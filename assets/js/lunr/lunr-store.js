var store = [{
        "title": "CS:APP - A Tour of Computer Systems",
        "excerpt":"Information Is Bits + Context  1 2 3 4 5 6 7 8 // hello.c  #include &lt;stdio.h&gt;  int main() {     printf(\"hello, world\\n\");     return 0; }   程式 hello 是 source program(source file) 經由程式設計師撰寫而成，其內容為 sequence of bits 數值為 0 或 1，8 個 bytes 組成 1 個 byte，而 1 個 byte 代表某個字元  大部分的電腦使用 ASCII 來表示一個字元  hello.c 展示了一個基本的概念，所有系統的資訊(包含: 硬碟檔案、存放在記憶體中的程式、使用者資料以及網路資料)都是由一連串的 bytes 所表示  如何區分記憶體中的資料取決於我們如何看待資料，在不同的情境下，有可能表示成 int、floating-point、character 以及其他   Programs Are Translated by Other Programs into Different Forms  hello 這個程式始於 hello.c 這個由 C 語言撰寫的高階(人類可閱讀)程式碼  為了要能夠在系統上執行，高階語法必須由另一個程式轉換成低階機器碼，而這些機器碼會被打包成 executable object program 並且以二進制的方式儲存於檔案系統中  在 unix 中，將高階語法轉換成低階語法的程式稱之為 compiler driver   1 linux&gt; gcc -o hello hello.c      Pre-processor phase            前置處理器 讀取程式原始碼，並將帶有 # 符號的指令展開，以這個例子就是 #include &lt;stdio.h&gt;，這行指令告訴前置處理器，在檔案系統中，找尋 stdio.h 並將其內容替換至 hello.c，將其輸出成 modified source program(hello.i)           Compilation phase            compiler 將 hello.i 轉換成 hello.s，其內容就是所謂的 assembly-language program         main:   subq    $8, %rsp   movl    $.LC0, %edi   call    puts   movl    $0, %eax   addq    $8, %rsp   ret                   Assembly phase            assembler 轉換 hello.s 的內容到 machine-language instructions，將其打包成 relocatable object programs(hello.o)           Linking phase            注意到在 hello.c 中我們有呼叫 C 語言標準函式庫中的 printf，而這個函式在另一個預先編譯好的檔案(printf.o)，為了能夠使用，他勢必要以某種形式插入到 hello.o 中，而這個過程是由 linker 所進行的           It Pays to Understand How Compilation Systems Work  在 hello.c 中，我們可以完全依靠 compilation system 幫我們產出正確且高效率的程式，但其中的細節是有必要了解一下的      Optimizing program performance            switch 一定比 if-else 有效率嗎?       while 比 for 有效率嗎?           Understanding link-time errors            static 跟 global variables 差在哪?       在兩個不同檔案中宣告相同名稱的變數會發生甚麼事?       static library 跟 dynamic library 差在哪?           Avoiding security holes            buffer overflow vulnerabilities 層出不窮，原因多半是因為開發者不了解系統運作的原理       隨意接受來源不明的資料導致系統出現危害，因此我們必須了解資料是如何排列在系統中           Processors Read and Interpret Instructions Stored in Memory  1 2 3 linux&gt; ./hello hello, world linux&gt;   在 unix 系統上，執行 executable file 需要透過 shell 的幫助，在 shell 載入 hello 程式並且等待其執行完畢，中間究竟發生了甚麼事?        Buses            匯流排 負責把資料(bits) 在元件中搬移，通常他一次是搬固定的大小(word, 4 bytes)           I/O devices   Main Memory            記憶體負責儲存正在運行中的程式以及資料，他是由一大堆的 Dynamic Random Access Memory 所組成。           Processor            中央處理器 是負責解析 instructions 的元件，program counter(PC) 是 CPU 中的一個儲存裝置，instruction 的位置是由 PC 所記錄的       算術邏輯單元 ALU 是主要負責運算的部分           綜上所述，我們可以得知，實際上電腦花了很多心力在移動資料，這裡就要提到一個重要的元件稱之為 cache  執行 ./hello 時，程式會從硬碟被載入至記憶體，當 CPU 執行 hello 時，會將 instruction 載入至 CPU register  從一個程式設計師的角度來看，以上操作耗費了很多移動資料的操作，而”真正”做事的部分卻相對比較少，所以我們要想一個辦法讓移動資料變快  根據物理法則，容量大的速度慢，容量小的速度快; CPU 與記憶體之間的速度差可以到很大，而 cache 作為暫存區域(安插於 CPU 與記憶體之間)，可以有效的減少時間差。\\   cache 分為 L1, L2, L3 cache，L1 cache 速度最快容量最小，L3 cache 速度相對慢但容量大; 縱使需要到 L3 cache 取得資料，相比要經過 I/O bridge 去取得記憶體內的資料來說，cache 在速度上依舊是相對較快的。     The Operating System Manages the Hardware  作業系統存在的目的      保護硬體，避免被亂用   作業系統提供一個統一的介面，讓應用程式不用理會過於艱澀的內容   回到 hello 本身，當他在執行階段時，hello 會以為他佔有系統的全部資源, 其實不然。實際上系統本身是 concurrently 的(透過 instruction pipelining)，如今多數的系統早已配備 multi-core processors 也就可以同時執行多個任務\\   為了達成 concurrency，作業系統提供了一個機制 context switching。所謂 context 指的是: PC、register、記憶體資料等等，在切換 process 的時候，會將以上 context 儲存起來，載入另一個 process 的 context，等到執行完成後再將原先的復原。負責這項工作的，自然是 kernel 了; kernel 常駐於記憶體當中，並且管理所有的 process，當程式需要執行，例如: 讀寫檔案 時，就需要使用 system call(這部分是由 kernel 完成的)    除單執行緒的程式外，現代作業系統提供了 threads 的功能，這在 concurrency 當中特別重要，相比另起一個 process 來說，threads 顯得更輕量化(因為 threads 一定程度的共享了資料)  前面提到，hello 在執行時會有 “全部系統資源都供我使用” 的錯覺。virtual memory 是作業系統提供的一種 abstraction，而 virtual address space 是每個 process 對於記憶體空間的 “logical” 觀點    對於 unix 來說，萬物皆可為檔案，舉凡: 硬碟、鍵盤以及網路等等，全部都可被視為是 檔案，這樣有一個好處是，對於各種看似毫不相干的操作，我可以用一個統一且優雅的方法去操作，而這類 system call 叫做 Unix I/O  ","categories": ["csapp"],
        "tags": ["operating system","c"],
        "url": "/csapp/csapp-A-Tour-of-Computer-Systems/",
        "teaser": null
      },{
        "title": "CS:APP - Representing and Manipulating Information",
        "excerpt":"Information Storage  相較於直接操作 bits，使用 bytes(8 bits) 會顯得方便得多。以 machine-level 來看待記憶體就會是一連串的 byte 陣列，每個 bytes 都有一個獨立的 id(address)，對於所有可能的 address 稱之為 virtual address space  所有關於 compiler, run-time system 如何分配記憶體，都是基於 virtual address space，諸如: 指標操作、結構操作等等。  C 語言擁有所謂 type 的資訊，以用於區分各種形態大小(指標要加多少 offset 才能正確取值)，不過在 machine-level 自然是沒有這種資訊的，對於機器而言，這就是一堆的 byte   pointer 大小一般為 word size, 而 virtual address 是以 word 做編碼的，若一機器為 w-bit word size，則最多可以定址 $2^w$ bytes   1 2 linux&gt; gcc -m32 prog.c linux&gt; gcc -m64 prog.c   -m32 編譯出來的程式可以跑在 32-bit 以及 64-bit 電腦上 -m64 只能跑在 64-bit 電腦上   對於不同機器，資料格式大小也會不同，比如說: int 可以為 2, 4 或 8 bytes  為了避免不同機器上有不同的大小，ISO C99 制定了一個規則，提供了 int32_t 保證大小為 4 bytes, int64_t 為 8 bytes   儘管大多數變數型態都有 signed 或 unsigned，那 char 也是嗎?  在 C 語言規格書 p.35 §6.2.5中，      The three types char, signed char, and unsigned char are collectively called the character types. The implementation shall define char to have the same range, representation, and behavior as either signed char or unsigned char. 35)    並沒有指定 char 要 signed 還是 unsigned，一切由 compiler 的實作而定，像是 gcc default 使用 unsigned char，並擁有如 -funsigned-char, -fsigned-char 的 compile flag   此外 C 語言也對於關鍵字的排列提供不同的方式，以下四種效果皆相同   1 2 3 4 unsigned long unsigned long int long unsigned long unsigned int   開發者需要自行解決在不同平台的不同變數大小的問題，在 C 語言規格書 p.33 §6.2.5 僅有規定大小的下界(lower-bound) 並沒有指定上界(upper-bound)      An object declared as type _Bool is large enough to store the values 0 and 1.     A ‘‘plain’’ int object has the natural size suggested by the architecture of the execution environment (large enough to contain any value in the range INT_MIN to INT_MAX as defined in the header ).    程式是由很多個 bytes 組成，我們必須考量兩個點      在某個地址上的 object 是甚麼   要怎麼在記憶體中排列 bytes   有些機器對於如何排列 bytes 有不同的意見，以下為兩種不同排列方式  假設 0x0 上有資料 $x_0x_1x_2x_3$                  little endian       $x_3$       $x_2$       $x_1$       $x_0$                       big endian       $x_0$       $x_1$       $x_2$       $x_3$           有些 IBM, Oracle 機器採用 big endian, 有些則採用 littel endian  對於哪種比較好，其實沒有一定，只要約定好就好  常見需要注意的場合有      網路通訊   查看 data sequences     注意到 int 與 float 的輸出結果不一致，這是因為他們使用了不同的編碼系統   binary values 是為組成編碼、操作訊息的重要理論基礎  boolean algebra 是由 {0, 1} 組成，並有 $\\neg , \\lor , \\land$ 等操作  C 語言也提供 bitwise 操作，需要注意 logical operator 與 bit operator 的差異，以下以 not 作為舉例      ~ 是將所有 bit 反轉   ! 是將數值取 negative(boolean)   同時還提供 shift operation，也就是將 bit 往左或右推      往左: 0b1010 :arrow_right: 0b0100   往右:            邏輯右移: 0b0101       算術右移: 0b1101           Integer Representations  C 語言可以表示多種的整數，包含: char, short 以及 long。C 語言僅規定各個型態最少需要表達多少範圍，並無界定上界，需要注意的是上界與下界的範圍並不一致   1 2 int : -32,767 ~ 32,767 long: -2,147,483,647 ~ 2,147,483,647   事實上，int 的數值範圍僅需 2 byte 即可表示，這就要追朔到 16-bit 機器，如今 int 大小，以 gcc 實作來說會是 4 byte   考慮用 bits 表示 int，以 unsigned 來說，最小值會是 0b0000…0000，最大值會是 0b1111…1111  unsigned 二進位表示法有一個很重要的特性，所有 int 數值在 binary 表示法中，都會有 唯一 的表示  如果是 signed int 呢，二補數(Two’s Complement) 就是用來表示負數的。具體的做法就是將 MSB(Most Significant Bit) 賦予負數權重    C 語言提供不同符號的數值切換，經由 casting 可以達成，但會不會有甚麼錯誤呢? 考慮以下例子   1 2 3 short int v = -12345; unsigned short uv = (unsigned short)v; printf(\"v = %d, uv = %u\\n\", v, uv);   結果   1 2 v  = -12345 uv = 53191   需要注意的是，bit 的數值並沒有任何改變，變的是解讀 bit 的方法。  多數 C 語言都是透過這樣子的機制去處理 signed 與 unsigned 之間的轉換      定義 B2U: Bytes to unsigned   定義 B2T: Bytes to 2’s complement   定義 T2U: 2’s complement to unsigned   定義 T2B: 2’s complement to bytes   \\[T2U =     \\begin{cases}       x + 2^w, &amp; x &lt; 0 \\\\       0, &amp; x \\geq 0     \\end{cases}  \\\\ U2T =     \\begin{cases}         u, &amp; u \\leq TMax_w \\\\         u - 2^w, &amp; u &gt; TMax_w     \\end{cases} \\\\\\]  輸出如下所示   \\[T2U(-12345) = 53191 \\\\ U2T(53191) = -12345 \\\\ T2U(-1) = 4294967295 \\\\ U2T(4294967295) = -1\\]       因為 unsigned 的部分不包含 $\\leq 0$，2 補數的部分則是橫跨正數與負數  觀察以上圖示可以得到以下結論   \\[B2U(T2B(x)) = T2U(x) = B2T + x_{w - 1}2^w \\\\ U2T(x) = -x_{w - 1}2^w + x\\]  儘管 C 語言沒有指定如何表示 signed numbers，大多數系統都採用二補數的方式進行，其中也提供 casting 機制作為 signed/unsigned 轉換，而方式分為      顯式轉換     1 2 3 4 5 int tx, ty; unsigned ux, uy;  tx = (unsigned)ux; uy = (int)ty;           隱式轉換     1 2 3 4 int tx, ty; unsigned ux, uy; tx = ux; uy = ty;           關於 C 語言這種隱式轉換，常常會帶來一些非預期的結果，比如說   1 -1 &lt; 0u   因為是 int 與 unsigned 的比較，所以他會把第一個 -1 轉換成 unsigned，因此其實是   1 4294967295u &lt; 0u   因此答案是 false，神奇吧   而在 printf 方面，可以使用 %d, %u, %x 等表示不同型態數值，不過 printf 不會用 type 相關資訊，因此我們可以用 int 搭配 %u，考慮以下   1 2 3 4 5 int x = -1; unsigned u = 2147483648;  printf(\"x = %u = %d\\n\", x, x); printf(\"u = %u = %d\\n\", u, u);   1 2 x = 4294967295 = -1 u = 2147483648 = -2147483648   casting 不僅可以用在同大小，也可以用在不同大小上。轉換到 larger data type 的時候要注意位移，前面提到會有 邏輯位移 以及 算術位移。考慮以下程式碼   1 2 3 4 5 6 7 8 9 10 11 12 13 short sx = -12345; unsigned short usx = sx; int x = sx; unsigned ux = usx;  printf(\"sx  = %d:\\t\", sx); show_bytes((byte_pointer)&amp;sx, sizeof(short)); printf(\"usx = %u:\\t\", usx); show_bytes((byte_pointer)&amp;usx, sizeof(unsigned short)); printf(\"x   = %d:\\t\", x); show_bytes((byte_pointer)&amp;x, sizeof(int)); printf(\"ux  = %u:\\t\", ux); show_bytes((byte_pointer)&amp;ux, sizeof(unsigned));   1 2 3 4 sx  = -12345: cf c7 usx = 53191:  cf c7 x   = -1234:  ff ff cf c7 ux  = 53191:  00 00 cf c7   其中算術位移是為了保證讓負數不會因為右移而導致變成正數  上述例子 x 與 ux 個別展示了算術位移以及邏輯位移   擴增變數大小的方法，綜上所述就是算術以及邏輯位移，不過如果我們想要縮減 number of bits 那會有怎麼樣的結果?  給定 $\\overrightarrow{x} = [x_{w-1}, x_{w-2}, …, x_0]$ 縮減到 k bits，實作上將會丟棄最高位元的 w-k bits，當然其中有可能會出問題，舉例來說，unsigned :arrow_right: signed 就有可能會發生 overflow 的情況  由於 binary 的特性，我們可以得出以下算式   \\[B2U_w([x_{w-1}, x_{w-2}, ..., x_0])\\ mod\\ 2^k = \\sum_{i=0}^{k-1}x_i2^i \\\\ c.f.\\ 2^i\\ mod\\ 2^k = 0, for\\ i \\geq k\\]  對於二補數的轉換也是同樣道理，只不過有東西需要做改變   \\[\\overrightarrow{x} = [x_{w-1}, x_{w-2}, ..., x_0] \\\\ \\overrightarrow{x}' = [x_{k-1}, x_{k-2}, ..., x_0]\\ \\text{縮減後的 bit vector}\\\\ x' = U2T_k(x\\ mod\\ 2^k)\\]  $x\\ mod\\ 2^k$ 的原因是，二補數最高位元是代表正負號的，如果也將其轉換數值會導致結果錯誤。      weight 會從 $2^{k-1}$ 變成 $-2^{k-1}$    總之，對於二進位的轉換我們可以歸納出以下   \\[B2U_k([x_{k-1}, x_{k-2}, ..., x_0]) = B2U_w([w_{k-1}, x_{w-2}, ..., x_0])\\ mod 2^k\\ \\\\ B2T_k([x_{k-1}, x_{k-2}, ..., x_0]) = U2T_k(B2U_w([w_{k-1}, x_{w-2}, ..., x_0])\\ mod 2^k)\\]  由於隱式轉換可能會隱藏錯誤，程式設計師若沒有充分了解數值系統將會導致錯誤。考慮以下例子   1 2 3 4 5 6 7 8 9 float sum_elements(float a[], unsigned length) {     int i;     float result = 0;      for (i = 0; i &lt;= length - 1; i++) {         result += sum[i];     }     return result; }   假設 length = 0, i &lt;= length - 1 會發生甚麼事情呢?  可以知道，這是 int 與 unsigned 的比較  根據 C 語言規格書 §6.2.5 第 9 項      The range of nonnegative values of a signed integer type is a subrange of the corresponding unsigned integer type, and the representation of the same value in each type is the same.31) A computation involving unsigned operands can never overflow, because a result that cannot be represented by the resulting unsigned integer type is reduced modulo the number that is one greater than the largest value that can be represented by the resulting type.    所以可以得知，0-1 的真實結果會是 -1 % UINT_MAX+1  上述式子會變成 0(int) &lt;= UINT_MAX(unsigned)   根據 C 語言規格書 §6.3.1.1      The rank of any unsigned integer type shall equal the rank of the corresponding signed integer type, if any.    根據 C 語言規格書 §6.3.1.8      Otherwise, if the operand that has unsigned integer type has rank greater or equal to the rank of the type of the other operand, then the operand with signed integer type is converted to the type of the operand with unsigned integer type    綜上所述，int 型態由於跟 unsigned 的 rank 是相同的，又因為 rank 相同所以將 int 轉型為 unsigned  所以最終比較式子會是 0(unsigned) &lt;= UINT_MAX(unsigned) 條件成立直到 i = UINT_MAX   再考慮以下例子   1 2 3 4 5 6 // prototype of strlen size_t strlen(const char *s);  int strlonger(char *s, char *t) {     returen strlen(s) - strlen(t) &gt; 0; }   在某些情況下，上述程式碼會出現不可預期的結果  因為 strlen 的實作是定義成 size_t，而 size_t 是根據 typedef 而來的，原型定義在 C 語言規格書 §7.17      The types are ptrdiff_t which is the signed integer type of the result of subtracting two pointers; size_t which is the unsigned integer type of the result of the sizeof operator; and wchar_t    因此 strlen(s) 與 strlen(t) 的結果都是 unsigned  若 strlen(s) 的結果比較小，得出來就會是負數  又因為 unsinged 所以結果會是一個很大的數字  很大的數字 &gt; 0 其結果為 true  進而導致錯誤   解決的方式是   1 return strlen(s) &gt; strlen(t);   講到 integer arithmetic  假設 x,y 為 postive number，有可能會遇到 x&lt;y 與 x-y&lt;0 結果不一樣的時候  假設 x,y 的數字區間範圍為 $2^w$，那麼其結果範圍可能是 $0 \\leq x+y \\leq 2^{w+1}$  為了要表示這個結果需要 w+1 bits   定義一個符號 $+^u_w$，$0 \\leq x, y \\leq 2^w$ 而其結果也落在 w bits 之間  假設使用 4 bit 表示法，x=9, y=12  x+y=21(0b10101) 很明顯超過 4 bit 表示範圍，如果我們捨棄高位元的數值就能得到 9(0b0101)，其值剛好是 21 mod 16  因此代表我們可以做 mod 運算   \\[x +^u_w y = \\begin{cases}       x + y, &amp; x+y&lt;2^w &amp; Normal \\\\       x + y - 2^w, &amp; 2^w \\leq x + y \\lt 2^{w+1} &amp; Overflow     \\end{cases}\\]  在 C 語言裡面，overflow 並不被視為錯誤的一種，甚至有時候我們會希望藉由此種機制用以檢測是否有 overflow 這種情況發生   那麼要如何檢測 overflow 呢? 給定 $s = x +^u_w y$  僅需考慮 $x + y \\geq x$ 或 $x + y \\geq y$ 與否即可  回顧上述例子， $9 +^u_w 12 = 5$ :arrow_right: $5 &lt; 9$      如果沒有 overflow :arrow_right: $s \\geq x$   如果有 overflow :arrow_right: $s = x + y - 2^w$   1 2 3 4 5 // return 1: overflow // return 0: no overflow int uadd_ok(unsigned x, unsigned y) {     return (x + y) &gt;= x; }   因為 two’s complment 的加法與 unsigned 的加法邏輯一樣，因此我們可以改寫成 $x +^t_w y = U2T_w(T2U_w(x) +^u_w T2U_w(y))$  先使用 unsigned 加法做完之後再轉換到 two’s complement 即可   \\[\\begin{align} x +^t_w y &amp;= U2T_w(T2U_w(x) +^u_w T2U_w(y)) \\\\           &amp;= U2T_w[(x_{w-1}2^w + x + y_{w-1}2^w + y mod\\ 2^w] \\\\           &amp;= U2T_w[(x + y)\\ mod\\ 2^w] \\end{align}\\]  T2U 可以改寫成 $x_{w-1}2^w + x$ 是因為只要讓最高位元採計並加上原本後面的資料就可以轉成 unsigned 了  考慮以下測驗題   1 2 3 4 int tadd_ok(int x, int y) {     int sum = x + y;     return (sum - x)  y &amp;&amp; (sum - y)  x; }   這個 overflow 檢查的實作是錯的，原因是 two’s complment addition 是一個 abliean group 所以 (sum - x) 其實跟 (x + y -x) 一樣，也就是說不管有沒有 overflow ，上述的結果都會是 y      Check of overflow in signed addition and abelian groups   ","categories": ["csapp"],
        "tags": ["operating system","c"],
        "url": "/csapp/csapp-Representing-and-Manipulating-Information/",
        "teaser": null
      },{
        "title": "Git 進階使用 - Git Hook",
        "excerpt":"What Is Git Hook  在開發過程當中，我們常常會遇到需要手動進行測試以及 format 程式碼等等的事情，那就會讓我思考  有沒有一種自動化的工具可以執行這些任務呢?   Git - 一個幾乎可以說每個軟體工程師都會使用的工具，提供了一個解方   git hook 簡單來說就是一個在 commit, push, receive 之前或之後跑的 script，可以讓開發者自定義需要執行的 script  這個 script 不限定使用何種語言撰寫皆可(e.g. python, perl, ruby 或 shell script … etc.)  而 git hook 通常分為兩大類      client-side: 透過 commit 或者是 merge 觸發   server-side: 多半是透過網路，當接收到 commit 時觸發   How to setup Git Hook  git hook 是內建的功能之一，我們可以在 ./git/hooks 底下找到許多範例檔案   1 2 3 4 5 6 ambersun@station:~/ambersun1234.github.io/.git/hooks$ ls applypatch-msg.sample      pre-applypatch.sample    pre-rebase.sample commit-msg.sample          pre-commit.sample        pre-receive.sample fsmonitor-watchman.sample  pre-merge-commit.sample  prepare-commit-msg.sample post-update.sample         pre-push.sample          update.sample ambersun@station:~/ambersun1234.github.io/.git/hooks$   使用 git hook 需要符合以下      將檔案置於 ./git/hooks 資料夾底下   客製化 git hook 不能 有任何副檔名   更改 file mode 成可執行     1 $ chmod +x my-hook           檔案名稱需要完全符合預設 hook 種類的名字(詳細可以參考 Git - githooks documentation) 不然有可能會無法 trigger   如果在 commit 的當下想要 pass 掉 hook 可以使用     1 $ git commit --no-verify           除了放在 ./git/hooks 底下之外，你也可以透過 git config 去設定   1 $ git config core.hooksPath MYPATH   Example  以我自己來說，安裝 git hook 這件事情我會寫成 Makefile 執行，然後呼叫安裝的 script 自動安裝  考慮以下安裝 script   1 2 3 4 5 6 7 8 9 10 11 12 #!/bin/sh  if ! test -d .git; then   echo \"Execute scripts/install-git-hooks in the top-level directory\"   exit 1 fi  ln -sf ../../scripts/pre-commit.hook .git/hooks/pre-commit || exit 1 chmod +x ./scripts/pre-commit.hook  echo echo \"Git hooks are installed successfully\"   可以看到以上做的事情相對單純，首先先檢查 .git 資料夾是否存在，之後再將 pre-commit.hook 檔案以 symbolic link 的方式安裝進 ./git/hook 中  注意到不可直接將 scripts/pre-commit.hook 安裝進去，會無法正確 trigger  最後在加上 execute 的權限即可   而 pre-commit.hook 的內容如下   1 2 3 4 5 6 7 8 9 10 11 12 13 14 #!/bin/bash  RED='\\033[0;31m' NC='\\033[0m' GREEN='\\033[0;32m' YELLOW='\\033[1;33m'  echo -e \"${YELLOW}Auto CI for all data structures${NC}\" bash ./ci.sh if [ \"$?\" -eq 0 ]; then   echo -e \"${GREEN}CI check done${NC}\" else   echo -e \"${RED}CI check failed${NC}\" fi   這個例子是針對所有子專案進行 compile 看說有沒有 compiler error 而已   在安裝了 pre-commit hook 之後每次執行 git commit 之前就會先跑一遍 local build 了  除此之外你也可以做比如說像是 format code 之類的事情      完整範例可參考: github.com/ambersun1234 - Data-Structures    Reference     Customizing Git - Git Hooks   Git - githooks Documentation  ","categories": ["git"],
        "tags": ["linux","version control"],
        "url": "/git/git-hook/",
        "teaser": null
      },{
        "title": "自架部落格 - 使用 GitHub Pages 以及 Jekyll Chirpy 為例",
        "excerpt":"What is GitHub Pages  GitHub 提供了一個免費的服務 - Github Pages，可以讓開發者建立屬於自己的專屬部落格網站  沒錯 是完全免費!   只不過，GitHub Pages 僅提供靜態網頁的服務，也就是說如果你想要做註冊、登入以及其他後端功能是沒辦法的哦   How to build GitHub Pages  首先，先建立一個新的 repository 名字必須要是 USERNAME.github.io    由於我已經有這個 repo 了所以會有紅字，如果是第一次撰寫則不會有這個問題   新增一個簡單的 index.html 內容如下   1 2 3 4 5 6 7 8 9 10 &lt;!DOCTYPE html&gt; &lt;html&gt;   &lt;head&gt;     &lt;meta charset=\"utf-8\" /&gt;     &lt;title&gt;Hello GitHub&lt;/title&gt;   &lt;/head&gt;   &lt;body&gt;     &lt;h1&gt;我的第一個 html&lt;/h1&gt;   &lt;/body&gt; &lt;/html&gt;   然後之後的操作就跟一般使用 git 的方法一樣  將新的檔案 commit, push 到 github 就完成啦  你的新部落格網址會是 USERNAME.github.io   新的修改需要等一下才會顯示在你的部落格上面(以我自己的例子約莫 10 分鐘以內就會有了)   Customize GitHub Pages  如同一般撰寫網站一樣，我們也可以客製化網站的樣貌，github page 有預設一些模板可以供你使用  你可以在 settings-&gt;pages 裡面找到\\      現在 GitHub 已經正式的將 theme picker 移除了，可以參考 GitHub Pages: Deprecating the theme picker    Jekyll Themes 裡面有很多種選擇，你可以依照自己的喜好套用模板\\   Jekyll Chirpy  我選擇的是 jekyll-theme-chirpy 這個主題，套用方式很簡單  按照官方說明，首先先把 chirpy repository fork 到你的 github 並且將專案名稱更改為 USERNAME.github.io 即可  然後設定 build(參照 deploy on github pages)      Ensure your Jekyll site has the file .github/workflows/pages-deploy.yml. Otherwise, create a new one and fill in the contents of the workflow file, and the value of the on.push.branches should be the same as your repo’s default branch name.     Ensure your Jekyll site has file tools/test.sh and tools/deploy.sh. Otherwise, copy them from this repo to your Jekyll site.       首先是檔案上述檔案要設定好   Github Action 要設定好   build source 要設定好   如果網頁並沒有正確顯示，要檢查上述步驟是否有漏掉  可能你已經發現了，chirpy 是使用 markdown 語法寫網頁，所以他在 build 的時候會使用 markdown compiler 將 .md 轉換為 .html(因此我們需要使用 GitHub Action 進行編譯以及部屬)   Highly customized your github page  細心的你肯定發現到，我的 github page 怎麼跟預設的樣板不太一樣  因為我還有另外對他的 source code 進行修改   主要更改的地方就是 side bar 顏色、side bar layout 以及 預設圖片的更改  以及最重要的，更新 font-family  因為原本預設是使用 微軟雅黑體，而這個字體在顯示繁體中文的時候會有高底差的問題  網站內容已經全部改成使用 思源黑體 了   除了上述的改動之外  你也可以在本機環境嘗試更改這些內容 客製化你的網頁   最後 chirpy 也提供了本機的開發環境(docker)，這樣就不用每次都需要 push 到 production 進行測試了  可以使用以下指令將測試環境跑起來(並且它會隨著檔案的更動自動重新 build，讓你看到最即時的改動)   1 2 3 4 $ cd PROJECT_ROOT $ docker run -it --rm --volume=\"$PWD:/srv/jekyll\" \\   -p 4000:4000 -p 35729:35729 \\   jekyll/jekyll jekyll serve --livereload      注意到使用 --livereload 必須將 port 35729 也一併 export    Reference     Websites for you and your projects.   Adding a theme to your GitHub Pages site with the theme chooser  ","categories": ["random"],
        "tags": ["github","blog"],
        "url": "/random/github-page/",
        "teaser": null
      },{
        "title": "Container 技術 - runC, containerd 傻傻分不清",
        "excerpt":"容器化技術  隨著 microservice 的發展，容器化技術在近幾年受到了廣大的歡迎  相較於傳統的虛擬機器(virtual machine)， container 擁有著輕量，快速等特性 隨即受到了開發者們的喜愛  而其中最廣為人知的便是 Docker  本文並不會贅述 Docker 工具的使用，我們將以其探討 container 背後的故事   OCI(Open Container Initiative)  在 2013 年 Docker 剛開始流行的時候，container 並沒有太多相關規範，而這造成了一些問題  比如說像是 kubernetes 以及其他第三方工具為了要能夠使用 Docker 的 “部分” 功能而必須 bypass 一些沒必要的功能(注意到這時候 Docker 的實作並沒有拆開成為獨立專案)，這使得一切都變得相當的複雜   所以在 2015 年的時候，Docker, CoreOS 與其他容器化工業的人一起啟動了 OCI - Open Container Initiative 專案(collaborative project at Linux foundation)  其目的在於說制定一系列 標準化 的 container spec，而 Docker 作為 container 的領航者也在制定標準時提供了 container format 以及 container runtime(runC) 作為 OCI 制定規範時的基石   OCI 最主要制定了兩個規範      Runtime spec: 關於作業系統如何 run container(包含執行環境、configuration 以及 lifecycle)   Image spec: 關於如何建立、準備 container image(包含像是 參數、指令以及環境變數等等)   而後 Docker 將其實作拆分出 containerd(high level runtime) 以及 runC(low level runtime)        ref: Docker, Containerd &amp; Standalone Runtimes — Here’s What You Should Know    至此，Docker 的 image 以及 runtime 都符合了 OCI 規範  除了 Docker 本身的 runc, 還有以下 runtime      crun   kata-runtime   既然 Docker image 符合了 OCI 規範，那麼我是不是可以使用其他 runtime 跑起來呢?   CRI(Container Runtimer Interface)  前面不是已經將 container 的規格都定義好了嗎? 為甚麼這裡又跑出來一個 CRI 呢?  CRI 是 kubernetes 為了要支援各種不同的 runtime 所開發的 plugin(在 kubernetes 1.5 Alpha 中釋出)   注意到 k8s 不與 Docker 完全綁定使用，使用者可以依據不同情況套用不同 runtime     只要符合 CRI 介面的 runtime 都可以跑在 kubernetes 上  例如      rtk   containerd            cri is a containerd plugin implementation of the Kubernetes container runtime interface (CRI). With it, you are able to use containerd as the container runtime for a Kubernetes cluster.            kata-runtime            The runtime is OCI-compatible, CRI-O-compatible, and Containerd-compatible, allowing it to work seamlessly with both Docker and Kubernetes respectively.            CRI-O            CRI-O is meant to provide an integration path between OCI conformant runtimes and the kubelet. Specifically, it implements the Kubelet Container Runtime Interface (CRI) using OCI conformant runtimes. The scope of CRI-O is tied to the scope of the CRI.            所以 k8s 可以達成動態更換 runtime 而不用重新編譯     Reference     What Is containerd, And How Does It Relate to Docker and Kubernetes?   How containerd compares to runC   The differences between Docker, containerd, CRI-O and runc   Introducing Container Runtime Interface (CRI) in Kubernetes   Kubernetes 1.5: Supporting Production Workloads  ","categories": ["container"],
        "tags": ["docker","kubernetes","linux"],
        "url": "/container/container-runc/",
        "teaser": null
      },{
        "title": "Container 技術 - 理解 Docker Container",
        "excerpt":"Virtualization  Docker 身為一個容器化技術的代表，與傳統 virtual machine 不同  Docker 擁有更快的啟動速度、對系統資源的極低要求以及輕量化的優點，既然同為 虛擬化技術，那麼他與傳統的 virtual machine 又有甚麼不一樣的地方呢?   傳統的虛擬機器如 virtual box, vmware，他們皆屬於一個完整的 作業系統，亦即他擁有獨立的 kernel space 以及 user space  若讀者有曾經使用過上述兩套軟體就可以發現，在建立環境的時候需要準備 disk image 並且如同一般安裝作業系統的方式安裝  也正是如此，所以跑虛擬機器會耗費大量的系統資源，如果說你的本機環境硬體設備不夠力，時常開一個 vm 就會很吃力了   看到這你可能會想說 甚麼? Docker 不是一個完整的作業系統?  對! Docker 本質上不是一個作業系統，而是一組 process  也就是說，Docker Container 是屬於一個 OS-level 的 user-space process  而 container 與 host machine 共享同一個 kernel        ref: Containers and VMs Together      在這裡做一個小小的表格比較一下                          container       virtual machine                       本質       process       operating system                 大小       輕量       較為笨重                 開起速度       快       慢                 可執行個體數量       多       少           How does Docker work  既然 Container 只是一組 process      那為甚麼使用如 $ ps 之類的指令能看到的進程數量異常稀少呢?            Linux kernel 提供了一個 namespaces system call，他可以限制一個 process 能夠看到的範圍。有了這個東西之後，做到基本的 隔離 就不成問題了           為甚麼用起來感覺跟用一個完整的作業系統沒有甚麼不同呢?            當我們在使用 $ docker run 的時候系統會將 base image (或是由 $ docker build 建起來的客製化 image) 藉由 container runtime 跑起來變成一個真正的服務。而 image 裡面包含了一個作業系統的檔案系統目錄結構，並且搭配 chroot system call 改變當前執行環境目錄，用以達到讓你身處於一個作業系統之中的錯覺              可參考 Linux Kernel - namespaces  可參考 Container and Layers    藉由使用 Linux Kernel 提供的 system call，我們可以很容易地建立一個類似於 Docker 的虛擬環境   Container and Layers       ref: Container and Layers    考慮以下 Dockerfile  1 2 3 4 5 6 FROM ubuntu:18.04 LABEL org.opencontainers.image.authors=\"org@example.com\" COPY . /app RUN make /app RUN rm -r $HOME/.cache CMD python /app/app.py   Docker 的運作方式是，將每一行指令都疊加在先前的 layer 上面     注意到只有 RUN, COPY, ADD 這三個指令會疊加 聰明的你必然得出一個結論，每一次的疊加都會增加 image 大小    所以以上的 Dockerfile 他的層數總共有 4 層   那我就好奇了  單純的減少層數，能夠縮減多少？  考慮以下實作程式碼  1 2 3 4 5 6 7 8 9 10 // origin version FROM ubuntu:22.04  RUN apt update &amp;&amp; apt upgrade -y RUN apt install vim -y RUN apt install curl -y RUN apt install wget -y RUN apt install build-essential -y RUN apt install make -y RUN apt install cmake -y  跟這種  1 2 3 4 5 // optimized version FROM ubuntu:22.04  RUN apt update &amp;&amp; apt upgrade -y RUN apt install vim curl wget build-essential make cmake -y   他們究竟有沒有差別？  build 起來之後，他們的大小, 層數分別是                          origin       optimized                       Size       527 MB       522 MB                 Layer       8       3           透過指令 $ docker image inspect --format \"{{json .RootFS.Layers}}\" xxxxx 分別查看   他們的結果是  1 2 3 4 5 6 7 8 9 10 11 $ docker image inspect --format \"{{json .RootFS.Layers}}\" minimized-layer-origin [   \"sha256:f4a670ac65b68f8757aea863ac0de19e627c0ea57165abad8094eae512ca7dad\",   \"sha256:ae52e443d3533afa7cdb2a56be73801a3fd82154e9b6d37ea9ca1b1a3f2fd6e1\",   \"sha256:469d1c4a0652e0da96c08c547cd8a2e398099e9428b9ebf90acaa0539c1d2b13\",   \"sha256:25e5cd9275c6c89686a96c5084ddc3038f3ae4fec68dc306ad3c2836a11b83e8\",   \"sha256:3ad9d501a659b754fb9ca008e00fc3e7378268958e606696b9a759540f6c421a\",   \"sha256:6dc9c5658e1222bc029ade98126ae5a6fb910c291a49db04c3396aff8abff2e0\",   \"sha256:7c586f24c4476990dbce13dbdbb18cc0ad204320fb0044e52e9c5243b11e74f3\",   \"sha256:2bb6f8743c914aa3c3f9afae740dbc654629b04f25242ea4143c319a76f24e16\" ]  1 2 3 4 5 6 $ docker image inspect --format \"{{json .RootFS.Layers}}\" minimized-layer-optimized [   \"sha256:f4a670ac65b68f8757aea863ac0de19e627c0ea57165abad8094eae512ca7dad\",   \"sha256:ae52e443d3533afa7cdb2a56be73801a3fd82154e9b6d37ea9ca1b1a3f2fd6e1\",   \"sha256:f9c2c95623fff836deba2495abd894325b8f804340e0c1f5f3e1829560c23307\" ]   可以看到 確實阿  只用一行指令執行完所有 package install 的 image 他的層數比較少 只有 3 層  相對的一個 package 一行的就有 8 層   仔細觀察你可以發現到  他們前兩個的 hash 是一樣的 不難可以想到因為他們跑得指令都相同(所以 docker 其實會共用 layer)  好比如說 apt update &amp;&amp; apt upgrade -y 這行的 hash 是 ae52e443d3533afa7cdb2a56be73801a3fd82154e9b6d37ea9ca1b1a3f2fd6e1  那我是不是可以大膽的假設 f4a670ac65b68f8757aea863ac0de19e627c0ea57165abad8094eae512ca7dad 這串 hash 是 ubuntu:22.04 的 image hash 呢？   用 docker inspect 檢查  1 2 3 4 5 6 7 8 9 10 11 12 $ docker inspect ubuntu:22.04          xxx          \"RootFS\": {             \"Type\": \"layers\",             \"Layers\": [                 \"sha256:f4a670ac65b68f8757aea863ac0de19e627c0ea57165abad8094eae512ca7dad\"             ]         },          xxx  在 RootFS 那裡你可以看到 layers 那邊正好就是我們這裡第一層 layer sha256 的結果  得證   實驗程式碼可以參考 ambersun1234/blog-labs/minimized-docker-image-lab   Distroless Image  在寫 dockerfile 的時候多半的開頭你會這樣寫   1 FROM ubuntu:22.04   ubuntu:22.04 是你的 base image  注意到你不一定非得要寫 base image，你也可以手動安裝你需要的 package  不過通常這個作法不特別推薦的原因是，網路上有許多現成的 base image 可以使用  他們通常都是經過測試的，也省去了複雜的手動操作的必要性   Google 在 2014 年的時候提出了一個概念叫做 distroless image  簡單來說，就是一個不包含任何作業系統的 image  使用 ubuntu:22.04 這種有 OS 的 image 會包含許多不必要的東西，例如說 package manager 以及 shell  這些東西在 production 環境下是不需要的，因為他們會增加 image 的大小，也會增加安全性的風險  distroless image 的目的在於提供一個環境，只包含你的應用程式以及他的相依性的 image   根據 GoogleContainerTools/distroless 內所述  distroless image 的大小甚至比 alpine 還要小了近 50%   Docker Networking  得益於優異的網路設定，其他服務可以很輕鬆的與 container 連接, 不須理會他是跑在哪一個 OS 上面，更甚至不用知道他是不是 container  接下來就看看，要如何設定網路吧   Bridge Network  顧名思義，就像是一座橋樑，連接著 host kernel 與 container  它可以是 software bridge 或是 hardware bridge(以 docker 來說當然是 software bridge)   預設的情況下，如果你沒有指定，container 預設將連接至 bridge network  default bridge network 下，container 要互相連接僅能依靠 ip address  並且，所有連接到 default bridge network 的 container 都能夠互相溝通，這其實不太好  我們不希望不相干的 service 有任何互相存取的可能性   User-defined Bridge Network  使用自定義的 bridge network 能夠手動的控制有哪些 container 可以加到這個網路  在安全性上會比較好  並且容器的相互溝通，可以使用 container name!   手動建立一個 bridge network  1 2 $ docker network create testBridge 7f8836b042f9398d3a48a3bd8e2e86b6306dfdb63c41b8e906970d7eff829f8a   建立一個 container 並使用前面定義的 bridge network  1 $ docker run -itd --network testBridge --name testContainer ubuntu   查看它是否使用我們定義的 network  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 $ docker inspect testContainer              \"Networks\": {                 \"testBridge\": {                     \"IPAMConfig\": null,                     \"Links\": null,                     \"Aliases\": [                         \"fc6a85559a10\"                     ],                     \"NetworkID\": \"7f8836b042f9398d3a48a3bd8e2e86b6306dfdb63c41b8e906970d7eff829f8a\",                     \"EndpointID\": \"0d3255316f378ff5ce7c6abc7c101598c036c4812326d5d498ecf8a11eee2700\",                     \"Gateway\": \"172.19.0.1\",                     \"IPAddress\": \"172.19.0.2\",                     \"IPPrefixLen\": 16,                     \"IPv6Gateway\": \"\",                     \"GlobalIPv6Address\": \"\",                     \"GlobalIPv6PrefixLen\": 0,                     \"MacAddress\": \"02:42:ac:13:00:02\",                     \"DriverOpts\": null                 }             }    可以看到在 NetworkID 的部份是一樣的   Overlay Network       ref: Day22：介紹 Docker 的 Network (三)    overlay network 允許 docker 連接不同 cluster 上的 docker container   Host Network  host network 的狀況下，container 的 network 設定將與 host machine 一致，也就是說你不需要 export port 就可以直接使用  這個模式下 container 將不會有自己的 ip   1 $ docker run -itd --network host --name testContainer ubuntu   Disable Network  將 network 設定為 none 即可關閉網路   1 $ docker run -itd --network none --name testContainer ubuntu   Reference     Containers From Scratch • Liz Rice • GOTO 2018   Mastering the Docker networking   Networking overview  ","categories": ["container"],
        "tags": ["docker","linux","container","distroless image","network"],
        "url": "/container/container-docker/",
        "teaser": null
      },{
        "title": "Linux Kernel - Clock",
        "excerpt":"Hardware and System Clock  Linux 的世界裡，有兩種時鐘，他們分別為      Hardware Clock            硬體時鐘，即擁有 獨立的供電系統以及電容等等的 硬體時鐘，以電腦來說就是 cmos clock(或稱為 RTC, BIOS clock)       硬體時鐘必須要是常駐於系統當中，即使作業系統關機之後也必須存在           System Clock            system clock 僅存於系統運行時期，當系統關機後，system clock 將不復存在       當 system clock 不處於運行時，hardware clock 就發揮了它的用處。當系統開機時，kernel 會透過 system call 去取得 hardware time 並初始化 system clock       driven by timer interrupt           Linux Kernel’s Clock  為了針對各種不同使用情境，測量各種不同的時間，kernel 提供了一系列的 clock  在此僅列出幾種常用的，完整支援列表可以參考 clock_gettime      CLOCK_REALTIME            真實世界的時間，比如現在是幾點幾分之類的( 可以被 adjtime(3) 以及 NTP 所更改)       也被稱為 wall-clock :arrow_right: 意即掛在牆壁上的時鐘       settable           CLOCK_MONOTONIC            這個時間是紀錄了自系統開機以來經過的時間(不包含系統休眠時間)，該數值是使用 jiffies 作為紀錄，並且此數值為單調遞增(透過 timer interrupt)       會受到 adjtime(3) 以及 NTP 的影響       nonsettable           CLOCK_BOOTTIME            類似於 CLOCK_MONOTONIC，只不過它記錄了系統休眠時間       nonsettable           Network Time Protocal(NTP)  前面提到，硬體時鐘是透過電腦本身內部的 原子鐘 去紀錄當前間的，不過由於每家硬體廠使用的硬體設備不同，進而導致時間上可能會有一點誤差  比如像是，跨時區，日光節約時間等等的  這時候正確的時間顯得相當的重要了, 因此，Network Time Protocal 誕生了。你可以使用 ntp daemon 或者是 ntpdate 用以進行網路校時      詳細內容可以參考 鳥哥的 Linux 私房菜 - 第十五章、時間伺服器： NTP 伺服器    11 minute mode  Kernel 提供了另外的一個功能: 11 minute mode  開啟這項功能等於說讓 kernel 每 11 分鐘自動更新 hardware clock(用以與 system clock 同步)   為甚麼要更新 hardware clock?  因為事實上，hardware clock 在某種程度上來說是不精準的，但是這個誤差值是可預測的!(每天慢個 10 second 之類的) :arrow_right: systematic drift  假設 11 minute mode 是開啟的狀態，然後你用 $ hwclock --adjust 去調整 hardware clock，可能過 11 分鐘之後時間又會跑掉了  所以 man page 裡面是建議，11 minute mode 與 $ hwclock --adjust 不要一起用會比較好   time_namespaces  在 linux kernel 5.6 釋出之後，time 也可以 virtualization 了!  但是，這個功能僅針對 CLOCK_MONOTONIC 以及 CLOCK_BOOTTIME  為甚麼 namespaces 不支援 CLOCK_REALTIME? 讓我們來看看 kernel 開發者的討論吧      [Y2038][time namespaces] Question regarding CLOCK_REALTIME support plans in Linux time namespaces    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 I hope you are aware that the time namespace offsets have to be set _before_ the process starts and can't be changed afterwards, i.e. settime() is not an option.  That might limit the usability for your use case and this can't be changed at all because there might be armed timers and other time related things which would start to go into full confusion mode.  The supported use case is container life migration and that _is_ very careful about restoring time and armed timers and if their user space tools screw it up then they can keep the bits and pieces.  So in order to utilize that you'd have to checkpoint the container, manipulate the offsets and restore it.  The point is that on changing the time offset after the fact the kernel would have to chase _all_ armed timers which belong to that namespace and are related to the affected clock and readjust them to the new distortion of namespace time. Otherwise they might expire way too late (which is kinda ok from a correctness POV, but not what you expect) or too early, which is clearly a NONO. Finding them is not trivial because some of them are part of a syscall and on stack.  ...  Aside of this, there are other things, e.g. file times, packet timestamps etc. which are based on CLOCK_REALTIME. What to do about them? Translate these to/from name space time or not? There is a long list of other horrors which are related to that.   mailing list 的內容實屬過於龐大了，有興趣的可以翻一下  大意是說，假設更改了 host 的系統時間，則 kernel 必須走訪每個 namespaces ，根據 offset 更改 namespace 裡面的時間。  先不說過多的 overhead，每次更改都需要花時間，那更改的過程中所產生的些微時間差要不要再更新一遍?   1 2 3 4 5 6 7 8 9 10 11 12 13 +static inline void timens_add_monotonic(struct timespec64 *ts) +{ +\tstruct timens_offsets *ns_offsets = &amp;current-&gt;nsproxy-&gt;time_ns-&gt;offsets; + +\t*ts = timespec64_add(*ts, ns_offsets-&gt;monotonic); +} + +static inline void timens_add_boottime(struct timespec64 *ts) +{ +\tstruct timens_offsets *ns_offsets = &amp;current-&gt;nsproxy-&gt;time_ns-&gt;offsets; + +\t*ts = timespec64_add(*ts, ns_offsets-&gt;boottime); +}      git commit: cad1bae    再者，因為 CLOCK_REALTIME 在很多地方被使用到，比如像是檔案的時間戳記，到底是要使用 host 的還是 namespace 的時間?  種種的原因致使 CLOCK_REALTIME 並未被納入 namespace 裡面     至於為甚麼要加入 time namespaces，由 man page 可以得知是為了要做 container migration checkpoint/restore 等工作   1 2 3 The motivation for adding time namespaces was to allow the monotonic and boot-time clocks to maintain consistent values during container migration and checkpoint/restore.   關於 container checkpoint/restore，可以參考這篇文章 CRIU 介绍   Measure Time With Clock  前面提到的 clock 種類，是不是每一種都適合用於測量時間呢?  不是的，比如說相比 CLOCK_REALTIME, CLOCK_MONOTONIC 更適合用於測量時間，因為 CLOCK_REALTIME 會受到校時的影響(手動或自動)，可能快一點或慢一點，對於要精準測量時間差的 prgram 來說可能不是那麼的適合   1 2 3 4 struct timespec start, end;  clock_gettime(CLOCK_MONOTONIC, &amp;start); clock_gettime(CLOCK_MONOTONIC, &amp;end);   struct timespec 定義於 time.h 裡面(參見 21.2 Time Types)，如下所示   1 2 3 4 struct timespec {     time_t tv_sec;     long int tv_nsec; }   考慮以下測量時間程式碼      to be continued    Reference     [Timer 学习]wall time 和 monotonic time   What is the use of CLOCK_REALTIME?  ","categories": ["linux-kernel"],
        "tags": ["clock"],
        "url": "/linux-kernel/linux-clock/",
        "teaser": null
      },{
        "title": "Linux Kernel - namespaces",
        "excerpt":"Introduce to namespaces  namespaces 是 linux kernel 的一種資源隔離機制，用以防止不同 process 看到不同資源      A namespace wraps a global system resource in an abstraction that makes it appear to the processes within the namespace that they have their own isolated instance of the global resource. Changes to the global resource are visible to other processes that are members of the namespace, but are invisible to other processes. One use of namespaces is to implement containers.    作為隔離機制，只有存在於 namespace 中的 process 可以互相看到彼此的資源，若不存在於 namespace 之中的 process 則無法看到其內容。而這正適合用來實作 container   Understanding namespaces  具體的來說 namespaces 隔離了以下資源                  Namespace       Flag       Isolates                       Cgroup       CLONE_NEWCGROUP       cgroup root directory                 IPC       CLONE_NEWIPC       system v IPC, POSIX message queues                 Network       CLONE_NEWNET       network devices, stack, port … etc.                 Mount       CLONE_NEWNS       mount points                 PID       CLONE_NEWPID       process ids                 Time       CLONE_NEWTIME       boot and monotonic clocks                 User       CLONE_NEWUSER       user and group ids                 UTS       CLONE_NEWUTS       hostname and NIS domain name           注意到 Time 是自從 linux kernel 5.6 起正式併入主線(詳見: The Time Namespace Appears To Finally Be On-Deck For The Mainline Linux Kernel)   其他資源有隔離的需求很合理，但是 Time 也需要隔離…?  在 30b67b1d9a2c50d5581cd3bdacf5f312ca4dfbaa 這筆 commit 中我們可以窺探其中的奧秘      For many users, the time namespace means the ability to changes date and time in a container (CLOCK_REALTIME). Providing per namespace notions of CLOCK_REALTIME would be complex with a massive overhead, but has a dubious value.     But in the context of checkpoint/restore functionality, monotonic and boottime clocks become interesting. Both clocks are monotonic with unspecified starting points. These clocks are widely used to measure time slices and set timers. After restoring or migrating processes, it has to be guaranteed that they never go backward. In an ideal case, the behavior of these clocks should be the same as for a case when a whole system is suspended. All this means that it is required to set CLOCK_MONOTONIC and CLOCK_BOOTTIME clocks, which can be achieved by adding per-namespace offsets for clocks.    在 container 裡面，使用者可能會想要出於某種原因，進而更改時間，這肯定是沒問題的  那當使用者退出 container 回到 host machine 時，問題就出現了  由於 CLOCK_MONOTONIC 以及 CLOCK_BOOTTIME 都是單調時間(亦即遞增的)，如果說這兩個時間倒退了，那顯然不合理對吧?  於是乎 time namespace 就被提出，針對以上兩種時間進行隔離，具體的做法是增加 offset 欄位  根據 man time_namespaces      /proc/PID/timens_offsets Associated with each time namespace are offsets, expressed with respect to the initial time namespace, that define the values of the monotonic and boot-time clocks in that namespace. These offsets are exposed via the file /proc/PID/timens_offsets. Within this file, the offsets are expressed as lines consisting of three space-delimited fields:     &lt;clock-id&gt; &lt;offset-secs&gt; &lt;offset-nanosecs&gt;    你說為甚麼不把 CLOCK_REALTIME 也一起加到 namespace 裡面? 因為加上去他的 overhead 會過多，詳細說明可以參考 Linux Kernel - Clock   Namespace life cycle  namespace 的作用域普遍的來說在 最後一個 process 離開 namespace 之後就會自動回收，不過仍有一些例外  概括地來說，就是當 namespace 有被直接或間接的使用的情況下，namespace 會持續存在      file 或 mount 存在於 /proc/[pid]/ns/* 下   hierarchical namespace(亦即有 child namespace)   PID/time namespace 存在 symbolic link   PID/time namespace 存在 mount filesystem   Reference     namespaces   time_namespaces  ","categories": ["linux-kernel"],
        "tags": ["namespaces"],
        "url": "/linux-kernel/linux-namespaces/",
        "teaser": null
      },{
        "title": "Git 進階使用 - Git Rebase",
        "excerpt":"Introduce to Git Rebase  Rebase 顧名思義，即更改目前的 base(分支基礎)  rebase 在很多地方都很有用，包含像是更改 commit message, re-order commits, squash commits 以及 pull base branch 的 changes   這個指令可謂是多人協作下最常使用的 command，話不多說就讓我們開始吧   Basic  rebase 的基本概念就是將 branch 的 base 進行更新，參考官網教學圖片  1 2 3           A---B---C topic          / D---E---F---G master  目前 topic 的 base branch master 已經有新的 commit 了，為了要 fetch base 的 changes 可以下  1 2 $ git checkout topic $ git rebase master  如此一來就會變成  1 2 3               A'--B'--C' topic              / D---E---F---G master     注意到新的 commit 的 hash 會發生改變    Move commit to another branch  有的時候，基於某種原因會需要將 C branch(based on B branch) 上的 commit 移到 A branch 之上  1 2 3 4 5 o---o---o---o---o  A      \\       o---o---o---o---o  B                        \\                         o---o---o  C  這時候有兩個解法   Cherry-Pick  使用 cherry-pick 一個一個將 commit 撿到 target branch 上  當然過程中可能會有 conflict 需要解   基本的過程如下所示  1 2 $ git checkout A $ git cherry-pick c1   如果遇到 conflict, cherry-pick 就會暫停  這時候你可以選擇 $ git cherry-pick --abort 取消 cherry-pick  或解完衝突之後  1 2 3 $ // fix conflict $ git add . $ git cherry-pick --continue     切記改完衝突之後不需要 commit changes    1 2 3 4 5 6 7 o---o---o---o---o  A     |            \\     \\             o'---o'---o' C'      \\       o---o---o---o---o  B                        \\                         o---o---o  C  最後 cherry-pick 完之後原來的 C branch 記得要刪掉即可   Rebase onto  1 $ git rebase --onto A B C  第一個參數是 target base branch 後面則是 source branch   Interactive Mode  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 pick af38288 Fix collaborator's repo pick 45ee847 Fix mv error pick 31c1b30 Add ignore file pick 8ee234f Fix token clone error pick d223bd6 Update README  # Rebase ddc2d6d..d223bd6 onto 8ee234f (5 commands) # # Commands: # p, pick &lt;commit&gt; = use commit # r, reword &lt;commit&gt; = use commit, but edit the commit message # e, edit &lt;commit&gt; = use commit, but stop for amending # s, squash &lt;commit&gt; = use commit, but meld into previous commit # f, fixup &lt;commit&gt; = like \"squash\", but discard this commit's log message # x, exec &lt;command&gt; = run command (the rest of the line) using shell # b, break = stop here (continue rebase later with 'git rebase --continue') # d, drop &lt;commit&gt; = remove commit # l, label &lt;label&gt; = label current HEAD with a name # t, reset &lt;label&gt; = reset HEAD to a label # m, merge [-C &lt;commit&gt; | -c &lt;commit&gt;] &lt;label&gt; [# &lt;oneline&gt;] # .       create a merge commit using the original merge commit's # .       message (or the oneline, if no original merge commit was # .       specified). Use -c &lt;commit&gt; to reword the commit message.  透過 interactive mode 可以更方便的執行各種操作  使用的方式為 $ git rebase -i HEAD~5     其中 HEAD~5 表示 rebase 到距離當前 HEAD 五筆的 commit  前一筆 : HEAD^  前二筆 : HEAD^^  前 n 筆: HEAD~n    你可以 更改 commit message, re-order commits, squash commits, remove commits  僅需要在 interactive editor 中將 command 更改並 儲存退出 就可以執行操作了，如下圖  1 2 3 4 5 6 7 pick af38288 Fix collaborator's repo s 45ee847 Fix mv error pick 31c1b30 Add ignore file r 8ee234f Fix token clone error d d223bd6 Update README  ...     你可以使用 short name(e.g. reword =&gt; r, drop =&gt; d)    如前所述，在 rebase 的過程中有可能出現 conflict，解決辦法也一樣  在解決完 conflict 之後執行以下指令即可  1 2 $ git add . $ git rebase --continue   Execute Command during Rebase  有時候我們會想要確保在 rebase 的過程中 code 沒有被改壞，通常都會跑 test 來確保對吧?  但是手動執行太費時費力了，rebase 的過程中可以安插 shell command 用以執行 test  使用方法為在 interactive mode 裡面加入 exec COMMAND，如下所示  1 2 3 4 5 6 pick af38288 Fix collaborator's repo pick 45ee847 Fix mv error pick 31c1b30 Add ignore file exec make pick 8ee234f Fix token clone error pick d223bd6 Update README  上述的例子是使用 make(如果你不是使用 Makefile 也可以改成其他指令執行)   Splitting Commit  其實並沒有甚麼 split commit 的指令啦  但是我們可以透過 git reset 先把 commit reset 回 staging 的狀態  然後分次 commit 部分 changes  如此一來就可以做到類似於 split commit 的作法了     詳細的操作可以參考 Git 進階使用 - Git Reset | Shawn Hsu    在 rebase 的過程中可以執行上述操作  就是將要 split 的 commit 在 interactive mode 中改成 edit command 即可   Reference     git-rebase   git-cherry-pick  ","categories": ["git"],
        "tags": ["linux","version control"],
        "url": "/git/git-rebase/",
        "teaser": null
      },{
        "title": "Git 進階使用 - Git Reset",
        "excerpt":"Introduce to Git Reset  開發者最喜歡 Git 的其中一個很大的原因就是即使你做錯了 仍然可以重來  使用 git reset 可以幾乎拯救所有 “不小心的操作”(只要 .git 資料夾還存在的情況下)   接下來就帶大家仔細看看這個指令吧!   Reset mode  git reset 總共擁有 6 種模式      --soft            當你想要做，比如說取消上一筆 commit 或者是 split commit 的時候可以用    用法為 $ git reset --soft HASH           這個參數對 commit history 以及 file changes 都沒有影響, 所有資料仍然會保留      如上圖所示，reset 之後更改完的檔案會在 staging area 裡面       --hard            使用 –hard 這個參數會將檔案以及 commit history 全部 reset   用法為 $ git reset --hard HASH              那萬一你想要回復 reset hard 的這個操作呢? 也是有辦法的    git reflog 裡面包含所有 操作歷史, 你可以 reset hard 到 reset hard 的前一個狀態(聽起來是不是有點繞口哈哈哈)          從上圖你可以很清楚的看到 git reflog 裡面包含了所有使用者執行的操作，也因此你可以透過 reset hard 到某個時間點(在你還沒搞砸之前)    reset hard 以及 reflog 可以說是我用版本控制以來最強的絕招之一了    學會了他想必你可以在事情搞砸的時候救回來       --mixed(default)            預設模式下, 所有已經改動的檔案全部會被 unstage, commit history 不會有任何更動           剩下還有 --keep, --merge, --[no-]recurse-submodules 就不一一列舉了   Reset to Remote branch  另外一個我很常用的情境就是當多人合作的情況下  當多個人在同一個 branch 底下做事情的時候，你 pull 其他人的 changes 卻出現 conflict 而你不想解的時候  就可以用 $ git reset --hard origin/BRACH-NAME 來全部重設   此情況僅適用於你確定 local changes 可以全部丟棄  當然如果你手殘還是可以用 git reflog 下去救   Split Commit  要怎麼把單一筆 commit 拆成多筆呢?  事實上 git 並沒有提供此類 command 但我們可以透過 git rebase 來達成   git rebase –soft 在上面有提到是將所有 file changes 全部變成 staged 的狀態(i.e. 退回到 commit 之前的狀態)  這時候你就可以手動挑你要的 file changes 逐一 commit     記得要將還沒要 commit 的檔案 unstage 哦($ git restore –staged FILE.NAME)    References     git-reset   git-reflog  ","categories": ["git"],
        "tags": ["linux","version control"],
        "url": "/git/git-reset/",
        "teaser": null
      },{
        "title": "關於 Python 你該知道的那些事 - GIL(Global Interpreter Lock)",
        "excerpt":"Preface  我一開始用 Python 開發多執行緒的程式的時候  學長姐都告誡我說不要使用 threading 而是要使用 multiprocessing 這個 library  因為後者才是 真正的 threading   我就好奇啦  甚麼叫做 真正的 threading?   Introduction  Python 擁有不只一套實作，包含 Jython, IronPython, Cpython 等等的  就如同 C 語言有不同的實作一樣 GCC, Visual c++ 等等的   GIL, Global Interpreter Lock 是一個 類 mutex，用於保護在多執行緒之下的物件 而他目前僅存在於 CPython 的實作當中(因為 CPython 的記憶體管理實作並非是 thread-safe 的)         Thread 只有 stack 是各自擁有的，其餘都是共享的    Atomic Operation  在多執行緒下首要目標即是確保資料的正確性，而多執行緒的程式在執行的時候，其執行順序有可能是混亂的  如果這時候你又在多條執行緒中共享變數 那麼就有可能造成資料在計算的過程中出現差錯   1 2 3 4 5 6 7 8 import dis  def add(num) -&gt; int:     num += 1     return num  if __name__ == \"__main__\":     dis.dis(add)   考慮以上程式碼，如果我們將上述 add 程式碼進行反組譯成 Python bytecode 我們可以得到以下   1 2 3 4 5 6 7   4           0 LOAD_FAST                0 (num)               2 LOAD_CONST               1 (1)               4 INPLACE_ADD               6 STORE_FAST               0 (num)    5           8 LOAD_FAST                0 (num)              10 RETURN_VALUE   第一列數字對應到原始程式碼行數 第二列是 Python bytecode 的執行指令  你可以很輕易的看到 在短短的 num += 1 這行裡面它實際上執行了 4 條指令  那有沒有可能你在執行到 INPLACE_ADD 的時候，中間被 thread1 被 swap out 然後換 thread2 執行，就拿到錯誤的資料了呢?  答案很明顯 如果沒有做適當的處理, 100% 一定會遇到這個問題(然後你的程式就出錯 你就會被老闆臭罵一頓)   上述的 LOAD_FAST, LOAD_CONST 就是所謂的 Atomic Operation  是不可被打斷的(意即不能被 interrupt)      有些 compiler 或 interpreter 在執行的時候會為了最佳化而移動指令順序  這種貼心的舉動有時候會造成問題  這時候就需要 memory barrier    題外話，這些 Python bytecode 是透過 Python Virtual Machine(PVM) 所執行的(跟 Java 一樣)  CPython 的實作包含了一個 virtual machine 用以執行上面我們看到的 Python bytecode      注意到 CPython 是 interpreter，Cython 是一種語言    Concurrency vs. Parallelism  單位時間內只有一個 thread 在跑的算是平行處理嗎? 那肯定是阿   Concurrency   concurrency 是將 一個切成不同的子部份，將這些 子部份 給不同 worker, 每個 worker 各司其職, 負責完成 不同部分      ref: Concurrency與Parallelism的不同之處    Parallelism   parallelism 是將 n 個 task 分割給多個 worker, 每個 worker 都執行 完整的 task 內容      ref: Concurrency與Parallelism的不同之處    Why Multicore Slower than Single Core  接下來我們就做點實驗來驗證以及探詢為甚麼 Python 多執行緒下反而會比較慢的情況  在這之前我們要準備點工具以及程式碼來幫助我們驗證   測試環境   1 2 3 4 $ uname -a Linux station 5.11.0-46-generic #51~20.04.1-Ubuntu SMP Fri Jan 7 06:51:40 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux $ python3 --version Python 3.8.10   我們先測量一下 threading 與單純的 for-loop 各個執行時間  考慮以下實作程式碼   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 import threading import sys import time  result = 0 thread_lock = threading.Lock()  def mysum(start, end) -&gt; int:     global result      for num in range(start, end):         thread_lock.acquire(blocking=True)         result += num         thread_lock.release()  if __name__ == \"__main__\":     result = 0     thread_pool = []     worker = 10     num = 1000000     start = 1      chunk = int(num / worker)     if num % worker:         print(\"Cannot divide equally to each thread\")         sys.exit(1)      # measure threading execution time     start_t = time.time()     for i in range(worker):         end = start + chunk         thread_pool.append(threading.Thread(target=mysum, args=(int(start), int(end), )))         start = end         thread_pool[i].start()      for i in range(worker):         thread_pool[i].join()     end_t = time.time()     print(end_t - start_t)      # measure for-loop execution time     result = 0     start_t = time.time()     for i in range(1, num + 1):       result += i     end_t = time.time()     print(end_t - start_t)   得到的執行結果為                          執行時間                       for-loop       0.085470438003540 sec                 single thread       0.282294273376464 sec                 5 條 thread       6.306507110595703 sec                 10 條 thread       6.836602449417114 sec           這時候你就發現奇怪  怎麼 thread 變多反而比較慢     讓我們來對其 profiling 一下  使用內建 cProfile 對其進行分析，需要注意的是 cProfile 僅能針對 main thread 進行 profile  我參考了 How can you profile a Python script? 的作法 override threading.Thread 的 start function  這樣就可以對每個 thread profile 了(見下圖)   1 2 3 4 5 6 7 8 9 10 11 12 class ProfiledThread(threading.Thread):     def start(self):       pr = cProfile.Profile()        try:           return pr.runcall(threading.Thread.start, self)       finally:           pstats.Stats(pr).sort_stats(\"time\").print_stats()  # usage # threading.Thread(target=mysum, args=(int(start), int(end),)) ProfiledThread(target=mysum, args=(int(start), int(end), ))      為什麼不 overwrite run? 因為 run 就真的只是單純的 run function，顯然我們在乎的效能瓶頸不在 run 的時候發生    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22          20 function calls in 0.005 seconds     Ordered by: internal time     ncalls  tottime  percall  cumtime  percall filename:lineno(function)         4    0.005    0.001    0.005    0.001 {method 'acquire' of '_thread.lock' objects}         1    0.000    0.000    0.000    0.000 {built-in method _thread.start_new_thread}         1    0.000    0.000    0.005    0.005 /usr/lib/python3.8/threading.py:270(wait)         1    0.000    0.000    0.005    0.005 /usr/lib/python3.8/threading.py:834(start)         1    0.000    0.000    0.005    0.005 /usr/lib/python3.8/threading.py:540(wait)         1    0.000    0.000    0.000    0.000 /usr/lib/python3.8/threading.py:249(__exit__)         1    0.000    0.000    0.000    0.000 /usr/lib/python3.8/threading.py:246(__enter__)         1    0.000    0.000    0.000    0.000 /usr/lib/python3.8/threading.py:255(_release_save)         1    0.000    0.000    0.000    0.000 /usr/lib/python3.8/threading.py:261(_is_owned)         1    0.000    0.000    0.000    0.000 /usr/lib/python3.8/threading.py:258(_acquire_restore)         1    0.000    0.000    0.000    0.000 {method '__enter__' of '_thread.lock' objects}         1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}         1    0.000    0.000    0.000    0.000 {method 'append' of 'collections.deque' objects}         1    0.000    0.000    0.000    0.000 {built-in method _thread.allocate_lock}         1    0.000    0.000    0.000    0.000 /usr/lib/python3.8/threading.py:513(is_set)         1    0.000    0.000    0.000    0.000 {method 'release' of '_thread.lock' objects}         1    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.lock' objects}   取其中一個 thread 的 profiling 結果來看  我們可以很清楚的看到是 acquire lock 以及 wait 佔用了最多的時間   What does Thread Waiting for?  讓我們來仔細看看 CPython 實作程式碼是怎麼做的   Lib/threading.py#34   1 2 3 4 5 6 7 _allocate_lock = _thread.allocate_lock  ...  Lock = _allocate_lock  ...   Lib/threading.py#494   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class Event:     def __init__(self):         self._cond = Condition(Lock())         self._flag = False      def wait(self, timeout=None):         \"\"\"Block until the internal flag is true.         If the internal flag is true on entry, return immediately. Otherwise,         block until another thread calls set() to set the flag to true, or until         the optional timeout occurs.         When the timeout argument is present and not None, it should be a         floating point number specifying a timeout for the operation in seconds         (or fractions thereof).         This method returns the internal flag on exit, so it will always return         True except if a timeout is given and the operation times out.         \"\"\"         with self._cond:             signaled = self._flag             if not signaled:                 signaled = self._cond.wait(timeout)             return signaled   Lib/threading.py#834   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 class Thread:     def __init__(self):         ...          self._started = Event()      ...      def start(self):         \"\"\"Start the thread's activity.         It must be called at most once per thread object. It arranges for the         object's run() method to be invoked in a separate thread of control.         This method will raise a RuntimeError if called more than once on the         same thread object.         \"\"\"         if not self._initialized:             raise RuntimeError(\"thread.__init__() not called\")          if self._started.is_set():             raise RuntimeError(\"threads can only be started once\")         with _active_limbo_lock:             _limbo[self] = self         try:             _start_new_thread(self._bootstrap, ())         except Exception:             with _active_limbo_lock:                 del _limbo[self]             raise         self._started.wait()   當我們 start 一條新的 thread 的時候, 它會初始化一些東西(self._bootstrap)  而當所有事情完成了之後，它會等待 event object 的 _flag(:arrow_right: 當這個 flag 為 true 的時候意味著該條 thread 可以開始執行)  乍看之下每條 thread 都各自擁有 event object，既然如此為什麼還需要 acquire lock?  event object 的 condition lock 實際上是 low-level 的 threading lock _thread.allocate_lock  而官方文件指出，在單位時間內只有一條 thread 可以成功 acquire lock      only one thread at a time can acquire a lock — that’s their reason for existence    CPU-bound vs. I/O-bound  在計算機系統裡面 任務大多分為兩大類      CPU-bound            任務多以吃重 cpu 運算為主, e.g. 圖形運算，數學運算等等的           I/O-bound            任務多以吃重 i/o 為主, e.g. 在電腦中尋找特定檔案，資料庫讀寫，網路資料傳輸           GIL - Global Interpreter Lock  既然 CPython 的實作並不保證 thread-safe，那麼最簡單的作法是甚麼呢? 加上一道鎖(鑰匙) lock   ㄟ等等 全域的 lock ?   沒錯，interpreter 為了確保資料的正確性，設計出了一種 類 mutex lock  由當前執行的 thread 持有  只有擁有 mutex lock 的 thread 可以操作 Python object  也就是上面實驗得出的結果(單位時間內只有一條 thread 可以操作)      /* A pthread mutex isn’t sufficient to model the Python lock type   * because, according to Draft 5 of the docs (P1003.4a/D5), both of the   * following are undefined:   * -&gt; a thread tries to lock a mutex it already has locked   * -&gt; a thread tries to unlock a mutex locked by a different thread   * pthread mutexes are designed for serializing threads over short pieces   * of code anyway, so wouldn’t be an appropriate implementation of   * Python’s locks regardless.   Cpython/Python/thread_pthread.h#177    1 2 3 4 5 6 typedef struct {     char             locked; /* 0=unlocked, 1=locked */     /* a &lt;cond, mutex&gt; pair to handle an acquire of a locked lock */     pthread_cond_t   lock_released;     pthread_mutex_t  mut; } pthread_lock;   原開發者在 pthread 的基礎上又多提供了 locked 變數，用以區別它是否被 locked 了      The Python interpreter is not fully thread-safe. In order to support multi-threaded Python programs, there’s a global lock, called the global interpreter lock or GIL - Thread State and the Global Interpreter Lock    When will GIL be Released  最直覺的想法，當當前 thread 執行結束之後就會 release GIL  但如果你的計算要持續一段時間呢？   所以一般來說 GIL 會有兩種時機會自動釋放 GIL, I/O 與 timeout   I/O  當你在做 I/O 的時候，你就不會動到 Python object 了，所以就可以 release GIL   timeout  為了鼓勵 thread 可以自動 release GIL, Python 內部有所謂的 timeout 機制  預設的 timeout 時間是 0.005 second(你可以用 sys.{get,set}switchinterval() 來查詢以及設定 timeout 時間)  當 timeout 到了的時候，它也不一定會理你(你可以用 FORCE_SWITCHING 來強制 scheduler 進行排程)  那為什麼 timeout 到了不一定有用？   根據 GIL implementation note, opcodes can take an arbitrary time to execute  因為 Python bytecode 並不能很好的反應到每一台機器的 machine code, 也因為 bytecode 可能包含 I/O 所以每個指令執行起來的時間都不盡相同  上面我們有稍微提到，Python 是執行在 PVM(Python Virtual Machine) 之上的，而這個 VM 他是採用 cooperative scheduling 的方式  所以這就是為什麼當 timeout 的時候，thread 是有可能不理你的，cooperative scheduling 講究的是主動放棄 lock(以 Python 來說是透過 yield)   另外就是如今 I/O 都有 buffering 的機制，即使 timeout 被 swap out，有了 buffer 的機制讓 I/O 的時間 短到可以再重新 acquire lock，造成其他 thread 等待 GIL 的時間越來越長(starving)   Why do we Need threading.Lock if we have GIL  我在做實驗的時候，發現了一個很神奇的現象  就是如果我不把 result 這個變數用 threading.Lock 鎖起來 好像…也不會錯阿？   稍微想了一下既然 GIL 的目的是確保同一個時間只有一條 thread 在使用 Python interpreter(或者說 同一時間只有一條 thread 可以存取 Python object)  那 “同一時間” 不就保證它一次只會有一個人存取了……嗎 :question:   如果有仔細看上面的 Atomic Operation 你就會意識到事情才沒有那麼簡單  因為如果說 Python bytecode 執行到一半被 interrupt，GIL 交給另外一條 thread 那你的計算結果就會出錯了   另一個直觀的方法是開多一點 thread 下去跑答案就會錯了(我是開 100 條 thread)      GIL 單位時間內只有一條 thread 可以擁有，但不代表它不會中途被搶走    How about Real Threading  既然 threading 是 concurrency 的一種，如果你要真正的 threading 你就必須要使用 multiprocessing  multiprocessing 實際上使用了一個巧妙的技巧去避開 GIL 的限制，既然 mutex lock 只有一把，那我 fork 出一個 subprocess 我不就也有一把鑰匙了嗎?  而事實上也的確如此，透過 fork 一個 child process 出來就能夠實現 真正的多工了   考慮以下程式碼   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 import multiprocessing as mp import os  def spin():     print(f\"child thread: {os.getpid()}\")     while True:         pass  if __name__ == \"__main__\":     proc = list()     num = 5      print(f\"main thread: {os.getpid()}\")      for i in range(num):       proc.append(mp.Process(target=spin))       proc[i].start()      for i in range(num):       proc[i].join()     為了驗證他的 parent-child 關係，我們打開 htop 進行驗證    透過 tree view 我們可以確認其 parent-child 的關係 證明了 multiprocessing 是透過 subprocess 的方式做到 parallel 的   但 multiprocessing 缺點也很明顯，解決了 race condition 造成的資料不正確，卻讓 share data 更困難了  作業系統課堂中有學過，process 之間是不會共享資料的  如果多個 process 要溝通必須要透過 InterProcess Communication(IPC) 的機制去執行，而他大致上分為兩種      shared memory: 透過建立一個 公有區域，將資料放置於其中，透過拿取(read)、寫入(write)以進行溝通(c.f. Producer–consumer problem)   message passing: 建立溝通渠道(pipe)以進行溝通   Eliminate GIL  如果要完全避開討人厭的 GIL  現今除了 CPython 的實作之外，也是有其他實作像是 Jython, IronPython 等等的可以使用   不過也是有人提出了可以完全去除 GIL 的實作，就列出來當作參考  colesbury/nogil   PEP-703  在 2023 年初，Python 內部提出了 PEP-703  旨在消滅 GIL, 而這項 proposal 已經被正式被接受了   消滅 GIL 是好事嗎？  開發者現在必須要手動的控制各種 lock 的狀態避免 race condition  給了更大的彈性，能夠更好的處理多執行緒  同一時間，在撰寫上也必須要更加的小心了   根據 PEP-703 Container Thread Safety 中所述      This PEP proposes using per-object locks to provide many of the same protections that the GIL provides.   For example, every list, dictionary, and set will have an associated lightweight lock.   All operations that modify the object must hold the object’s lock.   Most operations that read from the object should acquire the object’s lock as well;   the few read operations that can proceed without holding a lock are described below.    想當然，沒有全局的 lock, 區域的 lock 的引入勢必是必要的  他們是說這個是一個 lightweight lock 就是  不過對於效能的影響性大嗎？      一個簡單的概念就是盡量讓 critical section 小    根據 PEP-703 Performance 的結果                          Intel Skylake       AMD Zen 3                       One thread       6%       5%                 Multiple threads       8%       7%           它其實是增加 overhead 的！  因為引入了 per-object locks 的結果所以算是可以預期的   但為什麼多執行緒的情況下反而更糟呢？      For thread-safety reasons,   an application running with multiple threads will only specialize a given bytecode once;   this is why the overhead for programs that use multiple threads is larger compared to programs that only use one thread.    何謂 specialize?  根據 PEP-659      At runtime, Python will try to look for common patterns and type stability in the executing code.   Python will then replace the current operation with a more specialized one.    基本上因為 Python 是動態語言，而 CPython 內部實作有針對那些常見的 pattern 做了處理，將它替換成 specialized 的實作  想當然是為了優化的那類東西   而因為多執行緒的解放  同一段 code 的優化就不再只有單一解答  而是根據不同的 thread 處理的情況來做特別優化的處理(你怎麼拆 task 會導致優化出不同結果)  所以官方的 benchmark 結果，多執行緒下更慢   Reference     The Python GIL (Global Interpreter Lock)   Thread State and the Global Interpreter Lock   Understanding the Python GIL   Concurrency vs Parallelism: What’s the Difference?   How can you profile a Python script?   Not just CPU: writing custom profilers for Python   Why do we need locks for threads, if we have GIL?  ","categories": ["random"],
        "tags": ["python","gil","atomic operation","parallelism","concurrency","race condition","cpython","ironpython","jython","mutex","threading","multiprocessing","subprocess","fork","yield","timeout","cooperative scheduling","specialized","pep-703","thread-safe","lock","shared memory","message passing","python virtual machine","pvm","cprofile","pep-659","cooperative scheduling","starving","disassemble"],
        "url": "/random/python-gil/",
        "teaser": null
      },{
        "title": "網頁程式設計三兩事 - RESTful API",
        "excerpt":"What is API    API - Application Programming Interface 是一種 介面，他高度抽象化了背後的實作原理  使得呼叫端可以透過簡單的呼叫達成一件相對複雜的事情   而 API 的概念不僅限於網頁開發，包括像是 kernel system call 這種也是屬於 API  (因為你必須要透過作業系統操作各項事情, 諸如: 於螢幕上顯示字元)   說到底，程式設計師是一個非常懶惰的一群人，能夠重複使用的事情，我們就會把它獨立出來變成一個 function  這樣既節省時間也節省人力開發資源   RESTful API  假設現在要開發一組 user 的 API, 他必須要包含以下功能     查詢使用者，更新使用者名稱，刪除使用者   設計上的思維就是三支 API 對吧  1 2 3 4 5 6 7 8 9 10 11 12 GET /api/getUser/ {     \"userId\": \"xxx\" }  POST /api/UpdateUser {     \"userId\": \"xxx\",     \"userName: \"xxx\" }  GET /api/deleteUser   有覺得哪裡怪怪的嗎?  getUser, updateUser, deleteUser 是不是沒有必要寫那麼長?  但不加動詞要怎麼樣判斷當前的操作是屬於哪一種的?     RESTful API 的設計概念於 2000 年的時候提出  我們來看一下 REST 的幾個 architectural constrains   Uniform Interface  對於同一個 resource 盡量用同義化的字詞表示  有點抽象對吧 讓我們來看一下 best practice 裡面怎麼寫的      Use nouns to represent resource            上述 api 設計，是不是都可以改寫成 /api/user?           Consistency is the key            固定命名規則可以很有效的避免不必要的誤會以及最大化可讀性(e.g. /api/user)       在命名 URI 的時候，避免路徑中使用 _ 底線符號(有些瀏覽器無法正確顯示) :arrow_right: 改用 - dash 符號       在命名 URI 的時候，避免於路徑尾端使用 /(因為沒意義 而且會增加誤會的可能)           Never use CRUD function names in URIs            避免在 URI 當中指定操作類型(不要出現 create, update, delete … etc. 字眼)           我都遵照完 best practice 了  但問題是現在 API 都變成  1 2 3 GET /api/user POST /api/user GET /api/user  我要如何確定他是做甚麼用的?   REST guidelines 建議開發者可以多多利用 HTTP method 去對應 CRUD 的操作                  HTTP Method       Explanation                       GET       讀取資料                 POST       新增資料                 PUT       更新資料                 DELETE       刪除資料                 PATCH       更新資料           ㄟ等等  put, patch 都是屬於更新資料?  具體的來說     put 是更新 整體 資料   patch 是更新 部分 資料   Stateless  Server 並 不會儲存任何 session, history  所有跟狀態有關的都必須由 client 自行儲存   假設我們要實作 紀錄使用者登入狀態  傳統的設計上是使用 session 在 server 端註冊使用者狀態對吧?  或者是使用 cookie  而上述兩者的出現本來就是為了應對 HTTP stateless 的情況   既然不能儲存狀態在 server 端  client 端就勢必要儲存必要資訊(e.g. 使用者登入與否?)  其中一個做法就是使用 JWT 帶入資訊，詳細可以參考 網頁程式設計三兩事 - 不一樣的驗證思維 JWT(JSON Web Token) | Shawn Hsu  在每次發送 request 的時候帶上這些資訊提供 server 端做檢查  而這正好符合原本 HTTP stateless 的特性      這兩種做法並沒有好壞之分，單純是作法不同       有關 HTTP 的討論，可以參考 重新認識網路 - HTTP1 與他的小夥伴們 | Shawn Hsu    URL Length Limit of GET Request  URL 的長度是有限制的，粗略的估算大約是 2000 的字元  你說這跟 RESTful API 有什麼關係?   如果你有一隻 GET 的 API，然後參數是一個 id 陣列好了  因為參數實際上是帶在 URL 上的，所以如果參數太多太長就會撞到這個上限  撞到會怎麼樣呢？   有可能會遇到 HTTP 414 URI Too Long 的錯誤  那網路上有很多人會跟你說，改成用 POST 就好了  確實，實務上因為 POST 的 request body 是沒有大小限制的  你可以將資料一股腦塞過去不會報錯   用 POST 是合理的選擇，但這不符合 RESTful API 的設計  所以他其實算是必要之惡，不過這個惡能不能被改善？   Google 採取了 Batch Endpoint 的做法  就是將多個 request 組成一個 batch request  送到 /batch/{API_NAME}/{API_VERSION}(e.g. /batch/drive/v3) 之後再拆開來處理  根據 request body 轉發到不同的 endpoint 上面   與單純改成 POST 的差別在於有一個獨立的 batch endpoint 負責處理所有批次相關的  相比於原本的 POST 來說，我覺得 Google 的做法比較合理且較為彈性  他裡面也是遵循著 RESTful API 的設計   Version Control  隨著系統升級改版，API 也會隨之改變  這時候就會有一個問題，新版的 API 跟舊版的 API 該怎麼區分?   常見的一種作法是在 URI 中加入版本號  也就是 v1, v2 這樣的方式  這樣的好處是可以讓 client 端自行選擇要使用哪一個版本的 API  不過壞處是，當 API 版本越來越多的時候，管理維護的成本會越來越高  並且 version number 並不是強制性的   Backward Compatibility  即使 API endpoint 擁有不同的版本號做出別，我們仍然沒辦法阻止 client 呼叫舊版本 API  因此維持好向後相容性就很重要了   新的 API 實作出來之後，我們仍然可以保留舊版 API 的功能，只不過可以在 response body 中提及 deprecate 相關的訊息  注意到，做 301 redirect 不是一個好方法, 因為 redirect 指的只是單純 endpoint 搬家  但是新版 API 可能有新增欄位，導致 client 期待的回傳值不一致的行為      可以 301 頁面，但是 301 API 就不好了    References     REST Architectural Constraints   gRPC vs REST: Understanding gRPC, OpenAPI and REST and when to use them in API design   What is the maximum length of a URL in different browsers?   When do you use POST and when do you use GET?   Adding batch or bulk endpoints to your REST API   https://developers.google.com/workspace/drive/api/guides/performance?hl=zh-tw#details  ","categories": ["website"],
        "tags": ["api","restful","stateless","url length limit","version control","backward compatibility","batch endpoint","bulk operation","batch operation","http 414","http 301","redirect","session","jwt","cookie","http"],
        "url": "/website/website-restful-api/",
        "teaser": null
      },{
        "title": "網頁程式設計三兩事 - gRPC 與 JSON-RPC",
        "excerpt":"RPC  RPC(Remote Procedure Call) 是一種通信協定, 它能夠 允許本機電腦程式呼叫遠端電腦程式  聽起來好像還好? 重點是它能夠以 類似於呼叫本地 function 般輕鬆(稱為 location transparency)   1 2 3 4 5 6 7 8 // post.go import userClient  func CreatePost() error {     ...      user := userClient.GetUser() }   如上所示，我在 post.go 裡面透過 userClient 執行了一次 rpc call  而它的呼叫就跟一般 call function 一樣簡單，但 userClient 可能是遠在其他機器上的 service     RPC 的呼叫流程如下     client &gt; 呼叫 client stub(然後 push 到 stack，就像一般 function call)   client &gt; 打包呼叫參數(marshalling)   client &gt; 傳送資訊到遠端伺服器   server &gt; 傳送至 server stub   server &gt; 解析呼叫參數(unmarshalling)   server &gt; 呼叫 function   再依序返回   Proxy Pattern  而 RPC 的概念即是對應到 Design Pattern 裡的 Proxy Pattern 代理模式  用戶端藉由呼叫 stub 這個替身，這個替身會替它將 request forward 給真正的處理函式  用戶不需要管這個 function 在哪裡，替身都會幫它處理好   Definition  代理模式真正的定義如下  為物件提供一個代表或替身，藉以控制外界的接觸   在 RPC 中，替身單純做 request forward，並沒有 “控制外界的接觸”  這是因為 proxy pattern 有很多變體，如下所示                  Name       Description                       遠端代理       管理遠端與用戶端的互動                 虛擬代理       控制與 成本高昂的物件 的互動                 保護代理       控制用戶端與物件的接觸，通常與權限有關           Schema Evolution  資料格式可能會因為需求的改變而改變  這時候格式的變更可能會造成一些不相容的問題  而相容格式的情況包含兩種   Backward Compatibility  向後相容(Backward Compatibility)亦即 新 code 可以讀取舊的 format  因為你有辦法明確的處理舊的格式，你甚至知道它長怎樣   Forward Compatibility  向前相容(Forward Compatibility)的定亦是 舊的 code 有辦法讀取新的 format  這裡指的是即使遇到新的格式，我仍有辦法 不出錯  代表它可以忽略新格式裡的新東西   JSON-RPC  JSON-RPC 是一個輕量的 RPC 協定，其主要使用的資料格式是 JSON  它可以執行在 HTTP 或者是 websocket 之上   使用著方法滿簡單的，就像是一般呼叫 RESTful API 一樣  我們將要呼叫的 function name 指定在 body 裡面，並使用 POST method 送到伺服器上即可   詳細的 spec 可以參考 JSON-RPC 1.0 Specification 以及  JSON-RPC 2.0 Specification      需要注意的是如果你用的是 gorilla/rpc，他的 param 欄位長的不太一樣    1 2 3 4 5 6 7 8 {   \"jsonrpc\" : \"1.0\",   \"method\" : \"Server.Function\",   \"params\" : [     \"hello\", \"world\"   ],   \"id\": 1 }  以上是一個簡單的 JSON-RPC 呼叫範例，可以看到 JSON 裡面包含了 4 個欄位     jsonrpc 是 1.0 代表是 JSON-RPC 的版本，現在到 2.0 了   method 是呼叫的 function name   params 是呼叫的參數   id 是 client 的 id, 如果為空則代表為通知訊息   server 回傳的格式也類似  1 2 3 4 5 6 7 8 {   \"jsonrpc\" : \"1.0\",   \"result\" : { \t\"hello\": \"world\"   },   \"error\" : null,   \"id\": 1 }   server 回傳的 id 需要跟 client 發起的 id 一樣  格式方面則是多了 result 和 error   gRPC  gRPC 是 google 基於 rpc 所開發的一套 library, 其支援超過十幾種語言(包含 C++, Python, Go … etc.)  所以你可以作到像是 server side 用 GoLang 跑, client side 用 Python 跑這種     定義好了傳輸方式之後，資料傳輸格式以及 Interface Definition Language - IDL 的部份 gRPC 是使用 protocol buffer，其擁有以下特性     跨平台 跨語言   更快速 - 自行 encode 有可能會增加 run time cost   Protocol Buffer  Protocol Buffer 是一種資料編碼格式  由於其採用 binary encode 的方式，使得整體資料的大小相比 textual encode 還要更小  也因此傳輸速度可以更快   接下來就讓我們實際的來定義 protocol 檔案吧   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 syntax = \"proto3\"; package users;  option go_package = \".;users\";  service Users {   rpc GetUser (UserRequest) returns (User) {}; }  message UserRequest {   string user_id = 1; }  message User {   string user_id = 1;   string user_name = 2;   string first_name = 3;   string last_name = 4;   string email = 5; }   首先你會先定義 protobuf 的版本(現在都用 proto3)，以及 package name(避免撞名)  go_package 定義了 generated file 的檔案位置   Service  service 包含了所有你定義的 RPC 方法  而 gRPC 總共有 4 種 RPC 模式                  Method       Example                       Simple RPC(Unary RPC)       rpc SayHello(HelloRequest) returns (HelloResponse);                 Client-side Streaming       rpc LotsOfReplies(HelloRequest) returns (stream HelloResponse);                 Server-side Streaming       rpc LotsOfGreetings(stream HelloRequest) returns (HelloResponse);                 Bidirectional Streaming       rpc BidiHello(stream HelloRequest) returns (stream HelloResponse);              function 一定需要包含 request 嗎？  你一定遇過一些 function 是不需要參數的，這時候你可以使用 google.protobuf.Empty 這個型別  用以表示不需要參數    但是很多時候你需要考慮相容性(不管是向前還是向後相容)  推薦的做法是你一樣建立一個 request message，只是裡面的欄位都是空的  之後如果要新增欄位的時候，就可以直接在裡面加入新的欄位，而不用擔心相容性的問題  ref: Is google.protobuf.Empty dangerous for backwards compatibility?    本篇將專注在 Simple RPC 的部份   Message  message 區塊就是定義資料格式，我覺的有點像是 C 語言的 structure  裡面包含了                  Content Type       Description                       Field ID       ID 為對應每個欄位的號碼，以 = x 表示，其中 x 可以是 1 到 15 :arrow_right: 是使用 1 個 byte 16 到 2047 :arrow_right: 是使用 2 個 byte 2048 到 2^29 - 1 :arrow_right: 可以使用 19000 到 19999 :arrow_right: protobuf 保留用, 不能做使用                  Field Type       它可以是 string, int32, bool … etc.(詳細支援型別可以上 Scalar Value Types 查找)                 Field Data                   encode 完成之後，他的排列方式會長這樣  1 2 3 |----------|------------|------------| | Field ID | Field Type | Field Data | |----------|------------|------------|     為什麼需要 Field ID?  Field Name(i.e. user_id, user_name) 不就足以區分各個欄位了嗎？  沒錯！ 你說的對，但是為了 Backward Compatibility 以及 Forward Compatibility  Field ID 是必要的   根據上述的 protocol buffer 的定義，我們可以知道  tag 1 對應到 user_id  tag 2 對應到 user_name   如果今天我把 user_name 改成 user_fullname 會發生什麼事情？  舊的系統可以讀取新的資料格式嗎？  你當然可以直接改實作  但是有了 Field ID 之後，只要 ID 不變，不管 Field Name 怎麼改都不會有差  因此可以達到 Backward Compatibility   至於 Forward Compatibility  我要怎麼讀取新的資料格式？  如果遇到沒看過得 ID，略過往下一個繼續看不就行了？  因為 encode 過的資料都是排列緊湊在一起的，其中也包含了 偏移量(可以從 Field Type 得知)  所以利用 Field ID 與 Field Type 你可以輕易的達成向前相容   Repeated  1 2 3 message UserList {     repeated User users = 1; }   其實就是 array  他在格式裡面的表示方法也一樣就是  1 2 3 |----------|------------|------------| | Field ID | Field Type | Field Data | |----------|------------|------------|  只不過有很多組這樣  1 2 3 4 |---|-----|----|---|-----|----|-----| | 1 | int | 12 | 1 | int | 33 | ... | |---|-----|----|---|-----|----|-----|   ^              ^  上述等價於 []int{12, 33}   Compile protocol Buffer  撰寫完成之後，我們必須要把 proto 檔 compile 成我們能用的   需要使用到的工具有 protoc, protoc-gen-go, protoc-gen-go-grpc  安裝指令如下  1 2 3 4 $ sudo apt install protobuf-compiler -y $ go install google.golang.org/protobuf/cmd/protoc-gen-go@v1.26 $ go install google.golang.org/grpc/cmd/protoc-gen-go-grpc@v1.1 $ export PATH=\"$PATH:$(go env GOPATH)/bin\" &gt;&gt; ~/.bashrc   接下來使用以下指令編譯  1 2 3 $ protoc --go_out=. --go_opt=paths=source_relative \\         --go-grpc_out=. --go-grpc_out=paths=source_relative \\         users.proto   期間可能會遇到一些問題     protoc-gen-go-grpc: program not found or is not executable 或 protoc-gen-go: program not found or is not executable            這個狀況是你要正確的 install 這些 command 在機器上(也就是 go install google.golang.org/protobuf/cmd/proto-gen-go 之類的)       go package 的部份有分成 module 跟 command, 其中 command 的部份需要手動下載，你用 go mod download 是沒有用的           protoc: command not found            記得 export PATH=\"\\$PATH:$(go env GOPATH)/bin\" &gt;&gt; ~/.bashrc           protoc-gen-go-grpc: unable to determine Go import path for \"users.proto\"            可參考 proto编译组件使用, Windows 使用 protoc 编译 Go 语言的 protobuf 文件           當你克服萬難之後，你會得到兩個文件     *.pb.go :arrow_right: 包含各種序列化、反序列化、getter 以及 setter 的 message type   *_grpc.pb.go :arrow_right: 包含 server 以及 client 端的實作 interface 程式碼      在某些網站上，你會看到有人在 compile protobuf 的時候使用 --go_out=plugins=grpc=. 這個參數  這個參數在 github.com/golang/protobuf 這裡是支援的，但是在 google.golang.org/protobuf 這裡是不支援的  這裡都建議使用 google.golang.org 開頭的 :arrow_left: 這個是新版的  ref: Switch from –go_out=plugins to -go-grpc_out PATH problem [duplicate]    Pros and Cons  所以 Protocol Buffer 他有哪些優缺點？                  Pros       Cons                       特殊的 binary encode，減少資料大小，使得傳輸速度快       相比 textual encode(e.g. JSON, XML), binary encode 無法肉眼 decode                 支援 Backward Compatibility 以及 Forward Compatibility                   How do I use gRPC on website  根據 The state of gRPC in the browser 所述  很可惜的，即使 web 已經走到 HTTP3, 但是由於瀏覽器的 API 並沒有提供可以直接操作 HTTP2, 因此他們不能直接呼叫 gRPC 的 API      It is currently impossible to implement the HTTP/2 gRPC spec3 in the browser,   as there is simply no browser API with enough fine-grained control over the requests.  For example: there is no way to force the use of HTTP/2, and even if there was,   raw HTTP/2 frames are inaccessible in browsers.   The gRPC-Web spec starts from the point of view of the HTTP/2 spec,   and then defines the differences. These notably include:            Supporting both HTTP/1.1 and HTTP/2.     Sending of gRPC trailers at the very end of request/response bodies as indicated by a new bit in the gRPC message header4.     A mandatory proxy for translating between gRPC-Web requests and gRPC HTTP/2 responses.      那它沒用嗎？ 其實不然   gRPC 在 microservices 的架構下擁有出眾的效能  得益於 HTTP2，使得其與傳統 HTTP1 在速度上擁有著本質上的差異(因為 HTTP1 使用 plain text 進行傳輸，而 HTTP2 因為改進了壓縮演算法，效能提升)  更遑論提供可插拔的 auth, tracing, load balancing, health checking  並且支援多種語言的 gRPC 在開發上有更多種的選擇      可參考 重新認識網路 - HTTP1 與他的小夥伴們    回到正題，為了要讓現代瀏覽器能夠支援呼叫 gRPC API, 我們勢必要做一個 reverse proxy(反向代理)  讓 reverse proxy 將一般常見的 RESTful API 轉換成 gRPC(如下圖所示)     因此 gRPC-Gateway 就是為了處理這種情況而誕生的！  使用起來也不麻煩，除了準備原本的 proto 檔之後，接下來你要準備的就是 config yaml 檔   1 2 3 4 5 6 7 type: google.api.service config_version: 3  http:   rules:     - selector: users.Users.GetUser       get: /api/users/{user_id}      切記, yaml 檔必須使用 “空格” 進行縮排，不然會報錯    接著，簡單的 compile  1 2 3 4 5 $ protoc -I./proto --grpc-gateway_out ./proto/users \\ \t--grpc-gateway_opt logtostderr=true \\ \t--grpc-gateway_opt paths=source_relative \\ \t--grpc-gateway_opt grpc_api_configuration=./proto/users/users.yaml \\ \t./proto/users/users.proto  你就會得到 users.pb.gw.go  而這裡面就是將 RESTful-API request 轉換成為 gRPC call 的實作了   Proto Versioning  等到你完成基本的 server.go 並且滿心期待的要測試第一隻 gRPC API 的時候  1 2 3 4 5 # cloud/users/proto/users proto/users/users.pb.gw.go:56:2: cannot use msg (type *User) as type protoreflect.ProtoMessage in return argument: \t*User does not implement protoreflect.ProtoMessage (missing ProtoReflect method) proto/users/users.pb.gw.go:82:2: cannot use msg (type *User) as type protoreflect.ProtoMessage in return argument: \t*User does not implement protoreflect.ProtoMessage (missing ProtoReflect method)   哇這個問題阿，我在網路上看了很多文章阿  知識有點零散  總的來說呢，如果你仔細看 user.pb.go 以及 user.pb.gw.go 的 import 你會發現  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 // users.pb.gw.go import ( \t\"context\" \t\"io\" \t\"net/http\"  \t\"github.com/grpc-ecosystem/grpc-gateway/v2/runtime\" \t\"github.com/grpc-ecosystem/grpc-gateway/v2/utilities\" \t\"google.golang.org/grpc\" \t\"google.golang.org/grpc/codes\" \t\"google.golang.org/grpc/grpclog\" \t\"google.golang.org/grpc/metadata\" \t\"google.golang.org/grpc/status\" \t\"google.golang.org/protobuf/proto\" )  // users.pb.go import ( \tcontext \"context\" \tfmt \"fmt\" \tproto \"github.com/golang/protobuf/proto\" \tgrpc \"google.golang.org/grpc\" \tcodes \"google.golang.org/grpc/codes\" \tstatus \"google.golang.org/grpc/status\" \tmath \"math\" )  他們兩個用的 proto 有兩個版本 不一樣的版本  根據 Go Frequently Asked Questions - What’s the difference between github.com/golang/protobuf and google.golang.org/protobuf? 裡面提到     github.com/golang/protobuf/proto 是原始的 Go protocol buffer API   google.golang.org/protobuf/proto 則是更新版本的 Go protocol buffer API   他們之間有 breaking change(e.g. reflection)  雖然說 github.com/golang/protobuf v1.4.0 以上有針對新的 API 進行包裝，使其呼叫新版 API 不會報錯(向上相容？)  不過 proto Message interface 定義似乎並沒有相容到, 進而導致上述問題的發生     github.com/golang/protobuf/proto v1 message   google.golang.org/protobuf/proto v2 message(reflection 為 first-class function)   既然已經定義了問題所在，接下來只有兩個選擇     protoc-gen-go 使其使用新版實作(i.e. google.golang.org/protobuf/proto)   grpc-gateway 降版，然後 generate 出來的就會是使用舊版實作(i.e. github.com/golang/protobuf/proto)   前者我沒找到如何更改的相關資料  後者，根據 github.com/grpc-ecosystem/grpc-gateway #1989 的討論串可以得知  我們可以改用 v1 版本的 github.com/grpc-ecosystem/grpc-gateway/protoc-gen-grpc-gateway      Note: v2 的為 github.com/grpc-ecosystem/grpc-gateway/v2/protoc-gen-grpc-gateway    把所有的 import path 改完之後記得要在跑一次  1 $ go install github.com/grpc-ecosystem/grpc-gateway/protoc-gen-grpc-gateway  這樣才不會一樣抓到舊的   Result  經過了一段時間的修改與調整，最終 gRPC + gateway 已經可以成功運行了   1 2 3 4 5 // server $ go run server.go {\"address\":\"0.0.0.0:6666\",\"file\":\"/media/ambersun/ambersun1234/gitRepo/grpc-gateway-users/server.go:67\",\"func\":\"main.main\",\"level\":\"info\",\"msg\":\"gRPC server start\",\"service\":\"users\",\"time\":\"2022-05-14T21:20:04+08:00\"} {\"address\":\"0.0.0.0:7777\",\"file\":\"/media/ambersun/ambersun1234/gitRepo/grpc-gateway-users/server.go:80\",\"func\":\"main.main\",\"level\":\"info\",\"msg\":\"gateway server start\",\"service\":\"users\",\"time\":\"2022-05-14T21:20:04+08:00\"} {\"body\":{\"user_id\":\"1\"},\"file\":\"/media/ambersun/ambersun1234/gitRepo/grpc-gateway-users/server.go:32\",\"func\":\"main.(*UsersServer).GetUser\",\"level\":\"info\",\"msg\":\"Start GetUser request\",\"service\":\"users\",\"time\":\"2022-05-14T21:20:26+08:00\"}   1 2 3 // client $ curl localhost:6666/api/user/1 {\"user_id\":\"1\",\"user_name\":\"test_user_name\",\"first_name\":\"test_first_name\",\"last_name\":\"test_last_name\",\"email\":\"test@test.com\"}   詳細的實作程式碼可參考 ambersun1234/blog-labs/grpc-gateway-users   Compare with Traditional RESTful-API                          REST       gRPC       JSON-RPC                       Method       HTTP       HTTP2       HTTP websocket                 Data Exchange Format       JSON, XML       Binary       JSON                 Addressable Entities       Resource       Behaviour       Functions                 Speed       Slow       Fast       Fast                 Readable       Yes       No       Yes           看到上面的比較圖，你可能會好奇為什麼 RPC 會比 RESTful-API 還要來的快  更重要的問題是，快了多少？   Benchmark  為了使得效能測量誤差值不要太大，實驗準備如下     準備一個 echo api(執行簡單的操作，將其他 I/O 影響降到最低)   分別準備原生 server 接口與 rpc server 接口   分別進行 10000 次測量   我原本想要用 curl, grpcurl 進行 benchmark 測試  無奈 grpcurl 似乎並沒有提供 -w, –write-out 可以更好的進行測試  ref: How do I measure request and response times at once using cURL?   除此之外，我也查詢到可以利用 man 1 time，但是他的輸出精度僅到小數點後兩位(見下圖)  對於本次實驗需要高精度的需求屬實不是那麼的匹配  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 $ /usr/bin/time -v ls Command being timed: \"ls\" User time (seconds): 0.00 System time (seconds): 0.00 Percent of CPU this job got: 66% Elapsed (wall clock) time (h:mm:ss or m:ss): 0:00.00 Average shared text size (kbytes): 0 Average unshared data size (kbytes): 0 Average stack size (kbytes): 0 Average total size (kbytes): 0 Maximum resident set size (kbytes): 3180 Average resident set size (kbytes): 0 Major (requiring I/O) page faults: 1 Minor (reclaiming a frame) page faults: 135 Voluntary context switches: 2 Involuntary context switches: 0 Swaps: 0 File system inputs: 136 File system outputs: 0 Socket messages sent: 0 Socket messages received: 0 Signals delivered: 0 Page size (bytes): 4096 Exit status: 0   因此我決定使用 python - perf_counter_ns() 作為主要量測工具     Prerequisite  1 2 3 4 5 6 $ uname -a Linux station 5.13.0-30-generic #33~20.04.1-Ubuntu SMP Mon Feb 7 14:25:10 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux $ python3 --version Python 3.8.10 $ go version go versi9on go1.17.6 linux/amd64   Description  首先使用 Golang 分別架設 gRPC, JSON-RPC 與 RESTful server  client 端使用 Python 分別對其進行 一萬次的 benchmark testing   值得注意的是，gRPC 的部份 server 與 client 端分別使用 Golang 與 Python 實作  跨語言的支援同時也是 gRPC 的一大強項  就我這幾天的撰寫而言，就上手程度而言沒有太大的難度，基本上只要能夠順利 generate proto 就沒太大問題了   實驗相關程式碼可以在 ambersun1234/blog-labs/RESTful_gRPC_JSON-RPC-benchmark 中找到   Result     上述 benchmark 結果為 gRPC, JSON-RPC 與 RESTful API 的速度測試  其中綠色線代表 JSON-RPC, 藍色代表 RESTful, 紫色線代表 gRPC  這裡總共進行了 一萬次 的測試，y 軸代表執行時間(nanoseconds)   從上圖你可以很清楚的看到  JSON-RPC 跟 RESTful 平均呼叫時間都幾乎在 $1 \\times 10^6$ nanoseconds  但是你可以很明顯的看到，他們之間仍然有差別  即使兩者皆走 HTTP 協議，JSON-RPC 還是快那麼一點點   而 gRPC 則是完勝以上  根據 實驗數據, gRPC 相對 JSON-RPC 快了 5.77 倍   會有這樣的結果其實是因為 gRPC 是基於於 HTTP2  所以在速度上與傳統 API call(i.e. HTTP) 有著本質上的差異     有關 HTTP 的介紹，可以參考 重新認識網路 - HTTP1 與他的小夥伴們 | Shawn Hsu    References     深入淺出設計模式 第二版(ISBN: 978-986-502-936-4)   資料密集型應用系統設計(ISBN: 978-986-502-835-0)   gRPC Concepts Overview   Introduction to gRPC   Protocol Buffer Basics: Go   Quick start   gRPC API Configuration   Go Frequently Asked Questions   Why does a cURL request return a percent sign (%) with every request in ZSH?   关于makefile中，一直显示“XXX is up to date”的解决方法   Reflection not detected   How do I measure request and response times at once using cURL?   Why doesn’t the time command work with any option?   gnuplot 語法解說和示範   The state of gRPC in the browser   Core concepts, architecture and lifecycle   JSON-RPC  ","categories": ["website"],
        "tags": ["api","grpc","rpc","json-rpc","design pattern","protobuf","grpc-gateway","stub"],
        "url": "/website/website-rpc/",
        "teaser": null
      },{
        "title": "DevOps - 從 GitHub Actions 初探 CI/CD",
        "excerpt":"CI/CD  Continuous Integration - CI 是現今軟體開發流程當中的一種 best practice  開發的過程當中，我們有可能在實作中不小心改壞了一個東西，又剛好 QA 沒有測出來直上 production  這時候出問題就比較麻煩了對吧？   於是乎持續整合的概念就被提出來  我們可以透過某種方式在上版或是部屬到 production 上面之前先把我們的程式都完整的測試過一遍  這樣 出錯的機率是不是就會小的很多了   通常 CI 裡面會搭配各種測試  這些測試方法就讓我們拉出來獨立探討     可參考   DevOps - 單元測試 Unit Test | Shawn Hsu  DevOps - 整合測試 Integration Test | Shawn Hsu    而實務上來說 CI 就是負責執行以上的事物(包括但不限於 security check, code coverage, functional test and custom check)   Continuous Deployment - CD 持續部屬  傳統的部屬方式是手動部屬到遠端伺服器上，而現在你也可以透過自動部屬的方式上 code     透過自動化的 build code, test code 可以讓開發者更專注於專案開發   GitHub Actions  跑 CI/CD 有兩種方式，一個是在 local 自己起一個 CI server，另一個 solution 就是使用線上平台諸如 GitHub Actions, Travis CI, CircleCI 等等的   那我要怎麼樣 trigger CI 呢？  GitHub Actions 有多種 event 可以選擇(e.g. push code, new issue, schedule … etc.) 現在就讓我們來看看如何設定你的 GitHub Actions 吧   GitHub Actions Component  當某個 event 被觸發的時候, CI 就會執行某項動作，但我要怎麼指定他要跑哪些東西呢？  步驟、指令是由 YAML 檔撰寫而成，而裡面包含了若干 component   接下來就讓我們仔細的觀察每個 component 以及其關係圖       ref: https://morioh.com/p/aadcfe6cac57    Workflow  觸發執行單元，裡面包含了若干執行步驟  通常一個 repo 裡面可以有多個 workflow 分別對應到不同的場景(e.g. build and test 是一個, deploy 又是另外一個)  每個 workflow 都由一個 yaml 檔定義詳細的步驟   Events  觸發 workflow 的事件(e.g. push code)  1 2 3 4 on:   push:     branches:       - 'master'   完整 event 列表可以到 Events that trigger workflows 查詢   Jobs  裡面具體描述了該如何執行, 比如說 scripts 或是 actions  一個 job 可以對應一個 runner(意思是多個 job 可以平行化處理在多個 runner 上)   假設你要跑一個 unit test  但是，它可能會有一些前置步驟必須要做，jobs 裡面就是詳細定義這些 “步驟”  比如說     git pull source code   設定環境   下載第三方套件(e.g. $ go mod download)   跑測試   你可以看到，單單一個 unit test 的 job 需要做至少 4 個步驟  必須要完成一個，下一個才會執行   Actions  對於重複性 task(e.g. environment setup)  你可以把它寫成 task 然後在 job 裡面調用   如同你在 Jobs 裡面看到的範例一樣，我可以把其中一個 “步驟” 單獨的拉出來定義成 action  這樣就可以重複利用，在別的 jobs 可以直接 reuse   如果你願意，甚至可以將 action 上架到 GitHub Marketplace  比如說我的其中一個 action(Hardhat Test)       上架到 marketplace 需要設定 branding 相關參數，你可以參考 action.yaml  網路上也有人貼心的準備了一個 cheat sheet, 可參考 GitHub Actions Branding Cheat Sheet    Runner  CI 伺服器，可以是 local 或是 remote 的  GitHub Actions 提供了多種平台可以選擇(e.g. Linux, Windows 以及 macOS)      有關 local runner 的部分可以參考 DevOps - 透過 Helm Chart 建立你自己的 GitHub Action Local Runner | Shawn Hsu    Variables  Environment Variables   在 yaml 檔中你可以看到 ${{ xxx }}  他是代表你可以透過 context 使用所謂的環境變數  一種方式是在 yaml 當中直接定義(如下所示)   1 2 3 4 5 6 7 8 env:   DAY_OF_WEEK: Monday  inputs:   DAY:     description: 'Specify the day of week'     required: false     default: ${{ env.DAY_OF_WEEK }}   另一種是使用 GitHub 提供的環境變數                  env       description                       GITHUB_REPOSITORY_OWNER       repo owner’s name, e.g. ambersun1234                 GITHUB_REPOSITORY       owner 以及 repo name, e.g. ambersun1234/AART                 GITHUB_REF       trigger action 的各種資訊，它可以是 brach :arrow_right: refs/heads/&lt;brach-name&gt; tags :arrow_right: ref/tags/&lt;tag-name&gt; PR :arrow_right: refs/pull/&lt;pr-number&gt;/merge            上面的環境變數在 context 裡面多半都有對應可以使用  比方說 GITHUB_REF 與 github.ref 是等價的  關於 github context 的 document 可以參考 github context   其他內建提供的環境變數內容可以參考官方文件 Environment variables   GitHub Secrets  Secrets 顧名思義就是機密的資訊  什麼時候你會需要用到比較機密的資訊呢？   比方說你需要將 CI 完成的 docker image 推上 docker hub  聰明的你肯定發現，要上傳 image 需要做 authentication  最爛的作法當然是把你的密碼明文貼在程式碼裡面 ( :x:  所以這時候你就可以把密碼貼在所謂的 GitHub Secrets 裡面了  詳細的設定方法可以參考 Set up Secrets in GitHub Action workflows     每個 repo 擁有獨立的 secrets，目前沒有所謂的全局的 secrets    使用方式呢 一樣很簡單，語法跟 context 一樣   1 ${{ secrets.&lt;name&gt; }}           ref: GitHub Action YAML 撰寫技巧 - 環境變數(Environment Variables) 與 秘密 (Secrets)    注意到 secrets 的名字的使用，從上圖你可以看到 GitHub web UI 呈現的會是 全部大寫的  但是在你使用的時候，請記得一律是遵照 建立的時候的大小寫  也就是使用 ${{ secrets.APISecret }}     如果你在跑 action 發現了 Unrecognized named-value: 'secrets'  這邊要注意一件事  secrets 這個 context 只能在 workflow 存取  啥意思呢？   你在客製化 action 的時候會需要寫一份 action.yml 對吧  你要用客製化的 action 需要在寫一份 workflow  這兩個檔案是不同的，需要將它分清楚   secrets context 只能寫在 workflow 裡面(其他 context 可以在 action.yml 取得)  寫在 action.yml 它會抓不到   GitHub Token   要特別注意的是一個特殊的 secrets - GITHUB_TOKEN  這個是會自動建立的 secrets, 使用方法如上所示(${{ secrets.GITHUB_TOKEN }})  它可以 有限度的 存取 特定 GitHub 資源  比方說你想要有可以讀取或新增 Pull Request comment, 你可以透過 token 訪問 GitHub REST API 進行操作     secrets.GITHUB_TOKEN 你可以把它當作 Personal Access Token - PAT, 他們的作用大致上相同    Token Lifecycle  基於 token 安全性著想，GitHub 自動生成的 token 並不會永久的存在  token 是會 timeout 的，主要有兩個時間點     當 action job 完成的時候就會刪除   基於其他原因，token 最多也只能存活 24 小時   Token Permissions  你可以針對 repo 的 action 進行微調，基本上有三種模式(permissive, restricted 以及 fork)  前兩者你可以在 repo settings 裡面調整(可以參考 Setting the permissions of the GITHUB_TOKEN for your repository)，fork 是針對 fork 出去的 repo 做限制  這邊列出幾個比較重要的權限(完整權限可以參考 Permissions for the GITHUB_TOKEN)                  scope       Default access(permissive)       Default access(restricted)                       actions       read/write       none                 contents       read/write       read                 issues       read/write       none                 pull-requests       read/write       none                 pages       read/write       none           Implement Your Own GitHub Actions  定義一個客製化的 action 非常簡單，你只要指名 輸入, 輸出 以及 程式進入點 就可以了  而上述的資料必須寫在一個名為 action.yml(或 action.yaml) 當中就可以了   而 action 共有 3 大類                  Type       Docker container       JavaScript       Composite                       Operating System       Linux       Linux macOS Windows       Linux macOS Windows                 Speed       slow       fast       x                 Customizable       yes       no       x              Docker Container Actions            Docker container actions 因為是跑在 Docker 之上，所以其高度客製化,並且也由於容器的特性使得他的執行速度相較於 JavaScript actions 還要來的慢(因為你要啟動 container), 並且 runner machine 只支援 linux 以及上面必須安裝 Docker           Javascript Actions            JavaScript actions 可以以原生的方式跑在 3 大系統上面，在要求限制上面明顯沒有這麼多，你只能使用 pure JavaScript 以及不得依賴任何 binary(actions/toolkit 除外)       如果要用其他第三方的 package, 你可以用 webpack 之類的工具全部打包在一起，就不會受到限制了           Composite Actions            Composite actions 是將多個 actions 合併成一個 actions, 他的目的最主要是減少 duplication 而已, 詳細可以參考 GitHub Actions: Reduce duplication with action composition           Action.yaml   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 # action.yml  name: 'Issue assign all collaborators' description: 'Assign all collaborators to issues in repository' author: 'ambersun1234' inputs:   owner:     description: 'The owner of this repository'     required: true     default: ${{ github.repository_owner }}   repository:     description: 'The repo name of this repository'     required: true     default: ${{ github.repository }}   issue_num:     description: 'The issue number'     required: true     default: ${{ github.event.issue.number }}   api_url:     description: 'The GitHub REST API url'     required: true     default: ${{ github.api_url }}   token:     description: 'This is GitHub token'     required: true  runs:   using: 'docker'   image: 'Dockerfile'  branding:   icon: box   color: yellow   上述是最基本的 action.yaml  其中有幾個東西是必要的 name, description 以及 runs  如果有需要也可以視情況新增 inputs, outputs, branding      name            簡單，就是這個 action 的名字           description            action 的描述           runs            最重要的一部分，它定義了你的這個 action 該如何執行           inputs            定義輸入，可以有多個數值(e.g. inputs.my_name)                    而每個數值它裡面 必須 要有 description 以及 required, default 預設數值是可加可不加                       要如何在 JS runtime 或者是 docker container 裡面取得你的輸入呢？                    GitHub Action 會對所有的輸入值建立對應的 環境變數, 而他的形式是 INPUT_&lt;VARIABLE&gt;(以 inputs.my_name 來說，環境變數會變成 INPUT_MY_NAME)           它會是全大寫且會將 空格 替換成 底線                           outputs            注意到這裡的 output 不是拿來當作 console log 用的, 這裡的 output 是指將 action 輸出儲存下來，讓其他 step 可以透過 context 取得                    如果說你只是想要看它 log 到 console 那你其實用一般的 echo 就可以了           既然他的 output 是傳到其他 action 使用，所以你的 action.yml 裡面要定義輸出(如下所示)                   1 2 3 4 5 6 7 8 9 10 11   # action.yml    inputs:   my_name:       description: 'This is my name'       required: true       default: 'Shawn Hsu'    outputs:   my_name_uppercase:       description: 'This is my upper case name'                           branding            如果你要上架你的 action, branding 的部份可以參考，他是定義你的 action 的圖示與顏色           Docker Container Actions  一直以來我都是使用 GitHub issue 作為我部落格開發項目的紀錄  而當我新增一個新 issue 的時候 我都希望它可以自己將 assignee 自動填入我的帳號  所以 心動不如行動   actions.yaml 當中，如果是 docker container actions 的話，事情會有點不同，來看看吧     runs            runs.using :arrow_right: 只能是 docker       runs.image :arrow_right: 它可以是 Dockerfile 或是 public registry image(e.g. docker://debian:stretch-slim)           inputs            注意到如果是使用 docker container, 事情會有一點不同，我們必須手動將環境變數傳入 container                    也就是你在寫 runs 的時候要多加 args, 整體的寫法就會是這樣                   1 2 3 4 5 6 7 8 9 10 11 12   # action.yml    inputs:     my_name:       description: 'This is my name'       required: true       default: 'Shawn Hsu'   runs:     using: 'docker'     image: 'docker://debian:stretch-slim'     args:       - ${{ inputs.my_name }}                              那麼他在環境變數的使用上跟上面一樣, 可參考 Environments Variable                           outputs            為了使下一個 step 的 action 能夠取得上一層 action 的輸出，你在 Docker container 裡面的執行檔裡面要這樣寫           1 2   my_name_uppercase='SHAWN HSU'   echo \"::set-output name=my_name_uppercase::${my_name_uppercase}\"                  最後在 workflow 裡面你就可以拿到從其他 step 裡面傳出來的輸出了           1 2 3 4 5 6 7 8 9 10 11 12 13 14   # workflow    on: [push]    jobs:     issue-assign-all-collaborators:       runs-on: ubuntu-latest       name: Test on act       steps:         - name: Assign all collaborators           uses: ./action.yml           id: collaborators         - name: Get collaborators           run: echo \"${{ steps.collaborators.outputs.owner}}\"                  因為你要拿到上一個步驟的 action 值，所以你需要透過特定 id 存取特定步驟(像上面就是標了一個 id collaborators)              docker://debian:stretch-slim 對應到 Docker hub 上面的 debian:stretch-slim    詳細實作程式碼你可以在 ambersun1234/issue-assign-all-collaborators 中找到   Javascript Actions  相比於 Docker Container Actions, javascript actions 在實作上面會稍微方便一點      runs            runs.using :arrow_right: 定義了你要用哪一個 runtime(可以是 node12, node16)       runs.main :arrow_right: 定義了程式進入點，要用哪一個檔案跑 action(e.g. main.js, 其內容為客製化)           inputs &amp; outputs            相較於使用 Docker Container Actions 需要額外的動作傳遞參數，js 版本的完全不需要這樣做           接下來就看看 js 要怎麼寫吧  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 import * as core from \"@actions/core\"; import * as cli from \"@actions/exec\"; import * as fs from \"fs\"; import * as path from \"path\"; import { ethers } from \"ethers\";  const packageManagerFileMap = new Map&lt;string, string&gt;([     [\"yarn\", \"yarn.lock\"],     [\"npm\", \"package-lock.json\"] ]);  const packageManagerCommandMap = new Map&lt;string, string&gt;([     [\"yarn\", \"yarn install\"],     [\"npm\", \"npm install\"] ]);  const packageManagerRunCommandMap = new Map&lt;string, string&gt;([     [\"yarn\", \"yarn\"],     [\"npm\", \"npx\"] ]);  const localNetwork = \"hardhat\";  const fileExists = (lockFileName: string): boolean =&gt; {     return fs.existsSync(path.join(process.cwd(), lockFileName)); };  const main = async () =&gt; {     const network = core.getInput(\"network\");     const privateKey =         core.getInput(\"private_key\") ||         ethers.Wallet.createRandom().privateKey.slice(2);     const rpcUrl = core.getInput(\"rpc_url\");     const networkArgs = [\"--network\", network];      if (network !== localNetwork) {         if (privateKey === \"\") {             core.setFailed(\"Private key not found\");             return;         }         if (rpcUrl === \"\") {             core.setFailed(\"RPC url not found\");             return;         }     }      const content = `         PRIVATE_KEY=${privateKey}         ${network.toUpperCase()}_RPC_URL=${rpcUrl}     `;     fs.writeFileSync(path.join(process.cwd(), \".env\"), content, { flag: \"w\" });      for (let [packageManager, file] of packageManagerFileMap) {         if (fileExists(file)) {             await cli.exec(packageManagerCommandMap.get(packageManager)!);             await cli.exec(                 `${packageManagerRunCommandMap.get(                     packageManager                 )} hardhat test`,                 networkArgs             );             break;         }     } };  main().catch((e) =&gt; {     core.setFailed(e); });  不要看細部實作的話，是滿好懂的  定義一個 main function, 裡面透過 @actions/core 取得輸入，@actions/exec 執行指令  重點就只是 core.getInput() 以及 cli.exec() 僅此而已  當然你要設定輸出可以使用 core.setOutput()   詳細實作程式碼你可以在 ambersun1234/hardhat-test-action 中找到   Test GitHub Action locally  測試 GitHub Action 是一個有點尷尬的問題  開一個 repo 上去實測也..我覺的有點牛刀的感覺   nektos/act 是一款可以在本機測試 Action 的工具  因此我們就不用大費周章的建立測試環境了   Installation  1 2 3 $ wget https://raw.githubusercontent.com/nektos/act/master/install.sh $ sudo bash install.sh $ sudo mv bin/act /usr/bin   Test  安裝好之後你可以直接進行測試  1 2 3 4 5 6 7 8 9 10 11 $ cd issue-assign-all-collaborators $ act [issue.yml/test] 🚀  Start image=ghcr.io/catthehacker/ubuntu:full-20.04 [issue.yml/test]   🐳  docker pull image=ghcr.io/catthehacker/ubuntu:full-20.04 platform= username= forcePull=false [issue.yml/test]   🐳  docker create image=ghcr.io/catthehacker/ubuntu:full-20.04 platform= entrypoint=[\"/usr/bin/tail\" \"-f\" \"/dev/null\"] cmd=[] [issue.yml/test]   🐳  docker run image=ghcr.io/catthehacker/ubuntu:full-20.04 platform= entrypoint=[\"/usr/bin/tail\" \"-f\" \"/dev/null\"] cmd=[] [issue.yml/test]   🐳  docker exec cmd=[mkdir -m 0777 -p /var/run/act] user=root workdir= [issue.yml/test] ⭐  Run Run issue assign all collaborators [issue.yml/test]   ❌  Failure - Run issue assign all collaborators [issue.yml/test] file does not exist Error: Job 'test' failed   那尼？ 為什麼會這樣子呢？  後來我改了一下 action run step 發現到，container 裡面完全沒有 action 資料  1 2 3 4 [issue.yml/test]   🐳  docker exec cmd=[bash --noprofile --norc -e -o pipefail /var/run/act/workflow/0] user= workdir= | total 8 | drwxr-xr-x 2 root root 4096 Apr  8 07:41 . | drwxr-xr-x 3 root root 4096 Apr  8 07:41 ..   所以看起來是要 mount 或 copy 之類的，查找 README 果然有 -b binding 的參數(只不過它沒有特別標出來就是)  在跑之前你也可以先確定 act 有沒有正確讀到 action  1 2 3 4 5 $ cd issue-assign-all-collaborators $ act -l Stage  Job ID                          Job name  Workflow name  Workflow file  Events 0      issue-assign-all-collaborators  test      issue.yml      issue.yml      push $ act -b   跑下去之後發現 怎麼我改了 code 輸出沒改變呢？  因為你要重新 build image, 可以使用 --rebuild 讓每一次都使用最新 image  1 2 $ cd issue-assign-all-collaborators $ act -b --rebuild   如此一來，你就可以在本機測試了  不過我後來發現阿，因為我是跑 shell script, 所以不用 act 好像也沒什麼差別笑死   詳細實作程式碼你可以在 ambersun1234/issue-assign-all-collaborators 中找到   Continue on Error  基本上 workflow 的內容你都會希望他執行正確  但有時候又不是這麼一回事   舉例來說，有一個 job 的內容是讓他背景執行一些東西  由於 API limit 限制，他可能會失敗  需要等到 request 的內容減量他才會成功  所以這個是可以接受的   GitHub Action 提供了一個 continue-on-error 的參數  基本上可以 bypass 掉上面的問題   1 2 3 4 steps:     ...     - name: Create Search Index     continue-on-error: true   在有可能會失敗的 step 的地方可以加上這個參數  即使執行失敗，在 Action 裡面仍然視為成功並且會繼續執行下去   Skip workflow  有時候你可能需要跳過 workflow，不管是出於不想跑測試或者是需要快速上版  可以使用以下特殊指令                  First line commit message       Non-first line commit message                       [skip ci]       skip-checks:true                 [ci skip]       skip-checks: true                 [no ci]                         [skip actions]                         [actions skip]                   舉個例子，commit message 可以這樣寫  1 2 3 4 5 [skip ci] Add integration test setup  Due to chainlink vrf callback gas set limit to low Currently I couldn't test the code on chain Disable integration test action at GitHub, re-enable it when fix the above issue  ref: https://github.com/ambersun1234/nft/commit/95047600c90eb5d86e4cb8227f163c595ca45777     skip-checks: true 這種寫法必須在 commit message 保留兩行空白，接著 skip-checks: true 的指令  我試了一下發現是不行的，不太確定哪裡有做錯   Command in First line Message     to be continued    How to speed up Docker Container Action  從上面的討論你應該可以很清楚的發現到  因為 action.yml 裡面我們是定義 Dockerfile, 亦即每次都要跑 Docker build  那有沒有加速的方法？ ㄟ它除了每次 build 的選項以外，你還可以指定 public registry image 阿   所以我有特地分別觀察了一下實際執行時間     使用 Docker Build 耗時: 12 seconds   使用 pre build Docker image 耗時: 4 seconds   整整快了 3 倍阿  另外整體 duration time 提昇了約 66%  詳細的數據我沒有特別測試，但你可以在 issue-assign-all-collaborators#9 與 issue-assign-all-collaborators#10 找到相關數據   References     Understanding GitHub Actions   4 Steps to Creating a Custom GitHub Action   nektos/act   Get pull request number from action   Events that trigger workflows   Environment variables   About custom actions   Metadata syntax for GitHub Actions   Setting an output parameter   jobs id   Automatic token authentication   Webhook events and payloads   Skipping workflow runs  ","categories": ["devops"],
        "tags": ["github action","ci","cd"],
        "url": "/devops/devops-github-action/",
        "teaser": null
      },{
        "title": "GPG 與 YubiKey 的相遇之旅",
        "excerpt":"Introduction to YubiKey       ref: https://www.yubico.com    YubiKey 是 Yubico 公司生產的安全金鑰產品  透過實體金鑰，你不必在記一大堆密碼，即可進行身份認證   身份認證？ 沒錯，你也可以將 GPG 寫入 YubiKey 進行身份認證(可以參考 Enable GPG with YubiKey)  又或者是你跟我一樣，需要在多台機器上進行開發作業，又不想在你的 GitHub 上加了好幾把 key(可以參考 Enable SSH with YubiKey)   本文使用的 YubiKey 版本為 YubiKey 5 NFC   What can GPG Do?  GPG 除了可以加密訊息以及檔案之外 最重要的肯定是認證了   在這篇文章 How I got Linus Torvalds in my contributors on GitHub 中  作者演示了如何造假 commit log       ref: How I got Linus Torvalds in my contributors on GitHub    其實原理很簡單，GitHub 是使用 email 來辨識該筆 commit 的  所以你只要知道 target user 的 email 就可以進行偽造了 亦即  1 2 3 $ git -c user.name='Linus Torvalds' \\     -c user.email='torvalds@linux-foundation.org' \\     commit -m \"Fake commit\"   所以為了能夠證明該筆 commit 真的是你自己提交的，你可以使用 GPG 等其他工具來驗證     Managing commit signature verification    有經過認證的，在 commit 旁會看到 Verified 字樣  Unverified 必須要手動開啟顯示才有    PGP, OpenPGP and GPG - What’s The Difference  Pretty Good Privacy - PGP 是 Philip R. Zimmermann 在 1991 年創造的對稱式加密法，其目的正是為了能夠在 BBS 上安全的儲存訊息以及檔案所創造的  之後在 1997 年，IETF 為此制定了一系列的標準 - RFC 4880   有了這個標準之後，各種不同的實作相繼推出，其中最為人知的是 GnuPG - GPG   When will I Need GPG key  如果說 Git commit with GPG key 是個人要求所使用的  那有沒有在特定情況下必須要使用的呢？   事實上我曾經遇過必需要使用 GPG key 的情況  當時我在 Nvidia Jetson Nano 上面使用 Intel AX200 wifi 遇到了點問題  為了要將 log 那些等等的交給 kernel developers debug  他們要求我將以上資訊通通使用他們的 GPG public key 進行加密  這也算是我第一次實際使用 GPG key 的情境   除了與 kernel developers 在 email 上的對話之外，我也有將此情況 report 到 bugzilla.kernel.org  在這裡也順便紀錄一下 iwlwifi: AX200: Wifi not working on Nvidia Jetson Nano   What can SSH Do?  如果你需要在不同電腦上作業，可以將 SSH Key 寫進去 YubiKey 使用  等於說你只要金鑰在手上就可以登入不同主機，方便性大增   Enalbe GPG with YubiKey  Before Setup  在你參考 Yubikey 官方教學文寫入 GPG key 之前先等等  有一個很重要的東西必須先做   KDF - Key Derived Function 功能  預設的情況下是關閉的，而 YubiKey 在 KDF 功能關閉的情況下會使 PIN 碼以 明文(plaintext) 的方式儲存  然後官網教學完全沒寫 笑死(我全部做完之後繼續爬文改設定看到這一項做不了才在 issue 上面發現, 可參考 Mention Key Derived Format)   首先 插入 YubiKey(必須是完全沒動過得前提下才可以，如果有動過，那就參考 Resetting the OpenPGP Application on the YubiKey 囉)  1 2 3 4 5 6 7 8 9 10 $ gpg --card-edit gpg/card&gt; admin Admin commends are allowed  gpg/card&gt; kdf-setup  // Admin PIN 預設是: `12345678` // PIN 預設是: `123456`  gpg/card&gt; quit  做完以上，可以透過以下指令檢查  1 $ gpg --card-status   你應該要看到其中一行是這樣的(如果看到 off 表示失敗)  1 KDF setting ......: on   Generate GPG Key  一般來說，GPG key 可以有兩種生成方式     直接在 YubiKey 上面 generate   在 offline 作業系統上 generate   YubiKey 官方是推薦 offline 生成，避免之後 private key 遺失就沒救了   首先，插入 YubiKey, 並開啟 command line 輸入以下  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 $ gpg --expert --full-gen-key  Please select what kind of key you want:    (1) RSA and RSA (default)    (2) DSA and Elgamal    (3) DSA (sign only)    (4) RSA (sign only)    (7) DSA (set your own capabilities)    (8) RSA (set your own capabilities)    (9) ECC and ECC   (10) ECC (sign only)   (11) ECC (set your own capabilities)   (13) Existing key   (14) Existing key from card Your selection?  # 輸入: 8  Possible actions for a RSA key: Sign Certify Encrypt Authenticate Current allowed actions: Sign Certify Encrypt     (S) Toggle the sign capability    (E) Toggle the encrypt capability    (A) Toggle the authenticate capability    (Q) Finished  Your selection?  # 輸入: E # Note: 請務必確認經過上述指令後 Current allowed actions: 這行 # Note: 只有顯示 Sign Certify 而已 # 輸入: Q  RSA keys may be between 1024 and 4096 bits long. What keysize do you want? (3072)  # 輸入: 4096  Please specify how long the key should be valid.          0 = key does not expire       &lt;n&gt;  = key expires in n days       &lt;n&gt;w = key expires in n weeks       &lt;n&gt;m = key expires in n months       &lt;n&gt;y = key expires in n years Key is valid for? (0)  # 輸入: 0 # Note: 或者是你想要設定 expire time 也可以  # 接著輸入你的個人資訊 完成之後會長這樣 You selected this USER-ID:     \"Shawn Hsu (COMMENT) &lt;TEST@TEST.COM&gt;\"  Change (N)ame, (C)omment, (E)mail or (O)kay/(Q)uit?  # 完成輸入之後，必須輸入 passphrase(用於 GPG 驗證) # 接著等它跑完就可以了  gpg: key xxxxx marked as ultimately trusted gpg: revocation certificate stored as '/home/ambersun/.gnupg/openpgp-revocs.d/xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx.rev' public and secret key created and signed.  Note that this key cannot be used for encryption.  You may want to use the command \"--edit-key\" to generate a subkey for this purpose. pub   rsa4096 2022-03-11 [SC]       xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx uid                      Shawn Hsu (COMMENT) &lt;TEST@TEST.COM&gt; $   Generate GPG Subkey  primary key generate 完成之後，接下來要將 subkey 一起 generate(目的是為了使用其他 GPG 的功能)   Add Encryption Key  一樣，插入 YubiKey, 並開啟 command line 輸入以下  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 $ gpg --expert --edit-key xxxxx # 其中 xxxxx 為 GPG key id  gpg&gt; addkey  Please select what kind of key you want:    (3) DSA (sign only)    (4) RSA (sign only)    (5) Elgamal (encrypt only)    (6) RSA (encrypt only)    (7) DSA (set your own capabilities)    (8) RSA (set your own capabilities)   (10) ECC (sign only)   (11) ECC (set your own capabilities)   (12) ECC (encrypt only)   (13) Existing key   (14) Existing key from card Your selection?  # 輸入: 8  Possible actions for a RSA key: Sign Encrypt Authenticate Current allowed actions: Sign Encrypt     (S) Toggle the sign capability    (E) Toggle the encrypt capability    (A) Toggle the authenticate capability    (Q) Finished  Your selection?  # 輸入: S # Note: 請務必確認經過上述指令後 Current allowed actions: 這行 # Note: 只有顯示 Encrypt 而已 # 輸入: Q  RSA keys may be between 1024 and 4096 bits long. What keysize do you want? (3072)  # 輸入: 4096  Please specify how long the key should be valid.          0 = key does not expire       &lt;n&gt;  = key expires in n days       &lt;n&gt;w = key expires in n weeks       &lt;n&gt;m = key expires in n months       &lt;n&gt;y = key expires in n years Key is valid for? (0)  # 輸入: 0 # Note: 或者是你想要設定 expire time 也可以  # 接下來都確認就可以了   Add Authentication Key  其實步驟都跟 Add Encryption Key 一樣  主要的差異是在   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Possible actions for a RSA key: Sign Encrypt Authenticate Current allowed actions: Sign Encrypt     (S) Toggle the sign capability    (E) Toggle the encrypt capability    (A) Toggle the authenticate capability    (Q) Finished  Your selection?  # 輸入: S # 輸入: E # 輸入: A # Note: 請務必確認經過上述指令後 Current allowed actions: 這行 # Note: 只有顯示 Authenticate 而已 # 輸入: Q     全部都做完之後 你的畫面上應該會顯示如下圖  1 2 3 4 5 6 7 8 sec  rsa4096/xxxxxxxxxxxxxxxx      created: 2022-03-11  expires: never       usage: SC      trust: ultimate      validity: ultimate ssb  rsa4096/yyyyyyyyyyyyyyyy      created: 2022-03-11  expires: never       usage: E ssb  rsa4096/zzzzzzzzzzzzzzzz      created: 2022-03-11  expires: never       usage: A [ultimate] (1). Shawn Hsu (COMMENT) &lt;TEST@TEST.COM&gt;   或者是你可以跑 $ gpg --list-keys  1 2 3 4 5 pub   rsa4096 2022-03-11 [SC]       xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx uid           [ultimate] Shawn Hsu (COMMENT) &lt;TEST@TEST.COM&gt; sub   rsa4096 2022-03-11 [E] sub   rsa4096 2022-03-11 [A]     上述列出了現在電腦上的 GPG key 資訊，其中 usage: SC 這部份是什麼意思呢？  根據 trying to understand UID and subkeys 提到     (S)ign: sign some data (like a file)  (C)ertify: sign a key (this is called certification)  (A)uthenticate: authenticate yourself to a computer (for example, logging in)  (E)ncrypt: encrypt data       (S)ign            很好理解，對檔案簽名，證明該檔案是來自可信任的來源           (C)ertify            對 key 進行簽名認證       假設你有多把的 subkey, 那麼怎麼樣才能證明該 subkey 是屬於某個人的呢？ 因此 certify 就是為了驗證 subkey           (A)uthenticate            其實 GPG key 還可以配合 ssh 使用，詳見 GPG as SSH Key       ref: What is a GPG with “authenticate” capability used for?           (E)ncrypt            GPG key 可以針對訊息進行加解密       概念上是: 假設 A 要傳訊息給 B                    A 必須使用 B 的 public key 對訊息進行 加密           A 也可以同時使用 A 的 private key 對訊息簽名           B 收到訊息之後需要用 B 的 private key 對訊息進行 解密           B 為了驗證訊息是否來自 A, 它需要使用 A 的 public key 對簽名進行驗證                           Backup Your GPG Key  為了避免你後續出問題，YubiKey 的設計是一旦金鑰寫入就無法拿出來(同時它會把系統上的金鑰也一併刪除)  所以在此非常建議你對金鑰進行備份作業   而步驟就相對簡單了  1 2 3 $ gpg --export --armor xxxxxxxxxx $ gpg --export-secret-keys --armor xxxxxxxxxx $ gpg --export-secret-subkeys --armor xxxxxxxxxx   分別將以上三個 key 輸出儲存起來(e.g. 寫在紙上，或存在 usb 隨身碟裡面皆可)   How to Restore your GPG key to Yubikey  你有可能會因為各種情況而導致你必須重設你的金鑰  而這個步驟會重設你的 Yubikey, 這時候你就必須要 restore 你的資料回去了   相比於其他的資料，你的 GPG key 私鑰肯定是最重要的東西了  因為他的唯一資料是存在於你的 Yubikey 之中(如果你沒備份的話)  接下來就要教你，如何回復你的 GPG key   一開始最重要的事情當然是確認你有做好備份並且檔案健全(可參考Backup Your GPG Key)  如果連這個步驟都沒有做那就不用玩了(你可以重新開始了)   第一步驟當然是將備份好的資料 import 進機器裡面做備份  1 2 3 4 5 6 7 $ gpg --import gpg-private.key gpg: key ***************: \"Shawn Hsu (COMMENT) &lt;TEST@TEST.COM&gt;\" not changed gpg: key ***************: secret key imported gpg: Total number processed: 1 gpg:              unchanged: 1 gpg:       secret keys read: 1 gpg:  secret keys unchanged: 1   接著寫入 Yubikey  1 2 3 4 5 6 $ gpg --edit-key xxxxxxxxx gpg&gt; keytocard # 輸入 1 gpg: KEYTOCARD failed: Unusable secret key  gpg&gt;   疑？ 出現這種狀況是代表你的 private key 並不在機器上面  可是剛剛我們的確有正確引入對吧  為了確定我們有做好 import 這件事情，可以查看當前 key 的狀態   1 2 3 4 5 6 7 // 確認是否有正確被寫入 $ gpg --list-secret-keys pub&gt;  rsa4096 2022-03-11 [SC]       xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx uid           [ultimate] Shawn Hsu (COMMENT) &lt;TEST@TEST.COM&gt; sub&gt;  rsa4096 2022-03-11 [E] sub&gt;  rsa4096 2022-03-11 [A]      如果你有看到 key 旁邊有個 # 符號 :arrow_right: 代表找不到私鑰，但有個 reference  如果你有看到 key 旁邊有個 &gt; 符號 :arrow_right: 代表私鑰已經被移入 smartcard 裡面了    很明顯，在這裡你看到了 key 旁邊有一個 &gt; 符號，亦即私鑰並沒有被正確引入  出現這個狀況不用擔心，只要確保你的備份還在就可以了     根據 How to reimport gpg key replaced by stub 裡面所述     This is a known problem with GnuPG up to version 2.0.  You cannot import secret keys, if you already have some imported.       不過這個都多久了，居然還沒有修好    而我的 GPG 版本為  1 2 3 $ gpg --version gpg (GnuPG) 2.2.19 libgcrypt 2.2.19   也難怪我沒辦法正確的 import 私鑰   那麼接下來就很容易了  根據留言內的作法     因為你不能擁有部份的 key import，這樣會有問題，所以第一步先將所有機器上的 key 都刪除     1 2 3 4 5  $ gpg --delete-secret-and-public-keys xxxxxxxxx  // 接下來做確認刪除就可以了  // 你也可以用以下指令確認是否已經移除  $ gpg --list-keys  $ gpg --list-secret-keys           之後在用正常的方式一次性 import 進去就可以了     1 2 3 4 5 6  $ gpg --import gpg-public.key gpg-private.key gpg-sub.key  // 這裡依照需求將所有備份檔一併引入  // 這裡會要求輸入密碼  // 做好之後一樣可以使用以下指令做確認  $ gpg --list-keys  $ gpg --list-secret-keys           接下來的步驟就是將私鑰寫入 Yubikey  而這部份的操作與 Move Secret Key to Yubikey 一模一樣  跟著它操作，並且最後使用 $ gpg --card-status 查看是否正確寫入即可   Move Secret Key to YubiKey  在進行此步驟之前，請先確保上述步驟你都已經完成了   Move Primary Key into YubiKey  一樣，插入 YubiKey, 開啟 command line 輸入以下  1 2 3 4 5 6 7 8 9 10 11 $ gpg --edit-key xxxxxxxxx gpg&gt; keytocard  Really move the primary key? (y/N) y Please select where to store the key:    (1) Signature key    (3) Authentication key Your selection?  # 輸入: 1 # 輸入 passphrase 以及 Admin PIN   primary key 完成之後，接下來要把 Encrypt, Authenticate key 也都移進去 YubiKey   How to Choose Subkey  在你完成上述步驟之後，在 GPG console 上輸入 key 觀察目前狀態  1 2 3 4 5 6 7 8 9 gpg&gt; key sec  rsa4096/xxxxxxxxxxxxxxxx      created: 2022-03-11  expires: never       usage: SC      trust: ultimate      validity: ultimate ssb  rsa4096/yyyyyyyyyyyyyyyy      created: 2022-03-11  expires: never       usage: E ssb  rsa4096/zzzzzzzzzzzzzzzz      created: 2022-03-11  expires: never       usage: A [ultimate] (1). Shawn Hsu (COMMENT) &lt;TEST@TEST.COM&gt;  它應該要長成類似上面這樣   要選擇特定的 key 進行操作，他的使用方式是 gpg&gt; key {index}  index 是目前 key 的號碼(由上而下分別是 0 1 2)  亦即假設我現在要選擇 Encrypt Key(在上面輸出裡，他是排第 1 個位置), 選擇完成之後長這樣  1 2 3 4 5 6 7 8 9 gpg&gt; key 1 sec  rsa4096/xxxxxxxxxxxxxxxx      created: 2022-03-11  expires: never       usage: SC      trust: ultimate      validity: ultimate ssb* rsa4096/yyyyyyyyyyyyyyyy      created: 2022-03-11  expires: never       usage: E ssb  rsa4096/zzzzzzzzzzzzzzzz      created: 2022-03-11  expires: never       usage: A [ultimate] (1). Shawn Hsu (COMMENT) &lt;TEST@TEST.COM&gt;  可以看到 Encrypt Key 那一欄的 ssb 旁邊多了一個 * 號(這樣就代表目前選定這個 key 了)     如果要 “取消選擇” 就重新輸入一遍 key 1 即可(toggle)      Move Encrypt Key into YubiKey  一樣，插入 YubiKey, 開啟 command line 輸入以下  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 gpg&gt; key 1 sec  rsa4096/xxxxxxxxxxxxxxxx      created: 2022-03-11  expires: never       usage: SC      trust: ultimate      validity: ultimate ssb* rsa4096/yyyyyyyyyyyyyyyy      created: 2022-03-11  expires: never       usage: E ssb  rsa4096/zzzzzzzzzzzzzzzz      created: 2022-03-11  expires: never       usage: A [ultimate] (1). Shawn Hsu (COMMENT) &lt;TEST@TEST.COM&gt;  gpg&gt; keytocard Please select where to store the key:    (2) Encryption key Your selection?  # 輸入: 2 # 因為現在是對 Encrypt Key 的移動(這裡也很好的只有列出一個不會讓你選錯) # 輸入 passphrase 以及 Admin PIN   Move Authenticate Key into YubiKey  跟 Move Encrypt Key into YubiKey 一樣   只有 key index 以及 keytocard 那裡有點不一樣  1 2 3 Please select where to store the key:    (3) Authentication key Your selection? 3     Authentication Key 的目標儲存位置也只有一個    Verify YubiKey  上面我們成功的把 GPG Key 全部都寫入 YubiKey 裡面了  我們可以透過 $ gpg --card-status 觀察是否有成功寫入  1 2 3 4 5 6 7 8 ... Signature key ....: 0123 4567 89AB CDEF 0123  4567 89AB CDEF 0123 4567       created ....: 2022-03-11 12:34:56 Encryption key....: 0123 4567 89AB CDEF 0123  4567 89AB CDEF 0123 4567       created ....: 2022-03-11 12:34:56 Authentication key: 0123 4567 89AB CDEF 0123  4567 89AB CDEF 0123 4567       created ....: 2022-03-11 12:34:56 ...  如果說上述三者都有數值 基本上就完成了   Setup Info of your YubiKey  如果你有仔細看 $ gpg --card-status 你就會發現有一些欄位是空的(而它都可以透過 GPG 進行設定)   除了基本資料的修改，官方網站也強烈建議更改 PIN 以及 Admin PIN  1 2 3 4 5 6 7 $ gpg --card-edit gpg/card&gt; admin  # 啟用 admin function # 然後就可以改密碼了  gpg/card&gt; passwd   Test GPG sign works or not  為了要驗證是否可以進行簽名以及加密  考慮以下實驗   1 2 3 4 5 $ cd /tmp $ echo test &gt; gpg_test.txt # 將 YubiKey 插入 $ gpg --clearsign -o gpg_test2.txt gpg_test.txt $ gpg --verify  如果 GPG 成功簽名你會得到以下輸出  1 2 3 gpg: Signature made 廿廿二年三月十一日 (週五) 〇時〇分 gpg:                using RSA key xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx gpg: Good signature from \"Shawn Hsu (COMMENT) &lt;TEST@TEST.COM&gt;\" [ultimate]     如果把金鑰拔掉，就無法正確進行簽名      回歸我買 YubiKey 的重點，我想要能夠跨平台的使用 GPG key  所以接下來就是實測能不能跨平台使用   主要的概念其實很簡單 在使用的機器上必須要有 public key 就可以了  而要怎麼樣拿到 public key 呢？     將 public key 上傳到 keyserver(e.g. keyserver.ubuntu.com)   手動將 public key import 到機器上      Note: 上傳到 keyserver 的 public key 是沒辦法被刪掉的哦    手動上傳的方式就是你必須有 public 的檔案(或文字)  取得 GPG public key 的方法是  1 $ gpg --export --armor xxxxxxxxx &gt; gpg-public-key  接著在 target machine 上面執行  1 2 3 4 5 6 7 # prerequisite $ mkdir ~/.gnupg $ sudo apt update &amp;&amp; sudo apt install -y vim gnupg2 gnupg-agent scdaemon pcscd wget $ cd ~/.gnupg &amp;&amp; wget https://raw.githubusercontent.com/drduh/config/master/gpg.conf  # import public key $ gpg --import /PATH/TO/gpg-public-key   Error: gpg failed to sign the data  1 2 error: gpg failed to sign the data fatal: failed to write commit object   如果說你在 sign 資料的時候出現這個錯誤  在 .bashrc 裡面寫入以下  1 export GPG_TTY=$(tty)  基本上就可以動了   你可以透過以下指令測試  1 $ echo \"test\" | gpg --clearsign     如果還是不行  那八成是 gpg-agent 因為不明原因壞掉  簡單來說，重啟 gpg-agent 就可以了  1 $ pkill gpg-agent      如果使用 pkill 仍然無法解決，我的話是暫時把 Yubikey 從 usb 移除，跑一次 pkill 就可以了       我試過 gpgconf, systemctl, clear gpg cache 全都沒用    Delay or Disable Yubikey OTP  我在用 Yubikey 其實用得很開心  唯一不開心的點在於說只要我稍微不小心碰到它，它就會產生 OTP  然後它就會自動貼上 OTP   雖然說 OTP 被洩漏不算太大的問題，但三不五時出來還是很討厭  Accidentally Triggering OTP Codes with Your Nano YubiKey 官方文件有提到可以關掉或者是延長 OTP 的時間   一種方法是直接關了  反正要再打開也挺容易的      ref: YubiKey Manager    另一種則是把 OPT 的 trigger 方式改成 按住 3 秒才啟動  也就是 long press(Swapping Yubico OTP from Slot 1 to Slot 2)      slot 1 指的是 short press  slot 2 指的是 long press  ref: [question] newbie: what is slot 1 and slot 2?       把 slot 改成 long touch 就設定好了  然後測試的方式也挺簡單的，按上去試試，它會有反應，但如果時間不夠長，它不會發 OTP   Failed connecting to the Yubikey. Make sure the application has the required permissions.  單純沒權限，要用 root   如果是 AppImage 就直接 sudo ./YubiKeyManager.AppImage 來啟動   Enable SSh with YubiKey  有了 GPG key 之外，如果 SSH key 能夠寫在 YubiKey 上面那就太好了  你別說還真有, GitHub 於 2021/05 開始支援 U2F 以及 FIDO2 安全金鑰了(GitLab 同時也支援)   GPG as SSH Key  你可以透過 GPG subkey(Authentication Key) 用以替代 SSH private key  而 Authentication Key 在上面的教學中我們已經完成了(ref: Add Authentication Key)  所以直接跳到如何把你的 GPG key 變成 SSH key 吧！   Generate Public Key  因為 GPG key 終究是跟 SSH key 長的不一樣(而且你需要 public key)  所以我們要將 GPG key 轉換為 SSH public key  1 $ gpg --export-ssh-key YOUR_GPG_KEY_ID   然後你就可以把輸出的公鑰貼在任何你需要他的地方(e.g. GitHub SSH keys)      其實如果你已經設定過 Agent 然後忘記 public key 你可以直接 $ ssh-add -L 來看    你說，私鑰呢？ 當然是放在 YubiKey 本體之中  需要連線的時候，將 YubiKey 插入並且輸入對應的 PIN 碼即可正確登入  設定檔不需要改變，可以維持跟原本的一樣(如下所示)  1 2 3 Host github     Hostname github.com     User xxx      注意到不能加 IdentitiesOnly yes, 因為這個的意思是使用特定路徑下的金鑰  而 Yubikey 的金鑰不存在於系統內，它必須由 Agent 去取得  因此使用 Yubikey 的情況下 IdentitiesOnly 以及 IdentityFile 都不用加    你可以用 ssh 測試一下有沒有正確設定  有用好它應該會有以下輸出  1 2 3 $ ssh -T git@github.com Hi ambersun1234! You've successfully authenticated, but GitHub does not provide shell access.   Bad owner or permissions on ~/.ssh/config  我完成所有的步驟要測試 SSH 的時候遇到 Bad owner or permissions on ~/.ssh/config 這個問題   其實滿簡單的，因為 ssh config 算是滿機密的東西，如果被其他人可以改寫就不是太好  在底下的實例你也看到上面不小心將 write 權限給予 group 了  解決辦法也很簡單，用 chmod 把 file mode 改成 644 即可  1 2 3 4 5 6 7 8 $ ls -al -rw-rw-r-- 1 ambersun ambersun   53  三  17 03:05 config # 可以看到上面的 file mode 跑掉了  $ chmod 644 ./config $ ls -al -rw-r--r-- 1 ambersun ambersun   53  三  17 03:05 config # 正確的 file mode 應該只有 owner 可以有 write 的權限   GPG Agent Setup  1 2 3 4 5 6 7 8 9 10 # 啟用 GPG 對 SSH 的支援 $ echo enable-ssh-support &gt;&gt; ~/.gnupg/gpg-agent.conf  # 指定要用哪一把 key # 這裡 key id 要用 40 bits 的 fingerprint $ echo YOUR_GPG_KEY_ID &gt;&gt; ~/.gnupg/sshcontrol  # 指定 SSH 如何存取 GPG-agent $ echo $'export SSH_AUTH_SOCK=$(gpgconf --list-dirs agent-ssh-socket)\\ngpgconf --launch gpg-agent' &gt;&gt; ~/.bashrc $ source ~/.bashrc   40-bit 的 fingerprint 你可以用以下指令拿到  1 $ gpg --show-keys --with-fingerprint gpg-key.pub  其中 gpg-key.pub 的檔案內容為你的 gpg public key 公鑰   GPG Forward Agent  如果你不知道你在幹嘛，這部份請看看就好   以我自身的例子來說，我會需要在 remote 的機器上面進行 GPG 的簽章  很明顯，使用 remote 機器代表著我不能將 GPG key 放到伺服器上面  不過這節要講的方法也很危險，除非你能信任 server，否則不要這樣做   為了能夠讓 server 能夠存取 local 的金鑰  你需要把 Agent forward 給 server  為此你需要對伺服器進行一些設定   編輯 sshd_config($ sudo vim /etc/ssh/sshd_config) 並加入以下設定  1 StreamLocalBindUnlink yes  並重啟  1 $ sudo /etc/init.d/ssh restart   local 連線的設定檔要設定 forward socket(.ssh/config)  1 2 3 4 5 Host server      HostName xxx.xxx.xxx.xxx      User user      RemoteForward /run/user/1000/.gnupg/S.gpg-agent /run/user/1000/.gnupg/S.gpg-agent.extra      RemoteForward /run/user/1000/.gnupg/S.gpg-agent.ssh /run/user/1000/.gnupg/S.gpg-agent.ssh   他的 syntax 是 RemoteForward [remote] [local]  要取得路徑可以使用 $ gpgconf --list-dir agent-socket 以及 $ gpgconf --list-dir agent-extra-socket  如此一來，你的 remote server 可以吃的到你的 local Yubikey 了  可以在 remote server 上面使用 $ ssh-add -l 進行驗證   再次提醒，如果你不知道你在幹嘛  請不要開啟 Agent forward 的功能  攻擊者雖然無法存取你的金鑰，但仍然可以透過給定的 socket 進行驗證，進入別的系統      詳細的教學可以參考 Yubikey forwarding SSH keys    [Deprecated] Setup ed25519-sk SSH key  雖然標示成 Deprecated，但是裡面的資訊都是 正確且經過實驗 的  主要是我發現你可以使用 GPG key 直接生成 SSH key(ref: GPG as SSH Key)   如果你已經跟著底下做完成功生成 ed25519-sk key 並且想要復原  你一樣也可以使用以下指令  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 $ sudo apt install yubikey-manager -y # 如果你還沒裝 ykman, 可以用上面的指令安裝  # 列出當前 YubiKey 上的 credential $ ykman fido list WARNING: The use of this command is deprecated and will be removed! Replace with: ykman fido credentials list  Enter your PIN: ssh: 00000000000000000000000000000000000000 00000000000000000000000000 openssh  # 接著刪掉它 $ ykman fido delete ssh WARNING: The use of this command is deprecated and will be removed! Replace with: ykman fido credentials delete ssh  Enter your PIN: Delete credential ssh: 00000000000000000000000000000000000 00000000000000000000000000000 openssh? [y/N]: y  # 完事後確認 credential 已經被刪掉就可以了   Prerequisite  1 2 $ sudo apt install \\       libfido2-1 libfido2-dev libfido2-doc yubikey-manager -y   Add udev rule  為了能夠讓 Linux 能夠正確識別到 YubiKey  udev 的設定是必要的   1 $ sudo vim /etc/udev/rule.d/90-fido.rules   並填入以下資訊  1 2 3 4 #udev rule for allowing HID access to Yubico devices for FIDO support.  KERNEL==\"hidraw*\", SUBSYSTEM==\"hidraw\", \\   MODE=\"0664\", GROUP=\"plugdev\", ATTRS{idVendor}==\"1050\"   接著重新開機就可以了   Generate Key  接著生成 key     如果你在指令中有使用到 -O verify-required(就像以下範例)  那你的 OpenSSH 有可能需要做一些調整, 詳細可以參考 Compile OpenSSH 8.4    1 2 3 4 5 6 $ ssh-keygen -t ed25519-sk -O verify-required -O resident Generating public/private ed25519-sk key pair. You may need to touch your authenticator to authorize key generation. Enter PIN for authenticator: No FIDO SecurityKeyProvider specified Key enrollment failed: invalid format   這種情況是因為 你沒有新增 FIDO2 PIN，可參考 How to Change FIDO2 PIN   如果以上狀況都排除，你應該會得到以下輸出結果  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 $ ssh-keygen -t ed25519-sk -O verify-required -O resident Generating public/private ed25519-sk key pair. You may need to touch your authenticator to authorize key generation. Enter PIN for authenticator: Enter file in which to save the key (/home/ambersun/.ssh/id_ed25519_sk): Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /home/ambersun/.ssh/id_ed25519_sk Your public key has been saved in /home/ambersun/.ssh/id_ed25519_sk.pub The key fingerprint is: SHA256:SPZMDGk5TOuK21ONwTQkhaY1HpE8iDCLN4mRUbGrnqU ambersun@station The key's randomart image is: +[ED25519-SK 256]-+ |*=o+oO=o         | |+=.oX.O+         | |+ == B+oo        | | ..o.+o=         | |  .   o+S        | | . . .o .        | |. ....           | |. +o.            | | E. ..           | +----[SHA256]-----+  到此為止，你就已經成功的做出 ed25519-sk Key 了   How to Change FIDO2 PIN  預設情況下 YubiKey FIDO 是沒有密碼的，因此會出現 Key enrollment failed: invalid format 的問題(Generate Key)  你可以使用 ykman 進行修改  1 $ ykman fido access change-pin   Compile OpenSSH 8.4  GitHub now supports SSH security keys 裡面有提到可以使用 passwordless MFA  只要你的安全金鑰有支援 FIDO2 就可以使用，但它沒說的是這個功能要 OpenSSH 8.4 以上才支援   根據 OpenSSH Release Notes 裡面提到     ssh(1), ssh-keygen(1): support for FIDO keys that require a PIN for  each use. These keys may be generated using ssh-keygen using a new  “verify-required” option. When a PIN-required key is used, the user  will be prompted for a PIN to complete the signature operation.    由上可知，OpenSSH 必須是 8.3 以上才有支援 passwordless MFA(然後 document 寫 8.2)  在我的機器上 pre-installed 是 8.2 所以要自行 compile 新版本(8.4)的  指令如下   1 2 3 4 5 6 7 8 9 10 11 $ sudo apt install libpam0g-dev libselinux1-dev libkrb5-dev -y $ wget -c https://cdn.openbsd.org/pub/OpenBSD/OpenSSH/portable/openssh-8.4p1.tar.gz $ tar -xzf openssh-8.4p1.tar.gz $ cd openssh-8.4p1/ $ ./configure --with-kerberos5 --with-md5-passwords --with-pam --with-selinux --with-privsep-path=/var/lib/sshd/ --sysconfdir=/etc/ssh --with-security-key-builtin $ make -j`nproc` $ sudo make install -j`nproc`  # 你可以透過以下指令確認 ssh 版本 $ ssh -V OpenSSH_8.4p1, OpenSSL 1.1.1f  31 Mar 2020   Git with GPG  如果都設定好，那麼接下來就都挺簡單的了   1 2 $ git config --global commit.gpgsign true $ git config --global user.signingkey xxxxxxxxxxxx  其中 xxxxxxxxxxxx 為你的 GPG fingerprint  這樣設定完成之後，之後所有的 commit 都會預設要求簽名   40-bit 的 fingerprint 你可以用以下指令拿到  1 $ gpg --show-keys --with-fingerprint gpg-key.pub  其中 gpg-key.pub 的檔案內容為你的 gpg public key 公鑰     基本上這樣就可以了  可以試著簽名簽看看  簽完你可以使用 verify-commit 查看是不是有正確的簽章  1 2 3 4 $ git verify-commit HEAD gpg: Signature made 廿廿二年五月卅日 (週一) 十九時一� gpg:                using RSA key XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX gpg: Good signature from \"Shawn Hsu (COMMENT) &lt;TEST@TEST.com&gt;\" [ultimate]   Add more Accounts to GPG     如果你有多組 Email, 而他們都用同一組 GPG 的情況下  在 GitHub 上就會遇到即使你用了 GPG, 仍然是 Unverified 的情況   原因是因為你的 GPG 只有綁定一組 user id  以我的情況來說，我有兩組 email 需要使用(私人以及公司用)  所以正常情況下加一組 user id 就可以了   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 $ gpg --edit-key XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX gpg&gt; adduid # 然後輸入姓名，email, 註解, 密碼 Change (N)ame, (C)omment, (E)mail or (O)kay/(Q)uit? O # 確認輸入 # 接著輸入 Yubikey 的密碼驗證 [ultimate] (1)  Shawn Hsu &lt;TEST@TEST.com&gt; [ unknown] (2). Shawn Hsu &lt;TEST2@TEST2.com&gt; # 這裡它會列出目前的 user id # 可以看到第一組是我原本的，已經被設定為永遠信任 # 接下來就是要把新的 user id 加入信任名單裡面  gpg&gt; uid 2 # 選擇特定組別的 user id, 號碼是上面括號內的 [ultimate] (1)  Shawn Hsu &lt;TEST@TEST.com&gt; [ unknown] (2). Shawn Hsu &lt;TEST2@TEST2.com&gt; # 選定的會用星號標示，如果要取消選擇就在輸入一次 uid 2 就會取消了(toggle)  gpg&gt; trust # 可以依照你的喜好決定要多信任這組 user id # 我是選 5(亦即 ultimate trust) [ultimate] (1)  Shawn Hsu &lt;TEST@TEST.com&gt; [ultimate] (2). Shawn Hsu &lt;TEST2@TEST2.com&gt;  gpg&gt; save # 最後存檔離開就可以了     最後記得要更新公鑰  因為你已經新增了一組 user id, 公鑰的內容也會改變  1 $ gpg --export --armor xxxxxxxxxx  其中 xxxxxxxxxx 是 GPG 公鑰的 fingerprint  然後把公鑰更新到 GitHub 上   用好之後在 GitHub 上面看就會是長這樣  從原本的只有一個 email    變成有兩個 email       另外要移除 user id 也是類似  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 $ gpg --edit-key XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX [ultimate] (1)  Shawn Hsu &lt;TEST@TEST.com&gt; [ultimate] (2). Shawn Hsu &lt;TEST2@TEST2.com&gt;  gpg&gt; uid 2 # 選定第二組 user id  gpg&gt; revuid # 選 4(no longer valid) # 輸入密碼做確認即可以刪除 [ultimate] (1)  Shawn Hsu &lt;TEST@TEST.com&gt; [ revoked] (2). Shawn Hsu &lt;TEST2@TEST2.com&gt;  gpg&gt; save # 最後存檔離開就可以   也一樣要更新公鑰   YubiKey with USB Extension Cable  值得注意的是，如果你使用 USB 延長線可能會導致部份功能無法使用  根據我自己的實測顯示，延長兩段以上，YubiKey OTP 的功能在這個情況下是沒有反應的  不過 SSH 以及 GPG 的功能是正常的   比如說我的 GitHub 有使用 YubiKey 設定 MFA  在使用延長線的狀況下，你的金鑰會閃，但是你按上去是沒有反應的   References     Using Your YubiKey with OpenPGP   Resetting the OpenPGP Application on the YubiKey   drduh/YubiKey-Guide   PGP，OpenPGP和GnuPG加密之間的區別   Move pgp key to new Yubikey   Mention Key Derived Format   What is the difference between Key, Certificate and Signing in GPG?   How to gpg sign a file without encryption   如何調整 GnuPG 背景執行 gpg-agent 時的相關設定並延長密碼快取期限   安装selinux标头   openssh/openssh-portable   Is possible to upgrade openssh-server OpenSSH_7.6p1, to OpenSSH_8.0p1?   GitHub now supports SSH security keys   How to configure SSH with YubiKey Security Keys U2F Authentication on Ubuntu   How to use FIDO2 USB keys with SSH   Building OpenSSH 8.2 and using FIDO2 U2F on ssh authentication   libfido2   OpenSSH Release Notes   How to enable SSH access using a GPG key for authentication   Bad owner or permissions on ssh config file   Generate fingerprint with PGP Public Key   利用 GPG 簽署 git commit   Verifying signed git commits?   One GnuPG/PGP key pair, two emails?   How to remove an email address from a GPG key   Yubikey forwarding SSH keys   SSH Configuration  ","categories": ["random"],
        "tags": ["gpg","ssh","yubikey","mfa","extension-cable","subkey","keyserver","gpg agent"],
        "url": "/random/gpg/",
        "teaser": null
      },{
        "title": "重新認識網路 - OSI 七層模型",
        "excerpt":"Introduction  OSI 七層模型是由 國際電信聯盟電信標準化部門 - ITU-T 與 國際標準組織 - ISO 於 1989 年制定的 開放式系統互聯模型  標準的部份目前我有看到兩個版本     ISO/IEC 7498 - 1989 第一版   ISO/IEC 7498 - 1994 第二版   每個版本的標準都包含若干個部份     The Basic Model   Security Architecture   Naming and Addressing   Management Framework   本次探討主題主要會是七層模型的部份，所以只要看 ISO/IEC 7498-1 就可以了  網路上我找不到官網的 spec, 只有找到其他備份網站的 第二版 規格 ISO/IEC 7498-1  或是可以參考另一個版本的 ITU-T Rec.X.200(1994E)(內容與 ISO/IEC 7498-1 完全一樣，他有多個 release)  這邊強烈建議搭配 RFC 1122 服用     比如說 header 那些的定義其實都是寫在 RFC 1122 裡面的，網路上的資料多數沒有標明清楚  事實上 ISO/IEC 7498-1 只有寫說這一層應該要做哪些事情而已    這邊要特別注意的是，模型只是 參考用的  我一直以為說 OSI 七層模型是強制規定，但實際上並不是的   OSI 7 layer Model  Application Layer - 7  應用層提供了軟體跨網路之間溝通的 interface(唯一橋樑)  兩台機器之間的溝通(i.e. 資料交換如網頁伺服器)需要使用 應用層協議 以及 表達層 的服務   許多的協議都是跑在應用層之上  例如 HTTP, HTTPS, Telnet SSH … etc.   Presentation Layer - 6  表達層負責將資料轉換為 通用格式, 使的 application layer 能看懂資料   一開始看到這我有點看不懂這是啥意思 傳輸用的不都是 binary 嗎？  理論上只要 decode 不就可以看懂了   後來我想到 grpc 的 message encoding  它實際上並不是一般的 encode, 他有自己的一套方法(ref: Encoding)，而如果這些訊息沒有得到適當的 decode 是沒辦法讀出正確的資訊的  所以這就是 presentation layer 實際上在處理的事情     有關 gRPC 的相關介紹，可以參考 網頁程式設計三兩事 - gRPC | Shawn Hsu    Session Layer - 5  會議層主要的目的是維持溝通雙方的連線，確保資料交換的過程   Transport Layer - 4  傳輸層顧名思義，它為上層提供了一個可靠性傳輸的方式，讓上層不用擔心網路連線以及 routing 等等的問題   其中傳輸層的資料格式為 datagram(PDU)     通常在 TCP 裡面會稱為 segment, UDP 則是稱呼為 datagram    注意到這裡的 datagram 跟 RFC 1122 §1.3.3 裡面提到的 IP datagram 是不一樣的東西  這裡的 datagram 算是一個統稱, 可以參考 RFC 1594 §13     A self-contained, independent entity of data carrying  sufficient information to be routed from the source  to the destination computer without reliance on earlier  exchanges between this source and destination computer and  the transporting network.      常見的協議如 TCP 以及 UDP 都是跑在傳輸層之上的協議     詳細的討論可以參考 重新認識網路 - 從基礎開始 | Shawn Hsu    Network Layer - 3  網路層主要是作 routing 的功能  網際網路的連線主要連線是由數個開放網路的中繼站所組成  你可以做一個簡單的測試, 使用 traceroute 測試本機連線到 github.com 中間會經過多少 relay  1 2 $ sudo apt install traceroute -y $ traceroute github.com   預設跑出來的結果是只有 ip 以及域名解析，但我想要知道 ip 的所在地(i.e. 國家)  所以我參考 Associate IP addresses with countries [closed]，使用 MAXMIND 做查詢  它大概長這樣                  IP address       Location                       60.199.4.173       Taiwan,Asia                 52.95.218.48       Ashburn,Virginia,United States,North America                 52.93.95.79       United States,North America                 52.95.31.222       Tokyo,Tokyo,Japan,Asia                 52.93.73.224       United States,North America           上表你可以發現，它經過了至少 3 個地方(Taiwan, US, Japan)  你想哦 世界上開放網路的中繼站這麼多，他要怎麼知道怎麼走？  為什麼 traceroute 出來不是走 歐洲再到美國 而是走 日本到美國？  所以 network layer 會負責篩選出合適的路徑進行最佳化，避免你繞了地球走了三圈浪費網路資源     為了要找出最佳路徑，network layer 會帶一些重要的路由資訊  網路層的傳輸單位為 packet(PDU)  他是由 ip header + 資料組成     其中資料為真正要傳輸的 data    ip header 要帶的東西包含 source address, destination address, length, metadate(routing 相關) 以及 hop limit  hop 這個東西簡單講就是，經過的中繼站點個數, 它也可以用來粗略的估計兩台機器間的距離(路由個數)    以上述圖片來說，hop 的數值為 2(因為經過兩台 router)     那有哪些設備是跑在網路層的呢？  layer 3 switch 以及 IP 分享器 都是網路層的設備   除此之外，Internet Protocol IP 也是跑在網路層的上的協議     詳細的討論可以參考 重新認識網路 - 從基礎開始 | Shawn Hsu    Data Link Layer - 2  資料連結層為了提供 connectionless-mode 以及 connection-mode，所以它必須提供了一系列的連線建立、維護  而資料連結層會對實體層發出的錯誤訊號進行偵測以及 嘗試修正   資料連結層的傳輸單位為 frame(PDU)  他是由 link-layer header + packet(layer 3 SDU)  其中 link-layer header 的資料就跟 MAC 子層的內容是一樣的   資料連結層由兩個子層所構成 - LLC 以及 MAC  Logical Link Control - LLC  LLC(i.e. 邏輯鏈路控制) 子層在做的事情就是提供一個 interface, 讓上層可以不用管底層網路連接類型  而它還有一個更重要的功能是 multiplex(嘿對就是邏輯設計課堂教的 multiplexer 多工器)   你說為什麼這裡需要用到多工器  試想你平常在用電腦，你的網頁需要連網，line 需要連網，搞不好你還連了 NAS  是不是這些東西都需要用網路？ 理論上你電腦只有一張網卡，那當然不可能只能由一個程式霸佔網路對吧  所以你需要多工器，為每一個獨立運行的程式提供網路服務     每個 process 都能夠在一定的時間內分到網卡的資源(切換 logical link)       Media Access Control - MAC  logical link 的排程處理已經交由 LLC 子層處理了，那萬一我想要連線的對象不一樣呢？  假設你的電腦目前透過有線的方式連接了遠端伺服器 server1，以及透過無線的方式連接了你的另一台 server2  它會長的像下面這樣子(示意圖)       ref: Secure your Cloud Computing Architecture with a Bastion    很明顯你的 server1 以及 server2 的實體連線路徑並不相同  因此當你想要操作 server2 的時候，LLC 將網卡使用權交給連接 server2 的進程，同理他的底層連線路徑也需要做切換(不然你會連到別台機器)   所以切換 transmission medium(i.e. 實體連線路徑) 是 MAC 子層要做的事情  同時它還會將一些必要資訊塞入(e.g. MAC address, 必要 padding 以及 Frame Check Sequence - FCS)     除以上提到的功能之外，資料連結層也針對 區域網路 提供了 routing 的功能   layer2 交換器(記憶 MAC address 進行資料交換)是跑在資料連接層上面的設備   Physical Layer - 1  實體層定義了一種可以在多台機器間啟用、維護的物理連接，可以在這個連接上面傳輸 0, 1 位元資料(i.e. bits)   實體層傳輸的單位為 bit(PDU), 它通常為 一個 bit 或 一串 bits      實體層傳輸資料可以是 serial 或 parallel(看實作)   實體層傳輸可以是全雙工或半雙工(i.e. 可不可以同時收送資料)   那麼有哪些設備是跑在實體層的呢？  網路線(RJ45)、網路卡以及集線器     看了基本的 OSI 七層模型，想必你對網路基本架構原理有認識了  這裡還有幾張圖能夠幫助你更了解每層 layer 之間的關係\\        ref: Understanding the “OSI Model ” In Networking— All 7 Layers Explained.    Common Functionality in OSI Model  基本上撇除最高的 3 層(application, presentation and session layer)  其他的 layer 都擁有這以下功能     偵錯以及除錯            如果有發現錯誤，能解決的就會在該層解決，如果沒有，它就會將 error 往上送(propagate)           固定傳輸順序(sequential order)            傳輸資料時一定保證是依照一定的順序送資料(這樣才能確保資料讀取正確)       一般來說，網路之間傳遞資料是使用 Big endian(詳細可以參考 Endian)           Endian  Endian 指的是資料在記憶體中的排列順序  注意到 Endian 並不是受限於作業系統，而是受限於 CPU 架構  它一共分為兩種   Big-endian  big-endian 為 MSB 在 低位置 的排列方法   big-endian 多數用於網路資料傳輸，或者是少數的系統中如 Solaris(因為 Solaris 用的是 big-endian 的 CPU)   Little-endian  little-endian 為 LSB 在 低位置 的排列方法   little-endian 多數用於現今系統當中(像是我的 CPU 架構就是 little-endian)  但並不是所有系統都是 little-endian, 實際情況還是要進行確認  如果是 linux 系統你可以用以下指令進行確認  1 2 3 $ echo -n I | od -to2 | head -n1 | cut -f2 -d\" \" | cut -c6 // 0: big-endian // 1: littel-endian     ref: How to tell if a Linux system is big endian or little endian?      這裡做個簡單的比較讓你清楚的看懂他們之間的差別                  Big-endian       Little-endian                                           那麼要如何判斷高位置以及低位置？  記憶體的位置我們通常用 16 進位表示(hex) 例如 0x12345678  64 位元的 CPU 可以定址 $2^{64} - 1$ 這麼多空間, 它可以表示從 0x00000000 到 0xFFFFFFFF  我們說 0x00000000 是低位置，0xFFFFFFFF 是高位置      high, low memory 實際上還有別的意思, 是關於 kernel 對記憶體的分配(High memory)  有機會會針對這個主題寫一篇來探討    MSB and LSB  在二進位當中我們常常需要描述特定位元  其中又以 MSB, LSB 最常被提到     MSB - Most Significant Bit:            在一個長度為 n 的二進位數字下，在 第 n - 1 位 上的數字被視為是 MSB 因為它是最重要的       因為少了這個 bit 數字就跟原本的天差地遠了           LSB - Least Significant Bit:            同理，只是他是在 第 0 位 的數字       因為少了它也不會對結果造成太大影響           直接上圖比較好懂       ref: Binary Arithmetic and Bitwise Operations for Systems Programming    Will Endian Effect MSB?  既然 MSB 代表的是第 n - 1 位的數字，那 endian 會不會影響 MSB 的數值?  假設 num = 0x12345678  那麼他在記憶體中的表示法為下列兩種                  Endian       Value       MSB                       big       0x 78 56 34 12       0x7                 little       0x 12 34 56 78       0x1              上述 value 表示法，左邊為 高位置，右邊為 低位置  上述 MSB 為求方便辨識，取一個 byte 表示    那他們在不同的 endian 下，MSB 就不一樣了對吧！  答案是 對但也不對  事實上 endian 是一種表示法而已，當 endian 改變的時候，MSB 確實會改變  但不變的是，MSB 依舊是表示 第 n - 1 位 的數值      ref: Does bit-shift depend on endianness?    PDU and SDU  在 computer network layer 裡傳輸的資料，通常稱為 Protocol Data Unit - PDU  PDU 的內容由使用者資料以及 protocol control 相關資料所組成  在上面 OSI 七層我們討論的 frame, packet, datagram 都是被稱為 PDU   考慮 OSI 七層模型  假設，當我們從 network layer 將資料往下傳給 data link layer 的時候  資料將會從 packet 變成為 frame  但是 data link layer 面對新到來的資料 並不認識，它必須透過所謂的 encapsulation(像是增加 header field) 將資料轉換為我認識的格式  而這個新來不認識的資料稱之為 Service Data Unit - SDU   他們之間的關係就是 舉例來說                  layer       data       type                       transport       packet0       packet0 :arrow_right: PDU                 network       packet1       packet0 :arrow_right: SDU packet1 :arrow_right: PDU(ip address, length)                 data link       packet2       packet1 :arrow_right: SDU packet2 :arrow_right: PDU(Mac address, FCS)                 physical       packet3       packet2 :arrow_right: SDU packet3 :arrow_right: PDU           PDU 與 SDU 的關係是雙向的  亦即他的收送資料的封裝拆解是對稱的     sender: 送資料需要一層一層的 將資料加上各種 metadata(這些 metadata 又稱作 Protocol Control Information - PCI)   receiver: 收資料的時候需要一層一層的 將資料扒開   IP fragmentation  參照上面 PDU 與 SDU 的關係  我們不難推論出最後 physical layer 的資料會是最龐大的對吧(因為這層它塞了最多的東西)  在網路的世界裡，傳送封包(data packets)是有大小限制的, 這個限制稱作 Maximum Transmission Unit - MTU  如果說因為你在新增 header 的過程中，讓整體的封包變得超出 MTU 的大小，那就必須要將封包分批送  而這個分批次送的概念稱作 IP fragmentation   MTU 的預設大小可以參考下表(完整的可以參考 常見的媒體的 MTU 表)                  Network Type       MTU size(bytes)                       Ethernet       1500                 IEEE 802.2/802.3       1492           Transmission Medium  Transmission medium 的定義是 一條由 sender 以及 receiver 建立傳輸資料的溝通渠道  在資料傳輸的領域中泛指一條傳輸的實體通道  這個通道可以將資料(i.e. bits) 從 sender 經由通道傳送給 receiver   資料傳輸共分為兩大類   Guided Media  又稱作 bounded media, 簡單來說你可以理解成 有線傳輸   常見的 RJ45 網路線是屬於 guided media   Unguided Media  又稱作 unbounded media, 你可以理解為 無線傳輸   它可以經由空氣、水進行傳輸  主要的好處就是不用多那一條線  常見的像是 無線電波(radio wave) 以及 微波(micro wave)   References     電腦網路與連結技術：第七章 區域網路模型   Logical Link Control   網路通訊PDU和SDU的區別   什麼是OSI的7層架構？和常聽到的Layer 7有關？   OSI模型   Medium access control   Frame (networking)   Network packet   Difference between PACKETS and FRAMES   RFC 1042  ","categories": ["network"],
        "tags": ["osi","rfc","endian"],
        "url": "/network/network-osi/",
        "teaser": null
      },{
        "title": "重新認識網路 - HTTP1 與他的小夥伴們",
        "excerpt":"Introduction  在 1989 年，Tim Berners-Lee 提出了跨網路交換超文本資料的初始架構  它包含了以下     超文本資料: HTML   傳輸協議: HTTP   Client 以及 Server 直到 1990, 以上規範大致上都完成了  1991 年世界上第一台 server 正式開始啟用   這篇文章呢，主要會著重在 http protocol 的發展史以及其技術細節  讓我們開始吧   HTTP/0.9  HTTP 標準發展初期，只有定義一個基本的概念      Requests consisted of a single line and started with the only possible method GET followed by the path to the resource    換言之，就是 GET /index.html 這種  然後 server 必須回應一個 HTML 的檔案如下  1 2 3 &lt;html&gt;     Hello, World! &lt;html&gt;  定義非常的簡單，而上述的定義並沒有包含 headers, error code … etc.  我甚至沒有找到對應的 RFC 規格書，可見這是非常初期的定義(這個版本其實沒有編號，0.9 是為了與之後的版本做出區別而設立的)   HTTP/1.0  在 1996 年 5 月的時候，HTTP/1.0 的規範正式釋出，編號為 RFC 1945   HTTP - Hypertext Transfer Protocol 是跑在 應用層 之上的標準，且基於 request/response 架構  其工作流程如下     client 對 server 建立連線   client 對 server 發送 request(內容包含: request method, URI, protocol version, request modifiers, client information, body)   server 對 client 回應 response(內容包含: status, protocol version, error code, server information, entity metainformation, body)      client 通常會借助 user-agent 進行操作, user-agent 通常是: 瀏覽器，爬蟲或其他 end-user tools    HTTP 通常是基於 TCP/IP 進行通訊，但是 也不一定是必須要跑在 TCP/IP 之上，只要底層協議提供 “可靠性傳輸” 即可  何謂可靠性傳輸？ 很可惜的這個議題不在 RFC 1945 討論範圍之內     TCP default port 是 80    Entity  在 request/response 傳輸的過程中常常會需要帶所謂的 Entity  而 Entity 通常由 Entity-Header 以及 Entity-Body 所組成   Entity-Header  Entity-Header 包含了 Entity-Body 的 metainformation, 通常為以下     Allow: 列出支援的 method(e.g. GET, POST)   Content-Encoding: 壓縮方法(e.g. x-gzip)   Content-Length: Entity-Body 的長度   Content-Type: media type(e.g. text/html, 可參考 MIME - Multipurpose Internet Mail Extensions)   Expires: Expires 指定了一個時間(date)，代表該 response 視為是陳舊、過期的(完整的討論可參考 HTTP/1.0 Expires)   Last-Modified: 描述了該 resources 上一次被更改的時間(date)   extension-header            這個 header 提供了一個自定義的機制，但不保證 recipient 能夠正確辨認       常見的 extension-header 有 x-forwarded-for, x-real-ip, x-cache, x-xss-protection           Entity-Body  Entity-Body 是由一連串的八進位字串所表示，其資料型態須由 Content-Type(e.g. text/html) 以及 Content-Encoding(e.g. x-gzip) 決定   如果 Content-Type 並未指定，則資料接收端可以透過兩種方式去決定讀取方式     檢查內容 決定   透過 副檔名 決定   阿如果還是無法決定呢？ 你可以預設為 application/octet-stream   Connection  對於連線的建立與刪除，HTTP/1.0 的規範裡提到     connection 必須由 client 建立   在 server 回應 response 之後，connection 必須由 server 主動關閉   每一次的 request 都必須要使用新的 connection   connection 可能會因為使用者操作、timeout或程式錯誤提早關閉   connection 的關閉會 “永遠” 中斷目前 request，不論目前的狀態   Status Code  HTTP status code 是由 3 個數字組成的，第 1 位數字具有特定意義如下所示，2, 3 位則無特定意義     1xx: Informational: 保留以後使用   2xx: Success: 操作被接收、理解以及接受   3xx: Redirection: 需要進一步的動作以利完成 request   4xx: Client Error: request 內含有無法處理的語法   5xx: Server Error: 伺服器無法處理合法的 request   簡單來說呢，就是下面這張圖    Methods  HTTP/1.0 還定義了常用的 methods     GET: 從 URI 取得資源   HEAD: 與 GET 類似，只是 不包含 entity-body   POST: 將 request entity 新增到對應的 URI 之下(i.e. create)   Does HTTP run as Plain Text?  先前 Entity 提到，Entity-Body 是由一連串八進位的字串所組成  那麼要如何解析它呢？   HTTP character sets 是一種使用一個或多個 mapping table 將一連串八進位的資料解析成字串 的方法  而 mapping table 就是(e.g. ASCII)      ref: Python: ASCII value of letter in Python       這裡以 ASCII 為例，0x0A 是 LF, 0x0D 是 CR    所以你發現了嗎？ HTTP 是 plain text 傳輸  它只是將你的訊息 encode 成八進位     注意到 encode 跟 encrypt 不一樣    其中 HTTP/1.0 允許以下編碼方法: US-ASCII, ISO-8859-1, UNICODE-1-1-UTF-8 … etc.  如果 media subtype 為 text 且沒有指定 charset 的情況下，預設是 ISO-8859-1   Text Defaults  假設說你的訊息內容有包含 “換行”  HTTP 允許在 Entity-Body 內以 CR(0x0A) 以及 LF(0x0D) 作為換行符號  可是如果遇到 charset 並不支援的情況下，你可以自由更換 charset(只要他有相對應的控制字元即可)  所以總的來說會有以下幾種換行情況     支援 CRLF   僅支援 CR   僅支援 LF   指定 charset 中有可以表示 CRLF 的控制字元   以上四種狀況在 HTTP/1.0 當中都將視為合法的   MIME - Multipurpose Internet Mail Extensions  定義於 RFC 2045 以及 RFC 6838  MIME 又稱作 media types, 是一種表示文件、檔案的標準  其通常由至少兩部份構成 type 以及 subtype  形式為 type/subtype;parameter=value  1 2 application/javascript text/html; charset=utf-8   HTTP/1.1  在 1999 年的時候，HTTP/1.1 正式釋出，編號為 RFC 2616  為 RFC 2068(前 HTTP/1.1) 的更新版本     其實還有更新的 RFC 7231, RFC 7232, RFC 7234 只不過相對重要的討論沒有在此提及，所以我們還是以 2616 為主，必要的時候參考上述 RFC 做更新    HTTP/1.0 並沒有很好的考量到 proxy, cache, virtual host 以及 connection 的問題  就比如說 HTTP/1.0 的規範提到說 每一次的 request 都必須要使用新的 connection(ref: HTTP/1.0 Connections)  很明顯這樣的設計會造成網路資源的浪費   Persistent Connections  假設我今天要下載比較大的檔案，在 HTTP/1.0 當中我可能會需要進行多次 request(因為檔案過大)  由於 HTTP/1.0 當中指出 每一次的 request 都必須要使用新的 connection(ref: HTTP/1.0 Connections)  所以等於說我載一個檔案可能會有 n 個 TCP 連線  這樣會造成連線數量過多，CPU 負擔太大  為了克服這個問題  HTTP/1.1 預設 TCP 連線是持久的，意思就是說不論今天發生什麼事情(比如說 server 回了一個 error), client 都可以假定對 server 的連線都不會斷掉   這樣的好處多到炸     CPU, memory 可以減輕負擔   TCP connection 數量可以減少   latency(TCP 建立連線), elapsed time(HTTP request, response 時間) 可以減少   client 可以將 request 以 pipelining 的方式送出(i.e. 一直送 request 不等 response 回來)   在 client 想要關閉連線的時候只需要將 header 中的 connection 設為 close  在該次 response 完成之後就不能在送任何資料了   Methods  HTTP/1.1 新增了不少的 method     PUT: 替換(完整更新), 可參考 同樣是更新，HTTP 動詞中 PUT 和 PATCH的差別   DELETE: 刪除特定資源   TRACE            trace 通常是拿來 debug 用的，response 會帶有所有經過路徑上的所有資訊       因為現今的伺服器通常不會只是單純的 client –&gt; server 這樣，它可能是 client –&gt; proxy –&gt; server。意思就是它可能會包含你伺服器上的敏感資訊       所以他的 response body 會長的像這個樣子 :arrow_right: request body + 敏感資訊       response header 的 content-type 則是 message/http       通常會建議關閉這個 method, 因為它會有資安風險(CVE-2003-1567, CVE-2004-2320, CVE-2007-3008, CVE-2010-0386)           CONNECT: 保留 method, 它可以被用作 SSL tunneling   OPTIONS: 用於測試哪些 methods 可以在該 server 或 URI 上使用   Request Body with Get Request  根據 RFC 2616 §4.3 所述      A message-body MUST NOT be included in a request   if the specification of the request method (section 5.1.1) does not allow sending an entity-body in requests.    以及 RFC 2616 9.3 所述      The GET method means retrieve whatever information (in the form of an entity) is identified by the Request-URI.    言下之意是，你可以在 GET request 中帶有 body  不過他不應該有任何作用，GET request 僅能夠依據 URI 來取得資源  當然你可以帶 body 進去，但是 server 不應該對 body 有任何反應   Safe Methods  一個 method 被視為是安全的定義是 不會對 target resources 有任何狀態改變      Request methods are considered “safe” if their defined semantics are  essentially read-only; i.e., the client does not request, and does  not expect, any state change on the origin server as a result of  applying a safe method to a target resource.  Likewise, reasonable  use of a safe method is not expected to cause any harm, loss of  property, or unusual burden on the origin server.    你可以把它想像成是只有 read-only 的 method 會被視為是 safe  像是 GET, HEAD, OPTIONS, TRACE   Idempotent Methods     idempotent methods 指的是說 當我進行某某操作的時候，不會有其他 side effect  就比如說當我 get 某某東西的時候不會意外的刪除某些東西這樣  以上是 RFC 2616 的定義    較新的定義是在 RFC 7231  他是說當我用同一個 request method, request parameter 打同一個伺服器很多次，他的結果跟我只打一次的結果一樣  這就被稱為是 idempotent method   那哪些 method 被視為是 idempotent 的呢？  PUT, DELETE 以及 safe methods 被視為是 idempotent      safe methods 很好理解，因為它不會對 target resources 有任何狀態上的改變，也就是說不管你打多少次都一樣   PUT 會是 idempotent 是因為你是更新整體的資料(或者你可以說替代)，不管更新多少次，他的資料都已經不會做更改(已經是最新的資料了)   DELETE 同理，刪都刪掉了，再多執行一次也不會刪更多東西   Cache       ref: HTTP caching    cache 的目的就是為了要提高 performance, 有了 cache 的機制，我們可以     減少 network round-trips            有了 cache 的機制你可以不需要真正的像 server 發出 request, 它可以在某些地方就取得資料(例如在 proxy, gateway, CDN 或 reverse proxy 上面就有)       而是不是我在 proxy 就拿到資料我就不用繼續往下走到 server 了           降低 network bandwidth            假設資料一樣，server 沒必要回一整包完整的 response body, 它只需要跟你說 哦 資料一樣哦, 你就能夠只從 cache 拿資料了       而做確認(validate) 這件事情，很明顯的比送一大包 response body 還要省資源           Correctness of Cache  cache 的重要性是整個機制當中最重要的一部分，錯誤的 cache 資訊可能會導致執行出錯   所謂合法的 cache 必須符合下列任一條件     當 cache 的內容跟 server 回傳的內容相同(revalidation)   當 cache 夠 fresh(i.e. 還沒過期)   當 status code 為 304(Not Modified), 305(Proxy Redirect) 或是 error message(e.g. 4xx or 5xx)            target resources 找不到對應的資料，用 cache 也合法           如果 cache 過期，那麼在 response header 中必須帶入 Warning header  warning 由 3 個數字所組成  其中 warning code 的規則如下(這裡的 warning code 跟 http status code 沒有任何關係)     1xx: 當 cache 做完 revalidation 之後，過期的 warning 就必須要被拿掉，而 1xx 的 warning code 就是告知可以移除警告, 此 warning code 僅能由 server 產生   2xx: 當 cache 因為某種原因 造成 cache 資料沒有被更新到，這時候就必須回傳 2xx warning code, 並且 warning 不能被拿掉   Cache Validators  cache validators 主要的功用是作為檢查 cache 有沒有合法的一個手段  主要分為兩大類      strong validators            每一個小小的改動，都會造成 validators 大大的不同(i.e. octet equality)       e.g. ETag           weak validators            每一個小小的改動，不一定會造成 validators 改動       e.g. Last-Modified Dates           假設我有一個檔案在一秒之內更改了兩次 稱為 file1 以及 file2, 又因為 HTTP Last-Modified Dates 精度到秒而已    那麼他們的 cache validators 會長這樣                                  Entity           Cache Validators                                           file1           2022-05-01 00:23:10                             file2           2022-05-01 00:23:10                           這樣 validation 會過，但是檔案不一樣餒       How does Cache Works  cache 的目的是減少 request server 的次數  最好的方法就是跟 client 說某某檔案在未來的 48 小時以內都不會改變，請你 cache 起來就好對吧  如果你超過 48 小時跟我拿資料，我不保證它一定還是正確的      請注意， server 不一定會跟你講 expire time, 可參考 Heuristic Expiration    那如果過期了資料就一定得重新拿嗎？  總有那麼幾個東西是很少在改變的對吧 比如說你的 facebook 大頭貼  有可能 expired 了但是資料一樣可以用(沒改過) 這時候 cache 的機制要怎麼做？     當 server 回了你一個 response, 裡面通常會帶有所謂的 cache validators  當 client 再次 request 同一個 resource 的時候，它會檢查 cache validators 是否一樣     如果一樣，它會回一個 304 Not Modified   如果不一樣，它就會給你新的資料   常見的 cache validators 像是 Last-Modified Dates 以及 ETag     Last-Modified :arrow_right: 如果上次改動的日期沒變，那我們就可以說檔案沒有改變過   ETag :arrow_right: 如果 ETag 一樣，那我們就可以說檔案沒有改變過     透過實際的例子你可能會比較懂  我拿自己的 GitHub 做實驗, 目標是首頁的大頭貼   假設一開始進入到新的頁面(強制 reload page) 你會得到這張結果     關於怎麼強制 reload page, 可以參考 HOW TO HARD REFRESH BROWSER AND CLEAR CACHE USING WINDOWS OR LINUX      從上圖你可以得知幾個結果     status: 200 OK   etag: 866467218c9675fca81f921d68841c2c272805dbe901025c54746e7759f1c831   max-age: 300 seconds(i.e. 5 分鐘)   再五分鐘之內拿取資料，根據 Cache-Control header max-age, 我們可以很有信心的告訴你他是從 cache 拿的    疑 這時候有點不一樣了哦     status: 200 (from memory cache)   是不是跟你想的一樣呢？ 因為 max-age 的關係，以及 ETag 對的上表示 resource 沒有改變(亦即檔案沒有任何改變)，因此 Chrome 直接從 memory cache 拿到資料     有關 ETag 的介紹，可以參考 ETag    那如果我超過 max-age 時間怎麼辦，local cache 裡面我有找到我要的檔案, 但是 max-age 告訴你它已經過期了，你不應該將它視為合法的  這時候你就應該要帶著 ETag 去跟 server 問說, 我這個檔案有沒有過期阿? 能不能繼續使用, 若得到 server 肯定的答覆，那麼代表你的 cache 還可以繼續用, 如同下面這張圖        note: 圖片的 date 時間點系屬截圖時間不一致，不影響答案    Heuristic Expiration  既然 server 不一定會給你一個 expiration time, HTTP cache 會拿 Last-Modified time 去預估  不過 HTTP/1.1 的協議還是希望 server 主動提供準確的 expiration time   以下列出幾個各大廠商的 heuristic expiration algorithm     Chrome HttpResponseHeaders::GetFreshnessLifetimes()            freshness_lifetime = (date_value - last_modified_value) * 0.10           FireFox nsHttpResponseHead::ComputeFreshnessLifetime()            freshnessLifetime = (date_value - last_modified_value) * 0.10           Common Headers with Cache  1. Cache-Control  Cache-Control header 用於控制 HTTP Cache 的 request 以及 response   什麼東西可以被視為可以 cache 呢？     request method(e.g. safe methods)   request header fields(e.g. Cache-Control)        response status                                  HTTP Status Code                                                       200 OK           203 Non-Authoritative Information                             206 Partial Content           300 Multiple Choices                             301 Moved Permanently           401 Unauthorized)                                  206 Partial Content 只有在 Range Header 或 Content-Range Header 支援的情況下才可以使用 cache            上述任一條件符合，它就能被 cache   Cache-Control 可以為以下(僅列出部份做參考)                  Value       Description                       public       response 可以被 cache                 private       response 不可以被 cache                 no-cache       no-cache 指的是 沒有經過 server 的重新驗證，不得將 response 作為 cache no-cache 可以帶參數(稱為 field-name) 如果 header field 出現在參數裡面，則不能作為 cache(避免重複使用特定 header)                 no-store       request 或 response 都不能存下來                 max-age       max-age 指的是 當 response 的年紀超過 max-age，該 response 則視為過期 max-age 的單位為 seconds           2. Expires  Expires 跟 max-age 很像，它的作用是 當 response 的年紀超過 expires，該 response 就當作過期  Expires 的格式為 Expires: Thu, 01 Dec 1994 16:00:00 GMT      要注意的是 max-age 的單位是 seconds  expires 的單位是 date      如果是 max-age 以及 expires 一同出現，那我要依哪一個為準？  根據 RFC 2616 §14.21 中提到的      Note: if a response includes a Cache-Control field with the max-  age directive (see section 14.9.3), that directive overrides the  Expires field.    所以是以 max-age 為準   Expire 設定有沒有過期的方法有以下兩種     已經過期 :arrow_right: 設定 expire 為當前時間 t，因為在 t + 1 的時間裡它就過期了   永不過期 :arrow_right: 你可以將 expire 的期限設為 一年(HTTP/1.1 建議最高一年為限), 則該 response 視為永不過期   max-age vs. no-store vs. expires  這幾個都是設定 cache 相關的 header field, 那麼他們之間的差別究竟為何   no-store 指的是 無論如何都不能將 response cache 起來(因為可能含有機密資訊)    max-age 以及 expires 都是設定 cache 的新鮮程度，前者使用 seconds, 後者使用 date  當兩者同時出現的時候，以 max-age 為準(可參考 Expires)  而他們是作為決定 response 新鮮程度(freshness) 的參考指標之一     當足夠 fresh 的情況下(i.e. 未過期) :arrow_right: 不會向 server 要   當已經過期了 :arrow_right: 像 server request 新版的資料      注意到 no-cache 指的是 每次都要向伺服器進行驗證，並非不 cache  所以它實際上會進行 cache, 只是說仍然要驗證，而驗證過的話，response payload 要帶的東西就可以是一個單純的 304    What Resources Can be Cached                  Method       Cacheable                       GET       ✅                 HEAD       ✅                 POST       ❌                 PUT       ❌                 DELETE       ❌                 TRACE       ❌                 CONNECT       ❌                 OPTIONS       ❌           ETag     此處討論請參考 RFC 7232    ETag 是 response header field 之一，用於標示資源版本  ETag 可用於 cache validation 的比較，前面有提到說 Last-Modified Dates 不太適合做驗證  ETag 由於每次更改 resource 內容都會改變 ETag 內容，因此更適合進行版本驗證      你可以把 ETag 想像成是 checksum, 每一次的改動 checksum 都會改變      如果我打開了一個檔案但沒有做改動，只有時間戳記不一樣，那他們是一樣的嗎(octet equal)？   不同的 web server 有不同的作法， 像是 Nginx 計算 ETag 的方式是會使用到 timestamp 的  參考 nginx/http/ngx_http_core_module.c  1 2 3 4 5 6 7 8 9 10 11 ngx_int_t ngx_http_set_etag(ngx_http_request_t *r) {     etag-&gt;value.len = ngx_sprintf(etag-&gt;value.data, \"\\\"%xT-%xO\\\"\",                                   r-&gt;headers_out.last_modified_time,                                   r-&gt;headers_out.content_length_n)                       - etag-&gt;value.data;      r-&gt;headers_out.etag = etag;      return NGX_OK;   Apache 也是  參考 httpd/server/util_etag.c  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 AP_DECLARE(char *) ap_make_etag_ex(request_rec *r, etag_rec *er) {     /*      * Make an ETag header out of various pieces of information. We use      * the last-modified date and, if we have a real file, the      * length and inode number - note that this doesn't have to match      * the content-length (i.e. includes), it just has to be unique      * for the file.      *      * If the request was made within a second of the last-modified date,      * we send a weak tag instead of a strong one, since it could      * be modified again later in the second, and the validation      * would be incorrect.      */     if ((er-&gt;request_time - er-&gt;finfo-&gt;mtime &lt; (1 * APR_USEC_PER_SEC))) {         weak = ETAG_WEAK;         weak_len = sizeof(ETAG_WEAK);     }      if (er-&gt;finfo-&gt;filetype != APR_NOFILE) {         /*          * ETag gets set to [W/]\"inode-size-mtime\", modulo any          * FileETag keywords.          */         etag = apr_palloc(r-&gt;pool, weak_len + sizeof(\"\\\"--\\\"\") +                           3 * CHARS_PER_UINT64 + vlv_len + 2);          etag_start(etag, weak, &amp;next);          bits_added = 0;         if (etag_bits &amp; ETAG_INODE) {             next = etag_uint64_to_hex(next, er-&gt;finfo-&gt;inode);             bits_added |= ETAG_INODE;         }         if (etag_bits &amp; ETAG_SIZE) {             if (bits_added != 0) {                 *next++ = '-';             }             next = etag_uint64_to_hex(next, er-&gt;finfo-&gt;size);             bits_added |= ETAG_SIZE;         }         if (etag_bits &amp; ETAG_MTIME) {             if (bits_added != 0) {                 *next++ = '-';             }             next = etag_uint64_to_hex(next, er-&gt;finfo-&gt;mtime);         }          etag_end(next, vlv, vlv_len);      }     else {         /*          * Not a file document, so just use the mtime: [W/]\"mtime\"          */         etag = apr_palloc(r-&gt;pool, weak_len + sizeof(\"\\\"\\\"\") +                           CHARS_PER_UINT64 + vlv_len + 2);          etag_start(etag, weak, &amp;next);         next = etag_uint64_to_hex(next, er-&gt;finfo-&gt;mtime);         etag_end(next, vlv, vlv_len);      }      return etag; }   所以看起來 timestamp 在現今 web server 實作 ETag 當中都是會用到的      Algorithm behind nginx etag generation      ETag 的表示法有兩種, 分別對應 cache validators     weak etags: W/\"xasdf\"   strong etags(default): \"xasdf\"   Comparison  同樣的，comparison 也有 weak, strong 之分      Strong Comparison :arrow_right: 兩個 ETag 不可為 weak 且內容相等   Weak Comparison :arrow_right: 兩個 ETag 內容相等                  ETag1       ETag2       Strong Comparison       Weak Comparison                       W/\"1\"       W/\"1\"       ❌       ✅                 W/\"1\"       W/\"2\"       ❌       ❌                 W/\"1\"       \"1\"       ❌       ✅                 \"1\"       \"1\"       ✅       ✅           Common Headers with ETag  1. If-Match  If-Match header 可以用於 conditional request  當 entity-tag 出現在 target resources 的時候才會執行，否則不會執行(會丟一個 412 Precondition Failed)   他的寫法如下  1 If-Match: \"asdf\", \"xxxzz\", \"qwer\"      server 端必須使用 strong comparison 進行 entity-tag 的比對    If-Match 常用於     state-changing methods 例如 PUT, DELETE, PATCH :arrow_right: 用於避免 lost update problem   驗證 cache 的 freshness :arrow_right: 如果 ETag 一樣代表 cache 的資料還可以使用      有關 lost update problem，可以參考 資料庫 - Transaction 與 Isolation | Shawn Hsu - Lost Update    2. If-None-Match  跟 If-Match 一樣 類似都是 conditional request  當 entity-tag 沒有出現在 target resources 的時候才會執行，否則不會執行(會丟一個 412 Precondition Failed)   他的寫法如下  1 2 If-None-Match: W/\"asdf\", W/\"qwer\" If-None-Match: \"xxxzzz\"      server 端必須使用 weak comparison 進行 entity-tag 比對    If-None-Match 常用於 cache validation  當過期的 ETag 隨著 If-None-Match header 送到 server 端的時候  如果一樣則表示 client 端存的 cache 依然是 fresh 的(可用的)  這個時候 server 端會回一個 304 Not Modified   Nginx with ETag  現今 web server 多數都有支援(Apache, Nginx)，其中 Nginx 在 1.3.3 之後新增了 ETag(只要版本沒有太舊基本上都有)   我們來做個小小實驗觀察 nginx  1 2 3 4 5 $ docker pull nginx $ docker run -d --name web nginx $ docker exec -it nginx bash root:/# cat /etc/nginx/nginx.conf # nginx 設定檔路徑為 /etc/nginx/nginx.conf   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 # /etc/nginx/nginx.conf  user  nginx; worker_processes  auto;  error_log  /var/log/nginx/error.log notice; pid        /var/run/nginx.pid;   events {     worker_connections  1024; }   http {     include       /etc/nginx/mime.types;     default_type  application/octet-stream;      log_format  main  '$remote_addr - $remote_user [$time_local] \"$request\" '                       '$status $body_bytes_sent \"$http_referer\" '                       '\"$http_user_agent\" \"$http_x_forwarded_for\"';      access_log  /var/log/nginx/access.log  main;      sendfile        on;     #tcp_nopush     on;      keepalive_timeout  65;      #gzip  on;      include /etc/nginx/conf.d/*.conf; }   config 檔裡面你可以看到一些像是 gzip, log, keepalive, worker connection 等等的設定   上述你沒有看到任何有關 ETag 的設定是正常的，預設 ETag 會是開啟的(ref: Nginx ETag documentation)     Syntax:\tetag on | off;  Default:  etag on;  Context:\thttp, server, location  This directive appeared in version 1.3.3.    如果你不希望 ETag 的設定為全域的，你也可以僅針對 location 設條件，像是  1 2 3 4 location /img {     xxxx     etag on; }   ETag and gzip  當我在 survey Nginx 的相關資料時，我看到了有些文章寫說在 Nginx 裡面，gzip 會跟 etag 衝突到  我就好奇啦 為什麼會衝突呢   根據 Nginx 1.7.3 release notes 指出     *) Feature: weak entity tags are now preserved on response    modifications, and strong ones are changed to weak.    對於那些修改過得 response body, weak ETag 會被保留，且 strong ETag 會被轉成 weak ETag   前者，ETag 會被保留這件事情，可以在 Entity tags: downgrade strong etags to weak ones as needed commit(def1674) 當中找到  1 2 -    ngx_http_clear_etag(r); +    ngx_http_weak_etag(r);  不難發現之前的實作是把 etag 清除掉的  為什麼之前的實作要移除 ETag 呢？ 根據 Nginx ticket 337 - etag не отдается с gzip 裡面所述      При использовании gzip - содержимое ответа меняется, и strong entity tag исходного ответа уже не может быть использован, иначе будут проблемы при byte-range запросах. Соответственно сейчас заголовок ETag при изменение ответа просто убирается (как gzip-фильтром, так и другими фильтрами, меняющими ответ, e.g. ssi).  Интересно, на что рассчитывает Chrome, используя ETag ответа, который гарантированно устарел (ему уже вернули новый ответ). ​RFC2616 как бы говорит нам:    If none of the entity tags match, then the server MAY perform the  requested method as if the If-None-Match header field did not exist,  but MUST also ignore any If-Modified-Since header field(s) in the  request. That is, if no entity tags match, then the server MUST NOT  return a 304 (Not Modified) response.    Т.е. 304 в описанной ситуации возвращён быть не может, никогда. Возможно, имеет смысл сообщить об этой проблеме разработчикам Chrome’а.  Вот что выглядит ошибкой - это возврат ETag’а для 304-го ответа при включённом gzip. Надо подумать, что с этим можно сделать…    大意是說，當使用 gzip 的時候，response 內容會因為壓縮的關係導致產出的 strong ETag 不正確(因為壓縮，使得它與原始資料的內容不相等, i.e. octet equality)     On Fri, Dec 13, 2013 at 6:12 AM, Maxim Dounin mdounin@mdounin.ru wrote:         gzipping may result in many different byte representations of  a resource. Strict entity tags aren’t allowed as a result.       所以當時最簡單的作法就是移除 ETag, 避免誤用   而如今(2014)，他們意識到沒有了 ETag 可能是個錯誤，雖然可以使用別的 cache validators(e.g. Last-Modified)  但它不夠通用、好用(因為 Last-Modified 較難實作 :arrow_left: 討論區的人說的)   於是乎經歷過兩年的討論(2013 ~ 2014)，最終社群決定將 weak etag 納入  如果啟用了 gzip, 則將 strong etag 轉換為 weak etag  可參考底下實作，nginx/modules/ngx_http_gzip_filter_module.c, nginx/http/ngx_http_core_module.c  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 static ngx_int_t ngx_http_gzip_header_filter(ngx_http_request_t *r) {     ...      h-&gt;hash = 1;     ngx_str_set(&amp;h-&gt;key, \"Content-Encoding\");     ngx_str_set(&amp;h-&gt;value, \"gzip\");     r-&gt;headers_out.content_encoding = h;      r-&gt;main_filter_need_in_memory = 1;      ngx_http_clear_content_length(r);     ngx_http_clear_accept_ranges(r);     ngx_http_weak_etag(r);      return ngx_http_next_header_filter(r); }     如果你對開發者的討論有興趣，這裡附上所有有關的 mailing list 討論連結(依照時間排序)     clearing etags when gzipping   Add Support for Weak ETags   Add Support for Variant ETags   [nginx] Adding Support for Weak ETags   References     超文本傳輸協定   MIME 類別 (IANA 媒體類別)   OSI與TCP/IP各層的結構與功能，都有哪些協議   網際網路協議套組   RFC 1122   RFC 793   RFC 2616   RFC 1945   HTTP 協議的Idempotent Methods   where is the storage location of the browser’s HTTP cache? disk or memory   Why both no-cache and no-store should be used in HTTP response?   循序漸進理解 HTTP Cache 機制   图解Http缓存控制之max-age=0、no-cache、no-store区别   What heuristics do browsers use to cache resources not explicitly set to be cachable?   HTTP 何時驗證快取 no-cache? no-store?   How to address weak etags conversion by nginx on gzip compression   E-tags missing from response headers with rails 3.2 / nginx / phusion passanger   HTTP GET with request body  ","categories": ["network"],
        "tags": ["http0.9","http1.0","http1.1","cache","etag","nginx"],
        "url": "/network/network-http1/",
        "teaser": null
      },{
        "title": "DevOps - 單元測試 Unit Test",
        "excerpt":"Introduction to Testing  在軟體開發的過程當中，QA 測試其實是很重要的一個環節  有了 QA 驗證，可以確保程式不會因為不當的輸入而產生不如預期的結果      QA - Quality Assurance  通常泛指 軟體測試工程師    就我所待過的公司，大部分的 測試 都是泛指 手動 QA  什麼意思呢？ 就是會有專門的 QA 人員，透過 web UI 或是其他方式手動進行測試  這會產生一個問題，如果目前系統更新得很頻繁，那 QA 人員會被累死吧  並且會有很大的機會，會漏掉某些 corner case   所以，你應該嘗試引入所謂的測試  而這個測試是由 程式 撰寫而成的  亦即 用程式測試程式   注意到我不是說手動 QA 沒必要可以廢掉  而是說，多加一層保障  RD 自己寫測試同樣會有盲點，手動測試可能可以 cover 到這些, 反之亦然  有了程式測試以及手動測試，你就可以很有信心的說，我的這個功能八成不會壞掉！      其實 測試程式(unit test, integration test) 若以專業分工的角度來說是 QA 要寫的  但在國內我看到的要不是沒有不然就是手動    Unit Test  單元測試為最基本且最容易實作的測試  單元測試的目的在於 測試 function 的 logic   所以 unit test 的 scope 僅限於 function 內部  實際來看個例子吧   假設你有一個 function 是用於計算加法，你想要對它做 unit test  它寫起來會長這樣   1 2 3 4 5 6 7 8 const addition = (addend, augend) =&gt; {     return addend + augend }  const testAddition() =&gt; {     expect(addition(1, 1)).toEqual(2)     expect(addition(2, 3)).toEqual(5) }   在上述 pseudo code，你可以看到我用了一個 expect 來確保兩個參數的結果是一致的  如果不一致，它會跳出錯誤  而這就是 unit test   這麼簡單的嗎？ 當然  這就是最基本的 unit test 的概念，它只會驗證 function 內部邏輯是否與你預期的一樣而已   How to do Unit Test in Real World  有的時候，在呼叫 function 之前，你可能必須要做一些必要的初始化動作  例如 初始化資料庫連線、設定 config 等等的  這些瑣碎的事情，會無形中增加測試的複雜程度   以 Jest 來說你可以這樣寫  1 2 3 4 5 6 7 it(\"should return 2\", () =&gt; {     setup()      expect(...).toEqual(...)      teardown() })  不過 setup, teardown 只會在該 test 當中跑一次而已  其實這樣是不夠用的 我們會希望每一次的 test 都是互相獨立的(亦即降低其他可能會影響測試結果的因素)  所以能夠在每一次 test 都重新初始化是最好的  你可以這樣做  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 it(\"should return 2\", () =&gt; {     setup()      expect(...).toEqual(...)      teardown() })  it(\"should return 5\", () =&gt; {     setup()      expect(...).toEqual(...)      teardown() })  不過顯然我們有更好的作法, 目前市面上的 testing framework 都有提供類似的作法  不會需要自己刻一個底層的邏輯(i.e. 怎麼呼叫 test, 怎麼控制流程 … etc.)     google/googletest :arrow_right: C, C++   facebook/TestSlide :arrow_right: Python   stretchr/testify :arrow_right: Golang   Jest :arrow_right: JavaScript   Jest 提供了一種簡單的方法  也就是使用 beforeEach 以及 afterEach  他的行為跟字面上的意思一樣，就是在每一個測試執行之前/之後，額外做一些事情  確保測試之間的相依性降至最低  為了能夠執行，我們需要一個程式進入點 describe, 在這裡初始化   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 describe(\"Test Addition\", () =&gt; {     beforeEach(() =&gt; {         // setup test     })      afterEach(() =&gt; {         // teardown test     })      it(\"should execute successfully with addition\", () =&gt; {         // test your function here     })      it(\"should return error with addition\", () =&gt; {         // test your function here     }) })   What if there’s a Function Inside?  當然，在測試的過程中，你的實作幾乎不可能像是上面的加法範例一樣簡單  它可能包含了許多的 sub-function, 可能是另一個檢查 function 或是 database 相關的  1 2 3 4 5 6 7 8 9 const getUser = async (userID) =&gt; {     ...      if (!db.isUserLogin(userID)) {         return null     }      return db.getUser(userID) }   這時候如果你要測試的對象包含 sub-function，那麼事情就會有一點點的不一樣  由於 unit test 的宗旨是 測試 function 內部的邏輯  所以按理來說 sub-function 會有他自己的 testing code  為避免相依於其他 dependency, 這時候你就需要 mock   mocking 可以替代原有的 function 或 object, 使其可以 模擬原有行為  這樣的好處是可以讓我們專注於要測試對象本身的邏輯      可參考 DevOps - 詳解 Mock 概念以及如何 Mock HTTP Request | Shawn Hsu    Dependency Inversion Principle    在撰寫單元測試的時候，我對依賴反轉這件事有更進一步的認識   前面提到，為了要測試 sub function, 我們可以使用 mock 建構假的物件方便測試  但是如果你的程式沒有寫好，導致相依性過高，那實作根本沒辦法拆掉，也就會變得難以測試   以我最近遇到的情況，我想要對一塊商業邏輯做測試  其中，裡面的資料庫實作是完全綁定在 service layer  也就是說它並沒有使用 interface 來降低各個 object 之間的耦合性  正確的作法是 將依賴的對象從 object 轉換成為 interface  最後在使用 Dependency Injection 針對不同的情況使用不同的實作  在測試的例子當中，就可以將實作抽換成 mock 了         可參考 如何寫出好的程式碼架構 | Shawn Hsu    Dependency Injection Framework  當元件越來越多的時候，手動注入可能是個問題  因此現在有一些框架有試著解決這些問題，比如說 uber/dig      可參考 實際上手體驗 Golang DI Framework 之 Uber Dig | Shawn Hsu    dig 的做法是將全部的元件都註冊到 container 裡面  container 類似一個管理中樞，所有元件都會在這裡註冊  你不需要知道元件彼此之間的相依關係，你只要知道，你需要他就好  dig 會負責幫你把所有需要的元件都注入(依靠 reflection)      此過程稱為 provide    舉例來說，我有 router, controller, service, database  具體上誰依賴誰我不知道，但你可能需要他們? 要用自己拿   本質上這些工具做的事情是解決了手動註冊管理的問題  並不會因為你用了他，你的 code 就解耦了(事實上我看過用了 Framework 還寫出黏在一起的程式碼)   此外，框架如 dig 你會沒辦法第一時間知道誰依賴誰  這對於剛接觸 codebase 的人來說可能會無所適從  必須要一層一層看他怎麼定義的才能明白   相對來說，也因為 DI(Dependency Injection) 相當好實作  手動建構依賴關係的也是大有人在，具體依照團隊需求各自決定   Issues that I have when Writing Tests  到這裡你已經足夠了解如何撰寫測試了  不過在一開始我寫測試的時候，錯誤的實作了一些東西  借這個機會，一起紀錄一下   Minimize Test Case Scope  拿 cursor based pagination 為例      可參考 資料庫 - 更好的分頁機制 Cursor Based Pagination | Shawn Hsu    假設你想要測試 cursor 是否能正確的查詢到下一頁的資料  一種寫法是   1 2 3 4 5 6 7 8 9 it(\"should return data if cursor is empty\", () =&gt; {     const response = request(router).get(\"/\").query({})     expect(response.body).toEqual(expected)      const response2 = request(router).get(\"/\").query({         cursor: response.next_cursor     })     expect(response2.body).toEqual(expected2) })  很直覺的一種寫法，為了測試 cursor 能不能正確動作  我做了兩次 API call, 一次是為了取得下一頁的 cursor，第二次是為了驗證他有沒有正確 paginate   這樣做是屬於 bad practice, 尤其是當 cursor 並沒有依賴於前者的 response  每一項的測試應該是獨立的，這樣做的好處在於出問題的時候你可以很清楚的找到問題點(i.e. root cause)   你可能會問，cursor 不是從上一次的 API call 的 response 拿回來的結果嗎  這樣就是有依賴關係阿，為什麼還是不推薦這樣寫   原因在於 unit test 中你的資料多半是使用 mock 之類的 Test Double  理所當然的你會知道他的回傳結果是什麼  所以在這個 context 下，拆成兩個部份是合理的     但假設，你測試的東西是比如說 … enum 好了  而該 enum 裡面包含了幾十個可以接受的數值  那麼很明顯的，將它獨立分開放，會使得程式碼過於冗長且難以維護  但是把它擺在一起，在出錯的時候你很難確定是哪一個 value 造成問題   多數的測試框架如 Jest 並沒有提供 expect with message 的功能  但仍然是有解的，像是 jest-expect-message 就擴充原本的功能  讓你的測試可以這樣寫   1 2 3 4 5 6 7 8 9 10 11 12 13 14 const SEARCH_TYPE = {     USER: 'USER',     GROUP: 'GROUP' }  const validateType = (type) =&gt; Object.values(SEARCH_TYPE).includes(type)  it(\"should pass if type is valid\", () =&gt; {     const validValues = ['USER', 'GROUP']      validValues.forEach(type =&gt; {         expect(validateType(type), `shouldn't return error if type is ${type}`).toBeTruthy()     }) })   Don’t Use Inconsistent Input to Test Implementation  測試本質上的目的是在於確保你的改動不會改壞東西  因此，你的測試資料它必須是固定不變的  目的是當測試出錯的時候，你能夠 重現它   假設你用了 random() 之類的東西當輸入，每一次跑測試的資料都不一樣  那我怎麼知道在什麼情況下，我的程式會出錯，並且對於開發者來說它很難查明 root cause  所以這其實是個 anti pattern   正確的作法，也相對簡單  每一筆的 test case 資料，都應該使用 fixed data  不要使用隨機產生的資料，去測試你的程式   Constant or Literal in Unit Test  既然我們已經知道要使用 fixed data 當作程式的 input  另一個問題接踵而至   我在 Do you use constants from the implementation in your test cases? 發現有人也遇到一樣的問題  大意是說  在測試的時候，你的 expected result 要使用 literal 還是實作當中的 constant   以連結內的例子來看  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 const float PI = 3.14; float getPi()  {     return PI; }  // 這樣子寫 void testPiIs3point14() {    AssertEquals(getPi(), 3.14); }  // 還是這樣子寫 void testPiIs3Point14() {    AssertEquals(getPi(), PI); }   一種是直接將期望結果寫死，一種是用個變數  我最先想到的問題是，如果常數改變了，我的 unit test 是不是抓不到錯誤  抓不到錯誤那我的 test case 不就白寫了   我的擔心是正確的，但我擔心的地方不該是這個 test case 該做的  用 constant 代表說我希望這個 function 回傳的是 PI 這個常數的數值  用 literal 代表我希望這個 function 回傳的是 3.14  高階一點的看法是     constant: 我希望這個 function 的 行為 是回傳 PI   unit test 的宗旨我們開頭有提過，是測試 function 的 logic  錯誤的數值是結果，導致這個結果的原因是 logic 不符合預期  以這樣的觀點下去看，數值錯誤不應該在這裡處理，我只關心我的邏輯有沒有正確  因此我們應該 使用 constant   但錯誤的結果需要有人負責  因此你該做的事情是，額外寫個 unit test 確保 PI 是 3.14   Test-Driven Development - TDD       ref: TDD (Test-Driven Development) 測試驅動開發（入門篇）    在敏捷開發的方法論裡，有這麼一個模式稱為 測試驅動開發 Test-Driven Development  他的概念是說，在開發功能之前，先寫測試  一直重複到測試全過   這個概念不僅限於 unit test, 也包含 integration test  他的概念是透過測試快速得到反饋，以此來建構你的功能   Steps to Run TDD     分析需求   撰寫 一個 test case   執行 test case :arrow_right: failed   修改 implementation 讓它可以通過 test case   重構   執行步驟一   為什麼是一次撰寫一個 test case?  因為 TDD 的目的是要讓你一次專注在一個 test case 上，當然你也可以一次寫完所有 test case  (只是一次寫完所有情境能享受到 TDD 的好處就會減少，因為你必須一次考慮所有情境，就沒有辦法寫出最 clean 的實作)  透過不斷的新增修改，你的實作最後會符合所有的需求場景，到此你的功能就開發完成了  而開發途中產生的 test case 可以當作 unit test 或 integration test 放在 CI/CD pipeline 上面執行，確保每一次的修改都是符合預期的      有關 integration test 的介紹可以參考 DevOps - 整合測試 Integration Test | Shawn Hsu  有關 CI/CD pipeline 的介紹可以參考 DevOps - 從 GitHub Actions 初探 CI/CD | Shawn Hsu    Struggles to Run TDD  通常如果跑不了 TDD 或者說寫不了測試會有幾個問題     Single Responsibility Principle            要說無法寫測試的最大原因基本上有可能是你的 function 做太多事情(i.e. God function)違反 Single Responsibility Principle 的原則       一個 function 耦合度太高會導致你無法切割邏輯，讓整個測試變得很困難       最後就導致你的測試寫的亂七八糟           Waste of Time            撰寫測試程式的確會導致開發速度減緩，但這只是表象       有了測試，你每一次的更動都不用擔心會不小心把 code 改壞       算上程式品質，你撰寫的測試是有用的           Example  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 describe(\"pageNumber\", () =&gt; {     const validators = [middleware.pageNumber()];      it(\"should pass if pageNumber is valid\", async () =&gt; {       const validValues = [1, 1e3];        for (const value of validValues) {         const request = createRequest({ query: { pageNumber: value } });         await testExpressValidatorMiddleware(request, validators);         const result = validationResult(request);          expect(result.array()).toEqual([]);       }     });      it(\"should return error if pageNumber is invalid\", async () =&gt; {       const invalidValues = [\"abc\", -100];        for (const value of invalidValues) {         const request = createRequest({ query: { pageNumber: value } });         await testExpressValidatorMiddleware(request, validators);         const result = validationResult(request);         expect(result.array()).toEqual([           {             ...RequestErrorTemplate,             value,             msg: Errors.InvalidPageNumber,             path: \"pageNumber\",             location: \"query\",           },         ]);       }     });   });   以上是一個簡單的單元測試的例子  主要的目的在於測試 middleware 各個 validation function 是否正確動作  以這個例子是測試 cursor based pagination 的 pageNumber  因為這個相對單純，所以沒有任何的 mock 以及 beforeEach, afterEach  詳細的程式碼可以參考 ambersun1234/blog-labs/cursor-based-pagination   References     软件敏捷开发 TDD 方案   使人瘋狂的 SOLID 原則：單一職責原則 (Single Responsibility Principle)   Testing in Go: Mocking MVC using Testify and Mockery   Test Doubles — Fakes, Mocks and Stubs.   Unit Test 中的替身：搞不清楚的Dummy 、Stub、Spy、Mock、Fake   How can I do test setup using the testing package in Go   Dependency inversion principle   Unit Test 觀念學習 - 3A Pattern、名詞 (SUT、DOC)   Unit Test 實踐守則 (五) - 如何有效使用 Test Double   unit test 該怎麼用? 又該如何在 express 開發上實作 unit test?  ","categories": ["devops"],
        "tags": ["unit test","TDD","dependency injection"],
        "url": "/devops/devops-unit-test/",
        "teaser": null
      },{
        "title": "單系統？ 何不多系統",
        "excerpt":"Multi System  多系統對於一般非資訊專業人士而言，基本上是一個不需要了解的知識  但是身為一個資訊專業人士，在某些情況下你可能會需要多開系統，比如說      當你的電腦是 MacBook Pro，而老師要求你明天交一個 word 檔   當你的電腦是 MacBook Pro, 而你很想要打遊戲   當你的電腦是 Windows, 你想跑得開源專案只支援 Linux   這時候多數人會選擇使用虛擬機的方式  像是 macOS 可以跑所謂的 Parallel Desktop 的方式運行  如果是 Linux 或是 Windows 系統，你可以使用 VirtualBox  這樣你就可以 同時 跑兩個系統，解決你的使用需求   普通的使用情境下這樣做當然是一個很好的選擇  但上述的操作會對你的機器造成較大的覆載  因為對於作業系統而言，它必須要模擬出上面的那個作業系統  而模擬這件事情是很耗效能的   相比 container 與 host machine 共用 kernel 這件事  顯然每個 VM(Virtual Machine 虛擬機) 都要自己模擬 kernel 會比較吃效能  (可以參考下圖的架構)      Introduction to Dual Boot  既然 虛擬機 會明顯比較吃效能  那有沒有一種方法是可以在一台電腦上裝上兩個系統 在我需要的時候就可以做切換了呢？   Dual Boot 就是這個問題的最佳解了！  在一台機器上安裝兩套系統，可以避免開虛擬機太耗效能的問題   Does Dual Boot Safe?  雙系統會對現有系統造成影響嗎？   先說結論 不會   作業系統本質上就是一堆 檔案  所以在電腦上安裝兩套的作業系統並不會對現有系統有任何影響(前提是你安裝的過程沒有失誤)   Can I Dual Boot at the Same Time?  雙系統跟傳統的虛擬機不同  虛擬機可以同時開很多個  比方說你的主機是 Windows, 你可以多開 Linux   而雙系統的意思是  電腦上安裝了兩套系統，但是 同一時間只能開一套起來使用   Dual Boot Installation  接下來會帶到所有安裝雙系統，你需要做的準備   Partitioning your Disk  這裡講的是單硬碟安裝多系統的情況  如果說你要安裝多硬碟，可以參考 Multiple Disk Installation   理所當然的，如果要在單顆硬碟上安裝多系統勢必要對硬碟做切割  但是你不能直接切個資料夾或是分區什麼的      資料夾真的不能拿來安裝系統    不能直接切分區的原因是因為  在 Windows 裡面，檔案系統預設是使用 NTFS  由於 NTFS 是使用 fixed-size block 進行檔案讀寫的，因此 NTFS 非常容易造成 Fragmentation  導致資料東一塊 西一塊  因此你在切磁碟分區的時候，必須先進行壓縮，將資料全部集中到一起，再切割就沒問題了      有關 Fragmentation, 可以參考 資料庫 - Index 與 Histogram 篇 | Shawn Hsu      一開始先使用 win + x 叫出磁碟管理應用程式(如下圖)    我自己的電腦只有一個硬碟，所以你有看到只有一列(其中包含開機磁區以及 C 槽)    對 C 槽的部份右鍵叫出選單，選擇 壓縮磁區    這裡我想要切 500 GB 出來，所以填 500000 MB    切好之後會長這樣     這樣就好  有些文章會希望你創建新的磁碟區  但是這樣會增加之後的操作的危險性  所以把它弄成為配置的空間即可   Prepare Boot Device  接下來是要準備你的系統的部份  你要準備的東西會是所謂的 映像檔   映像檔當初是為了進行跨裝置大檔案的傳輸所開發出來的格式  你可以將大檔案以無壓縮的方式打包成映像檔(i.e. iso), 之後就可以寫入 CD 或 DVD 之上(用於傳輸或儲存)  當然現在其實很少人在用這種傳統儲存媒體了，多數都是用於製作開機碟相關   那開機碟映像檔的內容包含     必要的作業系統檔案系統   必要開機程式   接下來就是選擇你的目標系統映像檔了     假設你想要 Dual Boot Windows            Windows 11 必須要透過安裝媒體 建立 Windows 11 安裝媒體 下載映像檔       Windows 10 則可以直接在官網 下載 Windows 10 光碟映像 (ISO 檔案) 下載映像檔           Linux 的部份則是可以直接下載(以 Ubuntu 為例)            到官網就可以下載映像檔了 Download Ubuntu Desktop             為了能夠讓 BIOS 可以用該 iso 檔進行開機  因此我們需要將它寫到 CD 上…嗎？  其實你可以用 usb 隨身碟就可以了(依照過往的經驗大概 8 GB 的就夠了)   你可以使用 rufus 製作開機碟  基本上只要將 裝置 以及 iso 檔案 選擇好  接著讓它跑完，開機碟就製作完成了   \\     ref: https://rufus.ie/zh_TW/    BIOS Setting  為了要能夠正確的讀取到剛剛製作好的開機碟  BIOS 的設定是需要做調整的   Fast Boot  快速開機，就是要快速  在 BIOS 層要達成這件事情就是必須得要省去檢查的步驟  當然不是說都不要檢查，而是略過一些相對不太重要的檢查程序  包括但不限於     不做 memory test   不載入外接設備   省去 BIOS 界面等待時間   這樣對於 user 來說，就可以更快的進入到系統辦正事了   但你有沒有發現，如果啟用快速開機  這樣它就不能從 usb 開機(因為 BIOS 不會檢查外接設備直到真正開機完成才會做載入)  所以要將 fast boot 設為 Disabled(如下圖)        注意到這裡的 Fast Boot 跟 windows 內建的快速啟動不一樣  windows 的快速啟動指的是藉由所謂的 假關機 來讓下一次的開機更快  具體來說它就是將你的 應用程式狀態寫到檔案存起來，下一次開機的時候就直接從檔案直接讀取狀態  也就是所謂的 休眠   Secure Boot  顧名思義，安全啟動   據說是為了確保電腦本身只能執行製造商所認可的韌體而設計的  因為我們要安裝第二個系統，需要用到的韌體肯定不會是預裝的  所以要將它 關閉      關於 UEFI 關閉 secure boot 的方法，可以參考 ASUS 主機板 關閉 Secure Boot 及 Fast Boot       CSM  CSM - Compatibility Support Module 是相容性支持模組  有些作業系統不支援 UEFI 開機，所以這個模組的用義是為了支持傳統 Legacy BIOS 開機  如今多數系統是都支援 UEFI 了，但保險起見，你還是可以把它打開(如下圖)         有關 UEFI 與 Legacy BIOS 的差別可以參考 UEFI vs. Legacy BIOS    Boot Order  前面提到過，電腦開機一次只能跑一個系統(撇除 VM 以及 Container 的情況)  那麼如果有兩個可以開機的系統，要如何選擇？  因此 BIOS 裡面會有所謂的開機順序(Boot Order)  必須將開機順序更改  \\     ref: The ASUS ROG Maximus X Apex Review: X Marks the Spot, Literally    找到你的 usb 隨身碟裝置(你通常可以透過製造商找到相對應的選項, e.g. SanDisk)  將前面製作的 usb 開機碟放在第一個順位      note: 每一家的 BIOS 都長的不一樣  有的可能是下拉式選單、可以滑鼠拖拉的  會需要找一下設定      全部做完存檔離開就可以進到下一步了   Install  本文將使用 Ubuntu 22.04 LTS 作為第二個系統   首先，先 boot 進去你的開機碟     如果你上述步驟有跟好，應該是沒問題的    接著就可以開始安裝了       如果有網路可以連，沒有也沒差      接下來這個，選擇 Something else(一定要選這個，不然就麻煩了)    在這裡你就可以看到有很多磁碟區  上面我們切了 500GB 的磁碟空間出來  為什麼說不要建立新的磁碟區，讓它保持 free space? 因為你可以從上圖看到，他的代號都會是 /dev/sdx 的格式  而且空間的計算方式會有誤差，所以你看到的空間 不一定是 500 GB, 以這個例子就是 524289 MB  而這個步驟是整個安裝雙系統最危險的地方，多數情況下如果你的磁碟空間分配的平均，如果沒有仔細查看，你可能會不小心選錯   因此這裡就是選擇之前切出來的 500 GB free space 空間  並且 boot loader 的部份，單硬碟的情況下選擇硬碟本身就可以了  多硬碟的情況，可參考 Multiple Disk Installation     這裡針對詞結空間的部份，檔案系統選擇 ext4 日誌系統  掛載目錄選擇 根目錄(i.e. /) 就可以了     有些人會選擇把 /home, /root 個別切開  這裡不建議這樣做，因為如果你切的空間不夠大，導致資料塞滿是沒有得救的(應該不是不行，但會很麻煩啦)  根目錄包含了所有的子目錄(e.g. /home, /root, … etc.), 所以你只要把全部空間都給根目錄就可以了        最後做幾個簡單的設定就可以等它跑   完成之後，恭喜你，你已經成功的安裝雙系統在你的電腦上面了   Restore BIOS Setting  在你完全安裝完成之後  BIOS 的設定必須改回去  其實主要要改的是 boot order, 其他的要不要改沒有什麼差別   不過當你安裝完成之後，系統會提示你將 usb 隨身碟移除  這個時候 boot order 預設的第一個順位就會是 GRUB2 的開機頁面了  就不用在做額外的調整了   CSM，Secure Boot, Fast Boot 你可以不用把它改回去  當然要改也是可以，看你   Multiple Disk Installation  多硬碟安裝會需要注意一件事情  就是你的 boot loader   只是需要注意的是你的 boot loader 要裝在 有 Windows 的那顆硬碟上  (或者是硬碟 A 有 Windows boot loader, 硬碟 B 有 GRUB2 boot loader, 然後將 boot order 設為硬碟 B 優先)  只有這樣開機的時候才可以選擇不同的 OS   其他的安裝步驟都是一樣的，可以參考 Dual Boot Installation   Timezone of Dual Boot  你可能會發現，當你切換不同 OS(作業系統) 的時候，時區會亂掉  會亂掉的通常都是 Windows   主要的原因呢  是因為 Hardware Clock 以及 System Clock 的關係  當你在作業系統裡面切換不同的時區(e.g. 台灣是 UTC+8)  它會嘗試將 System Clock 的時間寫入 Hardware Clock  又因為 Linux 與 Windows 對於時間的解讀是不一樣的     Linux 將 Hardware Clock 視為是 標準時間   Windows 將 Hardware Clock 視為是 當地時間   就是一個奇妙的解讀方式  導致說你在切換不同系統的時候，Windows 的時間永遠都是錯的(以台灣來說，Windows 顯示的時間永遠會慢 8 個小時，因為台灣是 UTC+8)   解決的辦法也很簡單  在 Linux 下這個指令，將 Kernel 設定成讀取 當地時間 就可以了  1 $ timedatectl set-local-rtc 1   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 // 設定前 $ timedatectl         Local time: 四 2022-05-26 02:04:01 CST     Universal time: 三 2022-05-25 18:04:01 UTC           RTC time: 三 2022-05-25 18:04:02       Time zone: Asia/Taipei (CST, +0800) System clock synchronized: yes           NTP service: active RTC in local TZ: no  // 設定後 $ timedatectl         Local time: 四 2022-05-26 02:04:01 CST     Universal time: 三 2022-05-25 18:04:01 UTC           RTC time: 四 2022-05-26 02:04:01 CST       Time zone: Asia/Taipei (CST, +0800) System clock synchronized: yes           NTP service: active RTC in local TZ: yes      c.f.  UTC: 世界標準時間  CST: CST 可以表示多種時區，不過以這個例子他是指 China Standard Time(i.e. UTC+8)       有關 Hardware Clock 以及 System Clock 的介紹可以參考 Linux Kernel - Clock | Shawn Hsu    GRUB  \\     ref: What Is GRUB Bootloader in Linux?    boot loader 是在作業系統前第一個執行的程式  它主要負責載入 Kernel, 隨後在交由 Kernel 啟動整個作業系統   Windows 有自己的開機程式  而 Linux 則是使用 GRUB2   GRUB 可以提供多個 OS 的開機選項  也就是說 GRUB 不只可以做到開機 Linux, 它也可以開機 Windows(透過 Chain-loading 的方式，亦即呼叫另一個 boot loader)  相反的，Windows 只能做到啟動 Windows      這也就是為什麼有人會說必須先裝 Windows 再裝 Linux 的原因  因為會被蓋掉，導致你沒辦法進入 Linux    GRUB Default Boot and Timeout  你可能會發現，GRUB 的選單如果你沒有手動指定要開哪個作業系統(使用鍵盤方向鍵上下移動)，它會在 10 秒之後自動進到第一個順位的 OS  對於常常使用 Windows 而非 Linux 的人來說，如果能夠讓它預設進入 Windows 就太好了對吧   透過修改 config 檔我們可以做到這個功能  1 2 3 4 5 6 7 8 $ sudo vim /etc/default/grub GRUB_DEFAULT=0 GRUB_HIDDEN_TIMEOUT=0 GRUB_HIDDEN_TIMEOUT_QUIET=true GRUB_TIMEOUT=10 GRUB_DISTRIBUTOR=`lsb_release -i -s 2&gt; /dev/null || echo Debian` GRUB_CMDLINE_LINUX_DEFAULT=\"quiet splash\" GRUB_CMDLINE_LINUX=\"\"   這裡我們關心的主要就是兩點  預設啟動 OS 以及選單的 timeout 時間     GRUB_DEFAULT            這裡的 0 指的是相對於選單的號碼，由上而下分別是(Ubuntu: 0, Advanced options for Ubuntu: 1, Windows Boot Manager (on /dev/sda1): 2, System setup: 3)       假設你想要預設是啟動 Windows, 就把設定改成 GRUB_DEFAULT=2           GRUB_TIMEOUT            timeout 指的是在選單停留的時間(不過當你有手動操作的時候就會取消倒數), 他的單位是 seconds       你可以把它改為 1 分鐘(也就是 60 seconds), 設定就會變成 GRUB_TIMEOUT=60           當你全部改完之後要對 GRUB 本身做更新  1 $ sudo update-grub  就可以了   How about Triple Boot or N Boot?  既然你可以裝雙系統，那麼理所當然的也可以裝多系統對吧   事實上當然可以，而且安裝過程基本一致  唯獨在多硬碟的情況下需要注意一下 boot loader 的位置  詳細可以參考 Multiple Disk Installation   UEFI vs. Legacy BIOS  其實本文對於 BIOS 的描述不算太嚴謹  因為實際上 UEFI 與 BIOS 是兩個不同的東西  不過為了方便起見，這裡都還是以 BIOS 代稱   BIOS  BIOS 或稱之為 Legacy BIOS 是 Basic Input Output System 的縮寫  它主要會在系統啟動之前做必要的檢查並且引導系統進行開機   BIOS 使用 MBR 格式(單一磁碟分割區可以到 16TB, 可以有 4 個 primary partition)，並且支援 32-bits 以及 64-bits 的操作系統  如果是稍有年代的電腦，就會配備 Legacy BIOS 的韌體   \\     ref: UEFI vs BIOS：有什麼差異以及哪一個更好    UEFI - Unified EFI Forum  相比舊版的 BIOS，UEFI 提供了更漂亮的畫面、更高的安全性、還可以連網  至於 UEFI 會不會做自檢(POST - Power-On Self Test)這件事情  答案是會的，但它其實簡化的 BIOS 的自檢流程   因為早期 BIOS 的年代，BIOS 是沒辦法儲存週邊設備的硬體情況的  導致每次開機都要全部重新做一次 POST  UEFI 可以儲存這些訊息，也就是說不用每次都做自檢(針對不常變動的設備)   UEFI 使用了 GPT 格式(單一磁碟分割區可以到 18EB，可以有 128 個分區)，並且支援 32-bits 以及 64-bits 的操作系統  現在多數電腦都配備了 UEFI 開機系統      儲存容量單位：Bit, Byte, KB, MB, GB, TB , PB, EB, ZB, YB       EFI - Extensible Firmware Interface         ref: ASUS ROG Strix B550-E Gaming – BIOS (1)    References     Unikernel: 从不入门到入门   How to Take A Screenshot in Your Motherboard BIOS   电脑BIOS设置里面的快速开机是什么原理?开启后对电脑有什么别的影响?   提供絕佳的啟動和關機體驗   Wrong Time Displayed in Windows-Linux Dual Boot Setup? Here’s How to Fix it   How do I set the grub timeout and the grub default boot entry?   ext4   An introduction to Linux’s EXT4 filesystem   File system fragmentation   NTFS Fragmentation   Linux磁區配置從頭開始 搞定MBR、GPT與UEFI   UEFI vs BIOS：有什麼差異以及哪一個更好   UEFI是不需要自检还是弱化了自检这个概念？   【問題】UEFI? BIOS? Legacy? 淺談主機板UEFI觀念與迷思  ","categories": ["random"],
        "tags": ["linux","dual boot"],
        "url": "/random/dual-boot/",
        "teaser": null
      },{
        "title": "Git 進階使用 - Git Add",
        "excerpt":"Recall the Basics       ref: 1.3 Getting Started - What is Git?    眾所周知，Git 會將檔案分成以下幾種狀態     untracked :arrow_right: 新檔案   modified :arrow_right: 檔案已被更改，尚未被 git 追蹤最新更改紀錄   staged :arrow_right: 檔案最新狀態已被更新至 staging area 了，等待 commit 進 local database   committed :arrow_right: 檔案狀態已經被寫入 local database   透過 $ git add 你可以將 modified 或 untracked 的檔案推入 staged 的狀態         ref: git commit 하기    Introduction to Git Add  git add 的操作簡單，卻也有不同的玩法   我最近遇到了一個狀況，是這樣子的  開發一個新 feature 的時候，想當然的會有許多的修改，動輒十幾個檔案的異動  我自己的習慣會是偏向一次寫完一個段落在 commit  但是在上版控的時候就是一個災難了   如先前所提到的，十幾個檔案 changes 必須合理的分配在不同的 commit 裡面  比方說  1 2 3 * Add router endpoint * Add user database implementation * Add user service layer implementation  而實作中可能會在同一個檔案裡面，參雜到不同 commit 的 changes   這時候 git add 如果能夠分不同部份 個別 commit  那麼對於整體 git history 就會更有善了   Git Add - Partial Add  要進行 partial add 只需要加一個參數 -p 在 git add 後面即可  1 $ git add -p 2022-09-24-git-add.md      git add -p 是 git add --interactive 的 patch mode    接下來它會提示你接下來的操作  1 Stage this hunk [y,n,q,a,d,/,j,J,g,s,e,?]?  hunk 表示一個區塊，上方顯示的所有修改都屬於同一個 hunk  所以你操作的單位都是以 hunk 為主   目前我比較常用的操作是                  operation       description                       y       stage 目前的 hunk                 n       不要 stage 目前的 hunk                 q       離開並且不 stage 任何東西                 s       將目前的 hunk 切成更小部份                 e       手動分割 hunk           你也可以輸入 ? 取得所有操作的手冊(它會顯示在 hunk 上方，所以要往上拉才看得到)   當你完成所有的 hunk 操作之後，你就完成 partial add 了  接下來的操作你就很熟悉了, commit 然後 push 到 remote   Hunk Slice  前面提到，你可以將一個 hunk 切成更小的部份  s 是自動切成更小的 hunk  有時候你會遇到他的 prompt 沒有 s 的情況  1 (1/1) Stage addition [y,n,q,a,d,e,?]?  這時候你就必須手動切 hunk 了(e)     輸入之後它會帶你到文字編輯器，長的像下面這個樣子  1 2 3 4 5 6 7 8 9 10 11 12 13 # Manual hunk edit mode -- see bottom for a quick guide  + ... + ... - ...  # --- # To remove '-' lines, make them ' ' lines (context). # To remove '+' lines, delete them. # Lines starting with # will be removed. # If it doesn't apply cleanly, you will be given an opportunity to # edit again. If all lines of the hunk are removed, then the edit is # aborted and the hunk is left unchanged.   中間顯示的部份是你這次新增的 changes, 預設都是會被加到 staging area  如果你 不希望把某一行加到這次的 commit,     對於 + 開頭的 :arrow_right: 就把它刪掉(這裡的刪掉不會刪掉你的 changes, 它只是標記說這行不要加進 staging area)   對於 - 開頭的 :arrow_right:            把開頭的 - 號 替換成 {empty space} (空白 space)           1   -this is the deleted line          變成      1    this is the deleted line                  這樣被刪除的 changes 就不會被加到 staging area 了           如果你改錯了 它會顯示  1 Your edited hunk does not apply. Edit again (saying \"no\" discards!) [y/n]?   How to do Partial Add for Untracked File  對於新的檔案，你可能會遇到  1 2 $ git add -p new_file No changes.   這是因為 git 並不認得新的檔案，它屬於 untracked file  只要將它加進去版控裡面就可以了  1 2 3 4 $ git add -N new_file $ git add -p new_file ... Stage this hunk [y,n,q,a,d,/,j,J,g,s,e,?]?   References     Commit only part of a file in Git   git add - Editing Patches  ","categories": ["git"],
        "tags": ["linux","version control"],
        "url": "/git/git-add/",
        "teaser": null
      },{
        "title": "資料庫 - Cache Strategies 與常見的 Solutions",
        "excerpt":"Introduction to Cache  Cache 快取是在計算機當中最重要的概念  作為當今最有效加速的手段之一，其重要程度在作業系統、網頁伺服器以及資料庫當中都可以看到他的身影   Cache 的概念其實很簡單  也就是 常用的東西 要能夠越快拿到   那麼問題來了，哪些是常用的東西？   Cache vs. Buffer  兩個很相似的概念  Cache 如同先前所述，是為了要更快的拿到，所以將資料放在 Cache 裡面  而 Buffer 是為了應對不同裝置速度而做出的機制，當所需的資料還沒準備好供 process 的時候，這時候你可以將資料先寫到 buffer 裡面，當資料 ready 好的時候，就能夠一次拿走  Buffer 不限於軟體，硬體層也有類似的東西                          Cache       Buffer                       Description       為了能夠快速的回應常存取的資料       儲存資料直到被使用                 Storage       原始資料的備份       原始資料           Memory Hierarchy       Memory hierarchy                   device       description       volatile                       register 暫存器       位於 CPU 內部       :heavy_check_mark:                 CPU cache       位於 CPU 內部(分為 L1, L2, L3 cache)       :heavy_check_mark:                 RAM 記憶體       我們常說的 8G, 16G 就是這個       :heavy_check_mark:                 flash       USB 隨身碟       :x:                 HDD       傳統硬碟       :x:                 磁帶       冷儲存用，現已少見於個人 PC       :x:              volatile 指的是易揮發，亦即斷電後資料就不見了       L1 cache 是不共享的，亦即一個 core 一個 L1 cache  L4 cache 的設計並不常見    上圖是電腦的 memory hierarchy  越上層速度越快，空間越小；越下層速度越慢，空間越大   我們所熟知的記憶體，硬碟分別對應到第三跟第五層  那麼既然要做 cache, 把資料放在 flash 以下顯然就不合適(因為速度慢)  那麼 register, cpu cache, ram 哪一個適合放資料呢？   register 主要用於 CPU 運算期間的暫存空間之用途，且空間極度狹小，雖然速度最快，但很不適合也沒辦法直接使用  cpu cache 的空間，以最新發布家用旗艦處理器 AMD Ryzen 9 7950x 來說，他的 L3 cache 大小為 64MB。大小是可以了，但你還是不能直接用   CPU 的 memory 不能讓程式設計師直接存取是很合理的事情  試想如果你能夠手動操作，那這將會是個災難(搞亂 cache data 有可能會導致一直 cache miss, 造成效能低下)  不過不要誤會，即使我們不能直接操作，作業系統也替我們做了許多的 cache 在 cpu cache 了   Cache Warming  cache 的容量通常不大，因此所儲存的資料是有限的  這就會遇到一些問題，當我需要的資料不在 cache 裡面的時候，是不是要去 database 撈資料出來放著  這個情況被稱之為 cache miss，反之則為 cache hit      c.f. array 操作這種，也跟 cache hit/miss 有關哦(CPU L1 cache)    cache miss 的情況下，很明顯的會比 cache hit 的 反應時間還要久  所以我們需要 cache warming   為了最大可能，避免 cache miss 的情況發生  你可以怎麼做？  手動的將需要的資料放到 cache 裡面，這樣對於 end-user 來說，沒有 cache miss, 載入速度更快，使用者體驗會更好  問題是 我要怎麼知道要放哪些資料到 cache 裡面？  crawler(爬蟲) 或許是個手段，爬完一次，所有 miss 的資料現在都會在 cache 裡面了  但是這樣會有一點問題     現今系統都不是 single server 這麼簡單，幾千、幾萬台的 cache server 要更新到哪時候？   為了防止 DDOS, 太頻繁的發 request 也會被擋   根據現有的手段，我們可以做的更好嗎？  試想，每一台 cache server 他們的資料是不是一樣的？  如果是一樣的，那為什麼需要用爬蟲重新爬一次？ 直接從現有的 cache server 複製一份不就好了？  而這正是 Netflix 的 Cache Warming 的機制          ref: Cache warming: Agility for a stateful service    從現有的正在運行的 cache node 完整的複製一份資料出來(i.e. dumper)  透過 populator 寫到新的 node 上面  不就好了嗎？   為了進一步縮減 warm up 的時間，我們不必等待 “全部資料複製出來” 再操作  可以把它切成一小塊一小塊，中間擺一個 message queue, 資料塞進去  兩邊同時作業(i.e. producer consumer problem)，可以更快完成      有關 message queue 的討論，可以參考 資料庫 - 從 Apache Kafka 認識 Message Queue | Shawn Hsu    Cache Issues  Cache Avalanche  cache 裡的資料通常會設定 expire time, 當時間到了之後，資料會被刪除  可是如果一堆資料 同時都過期，會導致所有 request 直接打到資料庫，然後直接被打爆   如果你有多台 cache server，那麼資料設定的 expire time 不太可能會一樣  這樣是不是就可以降低 同時都過期 的機率  套用到更廣泛的場景，也就是說在 expire time 的設定上面加一點隨機值(Jitter)，不要讓它都同時 expire 就可以了   Cache Hotspot Invalid  而如果 expire 的是熱門的資料，那麼擊穿的效果會更加嚴重  那不要讓它 expire 是不是一種選項   像我之前做的有一個也是用這種作法  資料是不會過期的，相反的我會定期去 overwrite 該筆資料  使其永遠都是最新的，而且你永遠在 cache 裡面可以找到   Cache Penetration  那如果，client 要求的資料本質上就不存在呢？  也就是說你在 cache 跟 database 都找不到該筆資料   cache 找不到，理論上就是去 database 找對吧  所以所有 request 都會打到 database，然後又會直接被打爆   這題如果你沒有聽過 Bloom Filter 是答不出來的  Bloom Filter 是一種機率性的資料結構，可以快速判斷資料是否存在  當資料不存在時，可以快速返回結果，避免無謂的查詢      可參考 資料庫 - 機率型資料結構 Bloom Filter 在 Cache 中的應用 | Shawn Hsu    所以如果你用 Bloom Filter 發現資料不存在，那就不用往後確認了      注意到 Bloom Filter 會有 false positive 的機率  也就是說，資料存在於 Bloom Filter 中，但實際上不存在    Cache Strategies  Cache Aside(Read Aside)       ref: Introduction to database caching    這大概是最常見的 cache 策略之一了  概念很簡單，因為資料庫的讀寫通常很慢，如果每一次都需要 access 資料庫，那麼響應時間會太久  假如在旁邊放一個 cache 呢？   cache 有資料(i.e. cache hit), 就不用 query 資料庫，效率也會變高  如果找不到資料(i.e. cache miss), 查詢資料庫撈資料，順便寫回 cache(讓下次存取不會 cache miss)   Cache Aside 在大多數情況下都是很好的解決辦法  尤其是在 read heavy 的情況，因為常用的資料會在 cache 裡面，而 cache 讀取效率高所以再多的讀取 request 都可以 handle  倘若 cache 意外下線了，也沒關係，因為資料都可以透過資料庫 restore        ref: Consistency between Cache and Database, Part 1    唯一算是缺點的，兩邊的資料可能會 不一致(可參考上圖)  為什麼？  我們的好朋友 Atomic Operation 解釋了一切  重新審視 cache aside 的實作，你會發現  當 cache miss 的時候，你會 query 資料庫，然後再寫入 cache  問題出在這裡，多執行緒的狀況下，你沒辦法保證他是 unit of work  所以會出現 inconsistency 的狀況      可參考 關於 Python 你該知道的那些事 - GIL(Global Interpreter Lock) | Shawn Hsu    Read Through       ref: Introduction to database caching    長的跟 Cache Aside 很像  不同的是，Read Through 是將 cache 放在中間  所有的讀取都跟 cache 讀取，然後 write 全部直接跟 database 溝通   一樣？ 我覺的是一模一樣阿  這麼說吧, 因為讀取全部都找 cache, 所以在某種程度上，application 是不會知道有 database 的存在的(所以架構上較簡單)      可是 cache miss 的時候不是要去 database 找？    對！  但這步驟是透過 library 或者是第三方的套件完成的  而 Cache Aside 是自己 手動 寫回 cache 的  所以這是他們的不同   缺點呢，也一樣會有資料 不一致 的問題   Write Through       ref: Introduction to database caching    解決 不一致 最簡單的解法就是  每一次的更新，都寫進去 cache 裡面，永遠保持 cache 的資料是最新的   Write Through 的概念就是，每一次的更新，都一起更新 cache 跟 database  這樣就不會 cache miss  當然這個作法也是有缺點的，同時更新 cache 與 database，會增加延遲      double write 不會遇到前面 Cache Aside 的問題嗎？  答案是不會，因為 Cache Aside 是 reader 更新，而 Write Through 是 writer 更新    通常使用 Write Through 會搭配使用 Read Through  因為 cache 的資料永遠都是 up-to-date 且與資料庫的資料始終同步  因此配合 Read Through 可以解決資料不一致的問題   Write Back(Write Behind)       ref: Introduction to database caching    Write Through 每一次都要寫回去，overhead 會太重  所以，Write Back 的策略是，每隔一段時間再寫回去  這樣，你可以增加 write performance  缺點也很明顯，如果你是用 Redis 這種 in-memory cache  當它掛的時候，會有 data loss   Write Back 可以搭配 Read Through 組合  合併兩者的優勢，組合成 最佳讀寫策略   Write Around       ref: Introduction to database caching    仔細看可以發現他是 Cache Aside 與 Read Through 的結合體  write 永遠寫入 database, read 永遠讀 cache  因此，兩者的缺點仍然存在，也就是資料會 不一致     稍微總結一下  只要是 reader 更新 cache 就有可能會導致資料不一致的情況  如果是 writer 更新 cache 就有可能會拖慢執行速度                  Strategy       Pros       Cons                       Cache Aside       容易實作，增加 read 效率       資料可能不一致                 Read Through       架構簡單，可增加 read 效率       資料可能不一致                 Write Through       解決資料不一致的問題       速度偏慢                 Write Back       可增加 write 效率       資料可能會遺失                 Write Around       read 效率可以最佳化       資料可能不一致           Redis  REmote DIctionary Server - Redis 是一款 in-memory 的 key-value 系統  Redis 可以拿來當作 cache、正規的 database 使用、streaming engine 或 message broker  由於其資料都是 in-memory 的特性，因此操作速度極快   除了上述特性，Redis 也提供 replication 以及 clustering 的功能      有關 clustering 可以參考 資料庫 - 如何正確設定高可用的 Redis    Redis Data Structures  Redis 提供了一套完整且常見的資料結構，常見的有以下     strings   lists   sets   sorted sets   hashes   使用方法你可以參考官方的 documentation, 操作直覺容易理解，我就不在這裡贅述了   Distributed Lock  你也可以利用 Redis 實作所謂的 distributed lock  使用單純的 key-value 來實作配合 NX 參數來達到 distributed lock 的效果   由於 Redis 本身是單線程的，所以並不會有競爭的問題存在  SetNX 如果沒有成功就表示已經有人拿到 lock 了，這樣就可以達到 distributed lock 的效果   另外你也可以設定一個 TTL，這樣即使你的程式掛掉，也不會造成 deadlock   Sorted Sets vs. Lists  我想把這個單獨拉出來做一個比較  因為我看到了一個討論(Why use Sorted Set instead of List Redis)說 sorted sets 跟 lists 在某些情況下效能會有差別           LIST can have duplicates.     Checking in an element exists is very efficient in ZSET, but very expansive in a LIST (especially if the element is not there).     Fetching non-edges elements from a LIST can be slow (depends on the size of the LIST and on the distance of the object from one of the edges).     LIST is most efficient when working with the edges (L/R PUSH/POP).     ZSET has the added functionality of unions and intersects, and you can sort by any other score/weight.     In ZSET, the score can be updated later on, and the order will change.      大致上都挺好理解的，唯獨第三以及第四點  另外我也想要 benchmark 一下 lists 跟 sorted sets 的速度差異  接下來我就會設計個簡單的實驗來驗證上述內容真偽     我會分別對 lists 以及 sorted sets 取值(在頭尾各取不定數量的資料)  透過時間分析不同 data structure 會如何影響取值效率  你可以在 ambersun1234/Redis Benchmark - Lists vs. Sorted Sets 這裡找到實驗程式碼   首先，先看看一次取 100 筆資料的結果    從上圖你可以很明顯的看到，lists 在頭尾的部份表現與 sorted sets 差距比中間來的小，換言之就是 lists 在頭尾的操作效率會比中間高   再來是取 1000 筆 以及 10000 資料      不難看出即使一次取一大段的資料，在多數情況下 lists 的效率仍然比 sorted sets 還要來的差(約 $0.5 * 10^6$ nanoseconds)   Redis Transactions  在讀寫資料的時候，不同執行緒同時對同一個變數讀寫有可能會造成資料不正確  而在 Redis 裡面也會遇到同樣的事情，有可能同一個 key 被塞了兩次的資料，這當然不會是我們所樂見的   但是 Redis 本身其實並不會有 data race 的問題  因為 Redis 本身其實是 single thread 的，亦即它不會有所謂的 atomic operation 的考量(因為一次只能做一個 operation)  參考 Redis benchmark - Pitfalls and misconceptions 中提到     Redis is, mostly, a single-threaded server from the POV of commands execution  (actually modern versions of Redis use threads for different things).  It is not designed to benefit from multiple CPU cores.  People are supposed to launch several Redis instances to scale out on several cores if needed.  It is not really fair to compare one single Redis instance to a multi-threaded data store.    那為什麼我在上面說它還是會造成資料不正確？  Redis 不單純只提供 strings 的資料結構，還有 lists, sets 等等的資料結構  如果不同人先後對 Redis 的 lists 進行寫入操作，同樣的資料，它最終還是會被寫兩次資料進去  因為他的 atomic operation 是 command level 的，並不是針對 key   所以最終你需要的是  確保我在真正寫入之前，我的 key 不會有任何改動(被其他人改動)  這需要 transaction     Redis 的 transaction 操作其實很簡單  1 2 3 4 5 6 7 8 9 &gt; MULTI OK &gt; INCR foo QUEUED &gt; INCR bar QUEUED &gt; EXEC 1) (integer) 1 2) (integer) 1  透過 MULTI 關鍵字宣告一個 transaction block  在這個 block 裡面的所有操作都會被 queue 起來直到執行或丟棄  EXEC 會將 block 裡面的 command 逐一執行(因為他是 single thread)  DISCARD 會將 block 裡面的 command 丟棄   但光是這樣仍然避免不了上述同時寫入的問題  WATCH 關鍵字可以起到 check-and-set 的作用，亦即如果某個 key 改變了，那麼 transaction 就會 failed(亦即所有 transaction block 內的 command 都不會執行)  而當 transaction 成功執行、失敗或者是 connection 斷線都會把全部的 key 給 UNWATCH   如此一來，透過 MULTI 以及 WATCH 的搭配，就可以避免 data race 了      有關 transaction 更詳細的介紹，可以參考 資料庫 - Transaction 與 Isolation | Shawn Hsu    Redis Persistence  在上面我們討論到了，Redis 本身是 in-memory 的設計，而 memory 是屬於易揮發的(i.e. 只要斷電資料就會消失)  對於多數系統，可能沒什麼差別，只是會多個幾秒鐘的等待將資料重新寫入，沒有太嚴重   只不過我們還是會希望，系統的 downtime 能夠越低越好  而 Redis 也提供了一些備份的機制，盡量降低 downtime 時間   RDB - Redis Database  RDB 的設計是會自動的對你的資料進行備份(可能是每個小時備份一次)  他的備份方法是由 parent process fork 出一個 child process  然後由 child process 進行資料備份(操作 disk io)，而 parent process 就繼續服務 server     聽起來沒啥問題，但是它可能會導致部份的 data loss  由於 fork 並不會真正的拷貝記憶體，直到某個人要改寫記憶體的時候，它才會做複製的動作(也就是 copy on write)   又因為 Redis 是 in-memory 的設計，所以當他正在備份的時候，萬一這時候 parent 用了一個 SET xxx yyy 更改資料，那麼 child 並不會拉到新的資料(因為 copy on write)  如果 Redis 的資料東西很多，備份很久，那麼以上的情況很可能會出現很多次，造成 data loss   AOF - Append Only File  AOF 很直覺，就是紀錄下你所有的操作 command(這樣就可以最完整的重建你的 Redis 資料庫)  當然啦 當你的檔案太大，它就會寫一個新檔案(用最短的指令重建你的資料，一模一樣的)，與此同時，它還是會繼續紀錄 log 在之前的檔案，當重建完成之後，它就會換到新的上面  整個 rewrite 的過程一樣由另一條 thread 執行，對主要服務不會有影響     預設情況下，Redis 是有啟用 RDB 的，每隔一段時間就會將資料快照寫入 persistence storage  AOF 需要手動啟用  1 appendonly yes   Redis vs. Memcached  除了 Redis, 另一個在 memory cache 領域很有名的就是 Memcached  同樣身為 in-memory cache，該如何正確的選擇？   我最近剛好要開發一個功能是需要使用到 object storage 的  但他又不需要到類似 MinIO 這種功能，只是單純的 cache  所以我就在思考說，到底哪種方案會比較適合      有關 MinIO 的介紹，可以參考 資料庫 - 大型物件儲存系統 MinIO 簡介 | Shawn Hsu    事情是這樣的  我需要儲存的資料結構是一個 key to object 的結構  其中 object 裡面可以包含若干個欄位  舉例來說，他會長這樣   1 2 3 4 5 6 7 8 user1: {     name: \"Alice\",     age: 20 } user2: {     name: \"Bob\",     age: 21 }   而這些資料是可以容許一定程度的 data loss 的  並且我們要儲存的數量目前不太確定  這麼看下來 in-memory 就可以先被排除了  因為把他直接做在 application 的 memory 裡面，我們會需要自己管控他的生命週期狀態等等的，重點是它的容量是有限的   針對資料結構的部分，其實 Redis 是比較適合的  因為 Redis 提供了一套完整的資料結構，而 Memcached 只有 key-value 的結構  以我的例子來說，僅僅使用 hash 就可以完美的解決這個功能   另外就是稍早提到 Redis 具有 transaction 的功能  而 Memcached 則沒有  同一個操作，我會需要使用 transaction 來確保資料的一致性   性能方面，Redis 是單執行緒的，對比 Memcached 多執行緒可能略輸  但是 Redis 可以透過 clustering 來提升效能   至於備份機制，雖然 Redis 提供了 RDB 以及 AOF，Memcached 則沒有  但是以這個例子來說，他沒有那麼重要，因為我們可以容許一定程度的 data loss   所以最後選擇了 Redis 作為我們的 cache server   References     Redis persistence   Difference between Buffering and Caching in OS   Cache Strategies   Introduction to database caching   Consistency between Cache and Database, Part 1   淺談各種資料庫cache策略: cache aside、read through、write through、write back   What is Cache Warming?   Cache warming: Agility for a stateful service   Cache Warming: Know it’s Importance to Improve Website Performance   Cache warming: Agility for a stateful service   比較 Redis OSS 與 Memcached   Distributed Locks with Redis   redis - 快取雪崩、擊穿、穿透  ","categories": ["database"],
        "tags": ["cache","redis","transaction","rdp","aof","memory hierarchy","cache warming","cache aside","read through","write through","write back","write around","redis cluster","memcached","distributed lock","bloom filter","cache avalanche","cache hotspot invalid","cache penetration"],
        "url": "/database/database-cache/",
        "teaser": null
      },{
        "title": "資料庫 - Transaction 與 Isolation",
        "excerpt":"Transaction    根據 google translate 的結果我們可以得知，transaction 就是交易  那麼 交易 本身需要有什麼樣的特性呢？      交易不是成功就是失敗   交易完成之後不能反悔   交易的過程中，交易的物品不該被拿去做別的交易   那麼 交易 具體來說怎麼做？     我付錢   你收錢   我拿到交易的物品   從上述其實你可以發現  一筆交易的步驟可能不只一個，而任何步驟都有可能會出錯  transaction 的概念是，我把一系列複雜的事情抽象化到只剩 交易 本身  我不用管中間的步驟哪裡錯掉 我只關心交易本身而已     交易失敗？ 我的錢要還給我   交易成功？ 我要拿到交易的物品   基本上這就是 transaction 的中心思想     對應到資料庫本身，為什麼資料庫需要 transaction?  你可能做了某個操作，需要同時更新 user 資料、permission 資料以及 notification 資料  你絕對不會希望更新完 user 資料，結果發現 permission, notification 資料都沒被更新到這種事情吧  透過 transaction 你可以保證，它會全部被更新或全部被丟棄      其實 transaction 不只可以用在資料庫，service 層也可以使用到相同的概念    讓我們用比較學術的方式梳理一下目前的概念   transaction 包含了一系列的操作(可以將它視為 unit of work)  transaction 只會有兩種狀態 - 成功或失敗     成功 :arrow_right: commit 操作   失敗 :arrow_right: rollback 操作(復原我的操作，當作什麼都沒發生過)   Need to Use Transaction for Single Query?  答案是 不用   根據 13.3.1 START TRANSACTION, COMMIT, and ROLLBACK Statements     By default, MySQL runs with autocommit mode enabled.   This means that, when not otherwise inside a transaction, each statement is atomic, as if it were surrounded by START TRANSACTION and COMMIT.   You cannot use ROLLBACK to undo the effect;   however, if an error occurs during statement execution, the statement is rolled back.    每個 sql statement 即使你沒有明確地使用 transaction 把它包住  它也仍然是一個 unit of work  transaction 多數情況下只用於 2 個以上的 sql statement  多包一層毫無意義，即使是 CREATE 或是 UPDATE 都一樣      What does a transaction around a single statement do?    Can Transaction Prevent Data Race?  會不會遇到 data race 取決於 Isolation Level  但是 如果設定成 Serializable 就不會有問題了嗎？   我最近在工作上就遇到這個問題了  我們的 code 有許多的測試進行保護，包含了 unit test 以及 integration test  其中我們發現，integration test 的部份近期突然開始出現 unique constraint 的錯誤  而這個很明顯的是，隔離機制沒有做好  其實我們很早就有發現這個問題，並將測試限制成單執行緒(從 jest 下手)  當時我們認為已經處理完成了，不過現在問題依然存在   我們使用的是 PostgreSQL  預設的隔離機制是 Read Commited(可參考 SET TRANSACTION)  提高到 Serializable 不太能解決這個問題，尤其是 PostgreSQL   根據 13.2.3. Serializable Isolation Level 所述     However, like the Repeatable Read level,   applications using this level must be prepared to retry transactions due to serialization failures.   In fact, this isolation level works exactly the same as Repeatable Read except that   it also monitors for conditions which could make execution of a concurrent set of serializable transactions behave in a manner inconsistent with all possible serial (one at a time) executions of those transactions.    也就是說，如果偵測到類似 unique constraint 這種 inconsistent  它仍然會失敗，它並不像我們所認知的，它會等到其他 transaction 執行完接著跑  所以回答這個小節的標題，使用 transaction 並沒有辦法保證不會出現 data race      當然這取決於不同資料庫的實作，有些是真的一個一個跑      至於我們最後採取了何種作法  我們選擇在 seed db 的時候，讓他的 id 是隨機產生的  雖說寫測試的時候，我們希望資料本身是 fixed data 而不是 random 的  但是對於測試 API 本身，我其實不太關心 id 是多少  id 本身就是 auto increment 或者是 UUID, ULID … 這種的  所以他不會對你的測試有任何影響，就算要 trace 也很好追   ACID  ACID 是一系列描述 transaction 該滿足的屬性, 他是由 4 個屬性組合而成  一個提供資料一致、穩定的系統 他的條件必定符合 ACID 原則  常見的 RDBMS 比如說 MySQL, PostgreSQL, Oracle, SQLite 以及 Microsoft SQL Server   Atomicity  前面提到，transaction 將一系列操作視為 unit of work  亦即 transaction 的操作在外人看來是一個整體的   Atomic 保證了這個操作是不能被中斷的(亦即不會有更新到一半的問題)  唯有這樣才不會造成更大的問題(e.g. Dirty Read, Lost Update)   Consistency  Consistency 是一個第一次有點難理解的東西   它描述的是 transaction 前後的系統狀態要是相同的  用以保證 database invariants   invariants 指的是系統不變量  是一個系統該遵循的規則 什麼規則呢？ 每個系統不一樣  以 binary search tree 來說，他的 invariants 就是左子樹所有 child 都一定比我還要小，右子樹所有 child 都一定比我還要大  這就是 binary search tree 的 invariants, 不論你對它進行哪種操作，它都一定會符合這些規則   所以 database 的不變量(或者說規則)是什麼？  舉幾個例子     Entity Integrity            這個規則指的是每個 table 都必須要有 primary key, 而且被指定為 primary key 的欄位值必須是 unique 且不為空           Referential Integrity            這個規則指的是 foreign key value 只能有兩種狀態                    foreign-key 有指到資料庫中某些 table 的 primary key 上           foreign-key 為空，那就代表目前沒有外鍵的關係或 relation 未知                              八卦是你塞入違反系統的 invariants 它也不會阻止你  也因此書中是說，consistency 應該要是 application 的責任，而非 database    所以回到 consistency  對資料庫的每一次操作，資料庫的狀態都必須要符合它該有的 invariants  這就是 consistency 在描述的東西   簡言之，處於 “良好狀態”   Isolation  若是多個 transaction 同時對同一個資源更新，它仍然會有 data race 的風險  注意到 transaction 本身並沒有防止多執行緒 data race 這件事情   透過不同程度的 isolation 可以有效的避免這種事情發生，詳細解釋可以參考 Isolation Level   Durability  簡單，一旦 transaction 被寫入，它就會永遠存在在儲存裝置裡(即使遇到系統損壞)   BASE  跟 ACID 相反，符合 BASE 的系統多半願意提供更高可用性，更高的效能(注意到 BASE 原則還是會關心資料一致性)  多數 NoSQL 資料庫都符合這個特性   Basically Available  滿足 BASE 條件的 NoSQL 資料庫通常提供水平擴展的功能  Basically available 指的是當某幾個 node 掛掉的狀態下，系統仍然有一定程度的可用性   這個部份可用性會 work 的原因是因為  資料庫會將資料複製到不同的 node 上面(database cluster)，用以維持可用性   Soft State  也因為水平擴展的特性，資料的同步會有延遲  有些機器還沒同步到最新的資料，所以資料狀態有可能會隨著時間而有不同   Eventually Consistent  雖然 Soft State 告訴你，不同機器上的資料狀態可能不一樣  但是它保證系統最終會達到一致性   只是要花多久？ 不知道  就像臉書按讚數，不同的 replica 上面的讚數量可能不一樣，但最後一定會同步完成      ref: [Day 4] NoSQL Database 的 BASE 特性    Database Read Write Phenomena  以下將會介紹各種可能會遇到的 race condition 狀況   Dirty Read     讀到還沒 commit 的資料，稱為 dirty read    兩個 transaction 同時對同個 row data 進行讀寫  其中 A transaction 更新了資料(尚未 commit)  另一個 B transaction 卻讀取到更新後的資料        ref: 複習資料庫的 Isolation Level 與圖解五個常見的 Race Conditions    Non-repeatable Read(Read Skew)     query 得到的相同 row, data 卻有不同結果，稱為 non-repeatable read    兩個 transaction 同時對同個 row data 進行讀寫  其中 A transaction commit 了資料  另一個 B transaction 讀取到更新後的資料        ref: 複習資料庫的 Isolation Level 與圖解五個常見的 Race Conditions    Phantom Read     相同 query 得到不同 row, 稱為 phantom read    兩個 transaction  A transaction 撈資料  B transaction 對同個 table 新增了資料  A transaction 再撈一次資料(相同條件) 相比第一次撈 卻多了一筆        ref: 複習資料庫的 Isolation Level 與圖解五個常見的 Race Conditions    Write Skew     兩個 transaction 對不同 data 更新，造成違反某些條件    注意到它跟 lost update 是不同的狀況，write skew 是對 不同 資料進行更新(而 lost update 是對相同資料做更新)   所以何謂 write skew?       ref: Designing Data Intensive Applications: Write Operation Race Conditions    醫生 on call 班表，其中 Alice, Bob 皆為醫師  on call 班表有個限制，必須要有至少有一個人值班  Alice 以及 Bob 都不想值班，看到 on call 班表還有人  於是都更新了 各自的 on call 班表 :arrow_right: 結果造成沒人值班   Lost Update     兩個 transaction 對同一筆 data 更新，最後只有其中一個有成功    這個 case 其實滿好懂的  也就是說 B transaction 在更新的時候，它沒有意識到 A transaction 的存在(即使它已經 commit changes)  所以等到 B transaction 寫入的時候它會把 A transaction 的 changes 給覆蓋掉   其中 lost update 多半符合 read-modify-write 的特徵  也就是說在 application layer，programmer 會先讀取目標資料，再透過 transaction 更新以及 commit      在使用 ORM 的時候，由於不熟其特性即有可能發生此狀況         ref: 複習資料庫的 Isolation Level 與圖解五個常見的 Race Conditions    常見的 lost update 解決辦法包含  Last-modified Date  透過使用 last-modified date 當時間搓記，可以去判斷說寫入的資料是不是當前 transaction 所為      但這通常不是很好的方法，因為小數點只到第二位，存在精度問題    Atomic write  看了這麼多系列文章，想必你各位對 atomic operation 已經有一定的認知了  可以參考 關於 Python 你該知道的那些事 - GIL(Global Interpreter Lock) | Shawn Hsu   那麼為什麼 atomic write 可以避免 lost update?  read-modify-write 其實可以簡化成一段 sql statement, 沒有必要分成兩段以上撰寫   1 UPDATE user SET first_name = 'ambersun' WHERE id = 1      當然某些情況下沒辦法這樣寫就是，複雜的商業邏輯處理必須要採用其他種方式    Locking  最直覺的方法之一，既然同步讀寫這麼有問題，那我就強制加一個 lock  強迫所有操作必須等待其他 transaction      詳細的討論可以參考 Database Lock    Conflict Resolution and Replication  在 cluster mode 下的資料庫系統來說，會遇到多個 node 試圖去更新同一個 row data 要怎麼辦？  hmmm 那就要自己定規則去處理囉   他的衝突規則大約有以下這些種類                  Resolution Rule       Description                       Ignore       忽略                 Timestamp       apply 最新更新的資料                 SPL routine       提供最客製化的設定，透過 Store Procedure Language 自己制定規則                 Timestamp with SPL routine       如果時間搓記一樣，使用 SPL                 Delete wins       delete 以及 insert 的優先權高於 update, 否則依照時間 apply 最新的資料                 Always apply       忽略              以上出自 Conflict resolution rule      Database Lock  Shared Lock(Read Lock)  shared lock 可以被多個 transaction 持有，因為 shared lock 的特性是依舊可以讓你讀取資料  多個 transaction 讀取資料並不會改變資料本身 所以是安全的(前題是隔離機制設置正確)   那也因為 shared lock 只能讀取的特性，所以任何嘗試更新資料的行為是不被允許的   MySQL shared lock 的指令  1 2 SELECT ... FOR SHARE # 舊版的指令 SELECT ... LOCK IN SHARE MODE      15.7.2.4 Locking Reads    Exclusive Lock(Write Lock)  exclusive lock 可以讓持有的 transaction 進行更新資料的操作  不同的是，為了保證 data integrity, exclusive lock 同一時間只能有 一個 transaction 持有  剩下的 transaction 只能排隊等待 exclusive lock   exclusive lock 不能與 shared lock 並存  因為它不能保證資料正確性   MySQL exclusive lock 的指令  1 SELECT ... FOR UPDATE      15.7.2.4 Locking Reads    Predicate Lock  有別於 Shared Lock 與 Exclusive Lock  predicate lock 並不屬於某個特定物件，他是屬於匹配某些條件的物件的 lock   比如說  1 SELECT * FROM user WHERE created_at &gt; '2023-10-01' AND created_at &lt; '2023-10-31'   這些查詢出來的 rows 都擁有著 predicate lock      predicate: 一系列的表示式，用於 filter data row    Index Range Lock  一個區間內的 lock?  那不就是 Predicate Lock？ 顯然不是的   不一樣的地方在於，Range Lock 是建立在 index 之上的  所以速度上會快於 Predicate Lock   Database Locking Mechanism  Optimistic Locking  與其讓 transaction 一個一個等待，Optimistic Locking 的機制採用 先 commit 先贏 的方法  這樣的好處是讓所有人都有 commit 的機會，當某個天選 transaction 成功 commit 之後  其他的 transaction 就必須得 rollback 重來      其他 transaction 在 “交易後” 會檢查(e.g. version number, timestamp) 確保還沒有被其他人改過    Optimistic Locking 常用於不常更新的資料或是 locking overhead 很重的地方   Pessimistic Locking  我假設我要更新的資料會被弄髒，所以先使用 Exclusive Lock 鎖住  待我完成更新，釋放 lock 之前，沒有人可以進行更新   相對的，這樣的 lock 機制會造成資源的競爭，以及 overhead 的增加   Issues with Database Lock  lock 好用歸好用，但是過度的使用不僅會造成 overhead 還有其他問題   Lock Contention  過度的使用 lock 會導致資源的競爭  若干個 transaction 嘗試讀取更改資料，會因為前面排隊的 transaction 還很多進而拖累執行速度   而且不同的 transaction 可能由不同核心下去跑  而頻繁的存取改寫，會導致不同 core 的 cache line 資料過期(invalid)  也就是說 A transaction 改了數值，B transaction 的 cache line 也必須要同步更新(write broadcasting)  這就是所謂的 Cache Coherency Protocol   Deadlock  複習一下 deadlock 的四個條件                                                  Non-preemption       process 不能被 swap out                 Mutual Exclusion       資源一次只能一個人用                 Hold and Wait       吃碗裡看碗外                 Circular Wait       A 等 B, B 等 C, C 等 A           所以資料庫有可能會發生 deadlock, 它可能一次更新很多 row data   Lock Promotion  MySQL 8.0 InnoDB 預設 shared lock 以及 exclusive lock 都是以 row-level lock 為主      A shared (S) lock permits the transaction that holds the lock to read a row.  An exclusive (X) lock permits the transaction that holds the lock to update or delete a row.    當 row-level lock 太多的時候，因為佔用的記憶體空間變大，有可能 DBMS 選擇採用 table-level lock 以降低記憶體使用率  不過這部份文獻偏少，目前僅看到 SYBASE ASE 有支援相關功能      Once table scan accumulates more page or row locks than allowed by the lock promotion threshold,  SAP ASE tries to issue a partition or table lock,  depending on corresponding lock promotion thresholds.  If it succeeds, the page or row locks are no longer necessary and are released.    Isolation Level  Serializable  序列化隔離機制，有以下 2 幾種作法   Single Thread Approach  最安全的隔離機制之一，完全犧牲掉 concurrency 帶來的好處  換來的是最完美，不會有任何 read/write phenomena 的發生  也因為他是使用單執行緒進行操作，因此如果有一個特別慢的卡住，會讓後面的堵住   像是 Redis 底層就是使用單執行緒處理的  詳細可以參考 資料庫 - Cache Strategies 與常見的 Solutions | Shawn Hsu   Two-Phase Locking(2PL)  就是 Pessimistic Locking  那 2 phase 是哪兩個階段？  拿取(acquire) 跟 釋放(release) 的兩個時段   而正如上述提到的，悲觀鎖 只能讓擁有 Exclusive Lock 的人更改資料  與此同時，其他擁有 Shared Lock 的只能等待  至於要等多久？ 沒有人知道   缺點的話  由於 acquire/release lock 的過程是很耗費時間的  再加上若是並發寫入，n - 1 個 tx 都要等待，所以他的效能很差      可以用 Predicate Lock 或 Index Range Lock 實作  多數因為效能問題而普遍採用 Index Range Lock    Snapshot  snapshot 就是對資料庫進行快照  對於正在執行中的 transaction 不予理會   具體來說他是怎麼做的呢？  維護 一個物件的多版本(Multi-Version Concurrency Control, MVCC)        The “I” in ACID — Weak Isolation Levels    也就是說我把 UPDATE 轉換成 新增 以及 刪除  如此一來我就會擁有多個版本的歷史紀錄  當需要存取的時候，我只要撈特定版本資訊就可以了(created by current transaction)   而要注意的是, snapshot 仍然需要 Exclusive Lock(Write Lock)  因為他有可能是 concurrent 在執行  read 的時候不需要用 lock, 因為他是從 snapshot 中讀取的      即: read 不會 block write, 而 write 不會 block read    但是對於 Write Skew 的狀況用 snapshot 也沒辦法避免  因為 write skew 是更新兩個不同的 row data, 而有可能會違反特定的 condition  snapshot 的機制下，它也不會知道 condition 的存在(他是在 application layer 客製化的)   因此，snapshot 機制下，Write Skew 依然會發生   Serializable Snapshot Isolation(SSI)  一樣使用 snapshot 但是 不使用 locking  也就是 Optimistic Locking 的機制      早期的 OCC(Optimistic Concurrency Control) 不使用 snapshot, 這是它跟 SSI 主要的區別    一樣是對當前資料庫進行快照  但是讓若干個 transaction 同步執行  唯一不同的是，在 commit 的時候，它會將快照跟資料庫做一次比對，確保更新的部份沒有被其他人更新過  萬一更新的部份發生衝突了呢？ 那就是必須 rollback      這就是不需要用 lock 的原因，一次只會有一個 transaction 成功 commit    也因為我不使用 lock, 所有的 acquire/release overhead 都沒有  因此，他的效能是比 Snapshot 還要來的高的   Repeatable Reads  transaction 在交易期間會拿住 read lock 以及 write lock   也因為這樣，所以在 repeatable reads level 的隔離機制下  Phantom Read, Write Skew 也有可能會出現      由於 SQL standard 並未定義 Snapshot  後來的實作有的稱之為 Repeatable Reads，但它可能代表 Snapshot 或 Serializable    Read Committed  其餘 transaction 只能看到已經被 commit 過得資料(transaction 在交易期間會拿住 write lock)  所以任何 intermediate 的資料是沒辦法被讀取到的   read committed level 的隔離機制沒辦法防止 Non-repeatable Read(Read Skew)   Read Uncommitted  如他的名字所述，transaction 可以看到其他 “還沒被 commit 的 changes”   也就是說這個 level 的隔離，是 沒辦法防止 任何 read/write phenomena 的   Conclusion  總結一下                  isolation level       read lock       write lock       range lock                       serializable       :heavy_check_mark:       :heavy_check_mark:       :heavy_check_mark:                 snapshot       -       -       -                 repeatable reads       :heavy_check_mark:       :heavy_check_mark:       :x:                 read committed       :heavy_check_mark:       :x:       :x:                 read uncommitted       :x:       :x:       :x:                          isolation level       dirty read       non-repeatable reads       phantom read       write skew                       serializable       :x:       :x:       :x:       :x:                 snapshot       :x:       :x:       :x:       :heavy_check_mark:                 repeatable reads       :x:       :x:       :heavy_check_mark:       :heavy_check_mark:                 read committed       :x:       :heavy_check_mark:       :heavy_check_mark:       :heavy_check_mark:                 read uncommitted       :heavy_check_mark:       :heavy_check_mark:       :heavy_check_mark:       :heavy_check_mark:              :heavy_check_mark: 表示會發生    References     資料密集型應用系統設計(ISBN: 978-986-502-835-0)   內行人才知道的系統設計面試指南(ISBN: 978-986-502-885-5)   What is a database transaction?   ACID   Consistency (database systems)   複習資料庫的 Isolation Level 與圖解五個常見的 Race Conditions   [Day 17] Database Transaction &amp; ACID - (2)   Transactions (1) - ACID   What is an invariant?   ACID vs. BASE: Comparison of Database Transaction Models   What is the difference between Non-Repeatable Read and Phantom Read?   Transactions (4) - Concurrent Write   Write Skew   Lost Updates   Lost Update Problem   Conflict resolution rule   Locking in Databases and Isolation Mechanisms   Optimistic and pessimistic record locking   Preemption (computing)   Types of Locks in SAP ASE   What is thread contention?   Snapshot isolation   what are range-locks?   Do database transactions prevent race conditions?   Optimistic vs. Pessimistic locking  ","categories": ["database"],
        "tags": ["database","transaction","isolation level","mvcc","read uncommitted","read committed","repeatable reads","serializable","snapshot","two-phase locking","optimistic locking","pessimistic locking","write skew","phantom read","lost update","read-modify-write","lock promotion","predicate lock","index range lock","serializable snapshot isolation","ssi","dirty read","non-repeatable read","unit of work","atomicity","consistency","isolation","durability","base","basically available","soft state","eventually consistent"],
        "url": "/database/database-transaction/",
        "teaser": null
      },{
        "title": "Container 技術 - 最小化 Docker Image",
        "excerpt":"Why Do we Need to Minimize Image  Image 的大小對開發本身來說有很大的影響嗎？  考慮到要 deploy, pull image 這件事情來說，如果遇到網路速度慢的情況下  等個幾分鐘對於開發者來說真的會大幅度的拖慢進度  因此，如果 image 大小能維持在幾 MB, 是在好不過得事情了   換言之，好處就是     上傳下載 image 的時間可以縮短   佔用硬碟空間少   Ways to Minimized Image  Smaller Base Image  最直觀也最容易想到的方法之一，用小一點的 image 就可以縮小 docker image size 了  舉凡我們平常使用的 golang, ubuntu  他們的大小                  Image       Size                       golang 1.19       992 MB                 ubuntu 22.04       77.8 MB           但是這樣還不夠小    Alpine Linux 是安全且輕量的發行版 基於 musl libc 以及 BusyBox, 其大小只有 130 MB  而 Docker 版的 Alpine, image size 僅僅只有 7.05 MB  這使得目前主流推薦都使用這個發行版作為 base image   Don’t Install Unused Tools  舉凡像是 cURL, Vim … etc. 這些工具，都會極大的增加 image 大小  因此不建議安裝這些在 image 裡面(畢竟你可能久久才開一次)   但是如果說你的安裝過程需要用到 curl 安裝第三方 package 呢？  裝是當然要裝的，但是能不能不要把它包進去 image 裡面呢？ 可參考 Multi-stage Build   Volume  對於某些 application 會依賴相當多的 config 檔，如果檔案太大，與其把它寫入 image 裡面，也可以試著使用 volume 將資料掛載進去，減少 image 空間   Store only Binaries instead of Source Code  有一個絕大多數人會忽略的一個點是  把你的程式跑起來，只需要 binary 就行(source code 其實並不需要的)  因為相比於 binary 執行檔，你的 source code 大小肯定是大的多  那 binary 理所當然是從你的原始碼 compile 得到的      對於直譯式語言如 python, 仍然有 pyinstaller, p2exe 等等的解法可以打包成 binary    具體的作法有兩個選擇     將 source code 包進去 image, 並在 docker build 的時候生出 binary   在你的本地端直接 compile，單純的包 binary 進去 image   這兩種，無非都是很爛的選擇     第一種作法你還是會把 source code 包進去，對於最小化 docker image 可以說是一點幫助都沒有   這種作法看似沒問題，但是每個人的環境都不同，可能包進去雜七雜八的東西，可行性也不高   可行的作法是，一樣把 source code 塞進去，生成 binary 之後, 再把 source 拔掉就好     這裡說的不是單純的 rm -rf application/ (這樣還是沒用，可參考 Minimized Layer)  而是透過 Multi-stage Build 達成    我手上有一個數據，公司的 CI/CD pipeline 把整包 source code 一起包到 image 裡面  我把它改成只包 binary 的情況下，image 大小縮小的大約 5176%(從 2 GB 多直接縮減到 45.7 MB)     在 ambersun1234/blog-labs/minimized-docker-image-lab 裡面我提供了一個簡單的 echo API 實作  把 source code 包進去的跟只包 binary 的他們的差別如下                          with source code       binary only                       Implementation       Multi-stage Build       Non-optimized                 Size       1.18 GB       17.6 MB                 Layer       10       2           Decouple Applications  將 application 分成多個 container 也有助於降低 image size  同時，如果有需要做擴展，分開也有助於 scale   .dockerignore  跟 .gitignore 一樣，docker 也有 .dockerignore   ignore 檔裡面的規則最好是包含那些可以被自動生成的  像是 執行檔，暫存檔 等等的  將這些放到 image 裡面是完全沒有任何幫助的  因此將這些規則一併寫入 ignore file 也有助於縮小 image   Minimize Layer  Docker 是由多層 layer 所組成  每一筆 command(RUN, COPY, ADD) 都會增加一層 layer  進而導致佔用的空間變得更大   因此，試著減少 layer 層數，也有助於縮小 image size   有關 docker container layer 的部份，我把它獨立出來一篇  詳細可以參考 Container 技術 - 理解 Docker Container | Shawn Hsu   Multi-stage Build  multi stage build 是終極大招，它很好的解決了上述 multiple layer 以及 application binary 的問題  具體來說，他是在 build time 建構多個 stage image, 每個 stage 都自上一個 stage 拿取需要的東西  直接看一個例子吧  1 2 3 4 5 6 7 8 9 10 FROM golang:1.19 AS builder_stage WORKDIR / COPY ./echo ./ RUN go mod download RUN CGO_ENABLED=0 go build -o server  FROM alpine:latest AS final_stage WORKDIR / COPY --from=builder_stage /server . CMD [\"/server\"]   可以清楚的看到，這裡用了兩個 FROM, 代表這個 dockerfile 用了兩個 stage 下去操作  第一部份 builder_stage 主要是作 compile 的部份  而重點來囉，第二部份 final_stage 它從 builder_stage 直接複製 echo server 的 binary 資料！  所以 multi-stage build 的重點就在這，它可以只留下最重要的部份，不只 source code 我連第三方 package 的依賴都可以拔掉   讓我們來檢查一下他的 layer 吧   1 2 3 4 5 $ docker image inspect --format \"{{json .RootFS.Layers}}\" echo-multi-stage-optimized [   \"sha256:ded7a220bb058e28ee3254fbba04ca90b679070424424761a53a043b93b612bf\",   \"sha256:64394d25bf3e8126f87c418023130624fc841dfe79f3c88a6d20232196f7bad6\" ]  可以看到它的確只有兩層(FROM alpine:latest 以及 COPY --from=builder_layer /server .)  一樣照慣例，看一下第一層的 layer hash 是不是 alpine 的  1 2 3 4 5 6 7 8 9 10 11 $ docker inspect alpine:latest         xxx          \"RootFS\": {             \"Type\": \"layers\",             \"Layers\": [                 \"sha256:ded7a220bb058e28ee3254fbba04ca90b679070424424761a53a043b93b612bf\"             ]         },          xxx   實驗程式碼可以參考 ambersun1234/blog-labs/minimized-docker-image-lab   exec: no such file or directory  在執行 Multi-stage Build 的時候需要特別注意一件事情(一般的 dockerfile 也需要注意就是)  每一種 base image(e.g. alpine) 它底層提供的 standard library runtime 都不盡相同  因此，你可以會遇到很奇怪的錯誤  例如 exec /app/server: no such file or directory   明明你有正確的檔案路徑，但是 docker 怎麼樣都是啟動不了的狀態  而系統一直跟你說找不到檔案，實際上的問題其實是底層 runtime 不同的問題  你卻花了不少的時間在調試，尋找看似遺失的檔案   像是我在打包 golang 的小程式的時候，在 build 裡面我忘記指定 CGO_ENABLED=0  所以我在 build 的時候就會出現 exec /app/server: no such file or directory   這種小細節，在平常打包的時候要特別小心就是   References     Docker Image Size - Does It Matter?   Multi-stage builds   Best practices for writing Dockerfiles   About storage drivers   Multi-stage builds   When using CGO_ENABLED is must and what happens   exec /app/backendApp: no such file or directory in docker container  ","categories": ["container"],
        "tags": ["docker","linux"],
        "url": "/container/container-minimized-image/",
        "teaser": null
      },{
        "title": "資料庫 - SQL N + 1 問題",
        "excerpt":"Introduction to SQL N + 1 Problem  在使用 ORM 套件下，開發程式的過程中 你可能會不小心踩到所謂的 SQL N + 1 問題  假設你在開發一個社群網站 使用者可以發佈文章  現在你要實作一個功能 是要撈出所有符合條件的文章以及作者資訊(假設你想知道點讚數超過 10 的所有文章)  直覺來寫就會變成  1 2 3 4 5 6 7 data := make([]PostInfo, 0)  // find all post which it's likes count is greater than 10 for _, post := range posts {     // find author information via post foreign key     data = append(data, ...) }   沒毛病 嗎？  很合理跑起來也沒問題 東西都是正確的  但是效能上會影響很大   在 query 文章的過程中，是不是用 left join 就可以連作者的資訊都一併撈出了呢？  所以上面的作法實際上可以僅僅使用一條 SQL 語句就可以返回全部結果了  而上述的作法被稱為是 N + 1 問題 其中     1 :arrow_right: 找出有多少資料符合特定條件   N :arrow_right: 根據撈出的結果，再一個一個 query 關聯資料     這的 N + 1 問題概念很簡單，理解起來也沒啥難度  但是呢 好奇如我就想知道，到底對效能影響多大  所以我們就來實驗看看吧   Experiment  實驗基本上也相對簡單，準備的 API server 分別測量使用 N + 1 以及 JOIN 他們的速度差別   Environment  1 2 3 4 5 6 7 8 $ uname -a Linux station 5.16.0-051600-generic #202201092355 SMP PREEMPT Mon Jan 10 00:21:11 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux  $ go version go version go1.19.3 linux/amd64  $ mysql --version mysql  Ver 8.0.31 for Linux on x86_64 (MySQL Community Server - GPL)   Implementation  N + 1  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 func n1(db *gorm.DB) error {     posts := make([]*post.Post, 0)      if err := db.Model(&amp;post.Post{}).         Where(&amp;post.Post{LikesCount: 10}).         Select(\"*\").         Find(&amp;posts).Error; err != nil {         return err     }      for _, post := range posts {         var author user.User         if err := db.Model(&amp;user.User{}).             Where(&amp;user.User{ID: uint(post.UserID)}).             Select(\"*\").             Take(&amp;author).Error; err != nil {             return err         }     }      return nil }  N + 1 的實作當中  可以看到是由第一個 query 先篩選出按讚數等於 10 的文章(這裡的條件用啥都行，只要結果是一個 range 就行)  之後再由一個 for loop 個別 query 出相對應的作者資訊   JOIN  1 2 3 4 5 6 7 8 9 10 11 12 func optimize(db *gorm.DB) error {     posts := make([]*Data, 0)     if err := db.Model(&amp;post.Post{}).         Joins(\"LEFT JOIN user ON post.user_id = user.id\").         Where(&amp;post.Post{LikesCount: 10}).         Select(\"user.*, post.*\").         Find(&amp;posts).Error; err != nil {         return err     }      return nil }  用 left join 的實作明顯簡單許多，單純的撈一次資料即可得出所有結果     上述你可以看到，我並沒有將它組成一個完整的資料回傳  這麼做的原因也相對單純，因為我只想知道 query 總時長，對於組 response 這件事很明顯我們並不關心      話雖如此，這兩個 function 所 query 出的資料保證是相等的  畢竟撈出來的資料相同才有比較的基準      至於 benchmark 的部份就相對簡單  POST /init API 可以客製化初始資料庫資料數量  POST /benchmark API 就是單純的跑 benchmark(可以設定要測幾次)   結果如下    可以看到當資料庫的資料有 100 筆的情況下  就有不小的差距了，大約是 $2 \\times 10^7$     1000 筆的資料下就更慢了 大約是 $2 \\times 10^8$  當資料多了 10 倍後，效能也直接慢了 10 倍     10000 筆大約是 $3 \\times 10^9$  也是一樣慢了 10 倍   可見，使用了 N + 1 的寫法，當資料越多的情況下，他的速度是成倍數下降的   詳細的測試程式碼可以參考 ambersun1234/blog-labs/sql-n1-benchmark   Different Ways to Solve N + 1 Issue  PostgreSQL Temporary Table  上面我們提到的 N + 1 狀況屬於典型的新手錯誤  但是有時候你遇到的狀況，可能是必須要使用 N + 1 才可以解決的  什麼意思呢   比如說你的資料本身是放在兩個或以上的獨立資料庫  跨資料庫，當然是沒辦法做到 LEFT JOIN 這種東西的  那你可能就會使用 N + 1 query 去做了   不過聰明的開發者們提出了一個解法  既然它是因為跨資料庫受限，那我能不能把它塞到同一個資料庫裡面呢？   我們一樣用 user 跟 post 舉例  假設他們是在不同的資料庫上面  然後你得到一個需求是說 提供一隻熱門創作者的 API, 它會根據使用者文章的按讚數量排名  所以你要找出每一個 user 它所有文章的按讚數，在進行排名   PostgreSQL 中有一個東西叫做 Temporary Table  他的用處呢，就是建立一個暫存的資料表  其生命週期僅存在於當前 session 當中(亦即當 session close 的時候 temporary table 就會被刪除)      當 temporary table 與實際的 table 撞名的時候  會優先選擇 temporary table    基本的語法是  1 2 3 CREATE TEMPORARY TABLE (     userId int )  其實就跟一般 SQL 建立 table 一樣   那你會問說 temporary table 為什麼能解決 N + 1 問題？  因為我們可以把 user 的資料塞到 temporary table 裡面  這樣 user 跟 post 是不是就在同一個資料庫裡面，就可以用一個 JOIN query 解決了？   SQL Query Optimization  如果 Temporary Table 不適用你的狀況  其實也可以簡化成最多 2 個 query 的寫法   將 post 撈出來的時候，在 application layer 將 user_id 塞到一個陣列裡面當作參數  然後第二個 query 使用 IN 語法撈出所有相關 user 的資料   無論你的資料在何處，這種寫法都可以適用  唯一的缺點是要在 application layer 整理資料，相對來說比較麻煩  不過仍然可以節省 query 次數，降低資料庫的負擔，進而提昇效能   References     [科普文]什么是ORM中的N+1   [Day 15] 效能殺手 N+1 Query  ","categories": ["database"],
        "tags": ["orm","sql","n+1","temp table","temporary table"],
        "url": "/database/database-sqln1/",
        "teaser": null
      },{
        "title": "資料庫 - Index 與 Histogram 篇",
        "excerpt":"Speed up Search In Large Data        ref: Making Book    還記得小時候學習的過程中，一定會用到字典這個東西  在沒有網路的時代，字典可以說是查找單字以及成語的少數方法之一了  字典通常都很大本對吧 光是要從中找到目標資訊就很不容易了  所以字典通常都會有 部首查字, 筆劃查字 等的方法  而且都會用不同顏色標注在書的側面，讓你可以很快的就縮短查找範圍   這其實就是索引的一種  而資料庫說白了也就是一個很大的字典  因此接下來我們將探討 Index 在資料庫中的各種知識   What is Index  所以 Index 總的來說就是加速資料查詢的時間的一個方法  不同的是，Index 是透過拿取一個或多個 column data 建立另一個 table  由於這個 table 僅有存放少數 column data 加上它會自動幫你排序  搜尋的時候，因為已經排序過了，所以是用 binary search/tree traversal 的方式，也因此他的查詢會變得相對簡單且快速   Composite Index  有 single index 就有 composite index  顧名思義，他是由多個 column 組成的 index   需要注意的是，composite index 的使用條件相對嚴苛  假設你建立了一個 index (a, b, c)  index 的使用 必須要照順序  以下這些是 可以使用到 composite index 的  1 2 3 4 WHERE a=1 WHERE a&gt;=12 AND a&lt;15 WHERE a=1 AND b &lt; 5 WHERE a=1 AND b = 17 AND c &gt;= 40   以下是 沒辦法使用到 composite index 的  1 2 3 WHERE b=10 WHERE c=221 WHERE b&gt;=12 AND c=15   每一家資料庫的實作都不盡相同  就好比如說     IBM Informix 12.10 規定的是            query condition 第一個條件必須是 equality filter(只能用 =)       之後的條件必須是 range expressions(只能用 &gt;, &lt;, &gt;=, &lt;=)                  ref: Use composite indexes            MySQL 8.0 規定只要是            query 的條件順序是 composite index 的 prefix 就行(i.e. (a, b) is prefix of (a, b, c), (b, c) is not a prefix of (a, b, c))                  ref: 8.3.6 Multiple-Column Indexes            另外，在定義 composite index 的時候  遵循著一個原則，low cardinality 在前，high cardinality 在後  這樣的設計有助於再次提高效能      cardinality 是一個數值，用以表示資料的重複性，數字越大，重複性越小    總的來說，共通點都是  他們的 查詢順序要符合定義的順序  這樣才吃的到 index      注意到如果你使用 composite index 的時候沒有依照建構順序  在你嘗試使用 explain 去看 execution plan 也不見得會顯現出來差別  但實務上他的執行速度還是有差別的    Partial Index  既然 index 的使用，跟他的數值有極大的關係(i.e. cardinality)  太多重複的資料會導致資料庫甚至選擇不使用該 index 查詢  所以其實這種限制是不太方便的，因為很多時候你的查詢條件就長那樣，index 也用不到，是很可惜的  Partial Index 旨在解決這個問題      注意到 Partial Index 並非 SQL Standard 的一部分    原始問題是，index 內部儲存 “整張表” 的 column data  如果我可以調整儲存的資料內容，只保留部份資料，使得 cardinality 提高，是不是就可以讓 index 活過來，繼續被使用？  除此之外，因為我只 index 部份資料，所以佔用空間更小  並且資料更新時，也可以加速，因為只有部份需要更新   部份資料的作法，就是根據 query condition 來決定 index 的儲存內容  假設，我有 user 以及 post 兩個 table  我有一個 query 是這樣的   1 2 select * from post  where user_id = uid and visible = true   其實你可以針對 visible 建立一個 partial index  1 CREATE INDEX idx_post_visible ON post (visible) WHERE visible = true;   visible 欄位是 boolean, 所以重複的數值會很多(因為就兩種)  如果 true 的數量是相對於 false 小很多，這時候 partial index 的效益就會非常明顯   使用 partial index 你也可以直接過濾掉 “不感興趣” 的資料  如果只有單純使用 uid 的 index，visible = false 的資料也會被查詢到  index 撈出來的資料數量越來越小(或者說越接近你要的最終結果)，越可以加速查詢速度     使用 index 的好壞處，可以簡單的寫成以下                  Pros       Cons                       能夠更快速的讀取資料庫中的資料       因為 index 的建立是仰賴 空間換時間 並且 index 的 table 是保持著排序好的狀態 因此每一次的更新，除了更改原本 table 資料外，index table 也需要做同步的更新 所以可能會有較高的 overhead           Type of Index  Clustered Index  clustered index 是用於決定資料在 實際硬體上的儲存順序  通常 clustered index 在 table 建立之初就會決定好了   clustered index 儲存的資料是 key-pair 的方式 index column value :arrow_right: row data(如下圖)  也就是說，透過 clustered index 進行 query 不用作二次 look up 可以大幅度的縮短時間     換言之，沒有用到 clustered index 的資料就會是 Table Scan         ref: Clustered Index       需要注意的是，並不是每一種 DBMS 都使用 B+ Tree    如果對 clustered index 進行更新刪除會發生什麼事情  well 由於 clustered index 的順序是直接對應到 physical 儲存順序的  因此，如果你恰好更改了 index column 的數值，後果就是 DBMS 需要額外花力氣移動硬碟上的資料 進而導致效能減損  所以，使用 clustered index 的情況只對 SELECT 有幫助   MySQL 對於 clustered index 的建立，在官方文件中有說明到 MySQL 8.0 15.6.2.1 Clustered and Secondary Indexes      When you define a PRIMARY KEY on a table, InnoDB uses it as the clustered index.   A primary key should be defined for each table. If there is no logical unique and non-null column or set of columns to use a the primary key, add an auto-increment column.   Auto-increment column values are unique and are added automatically as new rows are inserted.     If you do not define a PRIMARY KEY for a table, InnoDB uses the first UNIQUE index with all key columns defined as NOT NULL as the clustered index.     If a table has no PRIMARY KEY or suitable UNIQUE index,   InnoDB generates a hidden clustered index named GEN_CLUST_INDEX on a synthetic column that contains row ID values.   The rows are ordered by the row ID that InnoDB assigns.   The row ID is a 6-byte field that increases monotonically as new rows are inserted. Thus, the rows ordered by the row ID are physically in order of insertion.    簡言之     有 primary key :arrow_right: clustered index = primary key   沒 primary key :arrow_right: 找第一個 unique field 當 clustered index   啥都沒有 :arrow_right: 我自己幫你偷偷建立一個   Covering Index(Index with Included Columns)  與其完全不儲存資料，Covering Index 僅儲存 部份 的 column data  算是一種介於 Clustered Index 與 Non-clustered Index 之間的解決方案   Non-clustered Index  就是我們常見自己加的 index  1 ALTER TABLE `table` ADD INDEX `product_id_index` (`product_id`)   與 clustered index 不同的是，non-clustered index 並不會儲存 row data, 他是儲存一個 pointer to original data(i.e. reference)  也因此在 query 的時候，使用 non-clustered index 會需要做 二次 look up(相對 clustered index 會慢一點)  更重要的是，non-clustered index 不一定是 unique 的        ref: Clustered Index    non-clustered index 又稱 secondary index     但 clustered index 不等於 primary index 請特別注意                     Index Type       Description                       Primary Index       不完全等於 Clustered Index                 Secondary Index       等於 Non-clustered index                 Bitmap Index       針對特定欄位做個 bitmap 用性別舉例, 男的 1 女的是 0 這時候如果要對性別 query, DBMS 可以很快的 apply bitmap 上去取得結果(bitwise operation)                 Dense Index       儲存所有 record 的 pair(1 對 1)                 Sparse Index       儲存 部份 record 的 block 起點(每個 block 有若干 record，亦即1 對 多) 找到該 block 後 sequential 尋找目標 record                 Reverse Index       將 key 反過來存(i.e. 24538 :arrow_right: 83542) 這樣可以減緩 leaf block contention, 因為原本緊鄰的 data, primary key 反轉之後位置會差很多 (24538 :arrow_right: 83542, 24539 :arrow_right: 93542)                 Inverted Index       多用於搜尋功能，其資料結構為 hashmap, key 為 content, value 為位置 比如說 cat 一詞出現在 document 1, 4, 22, 103 頁           Index Implementations  Hash Index  Hash index 顧名思義是用 hash 來達成的  用 hash 的好處之一就是快速，只要算一下就可以馬上定位到資料 $O(1)$      需要注意像是 wildcard query(e.g. WHERE username LIKE ‘john%’) 這種 hash index 也沒辦法處理就是    但同時缺點也很明顯，針對範圍查詢，效率也會很低  更甚至如果找不到資料的話就可能會是 Full Table Scan 了      它不全然會是 full table scan 的原因是，你的 query 可能剛好有其他 index 可以用，不需要 table scan    另一個問題是 hash collision  它取決於你的 hash function 怎麼設計，即使是 sha256 也有 $4.3 \\times 10^{-60}$ 的機率會碰撞到      出現 hash collision 有兩種解法  Linear Probing  Double Hashing    並且使用 hash index 對於 disk read-ahead 沒有幫助  因為下一頁的資料不一定在你的旁邊  因此這種操作屬於 Random I/O      詳細可以參考 資料庫 - 初探分散式資料庫 | Shawn Hsu    SSTable(LSM Tree)  另一種 index 實作的方式是稱為 SSTable(Sorted String Table)  亦即將 key 進行排序  排序過後的好處就是說，我能夠花較少的時間 query 到我想要的資料  比如說使用 binary search   為了避免日後 Random I/O 造成的效能瓶頸  SSTable 會先在記憶體中維護一個資料結構(e.g. AVL Tree, Red-Black Tree)  等到寫入超過一定的量之後，在存到硬碟裡面(segment file)  不同的 segment file 可能會包含相同 key 的 entry  所以也必須適時的進行 合併與壓縮的操作  又因為說 segment file 的內容是排序過的，所以合併的操作效率也不會太慢      符合 合併與壓縮 的儲存引擎，通稱 LSM Tree(Log-Structured Merge-Tree)  要注意的是，合併壓縮的操作會在某種程度上影響到效能，因為他是開一條 thread 下去處理，但同時你還在持續 serve client    另外 SSTable 也有所謂的 log file，避免你在寫入 disk 的時候意外中斷，造成資料遺失      作法跟 Redis 的 RDB, AOF 一樣，可參考 資料庫 - Cache Strategies 與常見的 Solutions | Shawn Hsu    重點來了  因為可能你有很多個 segment file  所以當你要查詢的時候，有可能會 miss 掉(i.e. 該 key 不存在於該 segment file 當中)  因此必須要往前一份資料找  你可以配合一些其他的工具避免此類的狀況(e.g. Bloom Filter)      Bloom Filter 相關的介紹可以參考 資料庫 - 機率型資料結構 Bloom Filter 在 Cache 中的應用 | Shawn Hsu    B Tree Index       ref: How Database B-Tree Indexing Works    B Tree 是一種自平衡的樹狀資料結構，也就是從 root 到每一個節點，所花費的時間最多不超過 $O(Log(n))$  SSTable 最差的情況會是 $O(n)$，相比 B Tree，它可以在最短時間內找到你要的資料   B Tree 將資料以 page 為單位分開，大小通常為 4KB(MySQL 的分頁大小為 16KB)  每個 page 對應樹狀結構中的 node，且每個 node 都包含有資料以及 child node 的 pointer   B Tree 的所有操作都是基於 page 的修改  新增，刪除都會重寫整個 page 的資料  但是當 page 剩餘空間不夠的時候，就會遇到 Fragmentation 的問題了  那，這個例子是屬於哪一種的斷裂？ 是 External Fragmentation   B+ Tree Index       ref: How Database B-Tree Indexing Works    B Tree 在 sequential scan 的情況下，必須來回 parent/child node 之間  而 B+ Tree 則是在園有的基礎上做了些改進      B+ Tree 僅有在 leaf node 儲存資料   leaf node 之間都會用 pointer 互相連接(linked list)   這樣的作法有助於 提昇 locality(資料庫多半會做 pre-load(i.e. disk read-ahead) 增進效能)  並且在 full table scan 下的效能海放 B Tree(因為後者必須執行 tree traversal 可能會 cache miss, 前者可以依靠連接的 pointer)   Fragmentation  Internal Fragmentation  內部斷裂指的是 當一個 process 被分配到的空間太大，導致說有部份的空間被浪費(i.e. 沒有被使用到)  且該未使用的空間太小不足以再塞入其他 process   這通常是因為每次分配的空間大小都一樣所導致的  解決辦法就是針對不同大小的 process 分配相對應的記憶體大小(動態大小記憶體分配)      External Fragmentation  外部斷裂指的是 剩餘記憶體空間足夠, 但單個記憶體空間不夠大   這通常是因為採用了動態大小的記憶體配置  解決辦法可以使用 磁碟重組 或者是使用 paging 的機制        檔案系統的 Fragmentation 會導致資料不連續  如果是使用傳統硬碟(HDD)，在讀寫資料會因為資料不連續的問題，必須額外的做 seek 的動作  進而造成效能瓶頸   Predicates  predicates 基本上分為     Access Predicates   Filter Predicates   Access Predicates  access predicates 指的是當 query 根據 condition 撈出相對應的資訊      如果 condition 用的欄位剛好有 index, 那就稱為 index access predicates(反之則是 table-level access predicates)    index access predicates 可以直接將資料限縮到某個區間範圍  比如說  1 SELECT * FROM customers WHERE last_name = 'John'  如此一來，我就能夠限縮上述的 sql query 到某個特定範圍了  由於 B+ Tree 的特性，資料都是儲存在 leaf nodes 上  藉由給定的條件，我可以輕鬆的減少資料範圍   Filter Predicates  filter predicates 也是根據 condition 撈資料  不同的是它只會用在 leaf node traversal 上面而已  也就是說當我在特定區間(由 access predicates 限縮後的)上找資料的時候，我會一個一個檢查他有沒有符合剩下的條件  (透過 B+ Tree leaf node 上的 linked list 達成 traversal)   所以 filter predicates 不會限縮資料區間範圍，它會在這個範圍內做 filter 的動作      如果 condition 用的欄位剛好有 index, 那就稱為 index filter predicates(反之則是 table-level filter predicates)    注意到並不是所有 multiple condition 都稱為 filter predicates  有可能你做 composite index 然後所有 query condition 都是使用 composite index 的欄位  那它本質上還是屬於 access predicates   Scans  Index Only Scan  1 2 SELECT * FROM customers WHERE id = 1 // assume id is primary key   這種情況就是 index only scan  因為上面說過，clustered index 上面是會儲存所有 table 資料的  然後你都用 clustered index 下去 query 了  並且撈出來的資料全部都在 table 上面(因為它就是原本放資料的 table)   所以這種情況是最佳的，只會做一次 look up(data table)   Index Scan  index scan 指的是 DBMS 會一個一個看過所有 index, 選出符合條件的 row  舉例來說，   1 SELECT * FROM customers WHERE last_name = 'John'   你說如果 last_name 有 index，那它還會一個一個看過嗎？  hmm 這其實是根據 DBMS 自己決定的  可能會左右他的決定的有以下情況     查詢條件   Index 的 cardinality   Table Size   Index Size   並不是說你有加了 index 它就一定會用  視情況不同 DBMS 會自己斟酌   Index Seek  index seek 就是完美的吃到了 index 的狀況  舉例來說，   1 2 SELECT * FROM customers WHERE id = 1 // assume id is secondary index   如果是 B+ Tree 它就能以最短時間查詢到對應的 row(複雜度 $O(Log(n))$)  由於 index table 並不包含全部 row data  所以接下來在找到它原本的 data 就完成了  總共是 2 次的 table look up(index table + data table)   如果你剛好用到 clustered index  那它就會變成 Index Only Scan   也要切記一件事情  並不是所有 case 都可以用到 index(即使你的 query 有用到)  影響條件如下     查詢條件   Index 的 cardinality   Table Size   Index Size   有的資料庫並沒有所謂的 index seek 的概念，如 PostgreSQL     There are different types of scan nodes for different table access methods:  sequential scans, index scans, and bitmap index scans.  ref: 14.1. Using EXPLAIN    這時候其實 index seek 的概念就是 Index Scan   Table Scan  Full Table Scan 指的是，資料庫引擎必須讀取 table 內的所有資料  如果說 table 資料量小，可能不會有太大的影響，但如果資料量大就另當別論了   1 SELECT * FROM customers WHERE last_name LIKE '%John%'  這種情況是百分之百的 full table scan  因為 index 的建立沒辦法判斷這種 wildcard 的情況   但是！  1 SELECT * FROM customers WHERE last_name LIKE 'John%'  這種情況就吃的到 index 所以它並不是 full table scan  原因在於 index 的建立是從左到右的  至少這個 case 它可以匹配前 4 個字母   1 SELECT * FROM customers WHERE last_name LIKE 'Jo%hn%'  wildcard 在中間的情況也不會是 full table scan  因為它也至少可以匹配前 2 個字母   要記住一件事情，對於電腦來說，I/O 是一件很花時間的事情(相比 register, cache, memory 來說)  因此我們必須要盡力避免過多的讀寫操作   資料庫查詢會慢，很大的原因是因為 Full Table Scan 的關係   When will Database do a Full Table Scan  那麼哪些會造成 Full Table Scan?  原因多半有以下      當你的資料量小於 10 筆的時候            Full Table Scan 的速度都比你去查 index 還要來的快       因為 index table 跟資料是分開存的, 等於要看兩次           當你的查詢條件(e.g. WHERE, ON) 沒有包含 index 的時候   當你用 WHERE 篩出來的 row(e.g. WHERE indexed_col = 1)包含了 很大部份的 table 資料(e.g. 90%)            那麼與其用 where clause 慢慢篩出來不如直接 Full Table Scan 然後再挑出你要的資料           當你的 index 屬於 low cardinality 的時候            cardinality 是一個數值，用以表示資料的重複性，數字越大，重複性越小       low cardinality 代表你的 index 內包含很多重複性資料，也就是你的 index 並不能指定到唯一的資料       那麼它還要經過更多次的 key lookup 可能才能找到你要的資料，那乾脆就直接 Full Table Scan           Histogram     to be continued    References     資料密集型應用系統設計(ISBN: 978-986-502-835-0)   SQL筆記：Index Scan vs Index Seek   Optimizing MySQL LIKE ‘%string%’ queries in innoDB   Performance of LIKE queries on multmillion row tables, MySQL   一文读懂MySQL 8.0直方图   Database index   Use composite indexes   8.3.6 Multiple-Column Indexes   Single vs Composite Indexes in Relational Databases   What is the point of reverse indexing?   3 Indexes and Index-Organized Tables   Indexing in DBMS: What is, Types of Indexes with EXAMPLES   What is difference between primary index and secondary index exactly? [duplicate]   30-12 之資料庫層的核心 - MySQL 的索引實現   淺談 InnoDB 的 Cluster Index 和 Secondary Index   What is the difference between Mysql InnoDB B+ tree index and hash index? Why does MongoDB use B-tree?   PostgreSQL B-Tree Index Explained - PART 1   【 翻譯 】How Database B-tree Indexing Works   Does MySQL use only one index per query/sub-query?   Distinguishing Access and Filter-Predicates   Index Seek和Index Scan的区别以及适用情况   What are the differences between B trees and B+ trees?   Difference between Internal and External fragmentation   Partial Indexes  ","categories": ["database"],
        "tags": ["index","histogram","partial index","secondary index","clustered index","non-clustered index","hash index","b-tree index","b+ tree index","index seek","index scan","table scan","full table scan","low cardinality","high cardinality","cardinality","composite index","bitmap index","dense index","sparse index","reverse index","inverted index","full table scan","sstable","lsm tree","avl tree","red-black tree","linear probing","double hashing","fragmentation","external fragmentation","internal fragmentation","locality","predicates","access predicates","filter predicates","index only scan","explain","execution plan","histogram"],
        "url": "/database/database-index-histogram/",
        "teaser": null
      },{
        "title": "Goroutine 與 Golang Runtime Scheduler",
        "excerpt":"Process, Thread and Coroutine  Process  Process 是跑起來的 Program, 它擁有自己的 memory space, system resources 以及 system state  在系統開機之初，init process(pid 1) 被建立之後，就可以透過 fork 的方式建立新的 process  具體來說，一個 process 擁有以下的資料     text :arrow_right: code   data :arrow_right: global or static variables   stack :arrow_right: local variables   heap :arrow_right: dynamic allocate memory        Processes    從上述你可以看到，process 本身的資料相當的多  也因此在建立，刪除上，相比其他如 thread 還要來的負擔更大   現代作業系統為了提高 degree of multiprogramming  process 會需要進行 context switch  context switch 實際上是 store 以及 restore 的過程(把目前進度儲存起來)，由於 process 本身是非常龐大的，也因此 process 在 context switch 下也是非常耗費資源的   Thread  thread 又稱為 LightWeight Process(LWP), 是 process 的組成最小單位  一個 process 可以擁有多個 thread, 但至少會有一條 main thread(kernel-level thread)   由於 thread 與 process 本身除了 stack, program counter 以及 register 是獨立擁有的之外，其餘的皆是共用  因此，它能夠快速的建立並刪除，使得相比 process 而言，他的建立，刪除以及 context switch 的成本都相對較低(相對 process)   Thread Model                  One to One       Many to One       Many to Many                                                   ref: Multi Threading Models in Process Management    User-level Thread  user-level thread 顧名思義是跑在 user space 上的 thread, 它可能是由 library 本身提供的  kernel 對 user-level thread 的存在是不知情的，也因此 user-level thread 的建立，排程，刪除皆由 library 控制   Kernel-level Thread  kernel-level thread 是由 kernel 所管理的，每一條 kernel thread 都會被 assign 到一個 cpu 實體核心上面執行  kernel scheduler 所排程的東西，是 kernel-level thread, 並不包含 user-level thread      對於 linux 來說，scheduler 處理的東西叫做 task  task 可以是 process, kernel-level thread      thread 本身可以增加效能，即使它只有一條 kernel-level thread 在實際工作  假設我有兩條 user-level thread A 跟 B 對到 一條 kernel-level thread C(也就是 Many to One 的架構)  thread A 正在執行一個任務，突然它需要進行 I/O  這時候 thread A 必須進行等待，這時候它就可以將操作權限交給 thread B 執行其他工作了  以 process 的角度來看，它並沒有浪費任何 CPU time, 也就是 process 不會 idle 浪費效能   更遑論如果 process 是處於 Many to Many 的架構下  因為 kernel-level thread 會分別對應到實體核心  這樣就是真正的多工了   Coroutine(Fiber, Green Threads)  基本上 coroutine 共享的資料與 thread 無異，主要的差異是在  coroutine 是採用 cooperatively scheduled 跟 process 還有 thread 的 preemptively scheduled 是不一樣的   cooperatively schedule 是 programmer 或語言實作決定何時要讓出 CPU time(user space context switch)      根據現有的資料，有的說 coroutine 共享的資料與 thread 一致  有的則說 coroutine 擁有自己的 stack  目前我並沒有找到一個完美的結論或證明    由於 coroutine 基本上都是在 user-space, kernel 對此可謂是毫不知情  亦即 coroutine 的排程是 不會被 kernel scheduler 影響的，而前面提到的 cooperatively scheduled 則是你可以自己管控何時要進行 context switch(這裡指的是語言實作自己的排程，而非 kernel scheduler)  這樣的好處是，你不會因為做事情做到一半就突然 timeout 而被 kernel swap out  壞處是，由於讓出 CPU time 這件事情必須是 主動且願意, 要是其中一個 coroutine 不願意 release CPU 那就會導致 starving 的問題      藉由 Yield 的行為主動讓出 CPU time    那 coroutine 相比 thread 來說，能提昇效能嗎？  hmm 效果不大  既然 coroutine 完全共享 thread 本身的資料(或是部份共享，fiber 擁有自己的 stack)，亦即他在 context switch(application code 執行) 的時候，fiber 是比較輕量的  也因為它完全共享的特性，因此建立 coroutine 的成本又比 thread 還低  既然如此那為什麼我說他的效果不大  原因是 fiber 是建立在同一條 thread 之上(也在同一條 thread 上做切換)，因此 coroutine 是沒有辦法拿到更多的 cpu time 的   Concurrency vs. Parallelism  可參考 關於 Python 你該知道的那些事 - GIL(Global Interpreter Lock) | Shawn Hsu - Concurrency vs. Parallelism   Introduction to Goroutine  根據 Effective Go 裡面所描述      A goroutine has a simple model:   it is a function executing concurrently with other goroutines in the same address space.   It is lightweight, costing little more than the allocation of stack space.   And the stacks start small, so they are cheap, and grow by allocating (and freeing) heap storage as required.     Goroutines are multiplexed onto multiple OS threads so if one should block,   such as while waiting for I/O, others continue to run.   Their design hides many of the complexities of thread creation and management.    基本上我們可以肯定它不是 process，但 goroutine 的本質到底是啥呢     goroutine 的排程是由 Golang Runtime Scheduler 決定的 :arrow_right: 他是 coroutine   goroutine 擁有自己獨立的 stack 而已 :arrow_right: 他是 coroutine   goroutine 直接對應到 kernel-level thread 之上 :arrow_right: 他是 user-level thread   我先不下定論，讓我們先往下看再說   GM Model  Effective Go 裡面提到 goroutine 會對應到 OS threads(kernel thread)  因此我們期待會看到兩個東西，一個 OS threads 一個 goroutine  在 golang source code 裡面，OS threads(i.e. kernel-level threads) 是以 m 來表示  其結構定義如下 runtime/runtime2.go  1 2 3 4 5 6 7 8 9 10 type m struct {     ...      g0      *g     // goroutine with scheduling stack     morebuf gobuf  // gobuf arg to morestack     divmod  uint32 // div/mod denominator for arm - known to liblink     _       uint32 // align next field to 8 bytes      ... }   goroutine 則是以 g 來表示  其結構定義如下 runtime/runtime2.go  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 type g struct {     ...      // Stack parameters.     // stack describes the actual stack memory: [stack.lo, stack.hi).     // stackguard0 is the stack pointer compared in the Go stack growth prologue.     // It is stack.lo+StackGuard normally, but can be StackPreempt to trigger a preemption.     // stackguard1 is the stack pointer compared in the C stack growth prologue.     // It is stack.lo+StackGuard on g0 and gsignal stacks.     // It is ~0 on other goroutine stacks, to trigger a call to morestackc (and crash).     stack       stack   // offset known to runtime/cgo     stackguard0 uintptr // offset known to liblink     stackguard1 uintptr // offset known to liblink      _panic    *_panic // innermost panic - offset known to liblink     _defer    *_defer // innermost defer     m         *m      // current m; offset known to arm liblink     sched     gobuf     syscallsp uintptr // if status==Gsyscall, syscallsp = sched.sp to use during gc     syscallpc uintptr // if status==Gsyscall, syscallpc = sched.pc to use during gc     stktopsp  uintptr // expected sp at top of stack, to check in traceback      ... }   根據我們先前的知識，我們可以得知  user-level thread 要跑，必須要對映到底層的 kernel-level thread  同理，coroutine 要跑，必須要對映到 user-level thread 之上  才可以拿到 cpu time   scheduler 會隨機挑選一個 goroutine 將它 map 到 kernel-level thread 之上取得 cpu time 執行      Java’s Thread Model and Golang Goroutine    GPM Model  GM Model 看起來不錯阿 對吧  但是語言開發者們發現了一些問題  根據 Scalable Go Scheduler Design Doc 所述           Single global mutex (Sched.Lock) and centralized state. The mutex protects all goroutine-related operations (creation, completion, rescheduling, etc).     Goroutine (G) hand-off (G.nextg). Worker threads (M’s) frequently hand-off runnable goroutines between each other, this may lead to increased latencies and additional overheads. Every M must be able to execute any runnable G, in particular the M that just created the G.     Per-M memory cache (M.mcache). Memory cache and other caches (stack alloc) are associated with all M’s, while they need to be associated only with M’s running Go code (an M blocked inside of syscall does not need mcache). A ratio between M’s running Go code and all M’s can be as high as 1:100. This leads to excessive resource consumption (each MCache can suck up up to 2M) and poor data locality.     Aggressive thread blocking/unblocking. In presence of syscalls worker threads are frequently blocked and unblocked. This adds a lot of overhead.         在 GM Model 的情況下，goroutine 要得到 cpu time 就必須得要依靠 scheduler 進行排程，那麼你一定會希望自己能夠早點被執行，所以多個 goroutine 會為了 scheduler 而 互相競爭, 爭取到 scheduler 替他們排程的機會，也就導致說 scheduler 的 mutex lock 會一直被爭奪   在只有 g 跟 m 的架構下，頻繁的切換會影響效能，講白話文就是頻繁的 store/restore   m 上面的 cache 只有需要存放跟當前 goroutine code 相關的資料就好，存放一些跟執行無關的 data 會導致 poor data(cache) locality(剛剛用到的東西有很大的機率會繼續用，塞太多不需要的東西會一直 cache miss 效能會差)   當遇到 blocking I/O 的時候，必須要等待嘛，既然要等待，我是不是就切到另一條 thread 繼續執行就行了(當然你 goroutine 要切換, kernel-level thread 也要)，但這樣頻繁的切換 store/restore 會影響效能   也因此，語言開發者們決定在中間多加一層 p(process)  架構會變成如下      Java’s Thread Model and Golang Goroutine    p 代表著需要執行 goroutine 的必要 resource  p(process) 的架構，大概會依照以下這個下去實作  1 2 3 4 5 6 7 8 9 10 11 12 13 14 struct P {     Lock;     G *gfree; // freelist, moved from sched     G *ghead; // runnable, moved from sched     G *gtail;     MCache *mcache; // moved from M     FixAlloc *stackalloc; // moved from M     uint64 ncgocall;     GCStats gcstats;     // etc     ... };   最新 p(process) 的 structure 定義如下  runtime/runtime2.go  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 type p struct {     ...      mcache      *mcache     pcache      pageCache      deferpool    []*_defer // pool of available defer structs (see panic.go)     deferpoolbuf [32]*_defer      // Cache of goroutine ids, amortizes accesses to runtime·sched.goidgen.     goidcache    uint64     goidcacheend uint64      // Queue of runnable goroutines. Accessed without lock.     runqhead uint32     runqtail uint32     runq     [256]guintptr      // Available G's (status == Gdead)     gFree struct {       gList       n int32     }      sudogcache []*sudog     sudogbuf   [128]*sudog      // Cache of mspan objects from the heap.     mspancache struct {       // We need an explicit length here because this field is used       // in allocation codepaths where write barriers are not allowed,       // and eliminating the write barrier/keeping it eliminated from       // slice updates is tricky, moreso than just managing the length       // ourselves.       len int       buf [128]*mspan     }      timersLock mutex      ... }     你可以看到，在 p 裡面包含了許多的 cache，這樣就解決了上述說的第三點的問題  m 上面就不會包含太多不相關的 cache data, 就可以提升 locality   那麼，多了 p(process) 之後，整個的架構要怎麼跑呢  要執行 goroutine 必須要有 resource(也就是 p) :arrow_right: 這部份是我們自己可以掌控的  再來就是 m 了，但這個就是單純的 thread，可以不太用理它  所以只要我們搞定 p 跟 g 就行了  最簡單的方式就是將它捆綁在一起   所以 golang 的實作方式是，將 goroutine(g) 塞到 process(p) 的 local run queue(p.gFree)裡面排隊等資源  再來就非常簡單了，我只要將 kernel-level thread(m) 發給 process(p) 執行就好了  而第一點提到的 scheduler global mutex lock 的問題就可以得到緩解(global lock 的問題被簡化成了 per-p lock)  所以整體的架構是長這樣      ref: 深度解密Go语言之 scheduler    每個 p 上面都有一個存放 g 的 local run queue(p.gFree), 然後 assign m 給 p 執行 g     你說為什麼會有多個 m 呢？ 如果 m1 要執行 blocking I/O, p 就會被 assign 新的 m    除了個別的 local run queue 之外，也有 Global Run Queue 的存在   那你說，這樣的架構哪裡減少了 overhead?  因為 resource 都被綁在 process(p) 而非 kernel-level thread(m) 上面  也因此遇到 blocking I/O 的時候，他的切換就便宜很多了阿！(你要 store/restore 的東西變少了，而且要切的對象是 m 不是 p)  也就是下面這張圖片      Java’s Thread Model and Golang Goroutine    Golang Runtime Scheduler  接下來來看看 golang runtime scheduler 怎麼執行的吧   Global Run Queue  既然每個 p 都有自己的 local run queue, 那我為什麼還需要一個全局的 run queue 呢？  我想答案其實相對簡單     local run queue 有大小限制(只能放 64 個 goroutine, 可參考 runtime/proc.go#4430), 多的會被送到 global run queue 做等待   當使用者 enable scheduling of user goroutines 的時候，scheduler 會將所有 disable run queue 裡面的 goroutine 全部排到 global run queue 裡面   當你跑太久被 swap out 的時候(可參考 Sysmon)   Work Steal  還記得前面說過得，每個 p 都會有自己的 local run queue 用以存放 runnable goroutine g  你有沒有想過一個問題，如果 p 上的 local run queue 沒有任何 g 可以執行呢？   根據 Scalable Go Scheduler Design Doc 所提到的      There is exactly GOMAXPROCS P’s.   All P’s are organized into an array, that is a requirement of work-stealing.   GOMAXPROCS change involves stop/start the world to resize array of P’s.    也就是說 p 的數量預設是 cpu 邏輯處理器 的數量  你可以透過更改 GOMAXPROCS 來限制要用多少 kernel-level thread     c.f. What if GOMAXPROCS is too large?    g 的數量是有可能不夠讓所有的 p 執行的  p 沒工作就讓它休息 嗎？ NoNoNo  golang scheduler 會幫你找到工作的  如果別的 p 上面有很多的任務要執行，那麼它就會執行所謂的 work steal(可參考 runtime/proc.go#3038), 將 一半的工作 拿過來幫你分擔(可參考 runtime/proc.go#6195)  work steal 的對象 只能是別的 p 上面的 local run queue  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 func stealWork(now int64) (gp *g, inheritTime bool, rnow, pollUntil int64, newWork bool) {     pp := getg().m.p.ptr()      ranTimer := false      const stealTries = 4     for i := 0; i &lt; stealTries; i++ {         stealTimersOrRunNextG := i == stealTries-1          for enum := stealOrder.start(fastrand()); !enum.done(); enum.next() {             if sched.gcwaiting.Load() {                 // GC work may be available.                 return nil, false, now, pollUntil, true             }             p2 := allp[enum.position()]             if pp == p2 {                 continue             }              // Steal timers from p2. This call to checkTimers is the only place             // where we might hold a lock on a different P's timers. We do this             // once on the last pass before checking runnext because stealing             // from the other P's runnext should be the last resort, so if there             // are timers to steal do that first.             //             // We only check timers on one of the stealing iterations because             // the time stored in now doesn't change in this loop and checking             // the timers for each P more than once with the same value of now             // is probably a waste of time.             //             // timerpMask tells us whether the P may have timers at all. If it             // can't, no need to check at all.             if stealTimersOrRunNextG &amp;&amp; timerpMask.read(enum.position()) {                 tnow, w, ran := checkTimers(p2, now)                 now = tnow                 if w != 0 &amp;&amp; (pollUntil == 0 || w &lt; pollUntil) {                     pollUntil = w                 }                 if ran {                     // Running the timers may have                     // made an arbitrary number of G's                     // ready and added them to this P's                     // local run queue. That invalidates                     // the assumption of runqsteal                     // that it always has room to add                     // stolen G's. So check now if there                     // is a local G to run.                     if gp, inheritTime := runqget(pp); gp != nil {                         return gp, inheritTime, now, pollUntil, ranTimer                     }                     ranTimer = true                 }             }              // Don't bother to attempt to steal if p2 is idle.             if !idlepMask.read(enum.position()) {                 if gp := runqsteal(pp, p2, stealTimersOrRunNextG); gp != nil {                     return gp, false, now, pollUntil, ranTimer                 }             }         }     }      // No goroutines found to steal. Regardless, running a timer may have     // made some goroutine ready that we missed. Indicate the next timer to     // wait for.     return nil, false, now, pollUntil, ranTimer }   你說它不能 steal global run queue 上面的 g 嗎？  hmm 我大概翻了一下 source code 在 work steal 裡面它並沒有檢查 global run queue 的哦  取而代之的是，是在 scheduler 裡面找 global run queue，它找 runnable goroutine 的方式是     先找 global run queue(61 次 scheduler call 之後會檢查一次，為了公平性，如果兩個 local run queue 頻繁的切換會導致 global run queue 裡的 goroutine starvation)   local run queue   再找 global run queue   可參考 runtime/proc.go#2672  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 func findRunnable() (gp *g, inheritTime, tryWakeP bool) {     ...      // Check the global runnable queue once in a while to ensure fairness.     // Otherwise two goroutines can completely occupy the local runqueue     // by constantly respawning each other.     if pp.schedtick%61 == 0 &amp;&amp; sched.runqsize &gt; 0 {         lock(&amp;sched.lock)         gp := globrunqget(pp, 1)         unlock(&amp;sched.lock)         if gp != nil {             return gp, false, false         }     }      // local runq     if gp, inheritTime := runqget(pp); gp != nil {       return gp, inheritTime, false     }      // global runq     if sched.runqsize != 0 {         lock(&amp;sched.lock)         gp := globrunqget(pp, 0)         unlock(&amp;sched.lock)         if gp != nil {             return gp, false, false         }     }      ... }   Sysmon  為系統層級監控進程，負責 goroutine 的監控與喚醒  因為 sysmon 為系統層級，所以它不需要 p 去執行  sysmon 會以一開始睡眠 20us ，1ms 之後每次 double 睡眠時間直到 10ms   喚醒的任務，當執行完畢 network I/O 的 goroutine 返回了之後  sysmon 就會負責將這些 goroutine 塞到 global run queue 讓他們等著被 schedule 執行      由於 sysmon 的執行不依靠 p, 因此 injectglist 會因為找不到 p 而將這些 goroutine 擺到 global run queue      那麼 sysmon 需要監控些啥呢  p 有若干個狀態如以下     可參考 runtime/runtime2.go#L106       _Pidle    :arrow_right: 可以被 scheduler 使用   _Prunning :arrow_right: 被 m 所擁有並且執行 user code 當中   _Psyscall :arrow_right: 正在執行 system call 並且可以被 work steal   _Pgcstop   _Pdead   假設遇到執行syscall 太久或者是單純跑太久的 goroutine(10 ms)  sysmon 會將他們的 p 奪走  可參考 runtime/prco.go#L5453      為什麼單純跑太久會被奪走 p? 就只是單純的 timeout 而已，符合 scheduler 的行為(Round Robin)  可是前面 Coroutine(Fiber, Green Threads) 不是說，讓出 CPU time 必須是要出於主動意願的情況下嗎  Golang Runtime Scheduler 並沒有可以讓 programmer 控制這個行為的操作(e.g. yield)  因此，scheduler 會主動進行 preempt, 盡可能不讓 starving 的情況發生    1 2 3 4 5 6 7 8 9 10 11 12 13 if s == _Prunning || s == _Psyscall {     // Preempt G if it's running for too long.     t := int64(pp.schedtick)     if int64(pd.schedtick) != t {         pd.schedtick = uint32(t)         pd.schedwhen = now     } else if pd.schedwhen+forcePreemptNS &lt;= now {         preemptone(pp)         // In case of syscall, preemptone() doesn't         // work, because there is no M wired to P.         sysretake = true     } }      preemptone() 可能會沒用，goroutine 有可能不理你，不過這個 function 會通知 goroutine 說你應該要停止執行了  真正交出 p 的 function 會是 handoffp()    被奪走 p 之後, 狀態會被設定為 idle，當你是 idle 的時候，如果別的 g 需要，原本的 p 會被 work steal 或者是維持 idle(目前很閒)  當執行完 syscall 之後，g 需要再度取得 p 才能再度執行  你原有的 p 要馬是     還在 idle(i.e. 沒有被 work steal)   不然就是已經被拿走了   被拿走的 p, findRunnable() 會幫你找一個讓你可以執行  幫你找伴侶的事情不是 sysmon 的職責   How to Use Goroutine  說了這麼多都沒有講到他的寫法  其實只要關鍵字 go 後面接 function 就可以了  比如說  1 2 3 4 5 6 7 go list.Sort()  // or  go func() {     fmt.Println(\"I'm in goroutine\") }()   就這樣  那你說我要怎麼知道它跑完了沒？ 答案是你不會知道，因為他是以 concurrent 的形式下去跑的  那有沒有辦法等待它跑完？ 你可以使用 WaitGroup, empty select 或者是 Channel      有關 channel 的介紹可以參考 Goroutine 與 Channel 的共舞 | Shawn Hsu    WaitGroup  來看個簡單的程式範例  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 package main  import (     \"fmt\"     \"sync\" )  func greeting(index int, wg *sync.WaitGroup) {     defer wg.Done()     fmt.Printf(\"Hello %v\\n\", index)     return }  func main() {     var (         wg    = &amp;sync.WaitGroup{}         count = 10     )      for i := 0; i &lt; count; i++ {         wg.Add(1)         go greeting(i, wg)     }      wg.Wait() }   結果如下  1 2 3 4 5 6 7 8 9 10 Hello 9 Hello 0 Hello 1 Hello 2 Hello 3 Hello 4 Hello 7 Hello 8 Hello 6 Hello 5  如果你沒有使用 wait group，隨著 main goroutine 的結束，所有的 child 也會一併結束的  那這樣的結果就會是輸出會來不及，導致你的 console 會是空的   Empty Select  另一個方法是使用 select  select 的用途是為了要處理 channel 的資料  不過空的 select 可以用於 阻塞目前 goroutine  與無窮迴圈 for {} 不一樣的是  select {} 會使得當前 goroutine 進到休眠狀態，而 for {} 的效果會是 spinlock(使用 cpu time, 100%)   如果參照上述的 goroutine 範例 wg.Wait() 改成 select {} 是 會錯的  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Hello 9 Hello 0 Hello 1 Hello 2 Hello 3 Hello 4 Hello 5 Hello 6 Hello 7 Hello 8 fatal error: all goroutines are asleep - deadlock!  goroutine 1 [select (no cases)]: main.main()         /tmp/main.go:26 +0xa5 exit status 2  為什麼會這樣呢？  因為當所有的 greeting 開始跑得時候，還不會 deadlock, 你也可以從上面看到 Hello x 的確也有輸出  可是當時間來到 t + 1 ，所有的 greeting 都執行完成了之後，剩下多少 goroutine 在跑？  1 個，只剩下 main goroutine 在跑  而 main goroutine 就像等著一個沒有結果的人類一樣癡癡的等著進而導致 deadlock      Deadlock 的四個條件(須全部滿足)         Non-preemption     Hold and wait     Circular wait     Mutual Exclusion      Goroutine Execution Sequence  考試的時候很常考一種問題，就是給定一段程式碼，問你輸出結果是什麼   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 func printNumbers() {     for i := 1; i &lt;= 5; i++ {         time.Sleep(100 * time.Millisecond)         fmt.Printf(\"%d \", i)     } }  func printChars() {     for i := 'a'; i &lt;= 'e'; i++ {         time.Sleep(40 * time.Millisecond)         fmt.Printf(\"%c \", i)     } }  func main() {     go printNumbers()     go printChars()     time.Sleep(time.Second)     fmt.Println(\"main terminated\") }   記住一件事情，goroutine 的執行是屬於 非同步的  剩下的變因就是時間而已   兩個 goroutine 會同時執行，但是因為他們各自的等待時間並不同  所以你的輸出結果會是不一樣的  main goroutine 會等待 1 秒之後才會結束  所以 printNumbers 會執行 10 次，而 printChars 只會執行 25 次   畫出來會長這樣  1 2 3 t             40 80 100 120 160 200 300 400 500 printChars:   a   b       c   d   e printNumbers:         1           2   3   4   5  因為 for loop 只有跑到 e 跟 5 而已，所以即使還有時間也不會有任何輸出  答案有兩種     ab1cde2345 main terminated   ab1cd2e345 main terminated   因為我們無法確定在 t = 200 的時候 goroutine 的執行順序   Memory Usage of Goroutine     可參考 多開 Goroutine 的效能瓶頸以及 Garbage Collection 對其的影響 | Shawn Hsu  程式碼 benchmark 的部份也可以參考 ambersun1234/blog-labs/golang-gc   References     Difference between Thread Context Switch and Process Context Switch   Difference between a “coroutine” and a “thread”?   What is the difference between a thread and a fiber?   [CS] 進程與線程的概念整理（process and thread）   Does linux schedule a process or a thread?   How do coroutines improve performance   Java’s Thread Model and Golang Goroutine   深度解密Go语言之 scheduler   Scalable Go Scheduler Design Doc   Golang Hacking.md   认识Golang中的sysmon监控线程   What does an empty select do?   deadlock with a empty select{} in main goroutine.   Memory Profiling a Go service   https://news.ycombinator.com/item?id=12460807  ","categories": ["random"],
        "tags": ["golang","coroutine","thread","scheduler","parallelism","concurrency","gm","gmp","work steal"],
        "url": "/random/golang-goroutine/",
        "teaser": null
      },{
        "title": "神奇的演算法 - Subarray Sum",
        "excerpt":"Subarray  Definition  subarray 為一個 array 的連續子集合  subarray 不可為空，subarray sum 則為這個子陣列的和   Properties  subarray 有一個特性，給定長度為 n 的 array  subarray 的總數為 1 + 2 + 3 + ... + n  舉例來說   1 2 3 4 5 6 7 假設 array = 1234  subarray 會是 1234, 234, 34, 4 (index 3, value 4) 123, 23, 3       (index 2, value 3) 12, 2            (index 1, value 2) 1                (index 0, value 1)   可以看到數量是 1 + 2 + 3 + 4 = 10  這個特性剛好對到 array 的 index  也就是說，當我在 index 3(value 4) 的時候，我可以知道前面總共有 1 + 2 + 3 + 4 = 10 個 subarray  針對 index 3(value 4) 自己來說，他有 index + 1(也就是 4) 個 subarray   LeetCode 560. Subarray Sum Equals K  Brute Force Approach  最直觀的作法當然是每個 subarray 都檢查一遍，確認該 subarray 的總和是否為 k 即可  pseudo code 為以下  for i from 0 to length(array)     for j from i + 1 to length(array)         if sum(array[i] to array[j]) equals k             answer += 1  這樣的作法是使用 2 層 for-loop 下去針對每一個可能的 subarray 進行檢查  雖然非常的直覺，但他的時間複雜度可以看到是屬於 $O(n^2)$  對於非常大的陣列來說，它很容易就會 TLE(Time Limit Exceeded)  所以很明顯的，這個作法有待改進   Cumulative Table     上圖為身高的累進圖表，可以看到分別紀錄了各個身高所對應的 累進比例  比方說，從 140 cm 到 160 cm 的人數佔了所有人的 62.5 %   Cumulative Sum of Array  同樣的概念可以套用到 array 上面  假設有一個 array [5, 4, -1, 7, 8]  那麼他的 cumulative sum of array 就會是                  index       0       1       2       3       4                       array       5       4       -1       7       8                 cumulative       5       9       8       15       23           累進的規則是  1 2 3 4 cumulative[0] = array[0] cumulative[1] = array[0] + array[1] cumulative[2] = array[0] + array[1] + array[2] cumulative[3] = array[0] + array[1] + array[2] + array[3]   依此類推   How to Get Subarray Sum from Cumulative Sum Array  假設我要找的 subarray sum 是 8  以肉眼觀察可以找到 2 組解答，分別為 [5, 4, -1] 以及 [8]  那要怎麼利用 cumulative sum array 來快速的找到呢？   根據上述的簡單累進推導規則，我們可以簡單的發現以下規則  1 2 3 array[1]                       = cumulative[1] - cumulative[0] array[1] + array[2]            = cumulative[2] - cumulative[0] array[1] + array[2] + array[3] = cumulative[3] - cumulative[0]   也就是說，要取得 array[i] ~ array[j] 的總和  $array[i] + array[i + 1] + … + array[j] = cumulative[j] - cumulative[i - 1]$   回到原本的例子                  index       0       1       2       3       4                       array       5       4       -1       7       8                 cumulative       5       9       8       15       23           [5, 4, -1] 的 sum 用 cumulative 寫出來就會是 cumulative[2] - cumulative[-1]      我的習慣會是建立 cumulative sum array 的時候在前面多塞一個數值為 0 的(上面的 cumulative[-1] 就會等於 0)    How Cumulative Sum Array Helps Speedup?  看到這你不難發現，使用 cumulative sum array 可以取得 任意區間 的 subarray sum(透過兩個 cumulative 的數值相減即可得到區間和)  亦即只要把 cumulative sum array 建立起來，你就不需要用 2 層 for-loop 暴力的窮舉出所有可能了   Cumulative Sum Approach                  index       -1       0       1       2       3       4                       array       0       5       4       -1       7       8                 cumulative       0       5       9       8       15       23           你可能會想，即使建立完 cumulative sum array，我不還是得用 2 層 for-loop 慢慢看區間和是否等於 k 嗎？  其實我們可以一邊建立一邊檢查區間和是否等於 k   前面提到，要取得 array[i] + ... + array[j] 可以使用 cumulative[j] - cumulative[i - 1] 取得  題目的要求是，區間和要等於 k  透過簡單的算式  1 2 3 4 5 因為 array[i] + ... + array[j] = k array[i] + ... + array[j] = cumulative[j] - cumulative[i - 1] 所以 cumulative[j] - cumulative[i - 1] = k  可以得知，目標為 cumulative[j] - cumulative[i - 1] = k, where i &lt; j     假設你建立 cumulative sum array 到一半，它應該會長成這樣                  index       -1       0       1       2       x       x                       array       0       5       4       -1       x       x                 cumulative       0       5       9       8       x       x           有了這個半完成的 cumulative sum array  你有辦法算出所有區間內的和，包含 [0], [1], [2], [0,1], [0,1,2], [1,2] 每個的區間和   既然我們的目標是區間和要等於 k  把目標稍微改寫能得到 cumulative[j] - k = cumulative[i - 1]  k 已經有了，題目給的  在建立 cumulative[j] 的時候，cumulative[i - 1] 已經有了(因為 i &lt; j)  所以！ 我只要往前看，找看看有沒有誰的區間和等於 cumulative[j] - k 就可以了！   依照現在這個例子，cumulative sum array 建立到 index 2  我的目標 k 是 8，我只要找有 哪個先前 cumulative 的數值等於 cumulative[2] - k(也就是 8 - 8)， 就代表該區間的和等於 k  為了使得尋找 cumulative[2] - k 有沒有存在於先前的區間和裡面，使用 hashmap 加速是一個可靠的選擇     講了那麼多，直接上 code  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 func subarraySum(nums []int, k int) int {     prefixSum := make([]int, 0)     prefixMap := make(map[int]int, 0)     prefixMap[0] = 1     previousPrefixSum := 0     answer := 0      for i, num := range nums {         prefixSum = append(prefixSum, previousPrefixSum + num)         if _, exists := prefixMap[prefixSum[i] - k]; exists {             answer += prefixMap[prefixSum[i] - k]         }         previousPrefixSum = prefixSum[i]          value, exists := prefixMap[prefixSum[i]]         if !exists {             prefixMap[prefixSum[i]] = 1         } else {             prefixMap[prefixSum[i]] = value + 1         }     }      return answer }   Why do we Need to Store Occurrence of cumulative[i] in Map  前面提到，為了加速尋找 cumulative[j] - k 能夠跑得更快，因此實作當中使用了 map 的結構  但是為什麼要紀錄 cumulative[j] - k 出現了幾次呢？  只要紀錄他有出現過不就好了嗎？   再看另一個例子                  index       -1       0       1       2       3       4       5       6       7       8                       array       0       3       4       7       2       -3       1       4       2       1                 cumulative       0       3       7       14       16       13       14       18       20       21           你可以看到 cumulative 陣列裡面出現了 2 次 14  假設你的條件剛好是 cumulative[j] - k = 14  如果你沒有紀錄出現的次數，在之後的答案當中你會少算了 1 次(以這個例子來說)   Do we Need to Consider -k Situation  我在嘗試理解這個算法的時候，一個問題油然而生  我到底需不需要考慮 -k 的情況？   還記得我們運算的陣列是 累進(累加) 的嗎？  意思就是說，如果考慮到 -k 的情況，你對回去原本的陣列看，他的 subarray sum 也會是 -k  所以只需要考慮 k 的情況就好了   LeetCode 53. Maximum Subarray  subarray 是由一個以上的連續元素所組成的，而 subarray sum 就是所有區間數值加總起來  要找到 maximum subarray sum 最直覺的方法就是窮舉出所有區間組合  但是這樣太複雜了，我們可以試著簡化問題   看一個實際例子比較快  如果之前的區間最大和(0 ~ n-1)是 10     num[n] = 2, 因為 10 + 2 &gt; 10, 所以目前 maximum subarray sum 就是 10 + 2(num[0 ~ n])   num[n] = -1, 因為 10 + (-1) &lt; 10，所以目前 maximum subarray sum 就是 10 + (-1)(num[0 ~ n])   num[n] = 20, 因為 20 &gt; 10, 所以目前 maximum subarray sum 就是 20(num[n])      注意第二點，為什麼最大區間和不是 10 而是 9?  因為我們要考慮 “連續的情況”  如果你把它寫成 10, 那麼最大區間和中間就會有空格，就不符合 subarray 的定義了    我們可以把上述的情況匯總成以下規則     如果 num[n] 小於 num[0 ~ n], 那 maximum subarray sum 就是 num[0 ~ n]   如果 num[n] 大於 num[0 ~ n], 那 maximum subarray sum 就是 num[n]   所以目前最大區間和，取決於 之前的最大區間和  這就是動態規劃(dynamic programming)  因為要我算出最大區間和實在是太困難了，當我知道之前的最大區間和(n-1)，在加上目前的數字，我可以很輕易的判斷現在的區間最大和為多少(n)      可參考 神奇的演算法 - 動態規劃 Dynamic Programming | Shawn Hsu    所以實作就很簡單了  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 func maxSubArray(nums []int) int {     size := len(nums)     dp := make([]int, size)     maxSubSum := nums[0]     dp[0] = nums[0]       for i := 1; i &lt; size; i++ {         dp[i] = max(nums[i], nums[i] + dp[i - 1])         maxSubSum = max(maxSubSum, dp[i])     }      return maxSubSum }  func max(a, b int) int {     if a &gt; b {         return a     }     return b }     另一種寫法一樣是使用累進的概念  定義一個 cumulative array  它負責的就是紀錄累進數字   區間和的寫法可以寫成 cumulative[j] - cumulative[i - 1]  所以換句話說，當前累進 - 最小的累進就是最大的區間和  只不過你還要跟 nums[i] 比較，因為有可能 nums[i] 比之前的區間和還大   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 func maxSubArray(nums []int) int {     size := len(nums)     cumulative := make([]int, size + 1)     cumulative[0] = 0      result := nums[0]     cumulativeMin := cumulative[0]     for i := 1; i &lt;= size; i++ {         cumulative[i] = cumulative[i - 1] + nums[i - 1]         result = max(nums[i - 1], max(result, cumulative[i] - cumulativeMin))         cumulativeMin = min(cumulativeMin, cumulative[i])     }      return result }  func min(a, b int) int {     if a &lt; b {         return a     }     return b }  func max(a, b int) int {     if a &gt; b {         return a     }     return b }   LeetCode 3147. Taking Maximum Energy From the Mystic Dungeon  另一種變形是這題，給定一個 array 以及一個間距 k  你可以從任一 index 開始，每次都走 K 步直到最後，要求求出你能夠拿到的最大值   你可能會想這跟 LeetCode 53. Maximum Subarray 很像  但差別在於本題是沒辦法 pick/skip 的  所以針對 [-8,10,-10] 以及 K = 1 這個 test case 來說答案會是 0 而不是 10  因為你不能略過任何一個數值  所以 Kadane’s Algorithm 並不適用於本題   但也還好對吧？ 就是稍微暴力的寫法而已  計算從每個 index 開始，走 K 步的最大值  不過效率有點低 而且會重複計算到  所以其實可以從最後面往前算   1 2 3 4 5 6 7 8 9 10 11 12 13 func maximumEnergy(energy []int, k int) int { \tn := len(energy) \tans := -1 &lt;&lt; 31  \tfor i := n - k; i &lt; n; i++ { \t\tsum := 0 \t\tfor j := i; j &gt;= 0; j -= k { \t\t\tsum += energy[j] \t\t\tans = max(ans, sum) \t\t} \t} \treturn ans }   原本是從 0 算到 n - k  因為你只能移動 K 步，所以最多只有到 n - k  那反過來就是從 n - k 算到 n   inner loop 其實滿有意思的  為什麼每一次計算都可以拿來更新答案？  他是往前算得對吧，你要 i 把它當成是 終點  所以每一次的計算其實都是完整的區間(起點可以任意，但終點都是沒辦法走下去的那個點，而它是由 i 決定的)  所以全部遍歷之後就可以得到最大值了   See Also     LeetCode 1171. Remove Zero Sum Consecutive Nodes from Linked List   References     C++| Full explained every step w/ Dry run | O(n^2) -&gt; O(n) | Two approaches  ","categories": ["algorithm"],
        "tags": ["kadane's algorithm","subarray","subarray sum","cumulative sum","dynamic programming","prefix sum"],
        "url": "/algorithm/algorithm-subarray-sum/",
        "teaser": null
      },{
        "title": "網頁程式設計三兩事 - 不一樣的驗證思維 JWT(JSON Web Token)",
        "excerpt":"Authorization  開發 API 的過程當中，我們提供了很多功能，其中可能包含較為隱私的功能(比如說，修改密碼，查詢個人資料等等的)  這個時候，你不會希望別人隨便修改你的密碼對吧？  所以就必須要驗證你的身份   現實生活中驗證身份的方式不外乎就是查看你的證件，要求輸入密碼 … etc.  本篇文章將會帶你了解不一樣的驗證方法 - JWT   Session Authorization  傳統上，要驗證一個人的身份，我們可能會這麼做     要求使用者輸入帳號密碼   將資料送到伺服器中做檢查   確認無誤後，在伺服器上儲存使用者狀態   伺服器回傳一的特殊識別字串，讓你在每一次 request 都帶著方便驗證        ref: [筆記] HTTP Cookies 和 Session 使用    上述的作法是利用了所謂的 session  當你驗證完成之後，下一次伺服器就會認得你，哦！ 你已經登入過了！ 可以放行 這樣   你不難免會好奇，這樣做安全嗎？  理論上，伺服器除非有漏洞，不然其他人是無法看到這些資訊的   OAuth 2.0 Framework  OAuth 2.0 定義於 RFC 6749  傳統上的 client server 架構下，身份認證會使用 user 的 credential  當這個情況衍生至第三方也需要存取 credential 的時候，事情會變得稍微複雜  你理所當然不會希望第三方擁有你的 credential 對吧  所以 OAuth 的標準立志於解決這種狀況   取而代之的是，OAuth 引入了 access token 的概念  token 包含了     可以存取 的範圍(e.g. 你的電話號碼，住址 … etc.)   token 的有效期限   OAuth 人物簡介     Resource Owner :arrow_right: user(i.e. 你)   Client :arrow_right: 欲取得你的授權拿資料的 application   Authorization Server :arrow_right: 驗證身份，並生成授權訪問 token 給 client   Resource Server :arrow_right: 儲存用戶機密資料的伺服器, client 必須帶著 token 才能存取   整個 OAuth 的 flow 大致如下      ref: An Introduction to OAuth 2    可以看到，client 先從 resource owner 這裡取得 授權  通常在這個步驟就會明確的指定可以存取的範圍  得到使用者的同意之後就會帶著 grant 向 authorization server 進行存取授權  拿到 token 之後就可以帶著去 resource server 拿資料了   Authorization Grant Type  Authorization grant 是一種 credential, 代表 resource owner 核可的授權  與上面講的傳統 client server 的 credential 不同  這裡的 credential 不會包含任何密碼什麼的，也就不會有任何洩漏機密資訊的風險   Authorization Code       ref: OpenID Connect    藉由將使用者導向 authorization server 的一種方法  當操作完成之後，你的 client 端會拿到一串 authorization code  你就可以使用這個 token 對 resource server 進行一系列的存取操作  上圖的 Google OAuth 2.0 API 就是一個很好的例子   Implicit  已經不推薦使用了   這個方法，client 會直接取得 access token  而且不會經過任何驗證   Resource Owner Password Credentials  已經不推薦使用了      顧名思義，使用 resource owner 的 credential(i.e. user id and password)  但它並不是把你的 credential 儲存起來  而是拿你的 password 跟 authorization server 換一個 access token      不推薦的原因是因為 OAuth Framework 是基於第三方存取而考量的  當然如果要你直接把密碼給第三方做驗證是我也不想       實務上，自己的網頁前後端用這種方法是 ok 的  spec 裡面有提到，除非 resource owner 跟 client 之間高度信任，否則不要使用這個方法    Client Credentials     長的跟 Resource Owner Password Credentials 很像  但不同的是，它 不需要 user 的 credential   JWT(JSON Web Token)  JWT 定義於 RFC 7519  他是一種 用以描述主體資訊的格式，且特別方便用於 對於空白很要求的環境 (e.g. http)  我們稱這種 描述主體的資訊 為 claim, 他是由 key/value pair 所組成      claim 裡面的 key/value pair, key 必須要是 unique 的  如果有出現重複的 key, 他要馬解析失敗，不然就是取最後一個 key 的值    JWT 有兩種主要實作方式，JWS 與 JWE   JWS(JSON Web Signature)  JWS 定義於 RFC 7515   JWS 是使用數位簽章(digital signature)或者 Message Authenticate Codes(MACs) 進行驗證的      兩者的不同在於，MACs 不能證明訊息來源的合法性  因為如果有別人知曉了你們之間的 shared key, 它也可以造出一樣的 MACs, 但來源是不可靠的  可參考 message authentication code (MAC)         ref: What is JWT (JSON Web Token)? How does JWT Authentication work?    整個 JWS 由三個部份構成，並以 .(dot) 分開，且每一個部份都由 base64 做 url encode     Header            {\"typ\":\"JWT\",\"alg\":\"HS256\"}           Claim payload            {\"iss\":\"joe\",\"exp\":1300819380,\"http://example.com/is_root\":true}           Signature            計算方式: sha256(base64(header) + \".\" + base64(payload), secret)              注意到 encode,encrypt 與 hash 的差別，前兩者可以被解碼/解密，後者不能  sha256 是一種 hash 的演算法    1 2 3 4 5 $ echo -n '{\"typ\":\"JWT\",\"alg\":\"HS256\"}' | basenc --base64url eyJ0eXAiOiJKV1QiLA0KICJhbGciOiJIUzI1NiJ9  $ echo -n '{\"iss\":\"joe\",\"exp\":1300819380,\"http://example.com/is_root\":true}' | basenc --baseurl eyJpc3MiOiJqb2UiLA0KICJleHAiOjEzMDA4MTkzODAsDQogImh0dHA6Ly9leGFtcGxlLmNvbS9pc19yb290Ijp0cnVlfQ      注意到不要用 base64 去做 encode  因為 base64 跟 base64url 兩個 encode 出來的東西不一樣  可參考 String based data encoding: Base64 vs Base64url 以及 How to encode and decode data in base64 and base64URL by using unix commands?    所以最終的 token 會長這樣  1 2 3 4 5 6 eyJ0eXAiOiJKV1QiLA0KICJhbGciOiJIUzI1NiJ9 . eyJpc3MiOiJqb2UiLA0KICJleHAiOjEzMDA4MTkzODAsDQogImh0dHA6Ly9leGFt cGxlLmNvbS9pc19yb290Ijp0cnVlfQ . dBjftJeZ4CVP-mB92K27uhbUJU1p1r_wW1gFWFOEjXk   JWE(JSON Web Encryption)  JWE 定義於 RFC 7516  與 JWS 不同的是，JWE 會將 payload 進行 加密   JWE 由五個部份構成，並以 .(dot) 分開，且每一個部份都由 base64 做 url encode     JWE protected header({\"alg\":\"RSA-OAEP\",\"enc\":\"A256GCM\"})   Random Content Encryption Key(CEK)   Random JWE Initialization Vector(IV)   JWE Cipher text   JWE Authentication Tag(CEK + IV + Additional Authenticated Data)   token 最終結果會長這樣  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 eyJhbGciOiJSU0EtT0FFUCIsImVuYyI6IkEyNTZHQ00ifQ . OKOawDo13gRp2ojaHV7LFpZcgV7T6DVZKTyKOMTYUmKoTCVJRgckCL9kiMT03JGe ipsEdY3mx_etLbbWSrFr05kLzcSr4qKAq7YN7e9jwQRb23nfa6c9d-StnImGyFDb Sv04uVuxIp5Zms1gNxKKK2Da14B8S4rzVRltdYwam_lDp5XnZAYpQdb76FdIKLaV mqgfwX7XWRxv2322i-vDxRfqNzo_tETKzpVLzfiwQyeyPGLBIO56YJ7eObdv0je8 1860ppamavo35UgoRdbYaBcoh9QcfylQr66oc6vFWXRcZ_ZT2LawVCWTIy3brGPi 6UklfCpIMfIjf7iGdXKHzg . 48V1_ALb6US04U3b . 5eym8TW_c8SuK0ltJ3rpYIzOeDQz7TALvtu6UG9oMo4vpzs9tX_EFShS8iB7j6ji SdiwkIr3ajwQzaBtQD_A . XFBoMYUZodetZdvTiFvSkQ   Header of JWE     enc            enc header 用於指定加密內容的方法，通常選擇 對稱式加密(e.g. AES-256-CBC)           alg            alg header 用於指定加密 CEK 的方法，為了確保安全性，通常選擇 非對稱式加密法(e.g. RSAES OAEP)           How does Encryption work  JWE 最特別的地方就是它會將 payload 加密成 cipher text  那麼具體來說他的加密方式如下      透過對稱式加密(使用 CEK 當作加密鑰匙)加密 payload   透過非對稱式加密 加密 CEK 鑰匙      為什麼不用非對稱式加密 payload?  那是因為 asymmetric encryption 通常有長度上限  大約只能 $floor(n/8) - 2 * ceil(h/8) - 2$  ref: What is the limit to the amount of data that can be encrypted with RSA?    Decryption  驗證完成之後，在使用 recipient 自己的 private key 進行解密  就可以讀取內容了     JWK(JSON Web Key)  JWK 定義於 RFC 7517  是有關於 cryptographic key 的 json 格式定義   1 2 3 4 5 6 7 {     \"kty\":\"EC\",     \"crv\":\"P-256\",     \"x\":\"f83OJ3D2xF1Bg8vub9tLe1gHMzV76e8Tus9uPHvRVEU\",     \"y\":\"x_FEzRu9m36HLN_tue659LNpXW6pCyStikYjKIWI5a0\",     \"kid\":\"1\" }      kty :arrow_right: 金鑰類型(key type), 可以是 RSA 或者是 EC(Elliptic Curve), case-sensitive   use :arrow_right: 金鑰用途, 可以是 sig(signature) 或是 enc(encryption), case-sensitive   alg :arrow_right: 金鑰演算法   kid :arrow_right: 金鑰唯一識別符(key id), 用來找特定的 public key   其他欄位是根據不同演算法而會有的類別，上述例子中的 x(公鑰), crv(公鑰), y(私鑰) 是 Elliptic Curve 才會出現的參數     ref: RFC 7518 §6.2    而 jwks 就是一堆的 jwk 所組成的 json 檔案  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 {\"keys\":        [          {\"kty\":\"EC\",           \"crv\":\"P-256\",           \"x\":\"MKBCTNIcKUSDii11ySs3526iDZ8AiTo7Tu6KPAqv7D4\",           \"y\":\"4Etl6SRW2YiLUrN5vfvVHuhp7x8PxltmWWlbbM4IFyM\",           \"use\":\"enc\",           \"kid\":\"1\"},           {\"kty\":\"RSA\",           \"n\": \"0vx7agoebGcQSuuPiLJXZptN9nndrQmbXEps2aiAFbWhM78LhWx                 4cbbfAAtVT86zwu1RK7aPFFxuhDR1L6tSoc_BJECPebWKRXjBZCiFV4n3oknjhMs                 tn64tZ_2W-5JsGY4Hc5n9yBXArwl93lqt7_RN5w6Cf0h4QyQ5v-65YGjQR0_FDW2                 QvzqY368QQMicAtaSqzs8KJZgnYb9c7d0zgdAZHzu6qMQvRL5hajrn1n91CbOpbI                 SD08qNLyrdkt-bFTWhAI4vMQFh6WeZu0fM4lFd2NcRwr3XPksINHaQ-G_xBniIqb                 w0Ls1jF44-csFCur-kEgU8awapJzKnqDKgw\",           \"e\":\"AQAB\",           \"alg\":\"RS256\",           \"kid\":\"2011-04-29\"}        ]      }   JWA(JSON Web Algorithms)  加密演算法定義，可參考 RFC 7518     Distinguish JWE from JWS  有以下幾點可以分辨      dot 數量不一樣(2 個以及 4 個，分別對應 JWS 與 JWE)   alg header(JWS 為 none, JWE 則有值)   enc header 存在與否(JWS 不存在，JWE 存在)        ref: JWT Hacking 101    Nested JWT  nested JWT 也是被支援的，只是根據 RFC 文件中所述，他是不建議這樣寫的  header 的部份需要帶入 cty 值為 JWT(建議大寫，用以兼容 legacy system)   Unsecure JWT  JWT 也支援不帶 signature 的寫法，在 header 當中帶入一個參數 alg 並將其設置為 none 即可  不帶 signature 也就表示最後出來的 JWT token string 只會有兩個部份，看以下例子   1 2 3 # header: {\"alg\":\"none\"} $ echo -n '{\"alg\":\"none\"}' | basenc --base64url eyJhbGciOiJub25lIn0      注意到不要用 base64 去做 encode  因為 base64 跟 base64url 兩個 encode 出來的東西不一樣  可參考 String based data encoding: Base64 vs Base64url 以及 How to encode and decode data in base64 and base64URL by using unix commands?    它最後會長成類似這樣  1 2 3 4 5 eyJhbGciOiJub25lIn0 . eyJpc3MiOiJqb2UiLA0KICJleHAiOjEzMDA4MTkzODAsDQogImh0dHA6Ly9leGFt cGxlLmNvbS9pc19yb290Ijp0cnVlfQ .   可以看到第二個點(dot)後面並沒有任何的 signature   Verifying a JWT  那我要怎麼驗證 JWT 的來源是否合法？   前面 JWK(JSON Web Key) 我們有提到  你可以 either 使用 symmetric 或 asymmetric key 當作 JWT signing key   Symmetric Key  既然你的簽章是用一把 “對稱式” 金鑰下去算的  client 要驗證的唯一方式 是不是用金鑰下去算算看？  如果用 header + payload 算出來的結果跟你傳過來的 signature 一樣  就代表這個 token 來源被驗證了！  嗎   記不記得在前面 JWS(JSON Web Signature) 有提到  簽章有兩種方式, digital signature 跟 Message Authentication Codes(MACs)      數位簽章(digital signature) 可以驗證 1. 資料完整性 2. 來源合法性  Message Authentication Codes(MACs) 可以驗證 1. 資料完整性  其中 MACs 是沒辦法驗證來源的    道理倒也淺顯易懂  如果第三方取得你的 signing key, 我是不是也能偽造簽章了？   也因此，對稱式的金鑰通常不會驗，也沒辦法驗   Asymmetric Key  非對稱式金鑰事情就變得有趣了      請注意，這跟 JWE(JSON Web Encryption) 不一樣！  我們 並沒有 加密 header 以及 payload  這裡單純講 簽章(signature) 使用 asymmetric encryption    How does Asymmetric Encryption Work  稍微複習一下，非對稱式加密的運作原理  context: A 與 B 要互相傳訊息     A 使用 B 的 public key 加密 payload, 並使用 A 的 private key 簽名(sign)   B 收到訊息，使用 A 的 public key 驗證, 並使用 B 的 private key 解密     client 可以透過取得 public key 的方式驗證  那這個 public key 在哪呢？  根據 OpenID Connect Discovery 1.0 incorporating errata set 1 public key 是必須儲存在每一台伺服器上的  你必須要先發起一個 get request 到 /.well-known/openid-configuration  伺服器要回一個 json 檔，大概長這樣  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 {    \"issuer\":      \"https://server.example.com\",    \"authorization_endpoint\":      \"https://server.example.com/connect/authorize\",    \"token_endpoint\":      \"https://server.example.com/connect/token\",    \"token_endpoint_auth_methods_supported\":      [\"client_secret_basic\", \"private_key_jwt\"],    \"token_endpoint_auth_signing_alg_values_supported\":      [\"RS256\", \"ES256\"],    \"userinfo_endpoint\":      \"https://server.example.com/connect/userinfo\",    \"check_session_iframe\":      \"https://server.example.com/connect/check_session\",    \"end_session_endpoint\":      \"https://server.example.com/connect/end_session\",    \"jwks_uri\":      \"https://server.example.com/jwks.json\",                 &lt;---      ... }  其中 public key 的位置就是 jwks_uri     或者是 https://{yourDomain}/.well-known/jwks.json    jwks.json 裡面可能包含了多組 key, 其中每一組 key 的格式可以參考 JWK(JSON Web Key)  拿到 public key 之後，之後的操作就相對簡單了      jwks 的運作流程，推薦可以去這篇文章 [OpenID] 使用 RS256 與 JWKS 驗證 JWT token 有效性    Server Side Verification  伺服器端的驗證則是  signature 的計算是使用存在 server 上的 signing key 加上前面 header 以及 payload 雜湊出來的  要驗證，就是再算一次，算出新的 new signature 與送過來的 signature 進行對比  如果兩個一樣，就代表資料沒有被竄改過   Is JWT Safe?  看了以上兩種 JWT 的實作，你應該有發現  JWS 並沒有針對 payload 進行加密，等同於他是裸奔在網路上的  那這樣是不是就等於 JWS 其實不怎麼安全？   如果真的有傳送機密訊息的需求，使用 JWE 是最好的選擇  不過普遍來說，我們認為 http 的 TLS 已經足夠應付大多數的場景了      有關 TLS 的介紹可以參考 重新認識網路 - 從基礎開始 | Shawn Hsu    既然 JWS 沒有進行加密  一般來說是不建議在上面塞入任何敏感資訊  以我自己來說，我通常只會帶個 user id 之類洩漏也不會怎麼樣的資料  畢竟你可以直接 decode 這串 eyJ1c2VySUQiOjF9  1 2 $ echo -n 'eyJ1c2VySUQiOjF9' | basenc --base64url -d {\"userID\":1}   Why JWT Popular than Session Authorization  session 逐漸式微的原因，有幾個面向可以討論   主要的原因是，現如今系統規模都不單只是一台伺服器的架構，你可能會跑 micro service 對吧？  那你要怎麼同步 session 就是一個大問題了  你的 token 可能在 A server 上，但是下一次 request 過來可能是 B server 處理，這時候你的 session 就不見了      這時候你就需要 sticky session 了    二來是，根據 RFC 1945, HTTP 1.0 的文件就已經開宗明義表明，HTTP 是屬於無狀態的 protocol      The Hypertext Transfer Protocol (HTTP) is an application-level   protocol with the lightness and speed necessary for distributed,   collaborative, hypermedia information systems. It is a generic,   stateless, object-oriented protocol which can be used for many tasks,   such as name servers and distributed object management systems,   through extension of its request methods (commands). A feature of   HTTP is the typing of data representation, allowing systems to be   built independently of the data being transferred.       有關更多 HTTP 相關探討，可以參考 重新認識網路 - HTTP1 與他的小夥伴們 | Shawn Hsu    所以總結起來就是兩點     多台伺服器下，session 如何同步   為了遵守 HTTP 無狀態的規範   那你說，session 就真的沒用了嗎？  client 必須在每一次的 request 都帶著 token 進行請求  萬一我伺服器想要主動踢掉他的訪問權限怎麼辦？ 比方說我要進行系統升級之類的  因為狀態完全由 client 管理，我 server 根本不知道目前誰可以存取，自然也就不知道怎麼設定黑名單了   難道我只能眼睜睜看著他的 token 到期才有辦法嗎？  所以現今的實作依然有保留 session 的 概念(注意是概念而已)  也就是說我伺服器上面同時紀錄著當前登入的人，那這樣我就可以隨時把他的權限移除(server side)  下次 client 再跑來請求的時候，當我發現 server 上面已經把它 revoke 它就無法存取  也就達到主動踢人的方法了      這部份可以考慮用 Redis 實作    HTTP Authorization Header  前面講了這麼多的 JWT token, 實際使用的時候  它應該放在哪裡呢   HTTP 的 request header 中提供了一個 Authorization header 讓你可以放所謂的 credentials  有了這個，你就可以存取受到保護的資源了   阿要記得 如果你是跨網域請求  你的後端必須額外設定一個 request header  Access-Control-Allow-Credentials 設定為 true      有關更多跨網域相關的請求，可以參考 網頁程式設計三兩事 - 萬惡的 Same Origin 與 CORS | Shawn Hsu    Authorization Header Schemes  Basic Scheme  Basic scheme 定義於 RFC 7617   basic scheme 帶的資料會是 userid 以及 password(兩者都使用 base64 做 url encode)  這個 scheme 由於是 plain text 的，因此被視為是不安全的，除非！ 你的 HTTP 走 TLS     那它整體驗證流程是要怎麼用？     如果 server 想要 client 進行驗證，那麼它會發一個 challenge 給 client(順便在帶一個 http 401)     1 2 3  HTTP/1.1 401 Unauthorized  Date: Mon, 04 Feb 2014 16:50:53 GMT  WWW-Authenticate: Basic realm=\"WallyWorld\"           client 必須要傳 user id 跟 password 回去 server 端進行驗證            user id 與 password 中間以 :(冒號隔開)         1  Authorization: Basic dGVzdDoxMjPCow==                       其中 dGVzdDoxMjPCow== 是 user id 與 password 的組合         1 2  $ echo -n 'dGVzdDoxMjPCow==' | basenc --base64url -d  test:123£                           就沒了      有關 realm，可參考 realm    Bearer Scheme  Bearer scheme 定義於 RFC 6750  不同於以往 直接使用使用者本身的 credential，Bearer scheme 是 使用一個字串代表允許授權訪問  而這個模式正是 OAuth 2.0 Framework   bearer scheme 使用的方式就相對簡單  1 2 3 4 Authorization: Bearer eyJ0eXAiOiJKV1QiLA0KICJhbGciOiJIUzI1NiJ9.                       eyJpc3MiOiJqb2UiLA0KICJleHAiOjEzMDA4MTkzODAsDQog                       Imh0dHA6Ly9leGFtcGxlLmNvbS9pc19yb290Ijp0cnVlfQ.                       dBjftJeZ4CVP-mB92K27uhbUJU1p1r_wW1gFWFOEjXk   其中後面那串就是 JWT token     跟 Basic Scheme 一樣，server 一樣有 challenge, 一樣有 realm  不過我個人的經驗上是不常看到就是   JWT in Cookie?  這聽起來很反人類，至少對我而言是如此   cookie 本身就是一個存放在 client 端的資料  為何 backend 能夠存取？  事實上每一次的 request 都會帶上 cookie，這是因為 browser 會自動幫你帶上去  簡單暴力的 Cookie header   因此，後端是可以存取 cookie 的  只不過要注意的是，cookie 本身是可以被 client 端修改的  所以安全性沒有那麼高     但為什麼要把 JWT token 放在 cookie 裡面呢？  有好好的 Authorization header 不用是為什麼?  安全性嗎？ 對，就是安全性   cookie 有一個特性，就是 httpOnly  這個特性可以讓 cookie 只能被 server 端存取，而無法被 client 端存取  也就是說，惡意的 javascript 不能夠存取你的 cookie 拿到你的 token 自然也就無法存取你的資源      也可以搭配 secure 這個特性，讓 cookie 只能在 https 下使用    所以這就是為什麼有些人會選擇把 JWT token 放在 cookie 裡面  但終究是不同的選擇，要看你的需求   realm  realm 指的是一個區域，要進行身份驗證的區域  啥意思呢？  一個網站之中，總有那麼幾個頁面是需要登入才能存取的吧？  而 realm 指的就是那些受到保護的區域   上述的 WallyWorld 是該區域的一個識別字串  只要你存取的頁面是在這個區域裡面，就通通都需要進行驗證  相同的 realm 代表，他們是屬於相同的驗證範圍   那麼能不能重複使用所謂的 credential 呢？  事實上是可以的，只要在相同 URI 底下 都可以重複使用 credential  什麼樣叫做相同 URI? 當然指的並不是網址完全一樣  以下這幾個 prefix 相同的 URI 被視為是可以重複使用 credential 的地方  1 2 3 http://example.com/docs/ http://example.com/docs/test.doc http://example.com/docs/?page=1   要注意的是，URI 本身可能會包含在多個 realm 底下(需要多個 authentication)  至於要用哪個 credential，就我目前看到的，並未特別定義   JWT Token in Golang  最後的最後，小小實戰一下   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 import (     \"time\"      \"github.com/golang-jwt/jwt\" )  type JWTAuth struct {     jwt.StandardClaims      UserID uint }  func GenerateJWTToken(userID uint, email string) (string, error) {     token := jwt.NewWithClaims(jwt.SigningMethodHS512, JWTAuth{         StandardClaims: jwt.StandardClaims{             Subject:   email,             ExpiresAt: time.Now().Add(constant.JWTExpire).Unix(),         },         UserID: userID,     })      tokenStr, err := token.SignedString(constant.JWTKey)     if err != nil {         return \"\", err     }      return tokenStr, nil }  生成 token 的方式，使用 github.com/golang-jwt/jwt 產生  其中你的 payload 可以塞任何你想塞的東西  standard claim 裡面有 subject 以及 expire time   這裡額外宣告了一個 structure, 繼承自 jwt.StandardClaims  如此一來你就可以在這塞入任何 payload   不過切記，這裡的東西可以簡單的被 base64 url decode  機密資料不要放入   準備好資料之後，要進行簽名  constant.JWTKey 是一串 byte array, 它可以是 symmetric key 或者是 asymmetric key      symmetric key 沒有什麼特定格式，它可以是隨便的字串(e.g. ‘abc’)  相反的 asymmetric key 就必須要用 openssl 或者是 ssh-keygen 之類的應該也行?    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 import (     \"time\"     \"fmt\"      \"github.com/golang-jwt/jwt\" )  type JWTAuth struct {     jwt.StandardClaims      UserID uint }  func ParseJWTToken(tokenString string) (*jwt.Token, *JWTAuth, error) {     var claims JWTAuth     token, err := jwt.ParseWithClaims(tokenString, &amp;claims, func(token *jwt.Token) (interface{}, error) {     if _, ok := token.Method.(*jwt.SigningMethodHMAC); !ok {             return nil, errors.New(fmt.Sprintf(\"unexpected signing method: %v\", token.Header[\"alg\"]))         }         return constant.JWTKey, nil     })      if err != nil {         return nil, nil, err     }      return token, &amp;claims, nil }   最後當然就是將 JWT 套到你的 application 上面  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 import (     \"net/http\"      \"github.com/gin-gonic/gin\"     \"github.com/golang-jwt/jwt\" )  func AuthMiddleware() func(ctx *gin.Context) {     return func(ctx *gin.Context) {         tokenString := ctx.Request.Header.Get(\"Authorization\")         if tokenString == \"\" {             ctx.AbortWithStatusJSON(http.StatusUnauthorized, nil)             return         }          token, claims, err := shared.ParseJWTToken(tokenString)         if err != nil {             if err.(*jwt.ValidationError).Errors&amp;jwt.ValidationErrorExpired != 0 {                 ctx.AbortWithStatusJSON(http.StatusUnauthorized, nil)             } else {                 ctx.AbortWithStatusJSON(http.StatusInternalServerError, nil)             }             return         }          if !token.Valid {             ctx.AbortWithStatusJSON(http.StatusInternalServerError, nil)             return         }          ctx.Set(\"userID\", claims.UserID)         ctx.Set(\"email\", claims.Subject)          ctx.Next()     } }   採用 middleware 的方式，針對每個需要進行 authorize 的資料進行保護  這段就普普通通  驗證 token 是否合法，並且取出我們塞的 payload 就大功告成了   References     是誰在敲打我窗？什麼是 JWT ？   Understanding JSON Web Encryption (JWE)   Where does jwt.io get the public key from JWT token?   JWT verify using public key   JSON Web Key Sets   Encrypt and Decrypt sensitive data with JSON Web Encryption(JWE)   JWT Private / Public Key Confusion   Authorization   Understanding the purpose of “realm” in Basic WWW Authentication   What is the “realm” in basic authentication   Difference between the “Resource Owner Password Flow” and the “Client Credentials Flow”   JWT Keys - Asymmetric and Symmetric   簡介其他 OpenID Connect 協定的內容   使用 HTTP Cookie  ","categories": ["website"],
        "tags": ["jwt","session","jws","jwe","jwk","golang","oauth","realm","cookie","httponly cookie","authorization"],
        "url": "/website/website-jwt/",
        "teaser": null
      },{
        "title": "網頁程式設計三兩事 - 萬惡的 Same Origin 與 CORS",
        "excerpt":"Preface  我最初遇到 CORS 的問題是在我的個人部落格上面，因為我引用了其他網站的圖片  某一天我突然發現圖片跑不出來了？ 思來想去我應該也沒有改到程式碼才對  後來看了一下發現好像是 CORS 的問題  所以今天要來講講 CORS   Same Origin Policy  來源相同的定義為何？      協定一樣   port 一樣   domain 一樣(包含 sub domain)   只要上述條件都符合即代表相同來源      反之違反任一條件即為不同來源         ref: [Day 27] Cross-Origin Resource Sharing (CORS)      網站的同源政策主要就是為了避免不同來源能夠透過 Document Object Model(DOM) 存取機密資料(e.g. cookies, session)      因此可以得知 Same Origin Policy 主要是實作在瀏覽器上面的    你說為什麼不同來源能夠存取 cookie?  那是因為早期的瀏覽器實作，即使是不同來源，它依然會帶 cookie 過去  就也因此可能會造成一些資安風險      延伸閱讀 [Day 26] Cookies - SameSite Attribute    所以引入了 Same Origin Policy 之後，可以有效的避免上述的事情發生   Introduction to CORS     CORS - Cross-Origin Resource Sharing 跨來源資源共用定義於 WHATWG’s Fetch Living Standard  CORS 不是一種安全機制，相反的，他是一種能夠突破 Same Origin Policy 限制的東西  有時候我們覺的 Same Origin Policy 太嚴格了，一些大網站用了不同的 sub domain 也被視為是不同來源實屬有點麻煩  因此 CORS 能夠使不同來源的要求被存取   Define Origin  那我要怎麼確定我的來源是誰  網站會使用 第一個 request 來確認你的來源      ref: Cross-Origin Resource Sharing (CORS)    How does CORS Work  前面有提到，普遍瀏覽器為了安全問題，都有實作 Same Origin Policy  為了有效的放寬此政策，便引入 CORS 的機制  允許部份的 origin 可以存取   如果瀏覽器要發起一個 cross origin request  它會先發一個 Preflight Request 跟目標 server 確認  server 會回傳一系列的 header 來描述哪些 request 可以被接受，可以被支援  確認可以支援之後，才會發起正式 request      cross origin request 不限於 api call, &lt;img&gt;, &lt;script&gt; 如果不同來源，也都算 cross origin request    你可以發現，基本上 CORS 不會管你不同來源是否合法  它只是要確認說 server 有支援你的 request 而已  安不安全跟它沒關係      所以基本上，如果你碰到 CORS 的問題，是你的後端需要做處理    注意到 CORS 的請求，預設是不會帶身份驗證相關的資料的(i.e. Authorization)  只有當 server 回傳特定 CORS header 它才會帶  設定的部份可參考 CORS Headers   Modify Origin Header  既然 Same Origin Policy 是用於保護網站被其他網站存取  而且他依靠的是 origin header  那有沒有可能我手動把它改掉，bypass 這個限制？   基本上你沒有辦法透過手動修改 origin header 來 bypass 這個限制  瀏覽器帶的 origin header 是不可更改的   但是你可以透過其他方式來達到這個目的  比如說 proxy  Nginx 的 proxy_set_header 可以新增或修改 header  以這個例子來說就是要修改 origin header   不過這種做法算沒必要  他的前提是你要能夠操作 server  以攻擊者的角度來說，其實使用 CSRF 來做攻擊更有效率   CORS Headers   這裡就大概列出幾個常用常見的 header  完整的 header list 可以造訪 HTTP 回應標頭                          Value       Description                       Access-Control-Allow-Credentials       true       是否允許帶 credential(e.g. cookies, authorization header, tls certificate)                 Access-Control-Allow-Headers       * Content-Type, Accept       允許的 header                 Access-Control-Allow-Methods       * GET, POST, PATCH       允許的 HTTP methods                 Access-Control-Allow-Origin       * https://example.com(僅允許 https://example.com) null(null origin)       允許的 request 來源                 Access-Control-Max-Age       86400       預檢後多久以內不需要在檢查           Credential with Wildcard Origin  當你的 origin 設為 wildcard 而且 credential 又為 true 的時候，會出現錯誤      Access-Control-Allow-Origin: '*'    注意到，這裡的 credential 不是 Access-Control-Allow-Credentials  它說的是 XMLHttpRequest 裡面的 withCredentials 的設定(e.g. Angular - HttpRequest)  那麼他有兩種解決方案     withCredentials 設定不能為 true(default 為 false)   explicit 設定 origin header :arrow_right: Access-Control-Allow-Origin: 'http://localhost:4200'   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 @Injectable({   providedIn: 'root', }) export class HttpInterceptorService implements HttpInterceptor {   constructor(private userStore: Store&lt;{ user: UserState }&gt;) {}    intercept(     req: HttpRequest&lt;any&gt;,     next: HttpHandler   ): Observable&lt;HttpEvent&lt;any&gt;&gt; {     return this.userStore.select('user').pipe(       first(),       mergeMap((userState) =&gt; {         req = req.clone({           setHeaders: {             Authorization: userState.Token,           },           withCredentials: true                &lt;---         });          return next.handle(req);       })     );   } }   null origin  不建議使用 null origin  因為如果 request scheme 非 http(e.g. data:, file:)  這些 scheme 的 origin 會預設為 null  那這樣就會有點危險，所以一般不建議這樣設定   Request Types  Simple Request  HTTP 的 request 當中，不受限於 Same Origin Policy 的 request 被稱之為 simple request  也就是說，符合下列規則的 request 不需要套用 CORS header 即可正常請求                                                  Methods       GET HEAD POST                 Headers       Accept Accept-Language Content-Language Content-Type Range                 Content-Type       application/x-www-form-urlencoded multipart/form-data text/plain           Preflight Request  預檢請求，亦即在正式 request 之前必須要先額外發一個 request 進行檢查  根據 server 的回應，來判斷是否可以往下執行   哪些 request 會屬於 preflight 的呢？ 簡單來說就是 非 Simple Request 的都是    上圖是一個完整 preflight request 的示意圖  可以看到 http://localhost:8888/me 的 request 被 call 了兩次  其中第一行即為 preflight request(他的 type 為 preflight)      如果你在 developer tools 沒有看到 preflight request, 記得把 filter 設為 all 才可以  做為 demo, http 401 可以先忽略(他是正常行為)     執行 preflight 的方式是  使用 HTTP Options method 並帶上一些 request header 進行檢查  Access-Control-Request-Headers: Authorization  Access-Control-Request-Method: GET      Options Method 為 safe method, 詳細可以參考 重新認識網路 - HTTP1 與他的小夥伴們 | Shawn Hsu  HTTP header field 並無大小寫之區分，可參考 RFC 1945 §4.2    這隻 API 主要會根據 user jwt token 取出對應 user 資料並回傳  所以他的 headers 帶了 authorization, 因為 credential 是存於 cookie 當中的  然後 server 這邊就要回應，它能夠處理的 CORS header 有哪些  有 4 個     Access-Control-Allow-Credentials 允不允許 request 帶 credential   Access-Control-Allow-Headers 允許哪些 HTTP header   Access-Control-Allow-Methods 允許哪些 methods   Access-Control-Allow-Origin 允許特定 request origin(i.e. 從哪裡來)   當所有條件都符合，都 ok，status code 會是 HTTP 204 No Content  你的 web browser 就會放行，進行真正的 request   CORS in Postman?  CORS 的問題基本上是為了解決瀏覽器實作的 Same Origin Policy  也因此，在你 debug 的時候使用 postman 或是 curl  CORS 的問題基本上不會出現(因為它不在瀏覽器裡面跑)   Refferer Policy: strict-origin-when-cross-origin  有的時候不是你設定有錯，是瀏覽器的問題   假設你 後端 正確的設定了 CORS header 了，但是你還是遇到問題  八成是瀏覽器在搞事，舉例來說 Google Chrome        ref: WebView2 Whether to allow private network request Settings？    Chrome 的選項 Block insecure private network requests 記得要把它關閉  然後你的網站就可以正常運作了   Configure CORS Support in Golang Gin  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 import (     \"net/http\"      \"github.com/gin-gonic/gin\" )  func CorsMiddleware() gin.HandlerFunc {     return func(ctx *gin.Context) {         allowHeaders := `             Content-Type, Content-Length,             Authorization, Accept,             Accept-Encoding, Origin,             DNT, User-Agent,             Referer         `         allowMethods := `             POST, GET, PUT,             DELETE, PATCH, OPTIONS         `          ctx.Writer.Header().Set(\"Access-Control-Allow-Origin\", \"*\")         ctx.Writer.Header().Set(\"Access-Control-Allow-Credentials\", \"true\")         ctx.Writer.Header().Set(\"Access-Control-Allow-Headers\", allowHeaders)         ctx.Writer.Header().Set(\"Access-Control-Allow-Methods\", allowMethods)          if ctx.Request.Method == \"OPTIONS\" {             ctx.AbortWithStatus(http.StatusNoContent)             return         }          ctx.Next()     } }      Referer 這個字其實是有故事的，可參考 HTTP 協定的悲劇    基本上就是定義你允許的各種條件  像是 origin 為 wildcard 代表你允許所有來源  allow credentials 可以允許攜帶 token 之類的東西  允許的 header 以及 method   為了處理 preflight request 不要出錯  會刻意抓 options request 出來，因為我們 router 並沒有定義相關 routing  沒有特別處理他到後面會 404 not found   Gin 其實有一套 CORS 的 library gin-contrib/cors  可以比較簡單的設定   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 package main  import (     \"time\"      \"github.com/gin-contrib/cors\"     \"github.com/gin-gonic/gin\" )  func main() {     router := gin.Default()     // CORS for https://foo.com and https://github.com origins, allowing:     // - PUT and PATCH methods     // - Origin header     // - Credentials share     // - Preflight requests cached for 12 hours     router.Use(cors.New(cors.Config{         AllowOrigins:     []string{\"https://foo.com\"},         AllowMethods:     []string{\"PUT\", \"PATCH\"},         AllowHeaders:     []string{\"Origin\"},         ExposeHeaders:    []string{\"Content-Length\"},         AllowCredentials: true,         AllowOriginFunc: func(origin string) bool {             return origin == \"https://github.com\"         },         MaxAge: 12 * time.Hour,     }))     router.Run() }     ref: gin-contrib/cors    References     跨來源資源共用（CORS）   What is the issue CORS is trying to solve?   同源政策 (Same-origin policy)   Cross-origin resource sharing   If browser cookies aren’t shared between different websites, then why is Same origin Policy useful?   [Day 26] Cookies - SameSite Attribute   Does every web request send the browser cookies?   how exactly CORS is improving security [duplicate]   In CORS, Are POST request with credentials pre-flighted ?   Why is jQuery’s .ajax() method not sending my session cookie?   What’s to stop malicious code from spoofing the “Origin” header to exploit CORS?   Same-origin policy   CORS - Is it a client-side thing, a server-side thing, or a transport level thing? [duplicate]   Reason: Credential is not supported if the CORS header ‘Access-Control-Allow-Origin’ is ‘*’   CORS error on request to localhost dev server from remote site  ","categories": ["website"],
        "tags": ["cors","website","preflight request","same origin","chrome","golang","gin"],
        "url": "/website/website-cors/",
        "teaser": null
      },{
        "title": "Goroutine 與 Channel 的共舞",
        "excerpt":"Preface  在了解 Channel 之前，我們需要先了解一些基本的概念  如果你已經很熟悉這些概念，可以直接跳到 Introduction to Golang Channel 部分   Synchronous vs. Asynchronous I/O  Blocking Send  在 receiver 收到資料之前，sender 不能在傳資料   Blocking Receive  在下一個資料送抵之前，receiver 會一直 block   Non-blocking Send  一直送資料，不管你有沒有收到   Non-blocking Receive  一直收資料，不管你有沒有送   Buffer  buffer 是用於暫存資料的一個空間，此時的資料是屬於 尚未處理過的      c.f. 資料庫 - Cache Strategies 與常見的 Solutions | Shawn Hsu    Zero Capacity Buffer  亦即沒有一個暫存空間，這會導致在 receiver 處理完資料之前，sender 必須等待(也就是所謂的 rendezvous)  在這個情況下，是屬於 blocking I/O   Bounded Capacity Buffer  bounded buffer 指的是這是一塊大小大於 0 的空間  當 buffer 已經填滿的情況下 sender 如果繼續送資料有可能導致     錯誤   或是直接扔掉   Unbounded Capacity Buffer  unbounded buffer 的大小為無上限(端看你機器有多大的 memory size)  但通常這個狀況是不建議的  我想理由也很簡單  如果說資料來不及消化，輕則 application crash 重則你的電腦掛掉   在這個情況下，sender 是不需要進行等待的   Ring Buffer  ring buffer 是一種特殊的資料結構，用以處理固定大小的資料  其實作通常是使用 circular queue，並頭尾相連   ring buffer 的好處如下     存取的時間複雜度為 $O(1)$, 而 linked list 則為 $O(n)$   ring buffer 空間大小固定，可以較省空間   當取出 data, 其餘的資料不需要做移動   ring buffer 由於頭尾相連的特性，因此在實作上需要注意一個細節  也就是我要怎麼分別 queue 是滿的還是空的 的情況  因為這兩種情況，他的 head 是等於 tail 的  這裡有幾個不一樣的判斷方法供參考   Counter Approach  最簡單的方法之一，既然 ring buffer 是固定大小的，那我加一個 counter variable 紀錄當前有多少 data 在 buffer 裡面就可以判斷是否為空值或者是已經滿了  唯一的缺點就是，多執行緒的情況下 counter variable 需要注意 race condition(可以用 mutex lock)   另一種的計數方法是分別紀錄讀寫次數，相差多少即為目前資料數量  不過我覺的既然都要記次數了，就沒必要紀錄兩次就是   Last Operation Approach  當 read/write index 相等的同時，我們知道會有兩種狀況  而紀錄最後的操作為何可以幫助我們分辨      最後的操作為 write :arrow_right: buffer 滿了   最後的操作為 read :arrow_right: buffer 為空   不難想到它也需要一個 variable 紀錄 last op  缺點也同上一個，多執行緒下需要進行 lock 保護資料   Store only Size - 1 Element  ring buffer 一定會有的資料結構是，一個 read index 一個 write index  基本的 ring buffer 概念, 有資料才能讀 :arrow_right: write index 相較於 read index 一定會比較後面(資料寫進去之後你才能讀取)  當 write index 超過 read index 的時候，就代表滿了 嗎  對 但是 buffer 滿的同時，你也不小心蓋掉了一個 element，而且是還沒有讀取過的   所以我們只能在它滿之前判斷出來，阿我們也不能用 read index == write index(因為它包含了兩種語意)  所以，要在 buffer 還剩一個空位的時候判斷，這時候 write index + 1 = read index  公式為 (read index - 1) % length == write index  而此時 write index 上面是沒有資料的  只能儲存 size - 1 個資料的原因在這   舉個例子, ring buffer 的大小為 5                          start                                                       index       0       1       2       3       4                         read write                                           寫了 4 筆資料之後，圖會長這樣                          start                                                       index       0       1       2       3       4                         read                               write           注意到此時 index 4 上面是沒有資料的  用公式計算是否已經滿了 (0 - 1) % 5 == 4 :arrow_right: true      也可以動手算算看，把 start 位置定在任意位置，ring buffer 不限制你從哪裡開始寫資料    Mirror Approach  維基百科上面寫的太難了  想的簡單點，它跟 Counter Approach 很像  除了 read/write index 以外，它引入了 read/write pointer 分別紀錄目前寫到了哪裡  你說 index 不就可以紀錄當前位置了嗎？ 問題是 read index == write index 的判斷是語意不清楚的   Mirror Approach 說，我讓 pointer 紀錄的位置長達 2n  也就是 read/write index 的區間為 0 到 n - 1，而 read/write pointer 的區間為 0 到 2n - 1  每次的寫入讀取，index 與 pointer 都往後移動一個位置  所以你可以把 pointer 想像成，目前寫了多少資料進去 ring buffer 裡面  亦即 read pointer 就是 已經讀取個數，而 write pointer 為 已經寫入個數  那麼，如果 read pointer == write pointer, 換成中文的意思就是，已經讀取個數等於已經寫入個數  不就是 counter approach 的思想了嗎      read/write pointer 只會一直遞增，當超過 2n 的時候，就回到 0  read/write pointer 最多只會相差 n  實際上可用空間只有 0 ~ n - 1 而已，pointer 只是起到紀錄的作用，不代表可以用到 2n 這麼多空間    兩個 condition 合併起來，當 read index == write index &amp;&amp; read pointer == write pointer 的時候，是不是就表明，ring buffer 已經滿了  當 read index != write index &amp;&amp; read pointer == write pointer 的時候，代表 ring buffer 為空   如果 ring buffer 長度為 2 的次方，可以簡化判斷式為 write pointer == (read pointer xor n)     n 為 2 的次方，亦即在二進位表示法中，只會有一個 bit 會有資料而已，所以如果 read/write pointer 只差一個 bit 就代表目前讀寫相差 n 個 element    這個方法的好處就是，不會像是 Store only Size - 1 Element 有空間沒利用到  Mirror Approach 可以完整利用到全部空間      可參考 2020q1 Homework (期末專題)    Inter-Process Communication     Shared Memory  顧名思義，是指兩個 process 共享一塊記憶體空間，將資料放進去這塊空間，從空間將資料讀出來的一個作法  也因為你是共享 一塊空間, 所以非常適合用於傳輸大量資料   kernel 僅須在建立 shared memory 的一開始介入，後續可以讓 process 之間處理  也因此，shared memory 的速度較快速(因為不用 kernel 幫忙)      process 的本質是與其他 process 相互隔離的，因此在實作上會相對困難，弄不好會有安全問題    Message Passing  使用 message passing 的機制可以讓 process 之間互相 同步(synchronize)  也因為它並不是採用共享空間的作法，因此 message passing 也可以應用在 distributed system 上面   message passing 基本上都會實作兩個 function     send   receive   Direct Communication  process 之間溝通的 communication link 為自動建立的  communication link 的建立是透過 kernel 完成的  與 shared memory 不一樣的是，message passing 每次都必須要經過 kernel   舉例來說  1 2 P: send(Q, message) Q: receive(P, message)   兩個 process P 以及 Q, 分別透過 send 與 receive 進行溝通  這樣的溝通方式稱之為 symmetric communication   另一種則是 asymmetric communication  也就是  1 2 P: send(Q, message) Q: receive(message)  與 symmetric communication 不同的是，receiver 並不知道訊息是從何而來的  它指關心有沒有收到資料而已   我就好奇啦，為什麼要區分這兩種方式？ 它又有什麼好處  asymmetric 的方式不會帶 message source, 意味著資料量可以減少  別忘了，message passing 是透過 kernel 進行處理的，少幾個 bit 都能夠一定程度的減少 overhead   Indirect Communication  與直接建立 communication link 不同，你也可以透過 mailbox 或是 port 的方式進行溝通  同樣的，這些基礎設施都是 kernel 幫忙建立維護的   P 將資料放到 mailbox 裡面，Q 則從 mailbox 裡將資料取出  你說這不就是 shared memory 嗎？  對 但是這塊 memory 並不是 process 自己處理的，是 kernel 負責維護的，所以它算是 message passing                            Shared Memory       Message Passing                       Communication       透過一塊共享空間       透過 kernel 提供的 message passing 設施                 Data Size       大       小                 Location       適用於同一台機器       適用於分散式系統或同一台機器                 Speed       快       慢                 kernel Intervention       kernel 不介入       kernel 每次都介入處理           Introduction to Golang Channel  多執行緒下溝通的方式，就如同前面所提到的 shared memory 或者是 message passing  Golang 作為一個強大的語言，它建議我們，可以採用 channel 的方式進行溝通，共享資料  所以 channel 本身是用來進行 資料的傳遞, 切記不要將它拿來當作 storage 用(會有效能問題而且它也不是設計來給你這樣用的)  接下來讓我們看看作為與 goroutine 相輔相成的 channel 實際上是如何運作的吧   How does Channel Work  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 type hchan struct {     qcount   uint           // total data in the queue     dataqsiz uint           // size of the circular queue     buf      unsafe.Pointer // points to an array of dataqsiz elements     elemsize uint16     closed   uint32     elemtype *_type // element type     sendx    uint   // send index     recvx    uint   // receive index     recvq    waitq  // list of recv waiters     sendq    waitq  // list of send waiters      // lock protects all fields in hchan, as well as several     // fields in sudogs blocked on this channel.     //     // Do not change another G's status while holding this lock     // (in particular, do not ready a G), as this can deadlock     // with stack shrinking.     lock mutex }   hchan 的結構中，包含了一個儲存資料的 circular queue(buf) 以及其他相關 variable(qcount, dataqsiz, elemsize, sendx 以及 receivex)  以及兩個 sender/receiver 的 wait queue(以 linked list 實作)  最後則是配上一個 mutex lock 保護 hchan 的資料   channel 儲存資料的方式是使用一個 circular queue 進行實作，也就是 Ring Buffer  golang 的 buf 是一個指向實際 ring buffer 的指標      ref: 深入理解 Golang Channel 結構    所以回到 hchan 的定義  因此我們可以發現到，sendx 與 receivex 分別代表 send(write) index 與 receive(read) index   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 func makechan(t *chantype, size int64) *hchan {     ...      mem, overflow := math.MulUintptr(elem.size, uintptr(size))     if overflow || mem &gt; maxAlloc-hchanSize || size &lt; 0 {         panic(plainError(\"makechan: size out of range\"))     }      // Hchan does not contain pointers interesting for GC when elements stored in buf do not contain pointers.     // buf points into the same allocation, elemtype is persistent.     // SudoG's are referenced from their owning thread so they can't be collected.     // TODO(dvyukov,rlh): Rethink when collector can move allocated objects.     var c *hchan     switch {     case mem == 0:         // Queue or element size is zero.         c = (*hchan)(mallocgc(hchanSize, nil, true))         // Race detector uses this location for synchronization.         c.buf = c.raceaddr()     case elem.ptrdata == 0:         // Elements do not contain pointers.         // Allocate hchan and buf in one call.         c = (*hchan)(mallocgc(hchanSize+mem, nil, true))         c.buf = add(unsafe.Pointer(c), hchanSize)     default:         // Elements contain pointers.         c = new(hchan)         c.buf = mallocgc(mem, elem, true)     }      c.elemsize = uint16(elem.size)     c.elemtype = elem     c.dataqsiz = uint(size)      ... }     makechan 的詳細實作可參考 src/runtime/chan.go    make 一個 channel 相對的簡單  從上述可以看得出來，首先先計算出總共需要多少的記憶體，接下來 switch case 就是分別 malloc  值得注意的是，如果是 unbuffered channel, 他的 buf pointer 會指向自己(跟 GC 有關，待補)  它並不會使用 buf pointer 存取資料，取而代之的是會將 data 存於 sudog 上面      sudog, 為 g 的封裝，有關 g 的部份，可以參考 Goroutine 與 Golang Runtime Scheduler | Shawn Hsu       element size is zero 的情況會是，如果 array 或者是 structure 並未擁有任何 field 或 element 的情況下，其大小為 0  比如說 myStruct{} 的大小會是 0    Send on Channel  對於 buffered channel 來說，寫資料進 channel 很直覺  就是將資料寫進去 buf 裡面就可以了   對於 unbuffered channel 來說，由於 hchan 上面沒有任何儲存單個資料的位置  實際上他是放到 sudog 上面的 elem 上面   如果有人提前在 receive queue 裡面等待的話，把資料寫進去 buffer 裡面再讀出來就會顯得很多餘  因此，可以先做檢查，就像這樣(go/src/runtime/chan.go#209)  其中 ep 為指向資料的指標  1 2 3 4 5 6 if sg := c.recvq.dequeue(); sg != nil {     // Found a waiting receiver. We pass the value we want to send     // directly to the receiver, bypassing the channel buffer (if any).     send(c, sg, ep, func() { unlock(&amp;c.lock) }, 3)     return true }   寫資料進 buffer 就相對單純(go/src/runtime/chan.go#216)  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 if c.qcount &lt; c.dataqsiz {     // Space is available in the channel buffer. Enqueue the element to send.     qp := chanbuf(c, c.sendx)     if raceenabled {         racenotify(c, c.sendx, nil)     }     typedmemmove(c.elemtype, qp, ep)     c.sendx++     if c.sendx == c.dataqsiz {         c.sendx = 0     }     c.qcount++     unlock(&amp;c.lock)     return true }  if !block {     unlock(&amp;c.lock)     return false }  當還有空間的時候，先將特定位置 malloc 出來(qp)  將資料移動到目標位置上(typedmemmove)  最後再做 counter 更新，釋放 mutex lock(保護 channel object)   如果 buffer 滿了呢？ 它會失敗  Bounded Buffer 在 golang 的 context 的意思是 non-blocking  看看上面的 code  non-blocking :arrow_right: block 會是 false  又因為 buffer 滿了，所以它會跑到最下面的那一個 if  也因此它會 return false     blocking 的情況下，就是將自己塞進去 sender wait queue(sendq)  然後 gopark 等待被叫醒      gopark 想的簡單點就是被暫停，swap out 的概念         ref: How Does Golang Channel Works      那如果往 closed channel 塞資料會發生什麼事情？  它會 直接 panic(可參考 go/src/runtime/chan.go#204)  1 2 3 4 if c.closed != 0 {     unlock(&amp;c.lock)     panic(plainError(\"send on closed channel\")) }   Receive on Channel  基本上 receive 的過程跟 send 差不多  也是先檢查 sendq 有沒有人存在，如果有，就直接從他的 sudog 拿資料並返回 go/src/runtime/chan.go#513  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 if c.closed != 0 {     if c.qcount == 0 {         if raceenabled {             raceacquire(c.raceaddr())         }         unlock(&amp;c.lock)         if ep != nil {             typedmemclr(c.elemtype, ep)         }         return true, false     }     // The channel has been closed, but the channel's buffer have data. } else {     // Just found waiting sender with not closed.     if sg := c.sendq.dequeue(); sg != nil {         // Found a waiting sender. If buffer is size 0, receive value         // directly from sender. Otherwise, receive from head of queue         // and add sender's value to the tail of the queue (both map to         // the same buffer slot because the queue is full).         recv(c, sg, ep, func() { unlock(&amp;c.lock) }, 3)         return true, true     } }   不然就是讀取 ring buffer(go/src/runtime/chan.go#537)  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 if c.qcount &gt; 0 {     // Receive directly from queue     qp := chanbuf(c, c.recvx)     if raceenabled {         racenotify(c, c.recvx, nil)     }     if ep != nil {         typedmemmove(c.elemtype, ep, qp)     }     typedmemclr(c.elemtype, qp)     c.recvx++     if c.recvx == c.dataqsiz {         c.recvx = 0     }     c.qcount--     unlock(&amp;c.lock)     return true, true }  if !block {     unlock(&amp;c.lock)     return false, false }   blocking 的情況下，將自己塞入 receiver wait queue(recvq)        ref: How Does Golang Channel Works      值得注意的是，讀取 closed channel 並不會造成 panic  根據 go/src/runtime/chan.go#513  1 2 3 4 5 6 7 8 9 10 11 12 13 if c.closed != 0 {     if c.qcount == 0 {         if raceenabled {             raceacquire(c.raceaddr())         }         unlock(&amp;c.lock)         if ep != nil {             typedmemclr(c.elemtype, ep)         }         return true, false     }     // The channel has been closed, but the channel's buffer have data. }  我們可以發現，當 channel 被 closed 的時候，他的返回值會被清空(typedmemclr)  所以你讀到的數值就是空值(它會根據型態自動轉型, e.g. false, 0)   Explicit Close Channel  當你不需要 channel 的時候，可以使用 close(channel) 來關閉 channel  不過通常來說你不需要手動關閉 channel   根據 A Tour of Go - Range and Close 所述      Channels aren’t like files;   you don’t usually need to close them.   Closing is only necessary when the receiver must be told there are no more values coming,   such as to terminate a range loop.    所以你不用 close 它，GC 會處理的   Share Memory by Communicating  Do not communicate by sharing memory; instead, share memory by communicating.   一般來說，當你要在不同執行緒共享資料(e.g. Shared Memory 或者是 Message Passing)，會順便加一個 mutex lock 的互斥鎖，確保在多執行緒的狀況下，不會出現 data race 等等不可預期的情況  golang 的 channel 提供了一個不同的思路   channel 在設計上，就根本的解決了 data race 的問題  在單位時間內，只會有一個 goroutine 有辦法存取資料  多個 goroutine 競爭要拿資料的時候，他們都會在 wait queue 等待  一次只會取一個 goroutine 出來取資料   你說 channel 還不是要用 mutex lock 保護  這樣做有什麼好處嗎  我個人是覺的透過 channel 的機制，可以讓你撰寫出容易理解並維護的程式  可以參考 Producer Consumer Example      注意到雖然 channel 可以避免 explicit locking  但未必 locking 是一個不好的選項，在實作時，應當就狀況考慮相關 trade-off    Select on Channel  select 用於在多個 channel 之間，選擇其中一個並 process  他的工作流程如下     對於所有 channel 進行洗牌(確保公平)   針對洗牌後的 channel 一個一個檢查看是否已就緒(ready)            檢查每個 channel 的 sendq 以及 recvq 就知道 channel ready or not 了           Block on all channel            如果 select 的 channel 們都被 block 住，我要怎麼知道哪個 channel 已經好了？   借用第二點的知識，我們知道當 sudog 出現在 wait queue 當中就代表該 channel 準備好了   所以這裡的檢查方式超暴力的，new 一個 sudog 把它塞進去該 channel 的 wait queue 裡面   如果我下一次進去該 channel，看到新的 sudog 還在裡面就代表它還在 block, 反之則 ready      可是這樣不就沒用了嗎？ 因為我檢查也是看 wait queue 阿？   如果是 select send, 那就把 sudog 加到 recvq, 依此類推   所以它可以正確的辨別 channel 的狀態這點是不用擔心的           Unblock on channel            前面為了要辨別哪個 channel 已經準備好了，我們將 sudog 加到 每一個 channel wait queue 上面，這裡，要 undo   走訪每個 channel，把 sudog dequeue 出來的同時，紀錄下被選中的幸運 channel   等到都處理完了之後，就可以針對該幸運 channel 做 send/receive    並且回傳 select 選中的 channel index           Nil Value from Channel  從 channel 拉資料的時候你需要同步檢查 channel 是否已經被關閉  也就是   1 2 3 4 5 value, ok := &lt;- a.ch  if !ok {     panic(\"channel is closed\") }   避免你直接存取 value 的時候，因為他是 nil 而導致 panic   Producer Consumer Example  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 package main  import (     \"fmt\"     \"math/rand\"     \"time\" )  const (     size = 100 )  type Job struct {     Target int }  func producer(jobChannel chan Job, statusChannel chan bool) {     for status := range statusChannel {         time.Sleep(1 * time.Second)          if status {             jobChannel &lt;- Job{Target: rand.Intn(100)}         }     } }  func consumer(id int, jobChannel chan Job, statusChannel chan bool) {     for job := range jobChannel {         success := rand.Intn(100) % 2 == 1          if success {             fmt.Printf(\"%3d finished the job\\n\\n\", id)             statusChannel &lt;- true         } else {             fmt.Printf(\"\\t%3d failed the job\\n\", id)             jobChannel &lt;- job         }     } }  func main() {     jobChannel := make(chan Job)     statusChannel := make(chan bool)     defer close(jobChannel)     defer close(statusChannel)      go producer(jobChannel, statusChannel)     statusChannel &lt;- true      for i := 0; i &lt; size; i++ {         go consumer(i, jobChannel, statusChannel)     }      select{} }   這是一個簡單版本的 producer consumer 的 goroutine 與 channel 的實作  其中，producer 負責生成一個 task 而 consumer 要嘗試解決這個 task  為了簡化程式，在 consumer 當中，透過簡單的擲骰子用以判斷 task 成功被解決與否   在這個例子裡面，是使用 unbuffered channel :arrow_right: 所以整隻程式會是 Blocking Send 與 Blocking Receive  另外不一樣的是，當某個 goroutine 失敗的時候，它必須要將原本的 task 讓出來給別人嘗試解決，因此需要額外的 statusChannel 紀錄說目前問題被解決了沒  statusChannel &lt;- true 必須寫在 go producer 後面的原因是因為，必須先要有人接收資料，才能開始送資料(不然會 all goroutines are deadlock)  由於本例是以 unbuffered channel 的方式撰寫  因此，當 consumer 的數量小於等於 1 的時候，將會觸發 deadlock，因為當 failed 的時候，重新 enqueue 寫進去 channel 的資料將沒有人可以讀取(all goroutines are asleep)   main 函數裡面的 select{} 是用以阻塞 main goroutine  因為所有主要執行程式都是以 goroutine 的方式下去執行，main 函數將會直接 return(它不會等待其他 goroutine)   注意到取出 channel 的資料有兩種寫法  第一個是範例中，for-range 的寫法是 unblocking 的  另一種是你比較熟悉的  1 2 3 4 5 6 7 8 9 for {     select {     case job, ok := &lt;- jobChannel:         // do something      default:         // nop     } }  這種寫法適用於你有多個 channel 需要接收資料  需要注意的是，加了 default 才是 non-blocking   跑起來的結果如下  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27          19 failed the job          31 failed the job          26 failed the job          27 failed the job  21 finished the job           28 failed the job          22 failed the job  29 finished the job           23 failed the job  30 finished the job           24 failed the job  34 finished the job           32 failed the job          25 failed the job  33 finished the job           36 failed the job          35 failed the job  37 finished the job            2 failed the job          45 failed the job  38 finished the job   References     Share Memory By Communicating   How to implement non-blocking write to an unbuffered channel?   Difference Between Shared Memory and Message Passing Process Communication   What is message passing technique in OS?   How are Go channels implemented?   How Does Golang Channel Works   Go channels on steroids   What are the uses of circular buffer?   which type’s size is zero in slice of golang?   環形緩衝區   ring buffer，一篇文章讲透它？   Empty a ring buffer when full   for select 與 for range   深入理解 Golang Channel 結構   7.1 channel   【我的架构师之路】- golang源码分析之channel的底层实现   2021q3 Homework3 (lfring)   Is it OK to leave a channel open?  ","categories": ["random"],
        "tags": ["golang","coroutine","goroutine","channel","ring buffer","shared memory","message passing","process","inter-process communication","makechan","send","receive","sudog","mutex","lock","select","producer consumer","blocking send","blocking receive","non-blocking send","non-blocking receive","counter approach","last operation approach","store only size - 1 element","mirror approach"],
        "url": "/random/golang-channel/",
        "teaser": null
      },{
        "title": "邁向 Angular 前端工程師之路 - Lifecycle Hooks",
        "excerpt":"Why do we Need Lifecycle Hook  Angular 在一開始初始化 component，他的生命週期就開始了  為了使 component 更人性化一點，舉例來說我們希望在特定的時間做特定的事情  當資料改變的時候，我們希望重新 render 畫面，這時候 lifecycle hook 就很好用了   Lifecycle Hook       ref: Lifecycle Hooks 學習筆記 (一)    Angular 提供多種的 lifecycle hook 供使用  你可以依照自己的需求，使用某些部份的 hook 即可，不需要全部都實作  @angular/core package 提供了一系列的 interface, 實作的時候，他的 function name 需要 prefix ng  也就是 OnChanges 會是 ngOnChanges   lifecycle hook 一個重點是，他的執行順序  上圖是所有的 hook 的執行順序，接下來就一個一個看下去吧   OnChanges  第一個執行的是，change detection hook, function 如其名，它會偵測所有 @Input 的異動  當有任何改變的時候，onChanges 就會被呼叫  看個例子  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 @Component({     selector: 'app-component',     templateUrl: './app.component.html',     styleUrls: './app.component.scss' }) export class AppComponent implements OnChanges {     @Input() currentBalance: BigNumber;      ngOnChanges(changes: SimpleChanges): void {         if (         changes['currentBalance'] != null &amp;&amp;         changes['currentBalance'].currentValue !== null         ) {             console.log(changes['currentBalance'].previousValue)             console.log(changes['currentBalance'].currentValue)         }     } }   onchange function 吃一個參數，SimpleChange 實作 SimpleChanges interface, 定義如下  1 2 3 4 5 6 7 class SimpleChange {   constructor(previousValue: any, currentValue: any, firstChange: boolean)   previousValue: any   currentValue: any   firstChange: boolean   isFirstChange(): boolean }   你可以透過檢查 changes 有沒有包含你的 input name, 判斷說當下這個值有沒有改變   要注意的是，如果你的 component 並不包含任何的 @Input, 即使你實作了 onChanges, 它也不會被呼叫   OnInit  跟 constructor 不同的是，onInit hook 被呼叫的時候，component 已經完成初始化了   constructor 不應該做任何讀取設定資料等等的  因為有可能初始化還未完成，因此這些事情適合等到 onInit 的時候處理  像是 @Input 在 constructor 當中是無法存取到的   與 constructor 相同的是，onInit 也只會被呼叫一次   constructor 就我的經驗來看，只會放 Dependency Injection 相關的初始化而已  像是 variable 的初始化或者是你有 Subject 或 Observable 之類的要定義，基本上都會在 onInit  for example  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 @Component({     selector: 'app-component',     templateUrl: './app-component.html',     styleUrls: './app-component.scss' }) export class AppComponent implements OnInit {     @Input() userState$: Observable&lt;UserState&gt;;      ngOnInit(): void {         this.userState$.pipe(filter((state) =&gt; !!state)).subscribe((state) =&gt; {         if (state.Token !== null) {             this.router.navigateByUrl('/');         }     });   } }     DoCheck  我們不是有了 onChanges hook 了嗎？ 為什麼還需要一個 change detection hook  仔細看 onChanges 的定義你會發現，它只會對 Input 有反應，亦即如果今天你要偵測的對象不是 Input, 那 Angular 就不會有反應  借用 Why do we need ngDoCheck 解答  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 @Component({    selector: 'test-do-check',    template: `       &lt;div [innerHtml]=\"obj.changer\"&gt;&lt;/div&gt;    `,     changeDetection: ChangeDetectionStrategy.OnPush }) export class TestDoCheckComponent implements DoCheck, OnInit {     public obj: any = {        changer: 1     };      private _oldValue: number = 1;      constructor(private _changeRef: ChangeDetectorRef){}      ngOnInit() {        setInterval(() =&gt; {           this.obj.changer += 1;        }, 1000);     }      ngDoCheck() {        if(this._oldValue !== this.obj.changer) {           this._oldValue = this.obj.changer;              //disable this line to see the counter not moving            this._changeRef.detectChanges();        }     } }   要注意的是, doCheck 是很昂貴的  為了要偵測這些 Angular 本身無法偵測到的事件  它必須以一個相對麻煩的方式做到  doCheck 有兩種呼叫時機     在 每一次 onChanges 之後   在 onInit 之後   也因此，doCheck 非常的耗資源  非常不建議在專案中大量使用，不然你的 user 會氣死   AfterContentInit, AfterContentChecked, AfterViewInit, AfterViewChecked  參考 Content vs. View 以及 Init vs. Checked Hooks  其執行時間基本上就是以上的排列組合  以個人的經驗來說，是很少會需要使用到   注意到由於 checked hook 昂貴的代價，使用時需要小心   OnDestroy  在 Angular 完全刪除 component 或 directives 之前呼叫，僅一次  類似 destructor, 你可以在這裡中止 subject 之類的  舉例來說   1 2 3 4 5 6 7 8 9 10 11 12 @Component({     selector: 'app-component',     templateUrl: './app-component.html',     styleUrls: './app-component.scss' }) export class AppComponent implements OnDestroy {     historyReady$: BehaviorSubject&lt;boolean&gt; = new BehaviorSubject&lt;boolean&gt;(false);      ngOnDestroy(): void {         this.historyReady$.complete();     } }     Content vs. View  Content  Angular 提供了 content projection 的機制，讓你可以從 parent 送一些 content 到 child  這個 content 不限於 html, content, text 也都可以   亦即 content 是還未被處理過得資料  所以 hook 的部份才有分兩種     View  view 則相對清楚，也就是最終呈現出來的畫面  你的 code 可能這樣寫   1 2 3 4 5 &lt;div&gt;   &lt;h1&gt;{{ user.name }}&lt;/h1&gt;   &lt;p&gt;Email: {{ user.email }}&lt;/p&gt;   &lt;p&gt;Bio: {{ user.bio }}&lt;/p&gt; &lt;/div&gt;   最終的 view 則是長這樣   1 2 3 4 5 &lt;div&gt;   &lt;h1&gt;John Smith&lt;/h1&gt;   &lt;p&gt;Email: john.smith@example.com&lt;/p&gt;   &lt;p&gt;Bio: I'm a software developer from San Francisco.&lt;/p&gt; &lt;/div&gt;   根據 Angular Glossary - View 的定義  view 是由 component class 加上 html template 組成的  view 可以被動態的調整，比如說 user 點了按鈕會出現其他畫面這樣       References     Lifecycle hooks   Angular Lifecycle Hooks: ngOnChanges, ngOnInit, and more   Difference between Constructor and ngOnInit   AfterViewInit, AfterViewChecked, AfterContentInit &amp; AfterContentChecked In Angular  ","categories": ["angular"],
        "tags": ["typescript","hooks"],
        "url": "/angular/angular-hooks/",
        "teaser": null
      },{
        "title": "邁向 Angular 前端工程師之路 - Obfuscation",
        "excerpt":"How Frontend Application Run in Real Life       ref: [javascript]如何用chrome,ie去debug javascript    如果你有曾經打開 F12(Web Developer Tool), 你可能會看過上圖的東西  仔細觀察你會發現，不只 html, css 都可以被看見，甚至是 JavaScript 原始碼也可以看得到  所以，我們可以做一個簡單的推論  你所看到的網頁基本上都是由這些 source code 跑出來的   這時候問題來了  為什麼我看得到 source code  根據過往的經驗，我應該只需要 binary 就可以跑了不是(想想你下載一些遊戲，是不是都是 .exe，頂多在搭配上圖片等 assets)  為什麼網頁的邏輯卻是直接將 source code 給 client 呢   Interpret vs. Compiled Language       ref: Compiler vs Interpreter – Difference Between Them    直譯式語言與編譯式語言最大的差別是在  編譯式出來的會是 machine code, 而直譯式的會是 intermediary code(中間碼)   Compiled Language  如 Golang, C, C++ 等語言都是屬於編譯式語言，所有的 source code 編譯過後會產生一個執行檔(binary)  而這個執行檔，會綁定在特定的作業系統(i.e. windows, ubuntu)以及系統架構(x86_64, arm64)，也因此 windows 上面 compile 出來的 .exe，ubuntu 沒辦法開啟   而通常編譯式語言跑起來會比 Interpret Language 來的快許多  原因是直譯式的中間碼還需要在解析一次，因此跑起來會比原生的還要慢      可參考 Cross Compile    Interpret Language  Python, JavaScript, Java 等等的是屬於直譯式語言，也因此他們 “直譯(interpret)” 出來會是 bytecode 的形式  比如說 Java “直譯(interpret)” 出來會是 Java bytecode, 之後在跑在 JVM 上面   也因為直譯式語言並不會直接產出 machine code, 因此，直譯式並不會有平台或系統的限制  只要執行環境看得懂 bytecode, 再來環境可以跑在 target os 上面  就可以很輕鬆的達到跨平台的特性   不過缺點很明顯，因為還要將 bytecode 翻譯成 machine code，因此在執行速度上會稍嫌緩慢  為了提高執行速度，JIT Compiler 出現了！  針對重複執行的程式片段，將它編譯成 machine code 以大幅度的提高執行速度     好，所以為什麼 JavaScript 還是要以 source code 的方式運行在瀏覽器上面呢  bytecode 不好嗎？   對，不好  記得上面說 bytecode 需要在轉換成 machine code 執行嗎？  也就是說所有的瀏覽器都必須要看得懂某種格式的 bytecode, 並且所有瀏覽器都必須使用相同的執行環境(像是 JVM) 來執行 bytecode  理想很豐滿，現實很骨感  雖然這是一個不錯的解法，但是由於歷史原因，要讓所有開發廠商全部採用相同格式本身就是一件很困難的事情(而且還牽扯到錢)  也因此 bytecode 這條路是行不通的，所以目前的主流作法是將 source code 給 client  讓瀏覽器自行決定是要怎麼執行，反正 JavaScript 的語言標準是通用的，跑出來肯定是相同的   Introduction to Webpack  既然要把 source code 給 client  那豈不是要上傳一堆東西？        ref: Heaviest Objects In The Universe    看看這精美的 node_modules, 少說幾 MB, 多則好幾 GB  先不說光是下載 package 載入都要一段時間了，我要怎麼把 package 安裝到 client 的瀏覽器上面?  總不能在上面跑 npm install 吧 這不太合理   把所有 node_modules 都放上伺服器讓 client 下載？  hmm not a good idea  但它是一個好方向   如果能有一種方法將所有第三方的 package 縮小塞在一起給 client  是不是就完美了   Webpack     webpack 可以將你的 static module 全部打包在一起變成一個檔案  注意到 webpack 僅有打包的功能，所以它並不會幫你優化程式碼之類的  話雖如此，webpack 在進行 production build 的時候會將程式碼內的空白以及換行全部拔掉  所以實際上 webpack 擁有可以縮小檔案大小的能力，但也僅此而已   要怎麼樣將 application 中所有用到的 package 打包起來呢？  直觀點的思考會是，一層一層的追下去，看說你引用了哪些，而那些 package 又引用了哪些  最後將這些做成一張地圖，稱之為 dependency graph      對於 webpack 的說明就到這裡，詳細的我會在開一篇文章下去探討  本文就先專注在 obfuscation 本身上面    Plain Text of Source Code?  終於，你成功的把 source code 全部放上去供 client 下載了  結果你發現你的 api key 居然赤裸裸的躺在別人的瀏覽器上面  這肯定是不行的   如果你 hard code 一些機密資訊在上面(e.g. 密碼，api key … etc.)  請愛用 environment variable  這樣就可以有效的避免資訊洩漏   但如果你還是不想讓其他人看到你寫的 code  最簡單的方式就是將它加密對吧  加密過後的 ciphertext 除非擁有解密鑰匙，不然打開只會看到一團亂碼  不過等等… 解密鑰匙要怎麼給 client?  非對稱式加密感覺不錯對吧，你有一把鑰匙，我有一把鑰匙(只需要交換公鑰就可以了也挺安全的不是)  但是你的 client 要跟全世界的 client 共用一把鑰匙？ 還是每個 client 都用自己獨一無二的 key?  這樣做起來不覺得很麻煩嗎  只是個 source code 不用那麼麻煩吧？ 而且做起來不符合成本   Introduction to Obfuscation  傳說中程式設計師要搞人的話，會在 source code 這邊動一些手腳  比如說  1 2 3 4 5 6 #include &lt;stdio.h&gt;  int main(int argc, const char *argv[]) {     printf(\"Hello World!\");     return 0; }  看起來沒問題，實際上  1 2 3 4 5 6 7 8 a.c: In function ‘main’: a.c:4:27: warning: `\\U0000037e' is not in NFC [-Wnormalized=]     4 |     printf(\"Hello World!\");       |                           ^ a.c:4:27: error: expected ‘;’ before ‘;’     4 |     printf(\"Hello World!\");       |                           ^       |  使用希臘文分號(U+037E, https://www.compart.com/en/unicode/U+037E)代替原本的分號  或者是將 1, I, l 混用等等的都有看過   把 source code 混淆好像是個好方法對吧  在不影響程式執行結果的情況下，將 source code 混淆使得人類無法輕易的破解     為了避免能夠輕易的讀取到 source code 的內容，主流的方法就是採用 obfuscation 的方式  接下來來看看如何在 Angular 的專案當中使用 obfuscation 吧   Angular Webpack Builder  首先，obfuscation 是針對最終的檔案，所以我們要先將 Angular 本身打包起來  並針對打包後的檔案，進行混淆的操作   所以，還是 Webpack  我們可以使用 @angular-builders/custom-webpack  基本上他是基於 @angular-devkit/build-angular 在往上做 custom webpack  也就是會將你自定義的 webpack config 跟原本定義的合併在一起      但像是 dev-server 它就不是使用 build-angular:dev-server 的 config    在 angular.json 裡面找到  1 2 3 4 5 6 7 8 9 \"architect\": {     \"build\": {      },     \"serve\": {      },     ... }   architect 裡面分別對應了，當你執行 ng build 或 ng serve 的時候，它應該要用哪一種設定  每個 entry 裡面都會有長的像下面的東西  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 \"builder\": \"@angular-devkit/build-angular:browser\",          &lt;--- \"options\": {     \"outputPath\": \"dist/app\",     \"index\": \"src/index.html\",     \"main\": \"src/main.ts\",     \"polyfills\": [\"zone.js\"],     \"tsConfig\": \"tsconfig.app.json\",     \"inlineStyleLanguage\": \"scss\",     \"assets\": [\"src/favicon.ico\", \"src/assets\"],     \"styles\": [\"src/styles.scss\"],     \"scripts\": [],     \"customWebpackConfig\": {     \"mergeRules\": {         \"externals\": \"replace\"     }     } }  多數的設定檔都是 anglar 產生的，我們要關注的只有 builder 以及等等會加的 customWebpackConfig     builder 要改成 custom webpack(因為我們要自訂 obfuscation 的設定)   然後還需要 webpack config 的路徑   所以 angular.json 會變成  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 \"builder\": \"@angular-builders/custom-webpack:browser\",          &lt;--- \"options\": {     \"outputPath\": \"dist/app\",     \"index\": \"src/index.html\",     \"main\": \"src/main.ts\",     \"polyfills\": [\"zone.js\"],     \"tsConfig\": \"tsconfig.app.json\",     \"inlineStyleLanguage\": \"scss\",     \"assets\": [\"src/favicon.ico\", \"src/assets\"],     \"styles\": [\"src/styles.scss\"],     \"scripts\": [],     \"customWebpackConfig\": {     \"path\": \"./webpack.config.js\",                              &lt;---     \"mergeRules\": {         \"externals\": \"replace\"     }     } }   至於 webpack config 裡面需要放入以下內容  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 // webpack.config.js  const JavaScriptObfuscator = require(\"webpack-obfuscator\"); module.exports = (config, options) =&gt; {     if (config.mode === \"production\") {         config.plugins.push(             new JavaScriptObfuscator(                 {                     rotateStringArray: true                 },                 [\"exclude_bundle.js\"]             )         );     } };      這裡的語法是 CommonJS, 原生的 JS 需要到 ES6 才支援  詳細的 CommonJS 可以參考 什麼？！我們竟然有 3 個標準？ - 你有聽過 CommonJS 嗎？(Day9)    webpack 除了基礎的打包功能以外，還提供了許多的 plugin 套件供使用  webpack-obfuscator 就是主要的混淆套件  透過指令安裝  1 $ npm i --dev webpack-obfuscator   webpack config 做的事情簡單明瞭  當 production build 的時候，將 webpack-obfuscator 加入 webpack config 裡面  就這樣   做完以上，你所得到完成打包的 source code 就具備基礎混淆能力了！   References     Interpreted vs. compiled languages: What’s the difference?   Why are webpages deployed as JavaScript source code instead of compiled bytecode?   Does JavaScript compile to binary?   Possible to compile/encode JavaScript to binary to hide code?   Day29-JS模組化！(套件結合篇)   5 Methods to Reduce JavaScript Bundle Size   什麼？！我們竟然有 3 個標準？ - 你有聽過 CommonJS 嗎？(Day9)  ","categories": ["angular"],
        "tags": ["typescript","obfuscation","webpack"],
        "url": "/angular/angular-obfuscation/",
        "teaser": null
      },{
        "title": "從 0 認識 Blockchain - Hardhat 全攻略",
        "excerpt":"Introduction to Hardhat       ref: NomicFoundation/hardhat    Hardhat 作為開發 smart contract 最受歡迎的整合開發環境，認識 hardhat 如何使用是有必要的  這篇文章當中，我會紀錄一路上我踩過的坑以及基本的 hardhat 使用   How Hardhat Works  基本上要測試 smart contract, 無非就是將它 deploy 到所謂的測試網路上面(e.g. Goerli, Sepolia)  但是每一次的測試都要這樣做，除了浪費時間之外，也浪費資源  hardhat 讓你可以在自己的電腦上跑一個 local blockchain, 測試以及部署都可以在本機完成，速度也比較快      testnet 節點的執行仰賴著一群熱心腸的開發者們的電腦，我們不希望你將 半成品 上傳並測試  使用 testnet 算是正式 release 到 mainnet 之前的手段      hardhat 主要是透過 runner 來執行各項 task, 包含像是 compile, test, deploy 等等的  除了內建的 functionality, 你可以透過安裝 plugin 擴充功能(e.g. hardhat-contract-sizer)   Install Hardhat  1 $ yarn add --dev hardhat  再來使用  1 $ yarn hardhat  開啟一個新的 Hardhat 專案   Command-line Completion  Hardhat 的指令基本上都圍繞著 yarn hardhat xxx(其中 xxx 為 task 的名字)  每次都要打這麼多，可以安裝一個全域套件縮短名字  1 $ yarn global add hardhat-shorthand  之後你就可以使用 hh 代替 yarn hardhat 了     另外一個套件，可以做到自動補全(列出所有可以執行的 task)  1 $ hardhat-completion install        ref: Command-line completion    Hardhat Config  基本上，所有 Hardhat 相關的設定，都是寫在 hardhat.config.ts，檔案會寫在 project root  而這裡除了定義 compiler version, private key 等等的，還有一個很重要的東西   所有需要用到的 plugin, 都要在這裡 import，否則它不會出現在 available task 裡面  比如說你要用到 solidity coverage  在 hardhat.config.ts 當中必須要 import \"solidity-coverage\";   Solidity  1 2 3 4 const config: HardhatUserConfig = {     solidity: \"0.8.18\" } export default config  solidity 的區塊，指定了需要使用的 compiler version, 以上述例子，是 0.8.18   如果需要使用到多種 compiler, 可以這樣指定  1 2 3 4 5 6 7 8 9 const config: HardhatUserConfig = {     solidity: {         compilers: [             {version: \"0.8.18\"},             {version: \"0.6.6\"}         ]     } } export default config  會需要用到多種 compiler 的情形是，你可能引用的 package 他的 compiler 定義可能比較舊  多個 solidity 的檔案 source code 需要編譯的時候會用到這種寫法   Network  前面提到，Hardhat 預設是使用你電腦上面的 local blockchain 執行測試以及部署的  但是同時，它也支持與 testnet 互動，因此需要定義相關的設定資料      Hardhat 預設是連接 local blockchain  透過更改 defaultNetwork: \"sepolia\" 可以改變預設值    Hardhat 對於 testnet 的支援，僅有 JSON-RPC based network   1 2 3 4 5 6 7 8 9 10 11 const config: HardhatUserConfig = {     networks: {         sepolia: {             url: SEPOLIA_URL,             accounts: [PRIVATE_KEY],             chainId: 11155111,             gasPrice: 5 * 1000000000         }     } } export default config   上述是 sepolia 測試網路的連接設定  針對非 Hardhat 以外的設定，其中僅有 url 為必要的     url(required)            透過第三方提供的 RPC node 用以連接整個 blockchain network，可以使用像是 Alchemy 或是 Infura           accounts            因為是跟真實世界的 blockchain network 互動，所以你至少需要測試用的開發幣付交易費用(i.e. sepoliaETH), 因此你需要提供擁有 testnet ETH 的帳號供使用       上述程式碼當中的 PRIVATE_KEY 即為你的帳號私鑰，注意到提供的時候，前綴 0x 需要移除，僅保留後面的私鑰部份                  私鑰的洩漏會極大的影響你的財產，擁有私鑰的人可以不經過你的同意將全部的 ETH 轉移出去  請務必不要將 private key 上傳到任何地方  開發使用可以考慮用 environment variable 的方式            chainId            用以驗證 Hardhat 是否連接到對的網路           gasPrice            手動指定 transaction 需要使用多少 gas price           這裡就列出幾個供參考，完整的 option 可以參考 JSON-RPC based networks   NamedAccounts  namedAccounts 是 hardhat-deploy 的一個欄位  我們可以透過設定 namedAccounts 將 wallet 帳號跟一個名字綁定在一起   前面 Network 提到我們可以設定 accounts 欄位，用以提供擁有 testnet ETH 的帳號  而如果使用預設 Hardhat network, Hardhat 會自己生成一個大小為 20 個假帳號們，每個假帳號用有 1000 ETH 可以使用   1 2 3 4 5 6 7 8 const config: HardhatUserConfig = {     namedAccounts: {         deployer: {             default: 0         }     } } export const config   上述我定義了一個名為 deployer 的名字，它對應到 accounts array 的第 0 個帳號      accounts 就是上面講的陣列，它可以是預設 20 個假帳號陣列，也可以是你定義的陣列(如果 Network 範例裡提到的)    更進階，你也可以針對不同網路，指定不同帳號  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 const config: HardhatUserConfig = {     namedAccounts: {         deployer: {             default: 0,             1: 0,             // chainId 為 1(mainnet) 的時候，使用第 0 個帳號              4: '0xA296a3d5F026953e17F472B497eC29a5631FB51B',             // chainId 為 4(rinkeby) 的時候，使用這個帳號              \"goerli\": '0x84b9514E013710b9dD0811c9Fe46b837a4A0d8E0',             // network 名字為 goerli 的時候，使用這個帳號             // 其中 network name 必須跟 config 裡面 networks.&lt;name&gt; 的 name 一樣         }     } } export const config   完整詳細 config 設定可以參考 hardhat-deploy   Path  Hardhat 各項預設路徑分別為以下     contracts: 存放所有 smart contract 的實作   deploy: 預設 deploy script 位置   test: 預測測試 source code 位置   Deploy  與 smart contract 互動的前提是，合約必須部署到區塊鏈上  我們可以撰寫 deploy script 將部署的部份全部自動化   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 const deployMarketPlace: DeployFunction = async (   hre: HardhatRuntimeEnvironment ) =&gt; {   const { deployments, getNamedAccounts } = hre;   const { deployer } = await getNamedAccounts();   const { deploy } = deployments;    await deploy(\"Marketplace\", {     from: deployer,     args: [],     log: true,     waitConfirmations: 1,   }); };  deployMarketPlace.tags = [\"marketplace\"];  export default deployMarketPlace;   部署合約，你會需要     deployer: 簽署 transaction 的帳號   合約本體   上述 deploy function 先從 HardhatRuntimeEnvironment 裡面拿到兩個東西，分別為部署用的 function 以及帳號  利用 getNamedAccounts 可以拿到事先在 hardhat.config.ts 裡面定義的 namedAccounts  透過語法糖，直接取出 deployer  再來就是用 deploy() 直接上傳合約，from 定義為 deployer, 比較特別的是 waitConfirmations, 要等到區塊鏈上傳完畢，所以要等一個 block      透過設定 tag 你可以只 deploy 特定的 contract, 以這個例子就是 hh deploy --tags marketplace      注意到，如果你有多個 contract 需要 deploy, 且 contract 具有相依性  這時候 deploy 的順序就相當的重要了   考慮我的 NFT 練習專案，ambersun1234/nft  其中 IpfsNFT 依賴於 VRFCoordinatorV2Mock, 也因此 mock 必須要比 IpfsNFT 還要早 deploy  解法為 調整 deploy script 的檔案順序，只要將要先執行的 deploy script 擺在前面即可(可以透過調整 filename 改變檔案順序)     deploy 到 testnet 也是用同一組 script, 什麼都不用改  不過為了能將合約給紀錄下來，最終你會在 project root 這裡找到 deployments 的資料夾  不同的 testnet 會歸類在不同的資料夾下(e.g. deployments/sepolia, deployments/goerli)  這樣下一次再次使用 hh deploy --network xxx 的時候，它就會 reuse 已經 deploy 過得合約      其中 xxx 為你在 hardhat.config.ts 裡面定義的 network name(networks.&lt;name&gt;)    如果你希望上傳新的，必須帶上 --reset 的 flag, 這樣舊的合約紀錄就會從硬碟上刪除   Test  測試的部份，可以透過 --network 參數指定要在哪個網路上測試，要注意的是，預設的情況下，hh test 會將所有測試檔案都執行一次  你可以透過 --grep 的方式指定要跑哪些符合規則的測試  比如說  1 2 3 4 5 6 7 8 9 describe(\"marketplace\", () =&gt; {     beforeEach(() =&gt; {         ...     })      it(\"Should initialize successfully\", async () =&gt; {         ...     }) })  跑測試的時候可以這樣做  1 $ hh test --grep marketplace  符合規則的 function 會被執行，以這個例子 describe(\"marketplace\", () =&gt; {}) 會被執行      或者可以使用第三方 mocha-tags    為了要能夠在 hardhat network 下測試 smart contract, 必須要 deploy 到 local hardhat network  又因為我們不希望測試互相干擾，就會希望每一次的測試都必須是乾淨的環境  deploy 在 local hardhat network 耗費不高，速度也不會說很慢，但有沒有更好的作法？  hardhat-deploy 提供了一個 feature, 我們可以將 deployment 進行快照(i.e. snapshot), 如此一來，便不用每次都重新 deploy, 僅需要重新使用 snapshot 即可      針對 testnet 上的測試，由於是跟真實世界的網路互動，就不用 snapshot 了    1 2 3 4 5 6 beforeEach(async () =&gt; {     deployer = (await getNamedAccounts())[\"deployer\"];     await deployments.fixture([\"marketplace\", \"nft\"]);      marketplace = await ethers.getContract(\"Marketplace\", deployer); })  上述就是一個 fixture 的例子，使用 beforeEach function 在每一次執行測試的時候運行  其中，deployments.fixture 裡面擺 tag, 在這個例子就是等待 marketplace 與 nft contract deploy 到 local hardhat network  最後在使用 getContract 取得合約   testnet 的部份，由於我們已經儲存合約相關資料在 deployments 裡面了  所以，這個合約已經在 blockchain 上面運作了，就不需要 fixture, 直接拿 contract 就可以了(如下所示)  1 2 3 4 5 beforeEach(async () =&gt; {     deployer = (await getNamedAccounts())[\"deployer\"];      marketplace = await ethers.getContract(\"Marketplace\", deployer); })      有關測試的細部討論，可以參考 DevOps - 單元測試 Unit Test | Shawn Hsu    Task  我們執行的 hh deploy, hh test, 每一個都是 task, 而 Hardhat 也支援讓你自己寫 task   1 2 3 4 5 6 7 8 9 10 11 12 13 14 // task/balance.ts  import { task } from \"hardhat/config\"; import { HardhatRuntimeEnvironment } from \"hardhat/types\";  task(\"balance\", \"Get balance from address\")     .addParam(\"address\", \"wallet address\")     .setAction(async (args: any, hre: HardhatRuntimeEnvironment) =&gt; {         const ethers = hre.ethers;         const balance = await ethers.provider.getBalance(args.address);          console.log(args.address);         console.log(balance.toString());     });   透過宣告一個 task 建立一個新的 task  task 可以吃 3 個參數     task 的名稱，用於呼叫(e.g. hh balance)   task 的 description   function 定義(也可以用 setAction 下去定義，看你高興)   另外你也可以額外定義 cli 參數，以這個例子來看，balance task 要求一個 address 的參數  你可以用 hh help balance 查看該 task 的描述，其中會包含 parameter 的定義  1 2 3 4 5 6 7 8 9 10 11 12 $ hh help balance Hardhat version 2.14.0  Usage: hardhat [GLOBAL OPTIONS] balance --address &lt;STRING&gt;  OPTIONS:    --address     wallet address   balance: Get balance from address  For global options help run: hardhat help     1 2 3 4 5 6 7 async (args: any, hre: HardhatRuntimeEnvironment) =&gt; {     const ethers = hre.ethers;     const balance = await ethers.provider.getBalance(args.address);      console.log(args.address);     console.log(balance.toString()); }   如果你要使用 ethers，一般都會 import { ethers } from \"hardhat\", 但是 task 沒辦法這樣做因為 hardhat 需要初始化，還沒初始化完成就使用 instance 是辦不到的  @nomiclabs/hardhat-ethers 套件會將 ethers object 注入 HardhatRuntimeEnvironment 讓我們可以取得並使用  task function 的參數實作定義是有一定規則的，如以下所示  1 (taskArgs: TaskArgumentsT, env: HardhatRuntimeEnvironment, runSuper: RunSuperFunction&lt;TaskArgumentsT&gt;)  不要亂調整參數順序，不然你 hre 印出來會是 undefined      @nomiclabs/hardhat-ethers 擴充 ethers  hardhat-deploy-ethers 擴充 @nomiclabs/hardhat-ethers      最後要使用套件之前，必須將它在 hardhat.config.ts 中引入  1 2 3 // hardhat.config.ts  import \"./tasks/balance.ts\"   使用起來會長這樣  1 2 3 $ hh balance --address xxx --network sepolia xxx yyyyy   Hardhat Network  看到這裡想必你已經了解到 Hardhat 是運作在 local Hardhat network 之上的了  每一次執行 test 都是使用 local Hardhat network  但是每一次的執行，都是建立一個新的 network，用完即刪  有時候我們希望可以重複利用它，或者說跟前後端一起開發的時候，總不希望 contract address 一直變動   透過手動建立一個持久 Hardhat network 可以幫助我們完成這件事情  1 $ hh node  上述指令會在你的電腦上面建立一個 node，而它不會被刪除，直到你手動中止  同樣的 hardhat-deploy 也稍微的擴充了 hh node  當你執行一個新的節點的時候，會執行所有的 deploy script, 自動的部署所有 contract  你也可以用 --tags 只部署特定合約  1 2 3 $ hh node --tags marketplace or $ hh node --tags marketplace,nft   要讓你的 test script 能夠使用節點而不是額外建立新的，需要新增一個 network  1 2 3 4 5 6 7 8 9 const config: HardhatUserConfig = {     networks: {         localhost: {             url: \"http:localhost:8545\",             chainId: 31337         }     } } export default config   並且於測試的時候，指定 network  1 $ hh test --network localhost   如果你需要與節點互動，可以使用 hardhat console 的功能，一樣別忘了要指定 network  1 $ hh console --network localhost      console 也可以跟 testnet, mainnet 互動    Network Forking  Hardhat 也可以複製 mainnet 或是 testnet 到你的電腦上，讓你的 local network 擁有 mainnet/testnet 的狀態  當然，它並不是全部複製下來，只有部份的資料   1 2 3 4 5 6 7 8 9 10 11 12 const config: HardhatUserConfig = {     networks: {         localhost: {             url: \"http:localhost:8545\",             chainId: 31337,             forking: {                 url: ALCHEMY_RPC_URL             }         }     } } export default config   或者是手動指定  1 $ hh node --fork ALCHEMY_RPC_URL   Error: No Contract deployed with name  代表說 Hardhat 並沒有抓到 contract, 其原因是 deploy 順序的關係導致  修改 deploy script 的順序即可抓到正確的 contract   詳細可參考 Deploy   TypeError: ethers.getContract is not a function  Hardhat plugin hardhat-deploy-ethers 在 @nomiclabs/hardhat-ethers 之上擴充了 ethers 的相關 functionality  所以，單純的 ethers.js 並沒有一些功能如 getContract   為了能夠順利的在 Hardhat 當中使用 plugin 的擴充功能，需要將 hardhat-deploy-ethers 跟 @nomiclabs/hardhat-ethers 安裝在一起，這樣它才可以吃到全部的功能  我們則需要借助 package alias 的功能全部安裝在一起  簡言之  1 $ yarn add --dev @nomiclabs/hardhat-ethers@npm:hardhat-deploy-ethers   最終的 package.json 會長成這樣  1 2 3 4 5 6 7 8 9 {     \"devDependencies\": {         ...          \"@nomiclabs/hardhat-ethers\": \"npm:hardhat-deploy-ethers\"          ...     } }  如此一來就解決問題了   References     Learn Blockchain, Solidity, and Full Stack Web3 Development with JavaScript – 32-Hour Course   Command-line completion   What is the difference between getSigners and getNamedAccounts?   Install A NPM Package Under An Alias   Hardhat deploy TypeError: ethers.getContract is not a function   How can I use hardhat.ethers inside a typescript task?   hardhat-ethers   Hardhat Runtime Environment (HRE)   Creating a task   Hardhat Network  ","categories": ["blockchain"],
        "tags": ["blockchain","ethereum","hardhat"],
        "url": "/blockchain/blockchain-hardhat/",
        "teaser": null
      },{
        "title": "從 0 認識 Blockchain - 區塊鏈基礎",
        "excerpt":"Introduction to Blockchain  Blockchain 技術的概念，始於 2009 年 由 Satoshi Nakamoto 建立的 Bitcoin  根據 Bitcoin 白皮書 中所述     A purely peer-to-peer version of electronic cash would allow online  payments to be sent directly from one party to another without going through a  financial institution    Bitcoin 擺脫了必須透過中心化交易中心進行交易的概念  使用了 P2P 的技術, 建構了 decentralized network 使得交易雙方可以直接交易  透過建立不可改變的 transactions 可以取代傳統中心化第三方交易中心的功用 :arrow_right: 信任      白皮書: 某項重要政策或提議的正式發表書, 可參考 White paper    Introduction to Ethereum  但是 Bitcoin 終究只能做金錢相關的事情  Vitalik Buterin 他想把 Blockchain 這個技術擴展到更高的層次，他想讓人們可以用 Blockchain 打造 decentralized application(不僅限於金錢)  其中一個很重要的概念就是所謂的 smart contract(decentralized agreement)  於是他在 2015 年發表了 Ethereum 這個 project  其白皮書可以在這裡找到 Ethereum Whitepaper   說到底 Ethereum Blockchain 就是一個大的 Excel 表格，紀錄了盤古開天以來所有的 transaction 紀錄以及 data   Smart Contract  根據 Ethereum Whitepapaer      The intent of Ethereum is to create an alternative protocol for building decentralized applications,   providing a different set of tradeoffs that we believe will be very useful for a large class of   decentralized applications, with particular emphasis on situations where rapid development time,   security for small and rarely used applications,   and the ability of different applications to very efficiently interact, are important.    為了打造去中心化的服務，Ethereum 創造了一門語言(solidity)用以實作 smart contracts  而智能合約是由一系列指令組成的程式，運行在 Ethereum Blockchain 之上   智能合約總結來說只幹三件事     定義規則   驗證規則   自我執行規則   smart contracts 本質上就是 code, 一旦合約成功部屬至鏈上，就 幾乎沒辦法修改  基於這樣的特性，合約變得有保障，且因為所有鏈上的資料都是公開透明的，所有的狀態更新都受到 network 的監督      八卦是 smart contract 並不聰明    Transaction  對 blockchain 造成 任何狀態更新, 都會建立一個新的 transaction  狀態更新包括     上傳新合約   更改合約(smart contract 實作上可以用成 configurable, 亦即你可以後期調整某些參數)   更新合約儲存的資料   而每一筆 transaction 你都必須要支付費用  讓 network 願意幫你更新區塊鏈狀態(i.e. 給錢做事 使用者付費)   transaction 會紀錄     誰要更改狀態   更改了誰的狀態   更改的數值      有關 transaction 的介紹，可以參考 從 0 認識 Blockchain - Transaction 以及你該知道的一切 | Shawn Hsu    Anonymous Identity  鏈上的資料完全透明！？ 那我的個資怎麼辦？   為了與 Blockchain 互動，你必須要創立一個錢包帳號  不同的是，這裡用的 account 是完全匿名的  也就是說 即使 blockchain 都是公開透明的，其他人也無從得知帳號主人的真實身份      可參考 從 0 認識 Blockchain - 錢包基礎原理 | Shawn Hsu    其中用到的，是非對稱式加密系統  利用公鑰 私鑰進行所有操作，既能完全匿名，也能夠驗證你的身份(透過 數位簽章 Digital Signature)   Cons of Centralized Service  中心化的伺服器缺點也滿明顯的 不透明   所有資料的傳輸的過程中，僅有 server client 參與  我們很難判斷說資料到底有沒有被更改過  即使現今擁有加密、驗證等等手段，還是沒辦法 100% 保證真實性(萬一 server 惡意竄改資料呢？)   Decentralized? How?  去中心化的作法是，擁有多台電腦執行 相同的共識機制，相同的程式, 擁有相同的資料備份  當有一台電腦想要搞事的時候，會因為算出來的結果跟別人不一樣，而遭淘汰  可以把他想像成 多數決  因為 network 的思考方式(共識機制)都一定一模一樣，如果有某個人的結果不同，那必定表示他在搞事      有關共識機制，可參考 資料庫 - 初探分散式資料庫 | Shawn Hsu    Chain Selection Mechanism  前面提到 blockchain 目前是採多數決的方式決定的  多數決顧名思義，必須要贊成或否決某項決議，但先決條件是，某個人要先提出某個決議才行  blockchain 要怎麼做呢？   首先，在眾多 blockchain 當中，選出 最多驗證者的鏈 當成 main chain  這時候！ 其他的 blockchain 會選擇 跟隨 main chain 的結果  接下來就可以開始進行多數決了  其餘的鏈，會一一驗算，計算 transaction 執行是否正確(e.g. from, to, function)選擇我要贊成或否決  遭到否決的話，將會找尋 第二多驗證者的鏈當成 main chain 並且處罰該節點  無限循環，無限驗證      經過 The merge(發生於 2022/09) 之後  Ethereum 迎來全新技術升級，chain selection 從原本的 最長的鏈 :arrow_right: 最多驗證者的鏈    如此一來，這樣的互相監督機制使得區塊鏈不易被更改  安全性也更加的高      要怎麼驗證 contract 內容合不合法？  這件事情不應該從技術端解決，而是立法機構要介入的  共識機制只確保我執行的結果是正確的    Sybil Attack  藉由創造大量的假帳號試圖影響網路  在 Blockchain 的世界裡，可以藉由一個惡意的節點，拒絕執行某些 transaction 拒絕寫入某些 block  當數量足夠多的時候，就會造成 51% Attack   Direct Sybil Attack  惡意的節點直接與真正的節點互動   Indirect Sybil Attack  透過一個中間(proxy)節點，對真正的節點進行攻擊  好處是 network 比較難發現惡意節點的存在  有點類似 中間人攻擊        ref: Executing a Man-in-the-Middle Attack in just 15 Minutes      51% Attack  當有過半數的人反對你的意見時，多數決的機制會排除你的意見  在 blockchain 的世界裡面，如果你控制了超過半數的 Ethereum node 那麼你將控制整個 Ethereum Blockchain  這時候你把黑的說成白的 也不會有人反對 畢竟現在你最有話語權      注意到，你不是能 100% 做你愛做的事情，比如說像是隨意移動他人 ETH 就是做不到的  控制網路能做的是類似，拒絕 transaction, rollback transaction … etc.         ref: Ethereum Mainnet Statistics    根據 Ethereum Mainnet Statistics 的大略統計結果  目前有將近 7700 多台 Ethereum Node 正在運行  意味著你要控制 3400 多台的機器，你才可能控制整個區塊鏈  不敢說多數決的機制是完美的，但是想要完整控制還是需要花很多的錢 也大幅度的減小了 51% Attack 的可能性      透過下載 go-ethereum 運行 Ethereum 節點你也可以為區塊鏈生態系貢獻心力    Chain Types        ref: Polkadot/波卡鏈｜平行鏈競拍為何成為市場熱話？DOT幣有何用途？    Relay Chain  對於使用相同基礎設施的區塊鏈們，他們可以透過中繼鏈進行溝通      Relay Chain 為 Polkadot 專有名詞    Parachain  跑在相同底層區塊鏈之上的 chain 被稱之為 平行鏈   Bridgechain  不同的區塊鏈不能互相傳遞資料，因此需要橋接鏈的幫助才可以溝通   Blockchain Layers  與 OSI 七層模型一樣，區塊鏈也有類似的架構      有關七層模型的介紹，可參考 重新認識網路 - OSI 七層模型 | Shawn Hsu         ref: Blockchain layers — What are they?    總共有 5 層架構，分別為      Physical Layer            就是基礎建設如硬體、網路以及虛擬機等等的           Data Layer            儲存數據的地方，並且由所有節點共同維護           Network Layer            所有節點構成分散式系統           Consensus Layer            區塊鏈的基礎之一，共識機制，並且所有節點可以從維護區塊鏈網路中獲得獎勵(i.e. Token)           Application Layer            最上層就是一些應用的部份了，包含像是 智能合約以及 Metamask 這種服務都算是應用層之上的服務           不過上述僅為架構，實際上我們在說的 layer 只有包含 3 層(3, 4, 5 層都有人講)      說目前 “只有” 3 層的原因很簡單，因為實際上區塊鏈仍在高度發展當中，搞不好以後會發展更多層也不一定         ref: Layers of the Crypto Universe    Layer 0  underlying infrastructure and network protocols(hardware, internet, protocol) layer 0 的定義實際上有點迷，比較能說服我的說法是  這層主要是包含一些 blockchain 的基礎設施，像是硬體，網路等等的   另一種說法是  layer 0 包含了以下三個元素   Mainnet  這裡說的 mainnet 可以用比較抽象的概念去理解  blockchain 一定會提供一個機制，讓整個網路運行起來可以完善且更加有效的機制  而這個機制是透過 primary chain 所提供的，可以確保網路的運作順利   同樣的，這裡的 primary chain 也是屬於比較抽象的概念  實際上 blockchain 是由多個節點共同維護運行的，並不存在所謂的 master chain 或是 slave chain  亦即，整個網路會致力於共同維護網路的資料正確性、安全性以及可信任性   Sidechain  sidechain 可以用於橋接其他 chain 的資料，進行驗證或是單純的移動資料過來處理   可參考 從 0 認識 Blockchain - Scaling Blockchain | Shawn Hsu - Sidechain   Cross-chain Operations  當多個 layer 1 chain 都使用相同 layer 0 基礎設施，之後就可以藉由 layer 0 進行跨鏈的功能(透過 Relay Chain)     有一些鏈是直接運行於 layer 0 之上的，比如說 Polkadot  為什麼它不屬於 layer 1?  因為 Polkadot 提供了一些基礎設施，特殊的架構設計(Relay Chain) 使其擁有類似於基礎建設的架構  因此我們稱它為 layer 0 blockchain   Layer 1  layer 1 是區塊鏈的 base layer, 包含 共識機制，transaction 驗證以及 block creation  我們熟知的 Ethereum, Bitcoin 都是屬於 layer 1  不同的 L1 chain 不能交換資料，互相溝通   Layer 2       ref: 【加密貨幣入門】3 分鐘了解 Layer 0 Layer 1 Layer 2 是什麼！    blockchain 由於其去中心化的特性，亦即所有 transaction 都必須由所有節點執行並驗證過才可以上鏈  這會導致單位時間內能處理的交易數量是有上限的  因此整個交易速度會被拖得很慢  而 layer 2 是 layer 1 blockchain 的解方，主要目的在於提昇速度，scalability  常見的 solution 可以參考 從 0 認識 Blockchain - Scaling Blockchain | Shawn Hsu   References     ETHEREUM DEVELOPMENT DOCUMENTATION   What is blockchain technology?   Introduction to smart contracts   How Smart Contracts Will Change the World | Olga Mack | TEDxSanFrancisco   Web3, Blockchain, cryptocurrency: a threat or an opportunity? | Shermin Voshmgir | TEDxCERN   Sybil Attacks Explained   51% Attack   Sybil Attack in Blockchain: Examples &amp; Prevention   區塊鏈的 Layer 是什麼？L1-4 差異與應用範例介紹   【加密貨幣入門】3 分鐘了解 Layer 0 Layer 1 Layer 2 是什麼！   What Are Blockchain Layers?   What Is a Layer 0 Blockchain?   Understand Layer 0, 1 and 2 of Blockchain   What is a Layer 0 Blockchain?   What Is a Layer 0 Blockchain?  ","categories": ["blockchain"],
        "tags": ["blockchain","ethereum"],
        "url": "/blockchain/blockchain-basics/",
        "teaser": null
      },{
        "title": "從 0 認識 Blockchain - Scaling Blockchain",
        "excerpt":"Why do we need to Scale Blockchain  Ethereum blockchain 仰賴著共識機制，同時也深受共識機制帶來的效能影響  由於要求所有 node 都執行計算 transaction 正確性，導致同一筆交易會被驗算數次  進而消耗整體網路資源   本篇文章將會探討一些 layer 2 的加速手法  一起看看吧   Proofs  Fraud Proof  將多筆 transaction 整合成一筆打包上鏈  一般來說是視為合法的，如果有人不服，可以提出 challenge  那麼 transaction 將會重新執行以驗證其正確合法性      可參考 fraud proof    Merkle Proof  驗證特定 transaction 是否屬於 block 或是 merkle tree 裡面      有關 Merkle Tree 的介紹可以參考 從 0 認識 Blockchain - Transaction 以及你該知道的一切 | Shawn Hsu  以及 merkle proof    Validity Proof  一樣會將多筆 transaction 打包成一筆上鏈  不同於 Fraud Proof 的是，會 事先 額外提供一個 “證明” 證明 transaction 的合法性      Fraud Proof 則是事後證明合法性，樂觀的認為提交上來的所有交易紀錄都是合法的       可參考 validity proof    Zero-knowledge Proof - ZK proof  zk proof 不只可以驗證交易正確以及合法性，還可以驗證整個 state 的轉換是否正確  也就是說它不用像 validity proof 一樣要有 challenge period  常用的 zk proof 有兩種，ZK-SNARK 以及 ZK-STARK  值得注意的是，zk proof 不需要重新執行交易就可以驗證正確性      可參考 ZK proof    Recursive Proof  ZK proof 的其中一個特性是，它可以驗證其他的 proof  也就是說我可以將多個 proof 組合在一起變成 recursive proof   根據 ZK-SNARK §4.3.2  recursive proof 可以使用 parallel 的方式大幅度的提高吞吐量  並且一旦驗證完成，所有相關的 block 都可以通過驗證      不過依照論文所述，PLONK 實作目前還是沒辦法達成並行計算    Optimistic Rollups  既然 layer 1 的運算成本太高，我可以把一連串的交易 off-chain 算完(透過 off-chain virtual machine)，紀錄打包好再一次上傳(交給 on-chain contract)  這樣就好了   optimistic 的原因在於，我相信 layer 2 的執行是完全合法的，而我不需要發布證明證明他是合法的  想當然這樣有點太樂觀，因此在主網正式接受這些紀錄的時候，會有一個緩衝期(i.e. challenge period, 通常為一個禮拜)  亦即任何人都可以提出他的質疑，透過計算 fraud proof 可以驗證紀錄真假  一旦被發現造假或著是紀錄有問題  該節點會受到處罰      有關 blockchain layer 的介紹，可參考 從 0 認識 Blockchain - 區塊鏈基礎 | Shawn Hsu    optimistic rollup 會將 所有交易紀錄 提交到主網  所以在費用上面相比是沒有比較划算的  它主要的目的在於，增加主網執行速度      注意到如果後來的 block(block1) 是跟著這個有問題的區塊(block0)，那麼 block1 也將會是非法的並且被 revert                   Pros       Cons                       效能提升       會有延遲(challenge period)                 可以輕易的驗證 transaction 正確性       交易紀錄順序可能改變                 因為 vm 是 Ethereum compatible 所以可以輕易的移植現有 contract       可以發布假的 block 資訊(在沒有誠實的 node 存在的情況下)                         必須提交所有交易紀錄，可能會增加成本           Zero-knowledge Rollups  相比於 Optimistic Rollups 將全部的紀錄寫回去，zk rollup 僅會將 部份更改寫回主網(通常只有 一筆，並且以 calldata 的形式)  亦即它會把交易紀錄們做一個總結，只將有更改的地方寫回去，並且同時附上 zk proof 以證明其正確性      calldata: 存在於 history log 而非 Ethereum state    執行的過程也與 Optimistic Rollups 相似，透過 off-chain virtual machine 執行計算，每隔一段時間題交給主網，並且由 on-chain contract 進行 zk proof 的驗證  不同的是，由於 zk-rollups 有提供相關證明，因此它不會有所謂的 challenge period, 當紀錄被寫回去主網的同時，驗證過後，就會被接受   zk-rollups 由於產生驗證用的 zk proof 需要使用特定的硬體協助  因此實務上會比較難使用                  Pros       Cons                       可提供 zk proof 證明 transaction       費用較高(除了計算 transaction 之外還要證明的費用)                 transaction 能較快被接受(免除 challenge period)       實作較困難                 提供較強安全機制       需要特殊硬體協助                 較少的資料需要被寫回主網       交易紀錄順序可能會改變           State Channel  每個 transaction 都要在每個 node 上面驗證過是一件費時的事情  有沒有一種方法能夠讓 transaction 的執行是 off-chain 的呢？   Channel  channel 提供了一個機制，允許交易能夠 off-chain 的執行，只須將最終結果寫回 blockchain  大幅度的提高執行速度  需要進行交易的所有參與者都必須 deploy multisig smart contract，並且存入一些 eth 用於交易(交易期間不用付費，僅須開始以及結束的時候需要)  然後就可以開始執行 off-chain 的快速交易了  最後執行完畢要關閉 channel 的時候，所有參與者提交最終交易結果，然後寫回去 blockchain   multisig 最少一個人參與交易, 假設為 m 個人  交易期間，muitlsig 需要至少有 n 把合法的 signature 才可以簽署交易  且同樣的，受限於共識機制，至少要有一半的人同意才可以執行交易(常見的配置為 n=3,m=5, n=4,m=7)   而 channel 又分為兩種     payment channel   state channel   Virtual State Channel  channel 的使用仰賴一開始雙方 deploy multisig contract, 這也違反了想要減少交易次數的初衷  virtual state channel 提供了完整的 off-chain 機制，從 建立、交易到最後的提交都是 off-chain 的  透過 on-chain 的 ledger channel 提供的 channel 實體，交易雙方可以使用它進行交易  而 ledger channel 可以當成是中立的第三方，當出現糾紛的時候可以出來調停     state channel 加強了 payment channel 只能用於金錢方面的限制，提供了更一般的解方  state channel 全部參與者們都必須對 transaction 簽名 方可執行，只要有一個人沒簽名，transaction 就不會執行   你說這樣安全嗎？  答案很明顯的，相較於整個網路維持共識機制，單靠交易雙方是沒辦法維持的  也就是說 state channel 其實是不怎麼安全的，當發生問題的時候，只能依靠爭議仲裁系統處理   Sidechain  sidechain 是另一個獨立於主網的 blockchain, 並且可以透過 two-way bridge 與主網連接  sidechain 可以擁有不同的 block 參數與共識機制，換言之不一定要跟原本的 chain 設定相同  不同的共識機制，目的是為了可以更有效率的處理交易  但這是有代價的，使用不同的機制代表他的安全性可能不如 layer 1 chain   注意到 sidechain 並不會將 transaction data 以及 state 寫回去主網      有關 blockchain layer 可以參考 從 0 認識 Blockchain - 區塊鏈基礎 | Shawn Hsu    常見的 sidechain 有 Polygon POS, Gnosis 等等的   Plasma  與 Sidechain 類似, 不同的是 plasma chain 是透過 smart contract 與主網連接的  並且它可以使用主網的安全機制(相對的 sidechain 依靠自己的安全機制)      Plasma 白皮書    主網，被稱之為 root chain, plasma 透過建立一個 child chain 與主網上面的 smart contract 進行溝通  要使用的時候，必須存入一些 ETH 或者是 ERC-20 token  這時候 plasma contract 會從主網複製一份資料下來到 child chain，之後所有的交易都是在 child chain 上面完成的  每隔一段時間提交一個 整合過的 commitment 寫回主網  這時候問題來了，我要怎麼確保你在 child chain 上面執行的交易紀錄是正確的？  merkle proof 可以當作一個證明  透過驗算 merkle proof 是否正確，能確保資料的正確性  注意到，root chain 僅有 整合過的狀態資料, 它對於每一筆 transaction 是毫無了解的，亦即 plasma 是將所有資料儲存於 plasma 本身的而已   當你要離開 plasma 並將資料寫回主網並提領 ETH 或 token 的時候  會有所謂的 challenge period(通常為一個禮拜), 如同先前提到的一樣，在這段期間內，所有人都可以質疑你的交易是否合法(透過 fraud proof)  稱之為 exit game      既然 merkle proof 可以作為證明，為什麼不能當作 withdraw 的有效證明？  因為 merkle proof 僅證明了某筆交易的正確性，但它不能保證整個 狀態轉換 是合法的    此外，plasma 並不能執行 smart contract  它只能做有關 金錢、token 等等的事情      plasma 通常只有一個 operator 在計算，惡意節點的可能性會更高    Validium  跟 Zero-Knowledge Rollups 類似，validium 可以使用 validity proof 或是 zk proof 驗證交易合法正確性，不同的是 validium 的資料不會儲存在主網上面   validium 是 off-chain 執行的，有可能是使用 Sidechain 或者是其他的設施  主要由兩個 contract 所執行的      Main Contract            負責儲存 state commitment           Verifier Contract            既然 validium 儲存的資料不是儲存在鏈上的，那麼當你在上傳 commitment 的時候是必須要驗證他的正確性           Data Availability Committee - DAC  off-chain 的其中一個隱憂就是，當 node operator 不在線上的時候  當你需要進行 withdraw 等等需要驗證的事情的時候(生成 merkle proof)，是沒辦法運作的  因為所有的資料都在節點上面，而目節點不可用   一種作法是將資料上傳到可信任的第三方，稱之為 Data Availability Committee - DAC      ref: Rollup 與資料可用性    Volition  Volition 提供了一個可以 選擇的方案，你可以選擇上傳資料到第三方，稱為 Data Availability Committee - DAC 或者是使用 on-chain data availability 的解決方案如 ZK Rollups        ref: Rollup 與資料可用性                     Pros       Cons                       proofs 可以保證資料正確以及合法性       可能需要特殊硬體協助(zk proof)                 減少 gas fee(資料在本地)       針對一般 use case 有較差的支援性                         節點不一定可用，無法 generate merkle root           Conclusion  最後做一個大點的總結好了，不然有點霧颯颯                          Optimistic Rollup       Zero-knowledge Rollup       State Channel       Sidechain         Plasma         Validium                       Security       main chain       main chain       Fraud Proof       Itself       main chain       main chain                 Proof       Fraud Proof       ZK proof       Fraud Proof       custom       Fraud Proof Merkle Proof or Validity Proof       Validity Proof or ZK proof                 Challenge period       :heavy_check_mark:       :x:       :x:       :x:       :heavy_check_mark:       :heavy_check_mark:                 Commit to mainnet       :heavy_check_mark:       :heavy_check_mark:       :heavy_check_mark:       :x:       :heavy_check_mark:       :heavy_check_mark:                 Data location       on-chain       on-chain off-chain       on-chain off-chain       off-chain       on-chain off-chain       on-chain off-chain                 Data on mainnet       full history       state       state       :x:       state       state                 Data on local       full history       tx data       tx data       :x:       tx data       tx data           References     What is a Sidechain?   OPTIMISTIC ROLLUPS   SIDECHAINS   ZERO-KNOWLEDGE ROLLUPS   STATE CHANNELS   PLASMA   VALIDIUM   Rollup 與資料可用性  ","categories": ["blockchain"],
        "tags": ["blockchain","ethereum"],
        "url": "/blockchain/blockchain-scaling/",
        "teaser": null
      },{
        "title": "從 0 認識 Blockchain - Transaction 以及你該知道的一切",
        "excerpt":"How does Blockchain Works  複習一下 blockchain 是如何運作的  blockchain 是由多個節點所組成的分散式計算網路  每個節點都嚴格遵循共識機制，共同維護區塊鏈上的資料   詳細的簡介，可以回去複習 從 0 認識 Blockchain - 區塊鏈基礎 | Shawn Hsu   Introduction to Transactions       ref: TRANSACTIONS    就如同基礎篇所提到的，每一次與區塊鏈的互動，都是一次的交易  這個交易可以是 金錢上的出入、合約的讀寫操作以及 Token 的往來   同時 Transaction 所代表的意思是 改變區塊鏈的狀態, 而這個狀態必須同步到整個網路上  交易內容一旦確認完畢後，便會 永久的儲存在鏈上 且沒有人能更改   而每一筆的交易都需要付費，並且 Transaction 將會打包，並且存於 Block 之中   Why do we need to Pay for Transactions  區塊鏈的資料是儲存在各個節點的硬碟上面，想當然這不會是免費的  為了能夠讓 node 願意幫你執行運算並儲存你的資料，付一點錢是必要的   Miners  在 Proof-of-Work(POW) 的時代，節點的工作者被稱之為 miners 礦工  也就是我們熟知的挖礦   要計算 Transaction 就像是解一題超爆難的數學題目  為了能得到所謂的 獎勵, 並且因為這個獎勵只會給第一個解出來的人  每個節點拼了命的瘋狂在解題  造成大量算力的浪費   POW 的獎勵為 block fee + transaction fee(base fee + priority fee)   Validators  為了解決 POW 所帶來大量的算力浪費，Proof-of-Stake(POS) 的機制可以大幅度的解決這件事情  其中 miner 變為了 validators 驗證者   驗證者的機制是說，我以我的金錢擔保這個交易內容沒問題(which is 質押)  而這個金錢是使用 ETH 的方式  驗證者需要存入一些 ETH 才能夠變成所謂的驗證者   一樣為了得到獎勵，不同的是 validator 透過投票的方式，贊成或是反對要不要將 Transaction 加入新的 Block  而你的金額的大小決定投票權重的大小   POS 的獎勵為 priority fee     有關獎勵機制，可參考 Gas      有關共識機制，之後會有一篇獨立出來     Ethereum Virtual Machine - EVM  用以執行智能合約的底層，被稱之為 Ethereum Virtual Machine  就跟我們在計算機組織裡學到的一樣，EVM 也有自己的一套 opcode(Ethereum Virtual Machine Opcodes)  所以執行合約的時候，EVM 能夠清楚的了解並解析 bytecode 然後執行   因為 EVM 是負責處理運算 Transaction，還記得我們說過交易有可能會改變區塊鏈的狀態嗎？  由於 EVM 是主要做這些狀態改變的人，因此 EVM 也被視為是 state machine, 負責將區塊鏈的狀態更新到新版        ref: ETHEREUM VIRTUAL MACHINE (EVM)    值得一提的是，EVM 是採用 stack-based approach 實作的，並且為 big endian  亦即他的資料都是存在 stack 當中的，當有需要的時候使用 push, pop 就可以拿取以及儲存資料了  而這個 stack 大小為 1024, 每個 element 大小為 256 bit      有關 endian 的介紹可以參考 重新認識網路 - OSI 七層模型 - Endian | Shawn Hsu    常見的 EVM 實作有     go-ethereum   Py-EVM   evmone      詳細可參考 Ethereum Yellowpaper    Interact with Smart Contract  與區塊鏈互動的其中一個方式就是透過智能合約  smart contract 本質上就是一連串自動執行的程式碼  那麼我們要怎麼跟它互動呢？   本質上就是 call function 嘛  所以你至少要知道是哪個 function, 以及 function 存在於哪個合約上面(因為別的合約可能有一模一樣的函式定義)，亦即     Contract Address   Application Binary Interface - ABI   Application Binary Interface - ABI  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 [     ...      {       \"inputs\": [         {           \"internalType\": \"address\",           \"name\": \"nftContractAddress\",           \"type\": \"address\"         },         {           \"internalType\": \"uint256\",           \"name\": \"tokenID\",           \"type\": \"uint256\"         },         {           \"internalType\": \"uint256\",           \"name\": \"price\",           \"type\": \"uint256\"         }       ],       \"name\": \"listNFT\",       \"outputs\": [],       \"stateMutability\": \"nonpayable\",       \"type\": \"function\"     }      ... ]  顧名思義，它就是 interface，定義了 contract 裡面的架構  如上圖，定義了一個 function, 它擁有 3 個 input，依序為 address, uint256 以及 uint256  透過 ABI 我不用了解你是如何實作的，只要我根據 interface 提供參數，我就會拿到想要的結果   看著是不是跟 API 很像  不同的是，ABI 不能直接呼叫，它只是個定義，只能透過 binary 的方式存取  而 API 是可以在 source code level 拿到的   Function Signature  為了能夠辨識所謂的 function, 我們需要給它一個名字(或者說識別符號)  而這個識別符號由以下組成     Function name   Type of function parameters   舉例來說，listNFT 定義如下  1 2 3 4 5 6 7 function listNFT(         address nftContractAddress,         uint256 tokenID,         uint256 price     ) {     // list nft implementation }   那麼他的 signature 就會是以下  1 listNFT(address,uint256,uint256)      組成的時候，需要去掉所有的空白    Function Selector  我們知道了可以透過 Application Binary Interface - ABI 取得 function 的定義  但是實務上我還是不知道這個 function 在哪裡對吧  我可以使用 Function Signature，這樣我就知道 function 的進入點位置了對吧  但是要稍微加工一下   將 signature 算 hash 之後，會得到一串字串  取 前 4 個 byte 當作 selector  當 Ethereum Virtual Machine - EVM 要執行某個 function 的時候  它會根據 function selector 找到相對應的 function 進入點進入執行   以 listNFT 來說，透過以下函式可以計算他的 hash  1 bytes4(keccak256(bytes(signature)))      它會不會發生碰撞？ 他的 signature 就是一串英文字母  相同的字串算出來的 hash 一定是一樣的  又因為你不能定義相同的 function，所以 signature 一定會是不同的    在你的 contract 裡面加一個 pure function  1 2 3 function signatureListNFT() public pure returns(bytes4) {     return bytes4(keccak256(bytes(\"listNFT(address,uint256,uint256)\"))); }  並且執行你就會得到一串神秘 signature 0xad05f1b4      pure :arrow_right: 不會讀取區塊鏈狀態  view :arrow_right: 讀取但不更改區塊鏈狀態    也可以使用線上的 hash tool 進行驗證    Block  blockchain 顧名思義是由一堆 block 所構成的 chain  儲存在鏈上的，會是一堆一堆的 block  每個 block，以 Ethereum 來說會是 12 秒 生成一個新的區塊  而 block 裡面包含了      若干個 Transaction   前一個 block 的 hash        ref: BLOCKS         ref: Blockchain    Validator 準確的來說，是要驗證 block, 也因為 Transaction 包含在其中，所以交易也會被驗證, by default  被選為要生成下一個 block 的 validator 會打包近期的 Transaction 並加上一些 header 如 Merkle Root 透過 gossip protocol 傳遞到其他的節點進行驗證  驗證通過之後，便會將該 block 寫到自己節點的鏈上   你可以到 andersbrownworth.com/blockchain/blockchain 實際的玩一下      ref: Blockchain Demo - Making Blockchain Accessible    Why do we need to Include Previous Block Information  為什麼要包含前一個 block 的資訊？  試想如果我貢獻了其中一個 block，但是我竄改了其中的交易資料，使得有些紀錄被我抹除了  又假如我擁有超過半數的網路節點，而每個節點我都這樣做，會發生什麼事情？   沒錯，51% Attack!  所以將前一個節點的資訊加入，會大幅度的增加 51% 攻擊的難度  由於 block 的資訊依賴於前者 block 的資訊  因此，之後的 block 必須要 全部重新計算, 導致攻擊者需要花費大幅度的心力去影響整個網路  進而增加難度   Block Size  在 London 升級之前  block size 是固定大小的，亦即每一個區塊能夠包含的交易數量是有上限的   固定大小的 block size 有幾個問題     因為大小固定，只有固定數量的交易能提交進網路，導致多出來的交易需要進行等待   為了能夠盡快的寫入 block, 你會提高手續費(可參考 Transaction Stuck)，進而導致整體網路交易費用大幅提高   在倫敦升級之後，引入動態大小的 Block size 能夠解決以上問題  平均的區塊大小會在 15 million gas, 最大可以到 2 倍(也就是 30 million gas)  詳細可以參考 EIP 1559   Genesis Block  盤古開天的第一個 block 稱之為 Genesis Block  第一個 block 是沒有辦法被挖礦的，因為它沒有前一個 block 可以參照  genesis block 是透過設定而生出來的   在 POW 的時代，可以透過 genesis file 啟動客戶端  代表他是從運作區塊鏈的一開始就存在的   Merkle Tree  Block 裡面包含了區間內所有交易資料，要如何快速的交易的正確性是一項挑戰   Merkle Tree 是一個樹狀的資料結構  其節點由資料雜湊而成(hash)  而 leaf node 則是由原始資料 hash 而成  就像下面圖片一樣      其中前綴有 h 的，代表 hash 過的數值          ref: Merkle Tree 和 Merkle Root 介紹    藉由一層一層的往上 hash, 最終你得到的 hABCDEFGH 即為所謂的 Merkle Root      注意到 Merkle Tree 不能反推 sub-Merkle Tree 哦(因為 hash 是單向的)      驗證某個 Transaction 是否是在這個 Block 裡面可以透過驗算 Merkle Root 得到  假設要驗算 D 有沒有被更改過(存在於該 block)  只要驗算 hAB, hC, hD 以及 hEFGH hash 過後的數值是否等於 hABCDEFGH 就可以了        ref: Merkle Tree 和 Merkle Root 介紹    Transactions  Type of Transaction                  Transaction Type       Description                       一般的交易       轉移金錢或 Token                 合約的建立       沒有 recipient(i.e. to) 並且 data 欄位包含了 contract bytecode                 合約的操作       to 指的就是 contract address data 則包含了必要的資料，可參考 Transaction Attributes           Mempool     待處理的 Transaction 會被放在 node 的 local memory, 稱之為 memory pool  一旦新的 Transaction 被提交到網路的時候， Miner 或者是 Validator 就會開始進行處理交易  完成計算整個 block 之後，會進行廣播，將算好的 block 傳遞到各個 node 進行同步  而其他的節點需要負責驗算結果是否合法(驗算 Merkle Root)  並且將新的 block 資料儲存於各自的節點硬碟當中   Transaction Response Attributes  最基礎的 Transaction Response 包含了至少以下欄位                                                  from       Transaction 的發起者的錢包地址，只能是外部錢包地址，智能合約無法發送交易請求                 recipient       1. 外部錢包地址 :arrow_right: 轉移數值(可能是 ETH 或是 Token) 2. 合約地址 :arrow_right: 執行合約程式碼                 signature       sender 的簽章，用以證明 Transaction 的發起者(通常以私鑰簽名)                 nonce       簡單的計數器，用以紀錄 Transaction 數字(number used only once)  &gt; ref: How to customize a transaction nonce                 value       轉移的數值，單位為 Wei 詳細可參考 Wei GWei ETH                 data       任意資料                 gasLimit       可參考 Gas                 maxPriorityFeePerGas       可參考 Gas                 maxFeePerGas       可參考 Gas           Data  data 的欄位通常是放一些跟 Application Binary Interface - ABI 有關的資料  比方說當我執行合約的函式的時候，data 欄位就會是放 Function Selector 以及參數資料  舉例來說，當執行 listNFT 的時候  1 2 3 4 5 6 7 function listNFT(         address nftContractAddress,         uint256 tokenID,         uint256 price     ) {  }  它執行完成的 response 的 data 欄位如以下所示  1 2 3 {   \"data\":\"0xad05f1b40000000000000000000000009fe46736679d2d9a65f0992f2272de9f3c7fa6e000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000001\", }   根據先前學到的知識，我們可以知道 Function Selector 佔前面 4 個 byte  也就是這個 Transaction 所執行的 function 為 0xad05f1b4(也就是 listNFT)      4 個 byte 總共為 32 的 bit  data 是使用 hex 表示法，亦即一個位置表示 4 個 bit(0x1 = 0b0001)  所以 function selector 要數 8 個字母(32 bit / 4 bit)    那麼後面的呢？ 讓我們先把輸出用的好看點  1 2 3 4 5 6 {   \"data\":\"0xad05f1b4         0000000000000000000000009fe46736679d2d9a65f0992f2272de9f3c7fa6e0         0000000000000000000000000000000000000000000000000000000000000000         0000000000000000000000000000000000000000000000000000000000000001\", }  剩下可以看出，有三組資料，而他們分別對應到 address nftContractAddress, uint256 tokenID, uint256 price  我們知道 EVM 的資料儲存的格式，是每一個資料為 256 bit 大小(i.e. 32 byte)  所以即使 address 是以 20 個 byte 表示，在 EVM 當中還是以 32 byte 呈現，差的部份則是使用 0 補齊      回顧 Ethereum Virtual Machine - EVM    第一個 0x9fe46736679d2d9a65f0992f2272de9f3c7fa6e0 為 nftContractAddress  第二個 0x0 為 tokenID  最後則是 0x1 為 price   你可以利用 Etherscan 等等的 block explore 去觀察 Transaction detail      ref: How to use Etherscan    Transaction Receipt Attributes  這裡列出幾項，常使用的欄位   Events  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 {   \"events\":[     {       \"transactionIndex\":0,       \"blockNumber\":10,       \"transactionHash\":\"0xa3e2d89e7a383f58a276118f99703d8e0eb166174c397920a45cf08e2fdef44d\",       \"address\":\"0x5FbDB2315678afecb367f032d93F642f64180aa3\",       \"topics\":[           \"0xd547e933094f12a9159076970143ebe73234e64480317844b0dcb36117116de4\",           \"0x000000000000000000000000f39fd6e51aad88f6f4ce6ab8827279cfffb92266\",           \"0x0000000000000000000000009fe46736679d2d9a65f0992f2272de9f3c7fa6e0\",           \"0x0000000000000000000000000000000000000000000000000000000000000000\"       ],       \"data\":\"0x0000000000000000000000000000000000000000000000000000000000000001\",       \"logIndex\":0,       \"blockHash\":\"0x8dc1ea78fc4bafaffed224b8b986e117e9d17666e7c9ac6ee275c94eb5db525e\",       \"args\":[           \"0xf39Fd6e51aad88F6F4ce6aB8827279cffFb92266\",           \"0x9fE46736679d2D9a65F0992F2272dE9f3c7fa6e0\",           {             \"type\":\"BigNumber\",             \"hex\":\"0x00\"           },           {             \"type\":\"BigNumber\",             \"hex\":\"0x01\"           }       ],       \"event\":\"ItemListed\",       \"eventSignature\":\"ItemListed(address,address,uint256,uint256)\"     }   ] }   智能合約能夠透過發送 event 來與外部的世界互動  上述就是一個簡單 event 的例子   可以看到，這個 event 的 signature 為 ItemListed(address,address,uint256,uint256)  那麼他的 selector 就是 d547e933094f12a9159076970143ebe73234e64480317844b0dcb36117116de4      回顧 Function Selector    event 原型定義為  1 2 3 4 5 6 event ItemListed(     address indexed seller,     address indexed nftContractAddress,     uint256 indexed tokenID,     uint256 price );  如上所示，event 可以傳送資料  其中，資料分為兩種     indexed parameter            indexed parameter 可以更快速的查詢特定的資訊       indexed parameter 的大小為 128 byte(4 * 32 byte), 亦即最多有 4 個參數可以放在 indexed parameter 裡面       indexed parameter 會儲存在 topics 的欄位中       topics[0] 為 Function Selector                    anonymous function 除外                           non-indexed parameter            non-indexed parameter 會儲存在 data 欄位中       不限定數量       資料為 ABI encoded, 亦即沒有 ABI 你是沒辦法 decode 的             根據我們的定義，有 3 個 indexed parameter 以及 1 個 non-indexed parameter  1 2 3 4 5 6 7 \"topics\":[     \"0xd547e933094f12a9159076970143ebe73234e64480317844b0dcb36117116de4\",     \"0x000000000000000000000000f39fd6e51aad88f6f4ce6ab8827279cfffb92266\",     \"0x0000000000000000000000009fe46736679d2d9a65f0992f2272de9f3c7fa6e0\",     \"0x0000000000000000000000000000000000000000000000000000000000000000\" ], \"data\":\"0x0000000000000000000000000000000000000000000000000000000000000001\",  其中 topics[0] 為 Function Selector  跟我們上面得出的結果一致  再來就是 address, address, tokenID 分別對應到 topics[1], topics[2] 以及 topics[3]   剩下的 1 個 non-indexed parameter 則儲存在 data, 為 price   args 的欄位則是包含了 topics 以及 data 的資料   GasUsed  1 2 3 4 5 6 {    \"gasUsed\":{       \"type\":\"BigNumber\",       \"hex\":\"0x01c7db\"    }, }  Gas 中提到，每一筆 Transaction 可能會執行多個運算步驟  gasUsed 就是代表 你使用了多少個 unit  要計算總 gas fee 就是 GasUsed * EffectiveGasPrice   EffectiveGasPrice  1 2 3 4 5 6 {   \"effectiveGasPrice\":{       \"type\":\"BigNumber\",       \"hex\":\"0x4a1d07d2\"    }, }  Gas 中，我們提到，手續費是由 base fee 以及 priority fee 所決定的  其中 base fee 由網路自行運算所得出來的  而實際上網路所消耗的實際手續費仍有所不同   effectiveGasPrice 即為 base fee + priority fee 所構成  在實際計算所消耗的手續費的時候，需要將其與 gas used 相乘  也就是 GasUsed * EffectiveGasPrice   Genesis Address  雖說 Genesis Block 是一開始運行客戶端就建立起來的 block  但是如果你到 Etherscan 去看第一個 block 的交易資訊  你會發現到，from 是有數值的，為 0x0   這個地址被稱之為 Genesis Address  而我們說，沒有人建立第一個 block, 亦即該 block 不會紀錄誰建立了這個 block(digital signature)  但是 from 的確是寫 genesis address 阿   沒有簽章，但有地址？  亦即，該地址沒有私鑰(因為沒有簽章資訊)  也就是說，它像是一個 黑洞 一樣(i.e. /dev/null)  沒有私鑰代表沒有人可以真的掌控該錢包地址  所有送進去的 ETH, Token 都沒有辦法拿出來  這也就是為什麼，genesis address 截至文章撰寫時間，擁有接近 1889.34 顆 ETH 以及 201 種不同 Token      可參考 https://etherscan.io/address/0x0000000000000000000000000000000000000000    如果只是一開始建立 Genesis Block 應該不用那麼多 ETH 跟 Token 對吧？  除了初始化區塊鏈之外，還有一些情況會需要用到 Genesis Address  根據 Etherscan 上面的 quote  1 2 3 This address is not owned by any user,  is often associated with token burn &amp; mint/genesis events  and used as a generic null address   當你需要銷毀 Token 的時候會需要用到 Genesis Address  那 ETH 呢？      有關 Token 的部份會獨立一篇出來     事實上 一些智能合約(通常是沒有寫好)  如果沒有指定 recipient(to), 預設會是 Genesis Address  可想而知，這樣會出大問題  就比如說這個交易 0x1c96608bda6ce4be0d0f30b3a5b3a9d9c94930291a168a0dbddfe9be24ac70d1 轉了 1493 顆 ETH 到一個沒有人拿的出來的地方  以一人之力貢獻了目前 Genesis Address 80% 的 ETH 存量   Transaction Stuck?  有時候你可能會遇到，你的 Transaction 卡在 Mempool 很久並且都還在 pending  原因其實很簡單，就是你錢付的不夠多   Miner 或是 Validator 傾向從手續費高的交易開始處理(因為我可以拿到比較多錢嘛)  常見的方法就是，你可以覆蓋之前的交易  用更多的手續費覆蓋先前的交易，可能可以讓 node operator 優先處理你的交易      為什麼說可能而不是一定，因為有可能其他人出價比你更高，那麼你還是得排在它後面    首先，你必須知道你要覆蓋哪一筆資料  nonce 作為 Transaction 的唯一識別數字，可以使用它  再來你需要手動指定交易手續費  做完以上就可以了   如果你是使用 Metamask 現在可以只按下 speed up 的按鈕就完成      早期 Metamask 還是必須用手動提高 gas fee 的作法，不過現在就是一鍵搞定         ref: How to speed up or cancel a pending transaction    Wei GWei ETH  1 ETH = $10^9$ GWei  1 GWei = $10^{10}$ Wei     Gas  每一筆 Transaction 所需要執行的運算資源，是需要付費的  而這個費用就是所謂的 Gas  不同量級的運算資源所需要耗費的 Gas 都不盡相同   手續費在 London 升級以前  由兩個數值所組成                  Gas Price(per unit)       Gas Limit(gas unit limit)                       針對每個 unit, 你願意付多少錢       一筆 Transaction 最多能使用多少 unit           Transaction 可能包含很複雜的邏輯，每一步的計算都需要手續費，unit 可以把它想像成每個步驟這樣  而 gas limit 就是每個步驟能夠使用的上限  因此，總共 最大可能耗費的金錢 為 gas price * gas limit      當然，有可能你花的錢少於最大金額，那麼剩下的會還給你    EIP 1559  我們上面有稍微提到，在執行 Transaction 的時候有可能會卡住  那是因為你付的手續費沒有高到 node operator 想要處理你的交易  Transaction Stuck 這裡提到的方法為手動設定手續費  但它顯然有點，怎麼說，如果你付了太多的錢，又會太過 有沒有一個好一點的方法，能夠改進它呢   London 升級的一大重點，就是 EIP 1559  旨在改善以下幾點     讓手續費能夠更容易預測   減少交易確認時間   改善使用者體驗   EIP 1559 引入了一個新的手續費機制，它主要包含了兩個新的概念   Base Fee  由於 gas price 是需要手動指定的，常常造成太多的錢被浪費  base fee 的引入，可以根據網路目前的使用狀態進行上下調整  當網路使用量大的時候，亦即 Block Size &gt; 50%, 則提升 base fee, 反之則降  而當 base fee 提高的時候，因為手續費變高，所以我們可以期待交易量會下降，進而達到網路穩定   與 Priority Fee 不同的是，Base fee 它不會到 node operator 的手上  它會把它銷毀  原因在於     避免通膨   提高 ETH 價值而非 ETH 數量   Priority Fee  priority fee 則是為了提高 Transaction 的處理速度，就像 Transaction Stuck 裡面提到的方法一樣  priority fee 是直接付給 Miner 或是 Validator  當作獎勵     gas price 被拆成兩個部份，base fee 與 priority fee  所以每一筆的 Transaction 計算方式就是  (Base Fee + Priority Fee) * 多少個 unit  在 Transaction Receipt 裡面你可以發現所需要的欄位  所以上述公式會變成 EffectiveGasPrice(每個 unit 要花多少錢) * GasUsed(使用了多少個 unit)      注意到 gas limit 依然存在，它並沒有被取代  你依舊可以手動設定一筆 Transaction 最多可以使用多少 unit    High Gas Price  高昂的 Gas Price 往往會造成使用者不願意付錢進行交易  Cryptokitties 在某一版本的實作當中  為了要列出使用者的第 n 隻貓咪，他們選擇用一個 for-loop 逐一檢查貓咪陣列，拉出符合條件的資料   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 function tokensOfOwnerByIndex(address _owner, uint256 _index)         external         view         returns (uint256 tokenId)     {         uint256 count = 0;         for (uint256 i = 1; i &lt;= totalSupply(); i++) {             if (kittyIndexToOwner[i] == _owner) {                 if (count == _index) {                     return i;                 } else {                     count++;                 }             }         }         revert();     }     ref: contracts/KittyOwnership.sol    我們上面有提到，Gas Fee 是基於你耗費了多少的算力而決定的  而 Cryptokitties 的實作，其執行時間會隨者 totalSupply 的大小而增加  換言之，運算的次數會隨之增加，最終導致高昂的 Gas Fee   那麼有人提出來一個改進的方法，我們可以紀錄一個 map  這樣就可以避免要逐一檢索全部的陣列資料   因此設計合約的時候，實作中你應該要考慮到耗費的資源  並且善用 hardhat gas reporter 等工具試圖優化   詳細可以參考原本的 bounty issue Listing all kitties owned by a user is O(n^2)   References     TRANSACTIONS   What is a function signature and function selector in solidity (and EVM languages)?   What is an application binary interface (ABI)?   ETHEREUM VIRTUAL MACHINE (EVM)   Ethereum Genesis Address: The “Black Hole” That Has Over $520 Million Worth Of Tokens   必讀指南 | 以太坊 PoS 時代：如何成為個人 ETH 驗證者？   PROOF-OF-STAKE (POS)   BLOCKS   CONNECTING THE EXECUTION AND CONSENSUS CLIENTS   Merkle Tree 和 Merkle Root 介紹   雜湊樹   GAS AND FEES   Beginners guide to Ethereum (3) — explain the genesis file and use it to customize your blockchain   What is Ethereum’s Genesis Address   Why the Ethereum genesis address holds over $500m worth of tokens   Understanding event logs on the Ethereum blockchain   【新手教學】到底什麼是Gas、Gas Price、Gas Limit？  ","categories": ["blockchain"],
        "tags": ["blockchain","ethereum","transaction","block"],
        "url": "/blockchain/blockchain-transaction/",
        "teaser": null
      },{
        "title": "設計模式 101 - Observer Pattern",
        "excerpt":"Observer Pattern  程式設計中，時常會需要處理到所謂的 “事件”  這些的事件的出現是 隨機的, 亦即你沒辦法判定何時何地會突然有一個事件送進來   而在傳統的實作當中，我們常常必須以 background-thread 對事件進行監聽  舉例來說，Polling，每隔一段時間主動詢問狀態是否更新  但這通常不是一個很好的辦法，因為很耗 cpu time   觀察者模式的出現可以很有效的解決上述問題  既然讓 Observer(觀察者) 一直去做監聽的事情，不如讓 Subject 在事件發生時，主動喚醒 Observer(就像 Webhook)  如此一來，觀察者就 不用一直傻傻的等待事件發生      注意到是 Subject 通知 Observer 有事件  所以 Observer Pattern 只是解決了等待事件而已    除了以上好處，Observer Pattern 還解決了哪些事情呢？  由 Observer 主動更新狀態而非由 Subject 主動更新所有 Observer, 這樣賦予了彈性     降低耦合性            更新的過程完全由 Observer 掌控可以大幅的降低耦合性(因為 Subject 內部要有每一個 Observer 的 update 方法)           依賴反轉            Subject 必須確保所有 Observer 都實作了相同的 interface(通知事件), 如果沒有統一規範，即使由 Subject 主動通知更新，也會遇到每個 class 的通知長的不一樣(e.g. getUpdate, update … etc.)       如此一來，Subject 依賴的對象就不會是 class 而是 interface 了           一對多關係            Subject 作為唯一擁有狀態的物件，負責將狀態同步更新至多個不同的觀察者(Observer)           Data  Observer pattern 主要以 Subject 進行主動推送資料至 Observer 實作  但是也有另一種方法是讓 Observer 主動更新(透過 Subject 的 public getter)  但一般來說是使用推送的方法居多   Implementation  這裡我們嘗試實作一次 observer pattern  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 from abc import ABCMeta, abstractclassmethod  # 定義 interface class Observer:     __metaclass__ = ABCMeta      @abstractclassmethod     def notify(self, value: int) -&gt; None: raise NotImplementedError  # 定義 subject, 發送事件 class Subject:     def __init__(self):         self.observers = list()         self.counter = 0      # 發送事件     def next(self, value: int) -&gt; None:         self.counter = value         self.notify_observers()      # 通知所有訂閱者     def notify_observers(self) -&gt; None:         for observer in self.observers:             observer.notify(self.counter)      # 訂閱     def subscribe(self, observer: Observer) -&gt; None:         self.observers.append(observer)      # 取消訂閱     def unsubscribe(self, observer: Observer) -&gt; bool:         try:             self.observers.remove(observer)         except ValueError as e:             pass         finally:             return True  # 訂閱者 john class John(Observer):     def __init__(self):         self.counter = 0      def notify(self, counter: int) -&gt; None:         self.counter = counter         print(f\"[John] new observable counter: {self.counter}\")  # 訂閱者 bob class Bob(Observer):     def __init__(self):         self.counter = 0      def notify(self, counter: int) -&gt; None:         self.counter = counter         print(f\"[Bob] new observable counter: {self.counter}\")  if __name__ == \"__main__\":     subject = Subject()     john = John()     bob = Bob()      subject.next(0)      subject.subscribe(john)     subject.subscribe(bob)     subject.next(1)      subject.unsubscribe(bob)     subject.next(2)   上述程式碼執行結果如下  1 2 3 4 $ python3 observer_pattern.py [John] new observable counter: 1 [Bob] new observable counter: 1 [John] new observable counter: 2   上述實作就是一個簡單的 observer pattern, 你可以看到當 subject 發送事件的時候，它會一一通知所有 observer list 裡面的訂閱者，使其主動更新資料     在 subject.next(0) 的時候因為目前沒有訂閱者，所以 john 跟 bob 都沒有收到上一個更新的資料(有時候你會希望說新的訂閱者能夠拿到上一個事件的資料，我們之後在 ReplaySubject 會提到)   在 subject.next(1) 的時候，john 以及 bob 都有拿到最新的訂閱資料   在 subject.next(2) 的時候，因為 bob 已經取消訂閱了，所以只有 john 有拿到更新的資料   Producer-Consumer Pattern  同樣都是發送訊息給訂閱者  Observer Pattern 是為了要讓 每個訂閱者都有資料  但是 Producer-Consumer 的目標在於，透過多個 consumer 消化資料  所以他的目的是 消化   所以我沒必要讓全部的訂閱者都拿到資料  因此，一筆資料只會由一個 consumer 處理  所以 Producer-Consumer Pattern 是屬於 1 To 1 的架構   Publisher-Subscriber Pattern  跟 Observer Pattern 一樣擁有 publisher(subject) 以及 subscriber(observer)  不同的是他們發送事件的方式     Observer Pattern 是直接通知訂閱者更新資料   Publisher-Subscriber Pattern 是透過 event bus 進行資料傳遞   也就是說 Publisher-Subscriber 可以用於不同系統上的事件監聽  透過將資料放在 event bus 當中讓訂閱者自行拿取  你有沒有覺的這個跟 shared memory 有點類似   同樣都是透過將資料放在同一個地方進行傳遞  並且也有生產者(publisher)以及消費者(subscriber)  而且他們都 不知道對方的存在，亦即誰送的資料誰收的資料其實對它來說都是未知的   唯一不同的是，訂閱者可能會訂閱不同的東西對吧？  假設 A 要訂閱 X, B 要訂閱 Y  那麼不同資料全部混在同一個 event bus 裡頭顯然是不合理的，因此我們需要對不同資料進行 filter 處理   Message Filter  Topic Based  publisher 送出的訊息中會帶有所謂的 topic, 而 subscriber 只會收到相對應的 topic 發送的訊息  你可以把它想像成標籤，它會對訊息進行分類   Content Based  根據內容屬性進行分類     當然也有部份系統支援兩種模式，亦即 publisher 發送帶有特定 topic 的訊息，而 subscriber 可以根據 topic 註冊其內容屬性  這邊附上兩張對比圖，可以更清楚的了解其差異         ref: Observer vs Pub-Sub Pattern    Differences Comparison                  Description       Observer Pattern       Publisher-Subscriber Pattern       Producer-Consumer Pattern                       Message Delivery       Synchronous       Asynchronous       Asynchronous                 Aware of Subscriber       :heavy_check_mark:       :x:       :x:                 Decouple       :x:       :heavy_check_mark:       :x:                 Broker       :x:       :heavy_check_mark:       :x:                 Type       1 to Many       1 to Many Many to Many       1 To 1           References     深入淺出設計模式 第二版(ISBN: 978-986-502-936-4)   Observer vs Pub-Sub Pattern   Publish-subscribe pattern   Publish/Subscribe vs Producer/Consumer?  ","categories": ["design pattern"],
        "tags": ["observer","observable","subject","publisher","subscriber","topic"],
        "url": "/design%20pattern/design-pattern-observer/",
        "teaser": null
      },{
        "title": "設定你的 Remote VS Code Server",
        "excerpt":"Preface  對於一個無時無刻都想寫 code 的工程師來說  如果能用手機，平板等等的設備開發，豈不美哉   不過常常受限於終端裝置的效能，抑或著是平台的關係  導致使用體驗並沒有很好   本篇文章，將會在你的開發電腦上面，設定一個遠端系統  允許你使用其他終端裝置存取，並享受隨時寫 code 的樂趣  對於會設定 SSH 相關的人，可以直接跳轉至 Code Server   SSH   Public/Private Key  SSH 仰賴公私鑰的系統進行運作  金鑰是使用 非對稱式加密法 生成的  而它的安全性是基於數學問題所提供的(大質因數分解)，也因此很難被破解  兩把鑰匙，一個加密，一個解密，是成雙成對的，使用其他把鑰匙是無法解出正確的資訊的  一般來說，公鑰可以在網路上裸奔沒有問題，但私鑰只能放在你自己身上且完全不能外流     生成一把金鑰最簡單的方式是透過以下指令  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 $ ssh-keygen -t rsa -b 8192 -C \"\" Generating public/private rsa key pair. Enter file in which to save the key (/home/user/.ssh/id_rsa):  Enter passphrase (empty for no passphrase):  Enter same passphrase again:  Your identification has been saved in /home/user/.ssh/id_rsa Your public key has been saved in /home/user/.ssh/id_rsa.pub The key fingerprint is: SHA256:vWQtmEj7WwgoQe7eGoOJ77jQD2hzg0nigQbdtC8mjK8  The key's randomart image is: +---[RSA 8192]----+ |  . .            | | + o .           | |. + o .          | |o+ . + o + .     | |++= + = S = .    | |*B+= . o + o     | |**Bo.   o o      | |o+o*.    o       | |Eo+ .   .        | +----[SHA256]-----+   其中                  Argument       Description                       -t       非對稱式加密演算法                 -b       金鑰長度                 -C       comment              詳細可以參考 man ssh-keygen    生成完畢之後會擁有兩個檔案，預設為     id_rsa :arrow_right: 私鑰   id_rsa.pub :arrow_right: 公鑰     另一種方式是透過 GPG 生成公鑰  取得公鑰的方法是  1 2 3 $ gpg --export-ssh-key xxx // xxx 可以讀取 Yubikey 取得 $ gpg --card-status  詳細可以參考 GPG 與 YubiKey 的相遇之旅 | Shawn Hsu     公鑰的 開頭 內容擁有特定的格式，如以下所示                  Valid public key format                                       ssh-rsa       ecdsa-sha2-nistp256       ecdsa-sha2-nistp384                 ecdsa-sha2-nistp521       ssh-ed25519       sk-ecdsa-sha2-nistp256@openssh.com                 sk-ssh-ed25519@openssh.com                           SSH Server  安裝 openssh server  1 $ sudo apt install openssh-server -y   更改 ssh 設定檔  1 $ sudo vim /etc/ssh/sshd_config  可以根據你的需要打開不同的設定  為了提高安全性，我只允許使用公鑰的登入方式並且將密碼登入停用  1 2 3 4 Port 22 PermitRootLogin no PubkeyAuthentication yes PasswordAuthentication no   設定檔更改完成之後  重啟 ssh server  1 $ sudo /etc/init.d/ssh restart     為了進一部增強安全性，特別是你的電腦可以對外的情況下  你可以設定允許連線的白名單   1 $ sudo vim /etc/hosts.allow  並填入白名單  1 sshd: 192.168.*.*     我只允許我內網的機器能夠連線    Upload Public Key to SSH Server  由於我們設定 ssh 的方式是不允許任何密碼登入  也因此不能使用 ssh-copy-id 的方式   如果你能夠直接存取伺服器，可以試試手動上傳金鑰  如果不行，那麼你可以先暫時打開密碼登入，並且使用 ssh-copy-id 的方式先上傳，完成之後在關閉密碼登入   將所有公鑰的內容複製並貼上至 ~/.ssh/authorized_keys 就可以了  1 2 3 4 5 $ cat ~/.ssh/id_rsa.pub $ touch ~/.ssh/authorized_keys // paste the public content $ chmod 700 ~/.ssh $ chmod 600 ~/.ssh/authorized_keys   ssh-copy-id  另一個方法就相對比較簡單，透過 ssh-copy-id 的指令上傳公鑰  1 $ ssh-copy-id -i ~/.ssh/id_rsa.pub -p port user@host   這個方式是使用密碼進行驗證  但即使你成功上傳了金鑰，沒有妥當的設定後續的登入也依然會使用密碼，詳細可以參考 SSH Config      port, user, host 要根據你的 SSH 伺服器位置而定    ERROR: failed to open ID file  如果你在使用 ssh-copy-id 的時候，遇到  1 ERROR: failed to open ID file   我遇到的情況有兩個問題點  第一個， -i 指定的檔案名稱後綴需要有 .pub  如果是單純 publickey 這種它會出錯   再來就是 Error 本身的錯誤  通常來說金鑰是成雙成對的出現，亦即擁有 public key 以及 private key  ssh-copy-id 在執行的時候會檢查，是否 key pair 都在該目錄底下  也因此，這個錯誤指的是它沒有找到私鑰      需要注意的是檔名的部份也要一致，也就是說 id_rsa 需要跟 id_rsa.pub 一起出現    你可以指定 -f 的 option 代表說不檢查私鑰  所以指令會變成  1 $ ssh-copy-id -i ./id_rsa.pub -f user@host   SSH Config  每次都要打那麼一長串的指令實屬麻煩  SSH 可以透過撰寫 config 的方式簡化   建立一個 config 檔並輸入以下資訊(touch ~/.ssh/config)  1 2 3 4 5 6 Host server     HostName 192.168.1.1     IdentitiesOnly yes     IdentityFile ~/.ssh/id_rsa     Port 22     User user   把所有連線資訊一併寫入設定檔，像是主機位置、port 以及使用者名稱  最最重要的是，使用金鑰驗證的方式     IdentitiesOnly :arrow_right: 要不要使用指定的私鑰   IdentityFile :arrow_right: 私鑰路徑(因為你可能有多個私鑰)   接著你就可以將  1 $ ssh user@192.168.1.1 -p 22  簡化成  1 $ ssh server   SSH Tunnel  Tunnel 指的是，將網路上的兩個端點以某種方式連起來，形成一個隧道的方式  SSH Tunnel 就是讓 SSH 建立這個隧道        ref: SSH Tunneling (Port Forwarding) 詳解    上圖可以看到，你沒有辦法直接連線 8080, 因為它沒有對外  因此透過 SSH Tunnel 的方式，我們可以稍微繞個路連接上我們需要的服務，就像下圖所示        ref: SSH Tunneling (Port Forwarding) 詳解    僅須一個簡單的指令，就可以建立 SSH Tunnel  1 $ ssh -L 8080:127.0.0.1:8080 server      詳細的 Tunnel 方式，可以參考 SSH Tunneling (Port Forwarding) 詳解    fail2ban  如果你的電腦是直接對外的，那麼最好要安裝 fail2ban  它可以 ban 掉登入失敗的機器   1 2 3 4 5 // install $ sudo apt install fail2ban  // enable on boot $ sudo systemctl enable fail2ban   新增規則設定檔($ sudo touch /etc/fail2ban/jail.local)，並填入以下  1 2 3 4 5 [sshd] enabled  = true maxretry = 2 findtime = 600 bantime  = 3600  重啟即可  1 $ sudo service fail2ban restart   Website Code Server  我們會使用 code server 作為遠端的伺服器  它可以讓我們以網頁的方式操作 vscode, 這通常很適合如果你是使用 iPad 或是手機操作的  當然，直接灌在系統上面有可能會污染系統，我個人較傾向使用 Docker 的方式   使用官方的 image 並且在 port 8080 打開對外服務  1 2 3 4 5 6 7 $ mkdir -p ~/.config $ docker run -it --name code-server -p 127.0.0.1:8080:8080 \\   -v \"$HOME/.config:/home/coder/.config\" \\   -v \"$PWD:/home/coder/project\" \\   -u \"$(id -u):$(id -g)\" \\   -e \"DOCKER_USER=$USER\" \\   codercom/code-server:latest   成功開啟之後，由於我們是使用 SSH 進行連接的，因此安全性可以由 SSH 提供，所以可以關閉 code server 本身的驗證  在你的伺服器執行以下指令關閉密碼驗證  1 $ sed -i.bak 's/auth: password/auth: none/' ~/.config/code-server/config.yaml  並重啟 code server($ docker restart code-server)   設定檔應該會長這樣  1 2 3 4 bind-addr: 127.0.0.1:8080 auth: none password: xxxxxxxxxxxxxxxxxxxxxxxx cert: false      注意到，這裡是 “關閉驗證”  但由於我們會透過 SSH 進行遠端連線，所以安全性是建構在 SSH 之上  如果不是，請務必打開驗證      最後，因為服務沒有對外  可以使用 SSH Tunnel 從外部連進 code server 進行開發  1 $ ssh -L 8080:127.0.0.1:8080 server   VSCode Code Server  除了網頁版的方式之外，VSCode 官方出的套件，可以讓你的本機電腦 VSCode 連線到遠端的 VSCode  聽起來有點雞肋，不過如果你像我一樣，可能剛好手邊的電腦效能沒有那麼好，就可以用這種方式   你需要的東西有     兩台電腦都必須要安裝 VSCode   下載 Remote - SSH 套件   然後點選左下角連接到 remote machine  如果你前面 SSH 都有設定好，它會跳出選項，跟著操作基本上就可以了   References     設定 Linux 開放 ssh 連線   How to Add SSH Public Key to Server   06. Symmetric and Asymmetric Encryption   How to setup SSH config ：使用 SSH 設定檔簡化指令與連線網址   Install   Port forwarding via SSH   $${HOME} or ${HOME} in Makefile?   SSH Tunneling (Port Forwarding) 詳解  ","categories": ["random"],
        "tags": ["vscode","linux","ssh"],
        "url": "/random/vscode-server/",
        "teaser": null
      },{
        "title": "設計模式 101 - Decorator Pattern",
        "excerpt":"Benchmark Time Elapsed  讓我們先從簡單的一個例子看起  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 import grpc import time from proto import echo_pb2 from proto import echo_pb2_grpc  # def benchmark(stub): #     stub.Echo(echo_pb2.EchoRequest(input=\"2\"))  def benchmark(stub, f):     start_time = time.perf_counter_ns()     stub.Echo(echo_pb2.EchoRequest(input=\"2\"))     end_time = time.perf_counter_ns()     time_diff = end_time - start_time      f.write(f\"{i + 1} {time_diff}\\n\")  if __name__ == \"__main__\":     round = 100000     with grpc.insecure_channel('localhost:6600') as channel:         stub = echo_pb2_grpc.EchoStub(channel)          with open(\"grpc-benchmark.txt\", \"w\") as f:             for i in range(round):                 benchmark(stub, f)   以我個人來說，我很常會需要對我的實作進行所謂的 benchmark  常見的方法即為，測試目標 function 所需要花費的時間，上述我想要測試 gRPC 的平均呼叫時間  可以看到，為了要加入測試時間的程式碼，我不得不重新修改實作本身，讓它得以 擴展 它原本的行為  除了執行基本的 gRPC 呼叫，它還可以測試執行時間，並寫到特定的檔案內   某種程度上它很丑對吧  為了新增某個功能而改變原本的實作本身屬實是不太好的行為  這個篇章我們將一窺 Decorator Pattern 可以如何解決這個問題      有關 gRPC 的相關介紹，可以參考 網頁程式設計三兩事 - gRPC | Shawn Hsu  詳細的實作程式碼可以參考 ambersun1234/blog-labs/RESTful_gRPC_JSON-RPC-benchmark    Open-Closed Principle     Class should be open for extension, but closed for modification    這個原則出現於 Bertrand Meyer 所撰寫的物件導向書籍   一個模組或者說物件，要怎麼樣同時保有這兩種特性？  不願意被修改可以很好理解，如果要新增功能，我們可以藉由 inheritance(繼承) 自己實作新功能  但是你又說歡迎擴展？ 不是不願意更改嗎？   回顧一下  一個物件由兩個基本的要素組成，屬性(data) 以及 行為(function)  一旦物件建立完成，我可以肯定的說，行為 是沒辦法輕易調整的對吧  屬性可以調整嗎？ 答案是可以的   在 設計模式 101 - Observer Pattern | Shawn Hsu 裡面我們就有幹過類似的事情  我們是不是可以 動態的 新增/刪除觀察者？  這些觀察者是 屬性(data), 但是更改的同時，我們並沒有動到任何行為(function)   Decorator Pattern  我們可以更進一步的用 callback function 改進我們的 benchmark 程式  1 2 3 4 5 6 7 8 9 10 11 12 13 def time_elapsed(func):     start_time = time.perf_counter_ns()     func()     end_time = time.perf_counter_ns()     time_diff = end_time - start_time      f.write(f\"{i + 1} {time_diff}\\n\")  def benchmark():     stub.Echo(echo_pb2.EchoRequest(input=\"2\"))  # caller time_elapsed(benchmark())   這本質上就是 Decorator Pattern 想達成的事情  我可以根據需要，動態的 加上額外的功能，而且我可以加很多層  且不會更改到原本的邏輯(即 Open-Closed Principle)   Object Oriented Programming  在 OOP 的世界裡，我們可以用比較 OO 的方法處理 Decorator Pattern  亦即不使用 function 包 function 的方式，而是採 class 包 class   設想我們有一個車輛訂購系統，使用者可以為他們的愛車新增選配(e.g. 行車記錄器、尾翼、避震 … etc.)  寫起來會長這樣  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 from abc import ABC, abstractclassmethod  # interfaces class CarInterface(ABC):     @abstractclassmethod     def cost(self) -&gt; int: raise NotImplementedError  class EquipmentInterface(ABC):     def __init__(self, car: CarInterface):         self.car = car      @abstractclassmethod     def cost(self) -&gt; int: raise NotImplementedError  # car class Benx(CarInterface):     def cost(self) -&gt; int:         return 100      # equipment class Recorder(EquipmentInterface):     def cost(self) -&gt; int:         return self.car.cost() + 12      class ShockAbsorber(EquipmentInterface):     def cost(self) -&gt; int:         return self.car.cost() + 80      if __name__ == \"__main__\":     car = Benx()     car = Recorder(car)     car = ShockAbsorber(car)      print(car.cost())   上述的實作可以這樣解讀  1 2 3 4 整輛車的價格 = 改避震器的價格 + 其他1 其他1 = 行車記錄器的價格 + 其他2 ... 依此類推  跟 divide and conquer 滿像的對吧？  老實說，這個方式跟非 OOP 的作法是一樣的，只不過我們是用 class 進行操作的     不論 OOP 與否，Decorator 的核心概念就是 動態的 新增額外功能，而不更改到原本的實作   Benchmark Time Elapsed with Decorator Pattern  Python 有內建提供給 Decorator Pattern 的 syntax sugar  當然，decorator 還是要由我們自己設計  於是可以改成最終完成版如下   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 import grpc import time from proto import echo_pb2 from proto import echo_pb2_grpc  def time_elapsed(func):     def measure_time():         start_time = time.perf_counter_ns()         func()         end_time = time.perf_counter_ns()         time_diff = end_time - start_time          f.write(f\"{i + 1} {time_diff}\\n\")      return measure_time  @time_elapsed def benchmark():     stub.Echo(echo_pb2.EchoRequest(input=\"2\"))  if __name__ == \"__main__\":     round = 100000     with grpc.insecure_channel('localhost:6600') as channel:         stub = echo_pb2_grpc.EchoStub(channel)          with open(\"grpc-benchmark.txt\", \"w\") as f:             for i in range(round):                 benchmark()      詳細的實作程式碼可以參考 ambersun1234/blog-labs/RESTful_gRPC_JSON-RPC-benchmark    採用 Decorator Pattern 之後，我既不會更改原本的實作，但我仍然可以擴充它，是不是很漂亮呢？      decorator 裡面的 i 與 f 都是取自 global variable scope  當僅僅讀取 global variable 的時候不需要使用 global xxx syntax       i 的 variable scope 不是存在於 for-loop, 而是存在於整個 function  可參考 Do iteration variable exist after the iteration statement in python? [duplicate]       if __name__ == \"__main__\" 的 variable scope 屬於 global  可參考 The scope of if __name__ == __main__    References     深入淺出設計模式 第二版(ISBN: 978-986-502-936-4)   Open–closed principle  ","categories": ["design pattern"],
        "tags": ["decorator","python"],
        "url": "/design%20pattern/design-pattern-decorator/",
        "teaser": null
      },{
        "title": "Prisma + Webpack + Docker 踩坑筆記",
        "excerpt":"Preface  前陣子為了其他系列的部落格文章的 lab，在練習一個簡單的 REST-ful API 的專案  途中遇到不少的困難，想說寫起來紀錄一下   用到的 tech stack 如題所述，稍微簡介一下  Prisma 是一款專為 Node.js 而誕生的 ORM 套件，透過 ORM 工具可以讓你輕鬆的序列化資料，不用自己硬刻轉換資料的部份  Webpack 則是負責將所有的檔案 “打包” 的一項工具  當然還有我們的老朋友 Docker   Environment  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 $ npx prisma version prisma                  : 4.16.2 @prisma/client          : 4.16.2 Current platform        : debian-openssl-3.0.x Query Engine (Node-API) : libquery-engine 4bc8b6e1b66cb932731fb1bdbbc550d1e010de81 (at node_modules/@prisma/engines/libquery_engine-debian-openssl-3.0.x.so.node) Migration Engine        : migration-engine-cli 4bc8b6e1b66cb932731fb1bdbbc550d1e010de81 (at node_modules/@prisma/engines/migration-engine-debian-openssl-3.0.x) Format Wasm             : @prisma/prisma-fmt-wasm 4.16.1-1.4bc8b6e1b66cb932731fb1bdbbc550d1e010de81 Default Engines Hash    : 4bc8b6e1b66cb932731fb1bdbbc550d1e010de81 Studio                  : 0.484.0  $ npx webpack version System:     OS: Linux 5.19 Ubuntu 22.04.2 LTS 22.04.2 LTS (Jammy Jellyfish)     CPU: (12) x64 AMD Ryzen 5 2600 Six-Core Processor     Memory: 22.04 GB / 31.28 GB   Binaries:     Node: 18.15.0 - /usr/local/bin/node     Yarn: 1.22.19 - /usr/local/bin/yarn     npm: 9.5.0 - /usr/local/bin/npm   Browsers:     Chrome: 114.0.5735.133   Packages:     copy-webpack-plugin: ^11.0.0 =&gt; 11.0.0      dotenv-webpack: 8.0.1 =&gt; 8.0.1      node-polyfill-webpack-plugin: ^2.0.1 =&gt; 2.0.1      ts-loader: ^9.4.4 =&gt; 9.4.4      webpack: ^5.88.1 =&gt; 5.88.1      webpack-cli: ^5.1.4 =&gt; 5.1.4      webpack-obfuscator: ^3.5.1 =&gt; 3.5.1  $ docker -v Docker version 24.0.2, build cb74dfc   How does Prisma Work  開發者透過定義 schema.prisma 檔案，定義資料庫的 table 結構，內容大概會長這樣  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 datasource db {     provider = \"mysql\"     url      = env(\"DATABASE_URL\") }  generator client {     provider      = \"prisma-client-js\" }  model User {     id            String       @id     username      String     created_at    DateTime     @default(now())     last_login_at DateTime     @updatedAt     RoomMember    RoomMember[]     Message       Message[] }  ...   主要就是三個部份     datasource            定義你用哪一種資料庫(e.g. mysql, postgresql)，然後他的 URL           generator            為了要生成對應的 typing(for TypeScript)       畫個重點，這裡很重要，跟 Webpack 有關           model            最後這裡就是定義 Table schema, 你可以定義多個 model             這時候，資料庫並沒有這些定義  所以要想辦法同步進去   由於 Prisma 本身是透過 JavaScript client 進行操作的(詳見 Prisma Architecture)  所以要先將客戶端生成出來  1 $ npx prisma generate --schema schema.prisma      generate 會在 npm i 的時候自動執行    然後就要同步 schema 了  1 $ npx prisma migrate dev --name init --schema schema.prisma      注意到一點，當你 migration 完成之後  migration 的 history 檔案也務必要加入版控裡面       預設定義檔路徑是 ./schema.prisma  如果你放在別的地方要指過去    到這裡，基本上你就設定完成了  然而，事情才剛剛開始   Prisma Architecture  Prisma v6(Legacy)  在眾多 Node.js ORM 框架裡，Prisma 是稍微年輕的後起之秀  而他的架構，是由 client 以及 server 所組成的，如下所示      ref: The query engine at runtime    Prisma 透過 JavaScript client 與 Query Engine 進行溝通，然後才到資料庫進行查詢  npx prisma generate 這行指令，上一節才看到，它會負責生成 client 以及 engine   注意到，query engine 是 binary(aka. 執行檔)  它會根據你目前的系統，自動下載相對應的 binary 到 node_modules/@prisma/engines 以及 node_modules/.prisma/client 裡頭      @prisma/client :arrow_right: prisma module 本體，下載後就不會改動了  .prisma/client :arrow_right: 根據你的 schema.prisma 動態生成的    檔案名稱的規則為                  Prefix       Platform       Postfix       Image                       libquery_engine-       Platform list       .so.node                        query-engine-       Platform list                          Prisma v7  新的 Prisma v7 改為使用 TypeScript 的 query compiler 取代了 Rust 的 query engine  你的 Query 會直接被編譯成 SQL(使用 query compiler)   所以 Prisma v6(Legacy) 裡面的 query engine 已經沒有作用了  也不需要下載，取而代之的是你需要接上 driver adapter 才能使用  而因為執行檔已經不需要了，所以你的專案實際上會更小，然後跨平台的支援度更高   Bundle all Source Code  當你完成你的程式碼，並且將他們打包的時候  你會意識到一個問題，在 Prisma Architecture 裡面有提到，Prisma 有一個 query engine 的 binary  而一般情況下，webpack 不會處理它，你需要手動將 binary 打包起來   這時候你需要 copy-webpack-plugin   copy-webpack-plugin  透過 npm 安裝  1 $ npm i -D copy-webpack-plugin   webpack.config.js 裡面加入 plugin  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 const CopyPlugin = require(\"copy-webpack-plugin\"); const path = require(\"path\");  module.exports = {     plugins: [         new CopyPlugin({             patterns: [                 {                     from: \"./src/database/prisma/schema.prisma\",                     to: \"./schema.prisma\",                 },                 {                     from: path.join(                         __dirname,                         \"./node_modules/.prisma/client/query-engine-linux-musl-openssl-3.0.x\"                     ),                     to: \"./query-engine-linux-musl-openssl-3.0.x\",                 },                 {                     from: path.join(                         __dirname,                         \"./node_modules/.prisma/client/query-engine-debian-openssl-3.0.x\"                     ),                     to: \"./query-engine-debian-openssl-3.0.x\",                 },             ],         }),     ] }   copy-webpack-plugin 基本用法就是這樣，複製某個檔案到某個位置  prisma 初始化的時候會跑兩個指令(generate 以及 migrate)  所以 schema 的定義檔也必須要複製進去，再來就是 query engine 的 binary 檔案   細心的你發現到，怎麼這裡複製的 binary 位置跟 Prisma Architecture 裡面講的不一樣?  prisma 預設會幫你下載跟系統一致的 binary(e.g. libquery_engine-debian-openssl-3.0.x.so.node) 要用它自然也是沒問題的  只不過我的例子是開發環境跟正式環境所使用的系統不一樣(ubuntu 以及 alpine)  prisma 提供了一個選項，可以指定你要用的 binary 有哪些  所以設定檔可以改成這樣寫  1 2 3 4 5 generator client {     provider      = \"prisma-client-js\"     binaryTargets = [\"linux-musl-openssl-3.0.x\", \"debian-openssl-3.0.x\"]     engineType    = \"binary\" }  其中，binaryTargets 可以在 Platform list 找到  然後切記 engineType 也要設定成 binary, 不然它會抓不到      這裡我有兩個 binary targets, 分別對應到 ubuntu 22.04 以及 alpine  你可以指定多個 binary, 它會自己抓合適的使用    而你指定的 binary, 會被下載到 node_modules/.prisma/client 裡面，如圖所示    Module not found: Error: Can’t resolve ‘fs’  webpack 5 以上，當你在打包的時候可能噴一堆 error 說它找不到 fs, http, crypto … etc.  一個簡單的作法是使用 node-polyfill-webpack-plugin plugin  你的 webpack.config.js 就會變成以下  1 2 3 4 5 6 7 const NodePolyfillPlugin = const NodePolyfillPlugin = require(\"node-polyfill-webpack-plugin\");  module.export = {     plugins: [         new NodePolyfillPlugin()     ] }   但是它會造成一點問題，可以參考 TypeError: argument entity must be string, Buffer, or fs.Stats  有一個更簡單的方法，只要將 target 設為 node 即可  1 2 3 4 5 // webpack.config.js  module.export = {     target: 'node', }   TypeError: argument entity must be string, Buffer, or fs.Stats  如果你跑起來，有遇到  1 2 3 4 5 6 TypeError: argument entity must be string, Buffer, or fs.Stats     at etag (/[...]/node_modules/etag/index.js:83:11)     at generateETag ([...]/node_modules/express/lib/utils.js:280:12)     at ServerResponse.send ([...]/node_modules/express/lib/response.js:200:17)     at ServerResponse.json ([...]/node_modules/express/lib/response.js:267:15)     at api.post (/the/code/above)  這個問題，是因為你在 webpack.config.js 裡面用了 node-polyfill-webpack-plugin plugin  把它移除之後就可以了  1 2 3 4 5 module.export = {     plugins: [         // new NodePolyfillPlugin()     ] }   Prisma Migration Inside Docker Container  將 application 容器化算是一個好習慣吧，至少對我來說這樣可以很方便的測試  但是碰上 Prisma 整體會稍微麻煩一點，且聽我娓娓道來   $ npx prisma migrate 的功用還記得嗎？ 就是將 table schema 同步到資料庫裡面  可是這行指令必須在你的 application container 裡面執行才可以(因為它需要 @prisma/client 以及你的 schema)  現今主流 container 的作法是把 app 跟 database 拆開  問題來了，你要怎麼同步 schema?   還有一點是，我們在 schema.prisma 裡面定義 URL 的時候是採環境變數的方式  1 2 3 4 datasource db {     provider = \"mysql\"     url      = env(\"DATABASE_URL\") }  要怎麼處理這塊也是一個問題     關於第二點其實如果你不用 environment variable 的寫法也不是不行  可以維護兩份不一樣的 schema 定義，在下 command 的時候指到不同的檔案  也可以 work 但顯然這樣有點蠢   Prisma Migration History  每一次你執行 $ npx prisma migrate 的時候，它會生成一個 sql 檔，存放於 ./migrations  裡面紀錄了每一次 migration 的 sql 檔  既然我有 .sql 那不用手動執行 $ npx prisma migrate 也沒差，還更省事   假設你用的 image 是 mariadb  有一個貼心的功能是，當資料庫 boot 的時候，它會自動執行所有放在 /docker-entrypoint-initdb.d 底下的檔案們(e.g. .sql, 可參考 Initializing a fresh instance)   因此你可以這樣做     複製最新的 migration sql 檔案   客製化 docker image, 將 sql 檔案塞入 /docker-entrypoint-initdb.d   如此一來，不需要手動執行指令，也可以初始化資料庫 table 了   1 2 3 4 5 6 7 8 9 10 11 USE restdb;  -- CreateTable CREATE TABLE `User` (     `id` VARCHAR(191) NOT NULL,     `username` VARCHAR(191) NOT NULL,     `created_at` DATETIME(3) NOT NULL DEFAULT CURRENT_TIMESTAMP(3),      UNIQUE INDEX `User_username_key`(`username`),     PRIMARY KEY (`id`) ) DEFAULT CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;      注意到 prisma 生成的 sql 裡面沒有指定資料庫  你需要手動指定(USE xxx)    1 2 FROM mariadb:latest COPY ./init.sql /docker-entrypoint-initdb.d/      另外如果你需要初始化資料 .csv 之類的檔案  記得放在 /var/lib/mysql 以外 的地方，由於權限問題，docker 沒辦法複製到這裡面    process.env undefined in Node Docker Image  同樣也是跟 prisma 有點相關的慘劇  1 2 3 4 datasource db {     provider = \"mysql\"     url      = env(\"DATABASE_URL\") }  除了 prisma 的環境變數，你可能有其他的環境變數設定要載入  不知道為啥，即使在 docker-compose.yaml 當中有設定環境變數，進去 container 裡面也看得到  但 application 就是啥都沒有   我原本用的 dotenv 它只會從檔案裡面讀取 .env  研判是這個導致的問題，因為在打包的過程中我並沒有將設定檔一併帶入，取而代之的是在 docker-compose.yaml 裡面定義   dotenv-webpack  後來我找到一款第三方的 webpack plugin dotenv-webpack  它可以讀取系統層級的環境變數，並載入使用  安裝也如同先前  1 $ npm i -D dotenv-webpack   webpack.config.js 可以改成以下  1 2 3 4 5 6 7 8 9 10 const Dotenv = require(\"dotenv-webpack\");  module.export = {     plugins: [         new Dotenv({             systemvars: true,             path: path.join(__dirname, \"./.env\"),         }),     ] }   除了載入 .env 之外，也將系統層級的環境變數寫入 process.env 裡面  這樣就可以 work 了   Example  有關上述所有的程式碼實作，你可以在 ambersun1234/blog-labs/cursor-based-pagination 找到   References     Express Response.send() throwing TypeError   Generating Prisma Client   Module bundlers   Configuring the query engine   Prisma Migrate   About migration histories  ","categories": ["random"],
        "tags": ["prisma","webpack","docker"],
        "url": "/random/prisma-webpack-docker/",
        "teaser": null
      },{
        "title": "資料庫 - 更好的分頁機制 Cursor Based Pagination",
        "excerpt":"Pagination Mechanism  相信一般做開發的，尤其是網頁相關  當資料量太大的時候，我們多半會選擇將資料切成多個部份傳輸  也就是分頁的機制   實作分頁機制可是有著大學問的  一起跟我看看吧        ref: Pagination    Page Number + Page Offset  說到分頁機制的實作，最簡單的當屬利用 SQL 的 LIMIT 以及 OFFSET 囉  只要在 query 資料庫的指令中，帶入前端帶的資料，設定好 LIMIT, OFFSET 就能夠實作  1 SELECT * FROM User LIMIT 10 OFFSET 10;  但，我們可不可以做的更好？   Cursor Based Pagination  這個的思想相對簡單  你想哦，如果是用 limit 跟 offset  假設要取第 10 頁的資料，是不是要從頭開始算，1, 2, 3, 4 ... n  好！ 找到了起始資料位置了，再來往後撈 n 筆就可以回傳資料了   這很有問題，對吧?  前面幾頁的速度可能差距較小，因為你很快就可以找到資料起始位置  但如果我要第 1000 頁的資料，你不就要數到頭暈   有沒有辦法直接 locate 起始資料的位置，然後從它開始往後拿就好  資料庫裡的什麼東西，有類似指標的功能，可以直接存取特定資料？  index 對吧   Index Recap  當然，大部分的 index 都是 B+ Tree index  所以沒辦法在 $O(1)$ 的時間內取得資料節點  不過相對於一個一個慢慢看還是快的多   資料庫為了要計算 offset  他是採用 full table scan 的(不然它怎麼知道前面有多少資料)  查詢了這麼多無用的資料，然後只要少少的幾個  當然很浪費時間，以及空間   我們是不是可以用 index  : 我要這個 index 以後的 10 筆資料 是不是快的多？  因為 index 可以快速的定位到資料的起始點，而不用載入我不需要的資料  找到起始點事情就簡單了，B+ Tree 的 leaf node 都是互相連接起來的，因此要查詢後面的資料很 easy 的      可參考 資料庫 - Index 與 Histogram 篇 | Shawn Hsu    Cursor Based Query  假設我有一個 User table 定義如下  1 2 3 4 5 model User {     id         Int      @id @default(autoincrement())     username   String        created_at DateTime @default(now()) }   你的 query 應該這樣寫  1 SELECT * FROM User WHERE id &gt; 9 LIMIT 10   寫起來跟 page number + page limit 有點不同  他的計算方法會有差  就好比如說 ?pageNumber=2&amp;pageLimit=10 跟 ?cursor=9&amp;pageLimit=10 的語意是一樣的   caller 負責紀錄目前的 cursor 位置在哪  在呼叫 API 的時候順便帶進去查詢即可  這一點的不同可以讓效能增加很多   How about Pagination with Sorted Field  道理我都懂，但是要怎麼將排序與 cursor 一起使用  很顯然的，ORDER BY 肯定要加，但 cursor based 可以直接套嗎？   來看個例子  假設我想要依據 username 以及 id 排序，他的結果會長這樣  1 &gt; SELECT * FROM User ORDER BY username, id     如果直接套上 cursor based 的方法，帶入 id 會如何  1 &gt; SELECT * FROM User WHERE id &gt; 7 ORDER BY username, id LIMIT 5     很顯然這不是我們要的，我期望的是，他的 username 應該至少是 z 開頭對吧  可是現在是 a 開頭，代表我們沒有選到正確的地方   排序的 cursor based, 必須確保 查詢條件能夠指到 \"一筆資料\" 而不是 \"一堆資料\"  你可以這樣寫  1 2 3 4 5 6 &gt; SELECT * FROM User      WHERE id &gt; 7 and username &gt; 'zzKSfVQhKK'      ORDER BY username, id LIMIT 5 &gt; SELECT * FROM User      WHERE username &gt; 'zzKSfVQhKK'      ORDER BY username, id LIMIT 5  可以這樣寫是因為 username 是 unique  但 id 不也是 unique 的嗎？  為什麼它指出來的是錯的？  那是因為我們的排序是下 ORDER BY username, id  先排 username 再排 id  username 靠前的 id 不一定小於 7  所以單靠 username 就可以正確的定位      這樣就正確了      詳細的實作細節，可以在 ambersun1234/blog-labs/cursor-based-pagination 找到    Sort with Non-unique Field  對於可能有重複值的欄位該怎麼辦   1 &gt; SELECT * FROM User ORDER BY created_at DESC, username     直接 query username 肯定是不行的  1 2 3 &gt; SELECT * FROM User      WHERE username &gt; 'enRakcznil'      ORDER BY created_at DESC, username LIMIT 5     因為你是先排時間，在根據 username 下去排  但是 created_at 它 不是 unique 的，因此直接寫也會錯  根據 sql 指令，當 created_at 相同的時候，才會依據 username 排序  所以這個功能的 cursor 應該這樣寫  1 2 3 4 &gt; SELECT * FROM User      WHERE (created_at &lt; '2023-07-10 06:50:36.000' OR          (created_at = '2023-07-10 06:50:36.000' AND username &gt; 'enRakcznil')     ) ORDER BY created_at DESC, username;      Previous Page and Next Page  前面提到的都是往後拿的，那往前的呢？   以 id 650 為中心       這裡的資料是 username DESC, id DESC 的結果    往後拿的 cursor 應該這樣寫  1 2 3 SELECT * FROM User  WHERE (username &lt; 'ZYruDDnmtB') OR (username = 'ZYruDDnmtB' AND id &lt; 650) ORDER BY username DESC, id DESC      因為原本的資料是倒序的，所以是 &lt;    然後拿到的資料長這樣    往前拿的 cursor 比想像種的還要簡單，就是將所有條件反過來  比方說比大小就由 &lt; 變成 &gt;, 然後排序就變成 ASC  1 2 3 SELECT * FROM User  WHERE (username &gt; 'ZYruDDnmtB') OR (username = 'ZYruDDnmtB' AND id &gt; 650) ORDER BY username, id     基本上你可以用同一個 cursor 使用它往前往後拿  其實沒有必要分成兩個 cursor，不過你應該會需要一個 previous 的 flag 來判斷方向的   ID, UUID or ULID  為了能夠讓 Cursor Based Pagination 可以正常運作  把一定的資訊透漏給外部是一件重要的事情   但一定要 id 嗎？  洩漏 id 通常是 bad practice  因為其他人可以拿他來做一點壞事  有幾種 alternatives 可以使用，就如同 title 所說  uuid, ulid 因為它們不會透漏太多訊息(亦即你看它就像個 random 的字串，無法解析出任何意義)  所以在實作的過程中，可以傳出去，到後端內部在自己轉就好了      cursor based pagination 裡面如果你選的欄位它能指到 “一筆特定的資料”  那也不需要使用 id 之類的，只要該 field 有 unique 即可    Benchmark Testing  起一個 Node.js 的後端系統  資料庫裡面包含了 10000 筆使用者資料  測試目標為，使用不同的方法對比查詢使用者資料的速度   測試是使用 python3 對 Backend system 進行 API 呼叫  取得往返時間差        上圖，是使用 offset 與 Cursor Based Pagination 的執行速度對比  y 軸為執行速度(單位為 nanosecond), x 軸則為資料起始點(i.e. 從第 n 筆資料開始往後拿 m 筆)  從上圖可以看到，使用 offset 的方法，它會隨著資料起始點的位置不同，而大幅度的增加查詢時間  而另一個方法，則是大約都維持在同一個水平      詳細的實驗細節，可以在 ambersun1234/blog-labs/cursor-based-pagination 找到    Offset Based faster than Cursor Based     你不難發現，在 query 前段的時候 offset based 是比較快的  這是因為 index lookup 要再看一次 table, 而一開始查詢 offset 的速度勝過 2 次 table lookup  因此，才有被反超的情況產生   Arbitrary Ordering Performance  另外我想測試的一個東西是，如果排序的欄位變多  效能影響有多大      排序的兩個測試，其欄位為 username 以及 created_at  都沒有 index, 而 without sort 則是使用 primary key   可以看到差了大概 $1 \\times 10^7$   Pros and Cons  儘管 Cursor Based Pagination 可以帶來很好的效能表現  但是也有一些事情是它做不到的  好比如說它沒辦法跳轉到指定的頁面  它只可以根據當前的 cursor 往前或往後   好處除了可以快速的定位資料，讀取之外  它不會受到資料新增刪減所影響  以往 offset based 的方法，如果新增一筆資料，end user 可能會在下一頁讀到相同的資訊  而 cursor based 則不受到影響   References     求求你别再用 MySQL offset 和 limit 分页了？   Do not expose database ids in your URLs   Cursor based pagination with arbitrary ordering   Is offset pagination dead? Why cursor pagination is taking over   Understanding the Offset and Cursor Pagination   MySQL - UUID/created_at cursor based pagination?   Mysql insert random datetime in a given datetime range   在 Linux 中以特定的 CPU 核心執行程式  ","categories": ["database"],
        "tags": ["sql","database","query","pagination","cursor","cursor based pagination","index","previous page","next page","uuid","ulid","page limit","page offset","full table scan","benchmark","prisma"],
        "url": "/database/database-cursor-pagination/",
        "teaser": null
      },{
        "title": "神奇的演算法 - Binary Indexed Tree",
        "excerpt":"Binary Indexed Tree  又名 Fenwick Tree, 是一種特殊資料結構，適用於需要大範圍的紀錄更新資料  像是下圖，假設我想要知道，達到 20% 採購率有哪些國家，達到 50% 的又有哪些  一般的作法是我可能開一個 map 去紀錄對吧 看起來會像以下這樣   1 2 3 4 5 6 7 8 map[rate][]string{     20: []string{         \"Finland\", \"United States\",     },     30: []string{         \"Spain\",     }, }   這種作法看似沒問題，但是當資料一多起來，你的程式跑起來將會非常的緩慢  線段樹的資料結構可以有效的利用最小空間，紀錄起這些龐大的資料，並且查詢速度相當的快速        5 Graph tables, add labels, make notes    Introduce to Binary Indexed Tree  Binary Indexed Tree(簡稱 BIT) 的核心思想就是建立一個表格，透過預先計算的方式，建構出完整的資料  BIT 透過將資料進行 分組 並紀錄於 一維陣列 當中  當使用者需要特定範圍的資料時，它可以以最小步數重建資料  重建資料？ 這樣不會太慢嗎   BIT 是以二進位的方式建立的，並且每個格子都存放著 不完全的累進的資料     為什麼是不完全？ 因為如果每個格子都紀錄完全的累進資料，那不就跟你用暴力法紀錄的一樣了(一樣慢阿)    Implementation  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 var (     size              = 100     binaryIndexedTree = make([]int, size + 1) )  func sum(index int) int {     sum := 0      for i := index; i &gt;= 1; i -= (i &amp; -i) {         sum += binaryIndexedTree[i]     }      return sum }  func update(index, value int) {     for i := index; i &lt;= size; i += (i &amp; -i) {         binaryIndexedTree[i] += value     } }  他的實作就只有上面這樣，非常的簡單阿  透過定義簡單的一維陣列，並初始化為 0  sum 計算從 1 ~ index 的累進數值  update 更新從 index ~ size 的累進數值   看到這，為什麼 update 要把後面的全部都更新呢？  它不能只更新 1 ~ index 就好嗎？  ㄟ還真的不行 且讓我娓娓道來   How does Binary Indexed Tree Works  前面提到，BIT 是透過將資料進行分組以達到高速計算的  那他是怎麼分組的？   BIT 借用了二進位的特性，亦即，所有正整數都可以以二進位的方式寫出來  考慮 19 這個數字，想像有一個大小為 19 的陣列且每個元素值為 1(下圖第一行)  接下來，19 這個數字可以被改寫成 $19 = 2^4 + 2^1 + 2^0$, 用顏色區分就會是 橘色 藍色 以及紅色(下圖第二行)  重點來了，我如果用迴圈慢慢加(i.e. 1+1+1+…+1) 是不是可以改寫成 16 個 1 + 2 個 1 + 1 個 1 呢？  那我為什麼不直接把數字直接標注在相對應的位子上呢！(下圖第三行)   仔細解析出來看就是  1 2 3 19: 0b10011 18: 0b10010 16: 0b10000  所以 19 的累進資料的算法是 (19 到 19) + (18 到 17) + (16 到 1) 上面個別紀錄不完全的累進資料相加就是了  再看一個例子  1 2 3 26: 0b11010 24: 0b11000 16: 0b10000  所以 26 的累進資料的算法是 (26 到 25) + (24 到 17) + (16 到 1) 加總     仔細觀察你就會發現，每一次的往下更新  它都是 移除最右邊數值為 1 的 bit  直到為 0   移除最右邊數值為 1 的 bit 可以用下列公式寫出來      重建的公式是 $x + (x\\ {\\&amp;}\\ {-x})$  取累進數值是 $x - (x\\ {\\&amp;}\\ {-x})$  你可以嘗試手算一下 19 跟 26 的推導累進    所以你可以看到，建資料的成本不高，複雜度才 $O(Log\\ n)$     回到上一節提到的問題  為什麼重建不能從 index ~ 1 而偏偏要 index ~ size 呢？  道理其實很簡單，因為 BIT 紀錄的是 累進資料  假設你要 update(10, 1) 好了，他的重點是，當我查詢的範圍有包含到 10 的時候，必須要算到  如果往下重建資料，它會連 1 ~ 9 都被 + 1  很明顯這不是正確的結果   LeetCode 1854. Maximum Population Year  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 var (     offset = 1949     size = 110 )  func sum(binaryIndexedTree []int, index int) int {     mysum := 0     for i := index; i &gt;= 1; i -= (i &amp; -i) {         mysum += binaryIndexedTree[i]     }     return mysum }  func update(binaryIndexedTree []int, index, value int) {     for i := index; i &lt; size; i += (i &amp; -i) {         binaryIndexedTree[i] += value     } }  func maximumPopulation(logs [][]int) int {     binaryIndexedTree := make([]int, size + 1)      if len(logs) == 1 {         return logs[0][0]     }      for _, log := range logs {         update(binaryIndexedTree, log[0] - offset, 1)         update(binaryIndexedTree, log[1] - offset, -1)     }          maxYear := 0     maxPopulation := 0     for i := 1950; i &lt;= 2050; i++ {         population := sum(binaryIndexedTree, i - offset)         if population &gt; maxPopulation {             maxPopulation = population             maxYear = i         }     }      return maxYear }   1854 這一題其實也可以使用 binary indexed tree 解  題目要求是要求出人口最多的所在年份是哪一年  然後他有提供每一個人的出生以及死亡日期   所以我的想法是，建立一個 array，每個欄位都儲存該年份，有多少人  因為 binary indexed tree 的特性，他是更新 n ~ size 的資料欄位  每個人的資料時長不同，僅會存在於 birth ~ dead 之間，因此我們要把這個操作改成     birth ~ size 是 1   dead ~ size 是 -1(要把它扣回來)      其中 birth &lt; dead    最後當我們把陣列建立完成之後，在從頭掃過一遍就好了      因為實際上的資料維度只有 100(1950 ~ 2050)  所以實際上不用將陣列大小開到 2000    References     树状数组（Binary Indexed Tree），看这一篇就够了   Binary Indexed Tree or Fenwick Tree  ","categories": ["algorithm"],
        "tags": ["binary indexed tree","binary tree","fenwick tree","cumulative"],
        "url": "/algorithm/algorithm-binary-indexed-tree/",
        "teaser": null
      },{
        "title": "資料庫 - 初探分散式資料庫",
        "excerpt":"Distributed System  Scale Out(Horizontal Scale) 的概念是利用多台電腦組成一個龐大的網路，進行運算提供服務  這個網路，稱為 cluster      注意到 cluster 跟 distributed system 在定義上是有所不同的         cluster :arrow_right: work as a single system     distributed system :arrow_right: work together to achieve common goal       cluster 為 distributed system 的一種    分散式系統通常由網路將節點(node)連接起來，提供服務   Brief Introduction of Issues with Distributed System  多節點在設計上需要注意很多事情   比如說最簡單的資料複製的問題  我有很多個節點，我希望不管我 query 哪一台，資料永遠會是一致的(i.e. Consistency)   我要怎麼確保並發(concurrent)讀寫的時候，衝突得以正確的被解決？   又或者網路出問題  我的系統是不是還能 work?(i.e. Availability 以及 Partition Tolerance)  更甚至節點掛掉怎麼辦？      有關分散式系統中，會遇到的問題  可以參考 資料庫 - 從 Netflix 的 Tudum 系統看分散式系統中那些 Read/Write 問題 | Shawn Hsu    CAP Theorem  CAP Theorem(又名 Brewer's Theorem)，是一個分散式系統的 trade off 規則   前面提到了一些分散式可能會有的問題，所以不難發現，使用分散式系統是會有代價的，而這個代價分為三種   Consistency(C)  資料的正確性對於某些系統而言，是很重要  如同先前提到的，要怎們確保資料在每一個節點都是正確的呢？   另外，它需要馬上正確嗎？ 還是它可以提供 最終正確性(Eventual Consistency) 就好？      Eventual Consistency 指的是總有一天它會同步完成  較正式的定義是: 暫停寫入，給一點時間，follower 最終會同步完成，保持資料一致    Availability(A)  可用性，指的是說，即使有部份節點意外下線(可能是系統重啟等等的)  我的系統仍然可以順暢的處理服務   Partition Tolerance(P)  分區容錯性的意思是，節點之間斷開連線(可能是網路問題)  系統依然可以正常的運作      你會想，它長的跟 Availability 可真像  回顧 distributed system 的定義，是指 “共同合作達成某個目標”  所以一件事情可能仰賴多個節點(每個 node 各負責不同的事情)     既然是 “共同合作”, 所以萬一途中有個節點無法建立連線  那麼目標就無法達成，因此 partition tolerance 是在確保這件事情      以上三種代價，根據 CAP Theorem 所述，分散式系統必須犧牲其中一個代價  因此，可以得知有三種系統                  System Type       Description                       CP       犧牲 Availability                 AP       犧牲 Consistency                 CA       犧牲 Partition Tolerance           具體要用哪一種，取決於你的需求  不過 CA 系統較少使用，因為其犧牲了 Partition Tolerance  分散式系統中，大量的依賴了網路的基礎建設，因此網路出問題是不可避免的，所以基本不會選擇 CA 系統   Replication  既然我們討論的 scope 是資料庫嘛，一定會遇到資料要複製到不同節點上的問題  擁有相同資料的節點稱為 replica(副本)   一般而言，資料複製可以簡略的分為兩種類別                  Category       Description                       Synchronous       當每個節點都同步完成之後，才視為操作完成，並返回 client                 Asynchronous       只要當前 node 寫入之後，立即返回，其餘節點的同步依靠背景程序           至於資料複製的路徑有這三種(如圖所示)      ref: Multi-Leader Replication Topologies    上圖適用於 Multi Leader 的複製(因為 Single Leader 的路徑是唯一的)  環狀跟星狀的會有缺點就是只要其中一個節點壞掉，它就不能往下同步   實務上，資料複製有幾種作法  一一看吧   Statement Based  最直覺想到的作法就是將 SQL 語句(e.g. INSERT INTO User xxx VALUES xxx)同步到各節點上，然後每個節點自己執行  但是要注意到一些 non deterministic function 如 NOW() 或是 RAND()  這些函數在不同節點上，它不一定會回復一樣的值，即使它用 Synchronous 的方法也一樣   解決辦法也算簡單，只要使用固定數值予以取代即可   Write-ahead Log  為了避免系統掛掉之後資料遺失，通常會有所謂的 log file  而資料庫系統也有，稱為 write-ahead log   真正寫入磁碟之前，會額外存一份在 log file 裡  而此類 log file 的內容通常會更靠近底層  因此它與     資料庫系統   資料庫系統版本   是緊密耦合的   在遇到不同系統，不同版本的節點，要做升級只有兩條路     zero downtime upgrade   停機升級 :arrow_left: 這種狀況要盡量避免(因為停機等於中斷服務，等於沒錢)   Logical Log  跟 Statement Based 的概念類似  不同的是 logical log 會將真正的 values 算出來   1 2 INSERT INTO User(userID, username, created_at)  Values(1, \"ambersun1234\", \"2023-08-07 15:40:06\")   可以看到這裡的 created_at 我先把它算出來  然後這串數值同步到其他節點上，就不會遇到 non-deterministic function 的問題了  記得 log 裡面要包含能指到特定 row 的資訊哦(e.g. primary key)      有關 index 的討論，可以參考 資料庫 - Index 與 Histogram 篇 | Shawn Hsu    Store Procedure Based  這個方法提供了一些客製化的彈性  就是說前面幾種都是完整的資料複製，透過 Store Procedure(SP) 可以對 log file 進行特定的操作  比如說你只想同步某幾筆這樣   由於其高度客製化的特性，也有不少人選擇使用它      有關 Store Procedure 可以參考 資料庫 - 最佳化 Read/Write 設計(軟體層面) | Shawn Hsu    Architecture  Single Leader(Master Slave)  叢集裡面，選一個節點當 leader(master), 剩下的人都 follower(slave)  這樣分要幹啥子呢        ref: Chatper 5. Replication    他的目的在於區分讀寫  write 只找 leader, 而 read 可以隨便找任意一個都行  這樣的好處在於說它可以 提昇讀取的效率  壞處也很明顯 寫入的速度會被受限(但寫入不會有衝突)   遇到節點損壞的時候，勢必要將其中一個 follower 升格為 leader  升格的過程你可能會遇到一些問題，舉例來說，有可能 old leader 沒有意識到它已經被降級了  它仍然在執行 leader 的功能，這就會造成同一時間有兩個 leader 存在的問題  而這稱為 Split Brain(腦分裂)      有關 Split Brain 的討論，可以參考 資料庫 - 從 Netflix 的 Tudum 系統看分散式系統中那些 Read/Write 問題 | Shawn Hsu    Multi Leader       ref: Use Cases for Multi-Leader Replication    既然 Single Leader 僅解決了讀取的效能問題  那麼允許多個節點提供寫入的功能不也行嗎  當然是沒問題的   使用 multi leader 的方法可以大幅度的的高讀寫的效能  它僅有一個缺點，寫入會有衝突，而這是一個很大的缺點  就像 git 在使用的時候一樣，如果多個人對同一份檔案進行更改  那要以誰為主？   因此實務上，Multi Leader 會增加系統的複雜度   Leaderless  顧名思義，沒有所謂 leader(master) 的存在  亦即每個節點都可以接受 read/write  通常實作都是同時的將請求發給 所有的 node      或者是由 coordinator 協助轉送 request    那我要怎麼確保每個節點的資料是一致的？  Multi Leader 會遇到的問題，我也會遇到  那它不就跟它長的一樣了   一樣都允許 並行寫入，一樣都會遇到 衝突  差別在於，當遇到衝突的時候     Multi Leader 僅需要 leader 們解決   Leaderless 需要全體成員參與決策   Consensus  所以你有多個節點，讀寫資料是一個問題  每台機器上保存的資料可能都有一點落差(因為同步的問題)，你要怎麼確定 “哪一個資料” 才是正確的？  前面看了 single leader, multi leader, leaderless  他們要怎麼互相的協調才能提供 正確的資料？   Quorum Consensus  想的簡單點，Quorum 共識機制其實就是 取得多數人的同意  什麼意思？ 當某個新的資料要寫入資料庫的時候，要怎麼確定資料已經寫入？  10 個節點只有其中 1 個人說寫入了，剩下 9 個人都說還沒  這樣應該不會視為是成功寫入   當多數人都同意寫入成功之後，這個資料才算是成功寫入  這就是基本的 Quorum Consensus 的概念   好處在於說，當一個節點掛掉的時候，其他人還認得說有這件事情發生過  換言之，資料不會不見   n 個節點，需要取得     w 個節點確認寫入成功才算數   r 個節點確認讀取成功才算數   要保證每次的讀取都有 最新 的值，可以遵照這個公式 w + r &gt; n  他可以確定至少有一個節點擁有最新的資料(前提是 w 跟 r 的副本有重疊到)   Tolerance to Node Failure  假設 11 個節點     w 有 6 個節點   r 有 7 個節點   那最多你可以容忍 4 個節點掛掉  因為     w 的情況下需要 6 個節點是好的，總共是 11 個節點，4 個壞掉，7 個還是好的，所以是 有餘欲的   r 的情況下需要 7 個節點是好的，總共是 11 個節點，4 個壞掉，7 個還是好的，所以是 剛好足夠的   Sloppy Quorum  但是網路是不可靠的，網路斷線是很常發生的事情  也就是說容錯能力是有限的，斷個網 quorum 就失效了  畢竟你就是要那麼多節點幫你確認   所以一個作法是，我一定需要 “那個” 節點嗎？  我可以有一個 hot standby 的節點，當 online 的節點掛了就將資料同步過去(i.e. Failover)  是不是也算滿足 quorum 了呢？   事實上也的確如此  等到節點恢復的時候在將資料同步回來即可   Raft Consensus  我們在 資料庫 - 從 Apache Kafka 認識 Message Queue | Shawn Hsu 裡大概知道 Raft Consensus 他是怎麼做的  那他跟 Quorum Consensus 有什麼不同呢？   主要差異在於 Raft 有選舉的機制  當 leader 掛掉的時候，Raft 會自動選出新的 leader  而 Quorum 是使用多數決，他沒有所謂的 leader 以及選舉制度   Different Approaches of Increasing Demands on Computer System       ref: Upgrade Button    Scale Up(Vertical Scale)  Random I/O vs. Sequential I/O  硬碟的讀寫有分隨機讀寫以及連續讀寫   他們的差別也很好理解  連續讀寫 - Sequential I/O 亦即 你要找的資料就在下一個，所以他是連續的  而 你要找的東西不在下一個區塊，就是屬於 隨機讀寫 - Random I/O   對於硬碟效能來說，這兩個讀取效率如果越高代表效能越好  你可以使用 CrystalDiskMark 這套軟體測量硬碟讀寫的效能       ref: 【問題】請問CrystalDiskMark的數值應該怎麼看?    那麼哪些東西屬於隨機讀寫，哪些又是連續讀寫呢？     連續讀寫 - Sequential I/O            當你需要 copy 大檔案到，比如說你的隨身碟上面，這時候使用的就會是連續讀寫(因為你的一個檔案肯定是一個 byte 接著一個 byte 的)           隨機讀寫 - Random I/O            其他剩下的基本上都屬於隨機讀寫(比如像是 掃描病毒 之類的)       其中 資料庫 也是屬於隨機讀寫           值得注意的是  傳統硬碟 HDD，針對隨機讀寫，會有影響  主要的原因是因為 HDD 採用機械結構，每一次的移動讀寫頭都是需要時間的  Disk Seek 在 HDD 幾乎不可能平行運算，因為機械裝置只有一個     Seek Time :arrow_right: 尋找 Track 的時間   Rotational Latency :arrow_right: 尋找 Sector 的時間   Transfer Time :arrow_right: 資料傳輸至記憶體的時間        ref: CS354: Machine Organization and Programming    至於 SSD 因為沒有讀寫頭這種機械裝置(而是採用 Nand 顆粒)，所以不會有這個問題     單純的堆料，提升單台伺服器的效能的方法  它終究是有其限制所在的，你無法加 “無限” 顆硬碟或 CPU 在一台電腦上  但你不得不承認他是一個簡單又暴力的解決辦法  這裡列出幾個有可能出現的瓶頸點供參考                  Hardware       Description                       CPU       為了要能夠處理從 Disk 撈出來的資料，CPU 扮演了一個很重要的角色，時脈的高低取決於你能夠以多快的速度處理這些資料                 Disk       毫無疑問的，硬碟是很重要的，從以前的 Magnetic Tape 磁帶，Hard Disk Drive 傳統硬碟 到現在的 Solid-state Drive 固態硬碟，硬碟速度越高，代表能夠處理的越快速                 Memory Bandwidth       當 memory 的頻寬來不及寫入 CPU cache, 這時候它就有可能為成為瓶頸所在。不過，通常這個不太會發生           Scale Out(Horizontal Scale)  一台不夠？ 多台來湊！  多台的架設成本可能會比升級 CPU, 硬碟等等還要來的划算  也因此 scale out 也成為現今的主流選擇之一   Summary       ref: https://www.researchgate.net/figure/Scale-out-storage-compared-to-scale-up-storage_fig1_273702105    References     資料密集型應用系統設計(ISBN: 978-986-502-835-0)   內行人才知道的系統設計面試指南(ISBN: 978-986-502-885-5)   Optimizing for Random I/O and Sequential I/O   【恐龍】理解 I/O：隨機與順序   Difference between Internal and External fragmentation   15.11.2 File Space Management   MySQL: What is a page?   How does mysql indexes turn random I/O into sequential I/O   Can we calculate the bandwidth for a CPU?   SDRAM 與 DDR：他們之間有何差異？   Cpu cache and memory bottleneck   Interview question: How to scale SQL database to allow more writes?   Difference between Distributed and Cluster? What is a cloud computing platform? Distributed application scenarios?   CAP theorem - Availability and Partition Tolerance   Quorum &amp; Raft &amp; Paxos  ","categories": ["database"],
        "tags": ["database","distributed","cluster","CAP","single leader","multi leader","replication","scale up","scale out","leaderless","sequential io","random io","quorum consensus","raft consensus","write ahead log","logical log","store procedure","split brain","quorum","raft"],
        "url": "/database/database-distributed-database/",
        "teaser": null
      },{
        "title": "如何正確模組化你的 OpenAPI 文件，以及如何建立 Mock Server",
        "excerpt":"Preface                          OpenAPI       Swagger                       Picture                               Picture Reference       https://swagger.io/       https://swagger.io/           OpenAPI 的規範在 2021/02 的時候來到 3.0，而它跟 Swagger 的關係就有點像是 Docker 跟 Open Container Initiative 的關係  也就是說是 Swagger 的開發團隊將他們的規範，貢獻出去給 OpenAPI Initiative  所以如果你看到 Swagger, 可以簡單的把它聯想為 OpenAPI(還是要注意他們的關係)   簡言之，OpenAPI 是文件的規範，Swagger 除了貢獻規範之外它還有其他的工具，這裡就不贅述   Introduction  OpenAPI 3.1.0 §4.3 裡提到     An OpenAPI document MAY be made up of a single document or be divided into multiple,   connected parts at the discretion of the author.    從上述可以得知，OpenAPI 文件是可以進行模組化處理的  不過使用 swagger-ui-express 的時候  需要用一點點的方法才可以 reference 到其他文件   所以這篇 blog 會詳細紀錄如何達到這件事情，以及有哪些坑   Reference Anchor $ref  OpenAPI 有一個我發現很討厭的事情是，它寫起來又臭又長  理所當然的能夠將文件拆分，不僅改寫容易，模組化也有助於 maintain   慶幸的是，OpenAPI 裡面你可以使用 $ref 的關鍵字  根據 OpenAPI 3.1.0 §4.8.23  $ref 可以用於 internal 或是 external reference, 其中 $ref 必須是 URI 的形式(注意到它跟 URL 的差異)   Internal Reference  1 2 3 # main.yaml  $ref: '#/components/schemas/Cat'   要指到同一個文件的 reference 是這樣寫  其中，# 字號代表該文件下的，後綴就代表指向文件內容的 URI   1 2 3 4 5 6 7 8 9 # main.yaml ...  components:   schemas:     Dog:       description: This is a dog     Cat:       description: This is a cat  所以它會從 document root 開始找，找到 components 再找到 schema 再找到 Pet  因此上述的 ref 它會被替換成  1 description: This is a pet   External Reference  1 2 # main.yaml $ref: './pet.yaml#/components/schemas/Cat'  1 2 3 4 5 6 7 # pet.yaml components:   schemas:     Dog:       description: This is a dog     Cat:       description: This is a cat   外部 reference 就是在前面新增檔案位置  所以 main.yaml 經過編譯後會長成這樣  1 description: This is a cat     需要注意 encode 的問題，可參考 JSON pointer    Circular Reference  在 OpenAPI 3.1.0 的規範中，並沒有實際的提到對於 circular reference 的限制  實際上在測試的時候，它也是允許的，並不會報錯   只是說有一些工具如 Redocly OpenAPI VS Code extension  對於 circular reference 沒辦法正確的識別，會造成套件部份失效  這個就要額外注意   JSON pointer  在使用 $ref 的時候有一點要注意，如果你是 reference 到 endpoint 的時候  該 endpoint url 必須要 encode   RFC 6901  encode 在一般情況下是使用 URL encode, 但是有些字元在 JSON pointer 裡有特殊的意義  根據 RFC 6901 所述      Because the characters ‘~’ (%x7E) and ‘/’ (%x2F) have special  meanings in JSON Pointer, ‘~’ needs to be encoded as ‘~0’ and ‘/’  needs to be encoded as ‘~1’ when these characters appear in a  reference token.    所以當你想要 reference url 的時候要稍微處理一下  比如說 /cat/{catId} 要被 encode 成  1 $ref: \"./cat/cat.yaml#/~1cat~1%7BcatId%7D\"   其中     / 是 ~1   { 是 %7B   } 是 %7D   Node.js Libraries   swagger-jsdoc  swagger-jsdoc 本身可以拿來組合多個 yaml 檔  將它合併成完整的一個 OpenAPI doc   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 // config.js import swaggerJSDoc from \"swagger-jsdoc\";  export const option = swaggerJSDoc({   definition: {     openapi: \"3.1.0\",     info: {       title: \"Swagger API Test\",       description:         \"This document shows how to use $ref in multiple swagger file\",       version: \"1.0.0\",       servers: [         {           url: \"http://localhost\",         },       ],     },   },   apis: [\"./doc/**/*.yaml\"], });  // doc.js import express from \"express\"; import swaggerUi from \"swagger-ui-express\";  import { option } from \"./swagger-jsdoc/config.js\";  const uiOption = {   swaggerOptions: {     docExpansion: false,   }, };  const app = express(); app.use(\"/\", swaggerUi.serve, swaggerUi.setup(option, uiOption)); app.listen(3000);   從上述設定檔可以看到，我們指定了 apis: [\"./doc/**/*.yaml\"]  因此，它會找到所有的文件檔並生成一個新的檔案，所以最後 express 的資料，會是處理過後的  值得一提的是，跨檔案使用 $ref 是沒有效果的  因為它合併成一個檔案了，所以那些 external link 都會找不到   swagger-combine  swagger-combine 也是另外一個可行的選擇   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 // config.js import swaggerCombine from \"swagger-combine\";  export const option = await swaggerCombine(\"./doc/api.yaml\");  // doc.js import express from \"express\"; import swaggerUi from \"swagger-ui-express\";  import { option } from \"./swagger-combine/config.js\";  const uiOption = {   swaggerOptions: {     docExpansion: false,   }, };  const app = express(); app.use(\"/\", swaggerUi.serve, swaggerUi.setup(option, uiOption)); app.listen(3000);   僅須簡單的將你的進入點丟給它處理，swagger-combine 就會自動 resolve 所有 reference  此外你也不需要像是 swagger-jsdoc 一樣重複定義區塊   Cons  對於 swagger-jsdoc 來說     跨檔案 $ref 無法使用   需要額外定義一次 info, servers … 等等的區塊      這裡要稍微提一下，其實你可以在 yaml 那裡單純的從 path 開始寫，info, servers 不需要兩邊寫  只不過如果你有用 redocly 這類的 linter, 針對單純的 yaml 它會報錯  但這部份就看你們的選擇    而 swagger-combine 的缺點則是     需要手動引入 endpoint   Implementation Example  上述的例子你可以在 ambersun1234/blog-labs/swagger 當中找到   Work with Docker  上述提到的做法是使用 Node.js 來處理，但是其實不用那麼麻煩  可以使用官方的 Docker image 來處理   1 2 3 4 5 $ docker run --rm -p 8080:8080 \\     --name swagger-api \\     -e SWAGGER_JSON=/swagger/openapi.yml \\     -v $(pwd)/api:/swagger \\     swaggerapi/swagger-ui   這裡會用到 swaggerapi/swagger-ui 這個 image  SWAGGER_JSON 是你的 OpenAPI document root  他不一定要是 json 檔，如上所示這裡是給 yaml 檔   然後他的路徑是 map 到 container 底下的路徑記得不要寫錯  跑起來之後你應該會看到正確的 API doc   每次文件的修改更新都會即時反映在 swagger-ui 上面  只是需要手動 reload 一下      如果你看到 document 的內容是 pet and store  這個代表 swagger 預設的文件，言下之意是你的路徑有錯  記得檢查一下    Create Mock Server with OpenAPI Document  寫文件除了是前後端溝通的橋樑之外，他也可以用來建立 Mock Server  以往的開發流程，前端往往需要依賴後端的實作，這對於開發效率來說是很差的  並且以開發階段來說，兩個部門也不應該有太多的依賴   若是能直接使用 OpenAPI 文件來建立 Mock Server，這樣就可以讓前端開發者不用等待後端的實作  並且能夠有一個真正的後端系統來測試   prism 這套工具可以無痛的幫你建立 Mock Server  僅僅需要簡單的 docker run 即可      prism 本身也會做 auto reload，即時的修改也會即時反映在 mock server 上面    1 2 3 4 $ docker run --init --rm \\   -v $(pwd)/api:/api \\   -p 4010:4010 stoplight/prism:4 \\   mock -h 0.0.0.0 \"/api/openapi.yml\"   然後你的 API 就會跑在 http://localhost:4010 上面  其中 /api/openapi.yml 是你的 OpenAPI 文件路徑  對於多檔案的情況，他會自己做處理，這個檔案是你的進入點即可      其實你也可以給一個 url，它會自動幫你下載    針對開發不同的 response 他也有相對應的機制  比如官方網站提到的 Modifying Responses   1 2 3 4 5 6 7 8 9 $ curl -v http://127.0.0.1:4010/pets/123 -H \"Prefer: code=404\"  HTTP/1.1 404 Not Found content-type: application/json content-length: 52 Date: Thu, 09 May 2019 15:26:07 GMT Connection: keep-alive  $ curl -v http://127.0.0.1:4010/pets/123 -H \"Prefer: example=exampleKey\"   可以透過 -H \"Prefer: code=404\" 或者是 -H \"Prefer: example=exampleKey\" 來設定相對應的 response  擁有這些基本的功能，我想對於開發來說已經是足夠的了   Integrate with GitHub Pages  你可以將你的 OpenAPI 文件放在 GitHub Pages 上面  swagger-ui-action 可以自動 build 你的文件  看起來就會像他 README 提到的那樣   1 2 3 4 5 6 7 8 9 10 11 12 13 - name: Generate Swagger UI   uses: Legion2/swagger-ui-action@v1   with:     output: swagger-ui     spec-file: openapi.json     GITHUB_TOKEN: $ - name: Copy doc   # copy other doc files if needed     - name: Deploy to GitHub Pages   uses: peaceiris/actions-gh-pages@v3   with:     github_token: $     publish_dir: swagger-ui      如果你的 doc 檔案是跟我一樣用 reference anchor 進行管理的，記得要把其他的檔案 copy 進去       有關 GitHub Action 的討論可以參考 DevOps - 從 GitHub Actions 初探 CI/CD | Shawn Hsu    Nginx as Reverse Proxy  編好的檔案，如果你需要在本機開起來看，記得要用 Nginx 來 reverse proxy  不然你會碰到   1 2 3 Access to fetch at 'file:///xxxx/swagger-config.json' from origin 'null'  has been blocked by CORS policy: Cross origin requests are only supported for protocol schemes: http, data, isolated-app, chrome-extension, chrome, https, chrome-untrusted.   老朋友了 CORS(可參考 網頁程式設計三兩事 - 萬惡的 Same Origin 與 CORS | Shawn Hsu)  但解法有點不一樣，這次需要一個 reverse proxy 來幫忙   1 2 3 4 5 6 7 8 9 10 server {     listen 80;     server_name swagger-doc;      # serve index.html     location / {         root /app;         try_files $uri /index.html;     } }   1 2 3 4 5 FROM nginx:alpine COPY . /app COPY nginx.conf /etc/nginx/conf.d/default.conf EXPOSE 80 CMD [\"nginx\", \"-g\", \"daemon off;\"]   把這個 container 跑起來就能夠正確顯示了  記得要 port forward 一下   References     maxdome/swagger-combine   How to split a Swagger spec into smaller files   Splitting your swagger spec into multiple files in a Node project   OpenAPI 和 Swagger 是什麼？他們是什麼關係？Swagger 規範和 Swagger 工具不同嗎？   Using $ref   When do we need to add file extension when importing JavaScript modules?   Including Multiple File Paths in Open API Doc  ","categories": ["random"],
        "tags": ["swagger","openapi","nodejs","swagger-ui","docker","anchor","mock server","anchor","json pointer"],
        "url": "/random/swagger/",
        "teaser": null
      },{
        "title": "資料庫 - 從 MySQL 到 PostgreSQL 一些新手會遇到的問題",
        "excerpt":"Preface  作為用了 MySQL 五年之久的我，原本以為同為 SQL  在語法上的差異不會影響到太多  實際上手之後，發現還是有點差異   因此這裡會稍微的紀錄一下，遇到的問題以及其解決方法  當然還有新的語法   Environment Setup  為了能夠更好的測試文中範例  這裡需要設定好測試用資料庫   MySQL(Mariadb)  1 2 3 4 5 6 7 $ docker run -d \\     -e MARIADB_ALLOW_EMPTY_ROOT_PASSWORD=true \\     -e MARIADB_DATABASE=test \\     --name test-mysql mariadb $ docker exec -it test-mysql bash &gt; mariadb -u root -p &gt; use test;      mariadb version 11.0.2    SQL Data  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 CREATE TABLE users (     id INT AUTO_INCREMENT,     username VARCHAR(255) NOT NULL,     email VARCHAR(255) NOT NULL,     CONSTRAINT pk_users PRIMARY KEY (id) ); CREATE TABLE posts (     id INT AUTO_INCREMENT,     user_id INT NOT NULL,     title TEXT NOT NULL,     content TEXT NOT NULL,     created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,     deleted_at TIMESTAMP,     CONSTRAINT pk_posts PRIMARY KEY (id),     CONSTRAINT fk_users FOREIGN KEY (user_id) REFERENCES users(id)     CONSTRAINT uk_posts UNIQUE (user_id, title, deleted_at) ); INSERT INTO users (username, email) VALUES ('john', 'john@example.com'),     ('alice', 'alice@example.com'),     ('bob', 'bob@example.com'),     ('bob', 'bob2@example.com'); INSERT INTO posts (user_id, title, content) VALUES (1, 'hello', 'This is the first post by john.'),     (1, '','Another post by john.'),     (2, 'hello', 'User2 is posting here.'),     (3, 'hello', 'Hello from bob.'),     (4, 'hello', 'Hello from bob.');   PostgreSQL  1 2 3 4 5 6 7 $ docker run -d \\     -e POSTGRES_PASSWORD=postgres \\     -e POSTGRES_DB=test \\     --name test-postgres postgres:15 $ docker exec -it test-postgres bash &gt; psql -U postgres &gt; \\c test      postgres version 15.4    SQL Data  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 CREATE TABLE users (     id SERIAL PRIMARY KEY,     username VARCHAR(255) NOT NULL,     email VARCHAR(255) NOT NULL ); CREATE TABLE posts (     id SERIAL PRIMARY KEY,     user_id INT NOT NULL,     title TEXT NOT NULL,     content TEXT NOT NULL,     created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,     deleted_at TIMESTAMP,     FOREIGN KEY (user_id) REFERENCES users (id),     UNIQUE (user_id, title, deleted_at) ); INSERT INTO users (username, email) VALUES ('john', 'john@example.com'),     ('alice', 'alice@example.com'),     ('bob', 'bob@example.com'),     ('bob', 'bob2@example.com'); INSERT INTO posts (user_id, title, content) VALUES (1, 'hello', 'This is the first post by john.'),     (1, '', 'Another post by john.'),     (2, 'hello', 'User2 is posting here.'),     (3, 'hello', 'Hello from bob.'),     (4, 'hello', 'Hello from bob.');   Column must appear in the GROUP BY clause or be used in an aggregate function  這大概是我撞過無數次的問題，直到現在還是偶爾會寫錯   其實它寫得很清楚，別擔心我第一次看到也是霧颯颯  看個例子比較直接   好比如說你想要計算 同名的 user 總共發布了多少的文章  當然在 MySQL 你可以使用  1 2 3 SELECT u.email, u.username, COUNT(*) AS num_posts FROM posts AS p LEFT JOIN users AS u ON p.user_id = u.id GROUP BY u.username   在 PostgreSQL 中，上述指令會出現  1 Error: Column \"u.email\" must appear in the GROUP BY clause or be used in an aggregate function  要了解為什麼會出現這個問題，首先來了解一下 SQL Standard   SQL Standard  SQL 標準的定義，你可以在這裡找到 ISO/IEC 9075:2023      很可惜的一點是，它並非 open source 的，所以找資料稍微困難    不過說起來為什麼我們要看 SQL standard 呢？  它跟 GROUP BY 又有什麼關係  讓我們來看看 MySQL 對於自家 GROUP BY 的解釋吧   根據 12.19.3 MySQL Handling of GROUP BY 所述     SQL-92 and earlier does not permit queries for which the   select list, HAVING condition, or ORDER BY list refer to nonaggregated columns   that are not named in the GROUP BY clause.    翻成白話文就是，在 SQL 92 以及以前的標準  1 2 3 任何使用到的 column(不論在 SELECT, HAVING 或者是 ORDER BY)  只要沒有使用 aggregation function 者 必須要出現在 GROUP BY 裡面   那 SQL 92 之後呢？  1 2 3 如果該 column 與 GROUP BY 的 column 有 functional dependence 的關係 就是合法的 比如說 user_id 與 username 是有 functional dependence 的關係   Functional Dependence  functional dependence 是說欄位 X 可以決定唯一的欄位 Y     X uniquely determines Y, so Y is functionally dependent on X    舉例來說，user table 中  user_id 可以決定唯一的 user_name  這樣就可以說 user_name functionally dependent on user_id     在隨後的文件中，MySQL 還提到      If ONLY_FULL_GROUP_BY is disabled,   a MySQL extension to the standard SQL use of GROUP BY permits the select list, HAVING condition, or ORDER BY list   to refer to nonaggregated columns even if the columns are not functionally dependent on GROUP BY columns.     This causes MySQL to accept the preceding query.   In this case, the server is free to choose any value from each group,   so unless they are the same, the values chosen are nondeterministic, which is probably not what you want    意思就是說在 ONLY_FULL_GROUP_BY disabled 的情況下，如果 不滿足 functional dependence  MySQL 會擴充 standard SQL，並視該 query 為合法的  只不過在 return row 的時候，它會 隨機的選一筆 當作 result(即使你把它排序過也不保證)   就拿上面的例子來說，MySQL 回傳的結果為  1 2 3 4 5 6 7 +-------------------+----------+-----------+ | email             | username | num_posts | +-------------------+----------+-----------+ | alice@example.com | alice    |         1 | | bob@example.com   | bob      |         2 | | john@example.com  | john     |         2 | +-------------------+----------+-----------+   bob 明明有兩個人，兩種不同的 email  可是在這裡卻只顯示出 bob@example.com, 而 bob2@example.com 卻莫名的不見了   而 MySQL 自己也提到，他的內部有針對 functional dependence 實作 detection 的機制  因此即使你 SELECT 一些 non-aggregated 的 column，MySQL 會自動推論它是否與 GROUP BY column 為 functional dependence 的關係   但是 PostgreSQL 內部，就我目前看到的資料來說，並沒有實作此類 detection 的機制  如同前一節看到的範例一樣，PostgreSQL 在這種狀況下會拋出 error  所以，你必須使用 aggregate function 或者是 GROUP BY  這樣回過頭來看這個 Error 是不是就很明確了   結論就是  MySQL 可以有限度的幫你做推論這件事情  但 PostgreSQL 就要求你 “明確的指定”   Window Function  所以針對上述的 sql query 要怎麼改才可以在 PostgreSQL 跑呢  透過簡單的 window function 可以輕易的達成   1 2 3 SELECT u.email, u.username, COUNT(*) OVER(PARTITION BY u.username)  FROM posts AS p LEFT JOIN users AS u ON p.user_id = u.id  GROUP BY u.email, u.username;   1 2 3 4 5 6        email       | username | count  -------------------+----------+-------  alice@example.com | alice    |     1  bob@example.com   | bob      |     2  bob2@example.com  | bob      |     2  john@example.com  | john     |     1     window function 根據 PostgreSQL 官方的定義如下  A window function performs a calculation across a set of table rows that are somehow related to the current row  也就是說我將同一種類的資料擺在一起做計算  以我們的例子來說，是把相同 username 擺在一起(PARTITION BY u.username)   而從上面的結果也可以得知  兩個不同的 bob 都有正確的顯示出來，而他的結果是可以預測的(相對於 MySQL 的實作是 nondeterministic 的)  我們更可以推測出一件事情，就是 window function 計算過得資料 並不會合併成一列，相反的彼此之間的前後關係仍然有所保留   LAG() vs. LEAD()  LAG() 是用以計算以目前為準，往前 N 筆的資料  LEAD() 是用以計算以目前為準，往後 N 筆的資料   舉例來說，你想要找出每個 user 的發文與前一筆發文  你可以這樣寫  1 2 3 4 SELECT  user_id, content,  LAG(content, 1) OVER(PARTITION BY user_id ORDER BY created_at) AS previous_content  FROM posts   1 2 3 4 5 6 7  user_id |             content             |        previous_content          ---------+---------------------------------+---------------------------------        1 | This is the first post by john. |         1 | Another post by john.           | This is the first post by john.        2 | User2 is posting here.          |         3 | Hello from bob.                 |         4 | Hello from bob.                 |    ROW_NUMBER()  就跟他的名字一樣，第一列為 1, 第二列為 2 … 以此類推  我們可以使用 ROW_NUMBER() 改寫 LAG() vs. LEAD() 中的範例   1 2 3 4 5 6 7 8 WITH posts_history AS (     SELECT *, ROW_NUMBER() OVER(PARTITION BY user_id ORDER BY created_at) AS rn     FROM posts ) SELECT p1.user_id, p1.content, p2.content AS previous_content FROM posts_history AS p1 LEFT JOIN posts_history AS p2  ON p1.rn = p2.rn + 1 AND p1.user_id = p2.user_id ORDER BY p1.user_id   將 目前所在列與他的下一列(p1.rn = p2.rn + 1) join 起來，由於我們的集合內部有對 created_at 排序過  所以他的偏移量剛好是下一筆資料   Explain SQL Query  Explain 可以用於查看 query 的 execution plan  在 MySQL 中，你可以看到各個階段，對於 Index 的使用程度為何  不過顯示出的資訊跟 PostgreSQL 裡的有一點落差   讓我們來分別看看 LAG() vs. LEAD() 與 ROW_NUMBER() 範例的執行計畫   PostgreSQL Explain SQL Query  1 2 3 4 5 6 7 8 9 10 11 EXPLAIN SELECT  user_id, content,  LAG(content, 1) OVER(PARTITION BY user_id ORDER BY created_at) AS previous_content  FROM posts                                QUERY PLAN                               ----------------------------------------------------------------------  WindowAgg  (cost=74.54..95.94 rows=1070 width=76)    -&gt;  Sort  (cost=74.54..77.21 rows=1070 width=44)          Sort Key: user_id, created_at          -&gt;  Seq Scan on posts  (cost=0.00..20.70 rows=1070 width=44)   在 query plan 當中，你總是會看到 (cost=74.54..95.94 rows=1070 width=76) 這麼一行  它代表的意思是這樣子的     估計啟動成本   估計總成本   估計輸出資料列數量   估計資料列平均資料大小   所以第一列的估計值，就是整體 query 的估計值  你可以看到，在 LAG() vs. LEAD() 中使用的 SQL query 他的總成本為 95.94  往下看它會分別列出每一個步驟所耗費的成本，舉例來說  第二列的 sort 是 window function 裡面我們用了排序造成的(user_id 則是預設分割的方式，所以它也有納入)   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 EXPLAIN WITH posts_history AS (     SELECT *, ROW_NUMBER() OVER(PARTITION BY user_id ORDER BY created_at) AS rn     FROM posts ) SELECT p1.user_id, p1.content, p2.content AS previous_content FROM posts_history AS p1 LEFT JOIN posts_history AS p2  ON p1.rn = p2.rn + 1 AND p1.user_id = p2.user_id ORDER BY p1.user_id;                                    QUERY PLAN                                     ---------------------------------------------------------------------------------  Merge Left Join  (cost=246.42..268.11 rows=1070 width=68)    Merge Cond: ((p1.user_id = p2.user_id) AND (p1.rn = ((p2.rn + 1))))    CTE posts_history      -&gt;  WindowAgg  (cost=74.54..95.94 rows=1070 width=56)            -&gt;  Sort  (cost=74.54..77.21 rows=1070 width=48)                  Sort Key: posts.user_id, posts.created_at                  -&gt;  Seq Scan on posts  (cost=0.00..20.70 rows=1070 width=48)    -&gt;  Sort  (cost=75.24..77.91 rows=1070 width=44)          Sort Key: p1.user_id, p1.rn          -&gt;  CTE Scan on posts_history p1  (cost=0.00..21.40 rows=1070 width=44)    -&gt;  Sort  (cost=75.24..77.91 rows=1070 width=44)          Sort Key: p2.user_id, ((p2.rn + 1))          -&gt;  CTE Scan on posts_history p2  (cost=0.00..21.40 rows=1070 width=44)      CTE 為在 materialized view 之上的 sequential scan  以這個例子來看就是 WITH posts_history as (…) 的部份    對於較為複雜的 SQL query 你可以看到  整體的執行計畫就變得很複雜了  而他的總成本 268.11 也明顯高於使用 LAG() 方法的 95.94   MySQL Explain SQL Query  1 2 3 4 5 6 7 SELECT p1.user_id, p1.content AS current_content, IFNULL(p2.content, '') AS previous_content FROM posts p1 LEFT JOIN posts p2 ON p1.user_id = p2.user_id AND p1.id &gt; p2.id ORDER BY p1.user_id, p1.id;      這個 query 實際上可能會有問題，因為它不一定可以 match 到 physical order 的前一筆  但我們只是要看他的執行計畫，所以可以忽略    1 2 3 4 5 6 +------+-------------+-------+------+------------------+----------+---------+-----------------+------+----------------+ | id   | select_type | table | type | possible_keys    | key      | key_len | ref             | rows | Extra          | +------+-------------+-------+------+------------------+----------+---------+-----------------+------+----------------+ |    1 | SIMPLE      | p1    | ALL  | NULL             | NULL     | NULL    | NULL            | 5    | Using filesort | |    1 | SIMPLE      | p2    | ref  | PRIMARY,fk_users | fk_users | 4       | test.p1.user_id | 1    | Using where    | +------+-------------+-------+------+------------------+----------+---------+-----------------+------+----------------+   要看的就幾個而已     type            這裡可以看到有 ALL(full table scan) 以及 ref(reference)           key            p2 表在 join 的時候有使用到 foreign key           rows            p2 的表只會回傳一行，代表說他是擁有 Functional Dependence 的特性           Unique Constraint  這個算是最近踩到的  我本身在測試的環境是 PostgreSQL 14  而這個版本的 PostgreSQL 在 unique constraint 上面有一個很特別的規則   根據 PostgreSQL E.5. Release 15 - E.5.3.1.2. Indexes 裡面提到的     Allow unique constraints and indexes to treat NULL values as not distinct (Peter Eisentraut)    Previously NULL entries were always treated as distinct values,   but this can now be changed by creating constraints and indexes using UNIQUE NULLS NOT DISTINCT.    大意是說，PostgreSQL 15 以前的所有版本，都將 null data 視為 distinct values   很明顯這不是我們所期望的  null 應該是一樣的才對？  如果你短時間內沒辦法升級成 15 該怎麼辦？  可以設定兩組 unique index, 亦即 (userID, title, deletedAt) 與 (userID, title)   假設你有一個需求是這樣子，我希望每個使用者發文的文章 title 都必須是 distinct 的  理所當然的你的 constraint 是設定 (userID, title, deletedAt)  然後你的 deletedAt 預設為 null(文章可以被刪除)  你已經有一筆資料 (12, 邁阿密旅遊記, null)  如果又有新的一筆，一樣是 (12, 邁阿密旅遊記, null)，它是可以被成功寫入的，在 15 以前        https://makeameme.org/meme/why-did-you-5c3700    Experiment  設定環境的步驟如同前面所述(可參考 Environment Setup)   PostgreSQL 15  在 PostgreSQL 15 中，如果你新增一筆 (1, 'hello') 會得到以下結果  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 test=# INSERT INTO posts(user_id, title, content) VALUES (1, 'hello', ''); ERROR:  null value in column \"content\" of relation \"posts\" violates not-null constraint DETAIL:  Failing row contains (6, 1, hello, null, 2023-10-08 16:11:34.111809, null). test=#  test=# \\d posts;                                         Table \"public.posts\"    Column   |            Type             | Collation | Nullable |              Default               ------------+-----------------------------+-----------+----------+-----------------------------------  id         | integer                     |           | not null | nextval('posts_id_seq'::regclass)  user_id    | integer                     |           | not null |   title      | text                        |           | not null |   content    | text                        |           | not null |   created_at | timestamp without time zone |           | not null | CURRENT_TIMESTAMP  deleted_at | timestamp without time zone |           |          |  Indexes:     \"posts_pkey\" PRIMARY KEY, btree (id)     \"posts_user_id_title_deleted_at_key\" UNIQUE CONSTRAINT, btree (user_id, title, deleted_at) Foreign-key constraints:     \"posts_user_id_fkey\" FOREIGN KEY (user_id) REFERENCES users(id)   符合預期，確實 null 已經 不在被視為是 distinct values   PostgreSQL 14     設定如先前的 docker command，替換掉 docker image 為 postgres:14 即可    那麼 PostgreSQL 14 呢  即使有新增 constraint 它也接受寫入，正如上一節提到在 15 以前 null value 都被視為 distinct values  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 test=# SELECT * FROM posts;  id | user_id | title |             content             |         created_at         | deleted_at  ----+---------+-------+---------------------------------+----------------------------+------------   1 |       1 | hello | This is the first post by john. | 2023-10-08 16:14:29.786042 |    2 |       1 |       | Another post by john.           | 2023-10-08 16:14:29.786042 |    3 |       2 | hello | User2 is posting here.          | 2023-10-08 16:14:29.786042 |    4 |       3 | hello | Hello from bob.                 | 2023-10-08 16:14:29.786042 |    5 |       4 | hello | Hello from bob.                 | 2023-10-08 16:14:29.786042 |    7 |       1 | hello |                                 | 2023-10-08 16:15:40.923995 |  (6 rows)  test=# \\d posts;                                         Table \"public.posts\"    Column   |            Type             | Collation | Nullable |              Default               ------------+-----------------------------+-----------+----------+-----------------------------------  id         | integer                     |           | not null | nextval('posts_id_seq'::regclass)  user_id    | integer                     |           | not null |   title      | text                        |           | not null |   content    | text                        |           | not null |   created_at | timestamp without time zone |           | not null | CURRENT_TIMESTAMP  deleted_at | timestamp without time zone |           |          |  Indexes:     \"posts_pkey\" PRIMARY KEY, btree (id)     \"posts_user_id_title_deleted_at_key\" UNIQUE CONSTRAINT, btree (user_id, title, deleted_at) Foreign-key constraints:     \"posts_user_id_fkey\" FOREIGN KEY (user_id) REFERENCES users(id)  test=#   前面還有提到一個解法，就是我再加一個 partial index  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 // 先把現有的 constraint 拿掉 test=# ALTER TABLE posts DROP CONSTRAINT posts_user_id_title_deleted_at_key;  // 新增兩個 partial index test=# CREATE UNIQUE INDEX uk_id_title on posts (user_id, title) WHERE deleted_at IS NULL; CREATE INDEX test=# CREATE UNIQUE INDEX uk_id_title_deleted_at on posts(user_id, title, deleted_at) WHERE deleted_at IS NOT NULL; CREATE INDEX  // 最後會長這樣 test=# \\d posts;                                         Table \"public.posts\"    Column   |            Type             | Collation | Nullable |              Default               ------------+-----------------------------+-----------+----------+-----------------------------------  id         | integer                     |           | not null | nextval('posts_id_seq'::regclass)  user_id    | integer                     |           | not null |   title      | text                        |           | not null |   content    | text                        |           | not null |   created_at | timestamp without time zone |           | not null | CURRENT_TIMESTAMP  deleted_at | timestamp without time zone |           |          |  Indexes:     \"posts_pkey\" PRIMARY KEY, btree (id)     \"uk_id_title\" UNIQUE, btree (user_id, title) WHERE deleted_at IS NULL     \"uk_id_title_deleted_at\" UNIQUE, btree (user_id, title, deleted_at) WHERE deleted_at IS NOT NULL Foreign-key constraints:     \"posts_user_id_fkey\" FOREIGN KEY (user_id) REFERENCES users(id   讓我們來測試看看 constraint 會不會正確動作  1 2 3 4 5 6 7 8 9 10 11 12 13 test=# SELECT * FROM posts;  id | user_id | title |             content             |         created_at         | deleted_at  ----+---------+-------+---------------------------------+----------------------------+------------   1 |       1 | hello | This is the first post by john. | 2023-10-08 16:14:29.786042 |    2 |       1 |       | Another post by john.           | 2023-10-08 16:14:29.786042 |    3 |       2 | hello | User2 is posting here.          | 2023-10-08 16:14:29.786042 |    4 |       3 | hello | Hello from bob.                 | 2023-10-08 16:14:29.786042 |    5 |       4 | hello | Hello from bob.                 | 2023-10-08 16:14:29.786042 |  (5 rows)  test=# INSERT INTO posts(user_id, title, content) VALUES (1, 'hello', ''); ERROR:  duplicate key value violates unique constraint \"uk_id_title\" DETAIL:  Key (user_id, title)=(1, hello) already exists.   當 deleted_at 為空的時候，因為 user_id 與 title 都重複到  因此 PostgreSQL 14 正確的回復了一個錯誤，說這個組合已經存在，不能寫入   1 2 3 4 5 6 7 8 9 10 11 12 test=# INSERT into posts(user_id, title, content, deleted_at) values(1, 'hello', '', now()); INSERT 0 1 test=# select * from posts;  id | user_id | title |             content             |         created_at         |         deleted_at          ----+---------+-------+---------------------------------+----------------------------+----------------------------   1 |       1 | hello | This is the first post by john. | 2023-10-08 16:14:29.786042 |    2 |       1 |       | Another post by john.           | 2023-10-08 16:14:29.786042 |    3 |       2 | hello | User2 is posting here.          | 2023-10-08 16:14:29.786042 |    4 |       3 | hello | Hello from bob.                 | 2023-10-08 16:14:29.786042 |    5 |       4 | hello | Hello from bob.                 | 2023-10-08 16:14:29.786042 |    9 |       1 | hello |                                 | 2023-10-08 16:34:45.991099 | 2023-10-08 16:34:45.991099 (6 rows)  而當 deleted_at 有值的時候，就允許寫入了   Prisma  另外值得注意的是，如果你使用 prisma  prisma 目前不支援 conditional index 的寫法(測試環境是 5.3 版)  也就是 CREATE UNIQUE INDEX ... WHERE ... 的寫法是沒有用的   你說手動更改 migration script 有效果嗎？  不但沒有用而且 prisma 會自動建立新的 migration，這是已知的行為  可參考     Partial unique indexes are being recreated on every migration with prisma migrate #13417   Prisma tries to re-create indexes in migration if they were previously created with “WHERE” #14651   解決的辦法我目前想到的就是  手動執行 custom sql script 而已  1 $ npx prisma db execute --file ./custom.sql -- schema prisma/schema.prisma   這個可能要等 prisma 官方之後的修改   References     How to Calculate Cumulative Sum-Running Total in PostgreSQL - PopSQL   3.5. Window Functions   9.22. Window Functions   Group by clause in mySQL and postgreSQL, why the error in postgreSQL?   The SQL Standard – ISO/IEC 9075:2023 (ANSI X3.135)   ANSI/ISO/IEC International Standard (IS) Database Language SQL — Part 2: Foundation (SQL/Foundation) «Part 2»   What is a CTE scan, and what are its implications for performance?   14.1. 善用 EXPLAIN   Day.23 分析語法效能必備 - MYSQL語法優化 (Explain)   Create unique constraint with null columns  ","categories": ["database"],
        "tags": ["database","postgresql","mysql","sql standard","sql"],
        "url": "/database/database-postgresql/",
        "teaser": null
      },{
        "title": "神奇的演算法 - Monotonic Stack",
        "excerpt":"Preface  千言萬語都比不上一個真實的範例   考慮以下 array [1,2,3,2,4,3]  求 Next Greater Element(i.e. 該位置下一個比我大的數值為何)  以上述的例子來看，答案會是 [2, 3, 4, 4, -1, -1]   當我看到這個題目的第一眼，直覺會是紀錄最大值(4)就好了  並且 由後往前看  在還沒有遇到 4 之前，答案是 -1   但如果事情真如我所想的，那麼第一格的答案應該要是 4 才對，怎麼會是 2 呢？  原因在於我們並不是要找尋最大值，而是 下一個比我大的值  也因為這樣，光是紀錄最大值顯然是不足的  正確的作法是要紀錄 歷史比我大的值   Monotonic  紀錄這種歷史紀錄，他的其中一個特性是必須是 單調的  維基百科的定義是這樣子的   1 2 3 4 在數學中，給定函數定義域， 當定義域中較小的自變量值小於較大的自變量值時， 較小的自變量值對應的因變量值總是小於較大的自變量值對應的因變量值， 那麼這個函數就是單調增加函數   我的理解是當前數字都比我小/大 就是 單調遞增/遞減函數        ref: 單調函數      上述的例子，比 2 大的數字有 3, 4  這個歷史紀錄是不是屬於單調遞增呢？   Introduction to Monotonic Stack  紀錄這些 history 可以用一個 stack 完成，並且 stack 裡的數字均為單調的(不論遞增或遞減)  聽起來不難，但怎麼建構一個 monotonic stack?   回到最初的例子 [1,2,3,2,4,3]，他的歷史數值應該會長這樣  1 2 3 4 5 6 7      |  0  |  1  |  2  |  3  |  4  |  5  |     index -----|-----|-----|-----|-----|-----|-----|      |  1  |  2  |  3  |  2  |  4  |  3  |     origin input -----|-----|-----|-----|-----|-----|-----|      |  2  |     |     |     |     |     |     history      |  3  |  3  |     |     |     |     |      |  4  |  4  |  4  |  4  |     |     |   history 為下一個比自己大的數值(從上而下)  可以看到它都是呈現單調的排列  值得注意的是在 index 為 2 的時候，他的歷史只有 [4] 並沒有包含 2(index 3 的數值)  原因是我們要找的是 比自己大的數值，因為 2 &lt; 3(自己)，所以它沒有包含進歷史當中   從上面的圖表你能推敲出第一件事情  就是必須從 右到左 建構  第二件事情是，歷史數值一定都比當前的數字都還要大，換言之，如果當前數字比歷史還要小，我就沒必要寫進歷史   所以，你要做的事情是，跟 monotonic stack 裡面的數字比大小，如果 stack 的數字比我還要小，就把 stack 的數字丟掉  重複一直做，你就會找到比自己大的數值了(or 沒找到)   Implementation  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 func nextGreaterValue(nums []int) []int {     monotonic := make([]int, 0)     nextGreater := make([]int, len(nums))      for i := len(nums) - 1; i &gt;= 0; i-- {         for {             if empty(monotonic) {                 break             }              if peek(monotonic) &gt; nums[i] {                 break             }             monotonic = pop(monotonic)            // 比當前數字小，丟掉最新一筆歷史紀錄         }          nextGreater[i] = peek(monotonic)         monotonic = push(monotonic, nums[i])      // 寫入歷史紀錄，要不要保留由 inner loop 決定     }      return nextGreater }  func empty(stack []int) bool {     return len(stack) == 0 }  func push(stack []int, input int) []int {     return append(stack, input) }  func peek(stack []int) int {     if empty(stack) {         return -1     }     return stack[len(stack) - 1] }  func pop(stack []int) []int {     if empty(stack) {         return stack     }     return stack[:len(stack) - 1] }   注意到你不能直接使用 monotonic stack 這個 array 當作結果  因為它只代表當前的歷史紀錄，因此你必須要使用額外的陣列儲存，以這個例子來說是 nextGreater   Time Complexity  兩層迴圈就是 $O(n^2)$ 嗎  顯然不是的   第一層迴圈顯然是 $O(n)$  然後 inner loop 是逐一檢查 stack 的內容  你說這樣還不是會跑完一次   換個角度思考，stack 裡面的資料進出幾次？  答案是 2 次，哪 2 次？  每個元素只會被寫入一次，在 outer loop 的時候做的，而 inner loop 只負責 pop(已經被移出 stack 的元素不可能再寫回去)  所以整體的複雜度為 $O(2n)$   LeetCode 2434. Using a Robot to Print the Lexicographically Smallest String  題目是這樣子的，給定一個字串 s 然後你可以做以下任一的一個操作     從 s 中移除 前面 的字元到 t 的後面   從 t 中移除 後面 的字元到 答案 的後面   重複操作直到 s 以及 t 皆為空，然後在眾多答案的組合中，找到字典序最小的答案是哪一個   比如說 vzhofnpo 的答案是 fnohopzv     看完你應該馬上要知道，這題會需要用到 stack 來解決  原因是從 s 移除前面的字元，又在 t 中移除後面的字元  這個情況是符合 FILO 的特性   第二個重點是，答案的組合是所有可能的組合中，字典序最小的那一個  字典序最小的範例是 abcdef...，也就是最小的在最前面  你可能會覺得我直接算字母頻率然後依照字典序組合起來就行，但由於轉換規則的限制，這並不是正確的  你也可以看到答案說，兩個字母 o 並沒有相依   題目本質是 stack, 答案又要是字典序最小的  意味著字典序小的字母會需要出現在前面，依序變大  這很明顯是符合 Monotonic Stack 的特性   為了需要知道哪些字母是當前最小的，你會需要一個 frequency map 來紀錄  如果 a 出現了三次，我會希望這三次都盡量靠前 甚至黏在一起 對吧？  但根據規則，這其實不太可能(就像例子中的 o 一樣)   但是為了能夠靠在一起，我需要等到最後一個字母出現的時候才開始將她寫入答案  什麼意思？ a----a------------a 的字串，如果我在第一個 a 的時候就寫入，答案會是 字典序最小 的可能性就會降低了對吧？  要最大機率可以得到正確的答案是不是要等所有 a 都出現了才開始寫入？  而事實證明，這的確可以得到全局最佳解，也因此這題會需要搭配 Greedy Algorithm 來解決   Implementation  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 func robotWithString(s string) string {     m := make(map[int]int, 0)      for _, c := range s {         m[int(c - 'a')] += 1     }      stack := make([]rune, 0)     result := make([]string, 0)     for _, c := range s {         stack = append(stack, c)         m[int(c - 'a')] -= 1          // 找到當前最小的字母         minc := 'a'         for minc &lt; 'z' &amp;&amp; m[int(minc - 'a')] == 0 {             minc++         }          // 執行 Monotonic 的操作         for len(stack) &gt; 0 &amp;&amp; stack[len(stack) - 1] &lt;= minc {             result = append(result, string(stack[len(stack) - 1]))             stack = stack[:len(stack) - 1]         }     }      for len(stack) &gt; 0 {         result = append(result, string(stack[len(stack) - 1]))         stack = stack[:len(stack) - 1]     }      return strings.Join(result, \"\") }   Examples                  Level       Link                       Easy       1475. Final Prices With a Special Discount in a Shop                 Easy       496. Next Greater Element I                 Medium       503. Next Greater Element II           References     Time complexity of Monotonic stack question  ","categories": ["algorithm"],
        "tags": ["array","stack","monotonic","next greater element","leetcode","leetcode-496","leetcode-503","leetcode-1475","leetcode-2434"],
        "url": "/algorithm/algorithm-monotonic-stack/",
        "teaser": null
      },{
        "title": "資料庫 - PostgreSQL 使用 Fuzzy Search 的效能測試",
        "excerpt":"Introduction to Fuzzy Search  就是字串匹配，只不過它即使是沒有完全的把字拼對，也可以找的到   那麼搜尋的單字到底拼的多對，才能被找到？  一個方法是，計算到底需要多少的步驟，才能將 search word 轉換成 target word  比如說當我輸入 coil 他有可能的單字為     foil :arrow_right: substitution   oil :arrow_right: deletion   coils :arrow_right: insertion   而這個步驟的數量，稱之為 edit distance   3 種不同的 primitive operation 可以組合出一定數量的結果  上述的結果他們的操作數量皆為 1  但是 foal 也可以作為答案，只不過他的操作數量為 2  於是你可以挑選操作數量最小的作為結果進行回傳   除此之外，你可以針對不同的 權重 來限制回傳結果  不同的 operation 有不同的權重等級  這樣可以更有利於回傳結果   Pros and Cons  使用 Fuzzy search 的好處其中一個是  可以在不需要知道確切拼法的情況下，搜尋到你要的東西   缺點就是，可能搜尋回來的結果數量太多，尤其是無關的數量太多  導致無法精準的找到你要的資訊   PostgreSQL Fuzzy Search Extension pg_trgm  總之 PostgreSQL 有一個套件是專門在做 Fuzzy search 的，叫做 pg_trgm  他的原理就是使用到了前面說的 edit distance   pg_trgm 是自帶的 extension  你可以使用以下指令啟用  1 CREATE EXTENSION IF NOT EXISTS pg_trgm;      用 SELECT * FROM pg_extension; 查看是否有正確載入    Trigraph  pg_trgm 是採用 trigraph matching 的機制  也就是說它一次是取 3 個字母(撇除特殊字元, i.e. non-alphanumerics)  將 search word 與 target word 一起做計算  看看全部的 trigraph 組合當中，它幾個符合  進而得出相似度   舉例來說 catalog 跟 cat  他們的 trigraph 分別為  1 2 3 4 5 catalog \"  c\",\" ca\",\"alo\",\"ata\",\"cat\",\"log\",\"og \",\"tal\"  cat \"  c\",\" ca\",\"at \",\"cat\"  他們的 trigram 相似度為 0.375  因為相同的 trigram 有 3 個，而 catalog 的 trigram 有 8 個  所以 3/8 等於 0.375      注意到這個數值是 word_similarity 而非 similarity    PostgreSQL Full Text Search Index  index 我們之前就有講過，詳細可以參考 資料庫 - Index 與 Histogram 篇 | Shawn Hsu  那我們這裡為什麼又要提一遍呢   PostgreSQL 在使用 Fuzzy search 的時候，有兩個特別的 index 類型可以加速這種計算   GiST(Generalized Search Tree based Index)  tree based index 亦即 GiST index 可以使用 B+ tree, R tree 等自平衡樹實作  所以你可以期待他的搜尋 速度是比較快的  除了本身資料結構的特性使得它速度快  另一個原因是，GiST index 將資料進行一定程度的壓縮，將它縮小至 n bit 的 signature  而這個過程自然是使用 hash 的方式達成的   由於他是使用 hash  這代表他有可能會出現碰撞，也就是說不同的資料可能會得到相同的 signature(i.e. hash value)  因此，在使用 GiST index 的時候，有可能會出現 false positive 的情況   而當這個情況出現的時候，PostgreSQL 則會自動的將欄位資料撈出並進行 二次檢查   GIN(Generalized Inverted Index based Index)  inverted index 的概念是 hashmap  將所有出現的單詞，建立一個 hashmap，其 value 儲存的是所有 occurrence 的位置      詳細可以參考 資料庫 - Index 與 Histogram 篇 | Shawn Hsu    所以 GIN index 你能夠推測出幾件事情     他的 index 更新會比較慢   查詢速度相比 GiST Index 還要快   index 大小會比較大(因為它沒有壓縮過)   為什麼 GIN index 比 GiST index 還要快？  自平衡樹的 query 時間雖然是 $O(Log(N))$, 但是 hashmap 的時間可是 $O(1)$  並且 GiST index 有可能會出現 false positive 導致需要進行二次確認的時間差  所以整體算起來，GIN index 速度上會比 GiST index 還快      你可能會問，平平都是 hash  為什麼 GIN index 不用進行二次確認？  因為它 沒有做壓縮，這也就是為什麼 GIN index 所需大小較大  卻又比較準確的原因    Pros and Cons  整理成表格大概會長下面這樣                          GiST index       GIN index                       查詢速度       Slow       Fast                 更新速度       Fast       Slow                 Index 大小       Small       Large                 資料結構       Self-Balanced Tree       Hashmap           tsvector and tsquery  根據 12.9. GiST and GIN Index Types  他是這麼說的      Creates a GiST (Generalized Search Tree)-based index. The column can be of tsvector or tsquery type.  Creates a GIN (Generalized Inverted Index)-based index. The column must be of tsvector type.    所以需要使用 GiST 與 GIN index 他們的資料型態必須是 tsvector 或者是 tsquery  問題來了，他們是什麼   為了要支援 full text search, PostgreSQL 開發出了兩款資料型態，特別 for 此類的需求   tsvector  tsvector 是為了 text search 而開發的，其中它儲存的資料是排序過的詞位  什麼意思呢  1 2 3 4 SELECT 'a fat cat sat on a mat and ate a fat rat'::tsvector;                       tsvector ----------------------------------------------------  'a' 'and' 'ate' 'cat' 'fat' 'mat' 'on' 'rat' 'sat'  上述例子，每個 “單詞” 就是一個詞位，所以你可以看到 a 這個單詞在 tsvector 中只有儲存一次  並且 tsvector 內部的資料是有排序過的   根據 9.13. Text Search Functions and Operators  tsvector 本身能支援的搜尋相對 tsquery 少很多  它只有支援 perfect match 的情況，如果要用到 contain 的功能就沒辦法   tsquery  tsquery 儲存的則是被搜尋的詞位(你可以把它想像成是 text query)  如果有多個詞位，它會使用不同的 operator 將它組合起來  1 2 3 4 SELECT 'hello &amp; world'::tsquery      tsquery ----------------- 'hello' &amp; 'world'   Benchmark Testing for GiST / GIN Index  根據 12.9. GiST and GIN Index Types 所述     As a rule of thumb, GIN indexes are best for static data because lookups are faster.   For dynamic data, GiST indexes are faster to update.   Specifically, GiST indexes are very good for dynamic data and fast if the number of unique words (lexemes) is under 100,000,   while GIN indexes will handle 100,000+ lexemes better but are slower to update.    什麼？ data 還有分成 dynamic 跟 static 的？  其實 dynamic data 就只是比較常更動的資料而已   光是知道它定義上的差別，並沒有辦法實際的了解他的差異  本次實驗將會著重在 index 的使用上，會對效能上有多大的影響  看看他們在不同大小的資料集中表現如何   Environment  1 2 3 4 5 6 7 8 9 10 11 $ docker -v Docker version 24.0.6, build ed223bc  $ node -v v20.5.1  $ python3 --version Python 3.10.12  $ postgres -V postgres (PostgreSQL) 15.4 (Debian 15.4-1.pgdg120+1)   Benchmark Steps  測試 index 基本上我的想法是  建一個 table, 裡面包含本次要測試的對象 GIN 以及 GiST  但同時我也想知道他們跟一般 secondary index 差距有多大   對於資料集的部份，使用 python 進行 data processing  data source 是從這裡拿到的 https://www.ssa.gov/oact/babynames/names.zip  用一個 set 塞入所有的 name  如此一來我便可以保證他是 unique 的了  為了測試大資料集，我也將 name 進行了一定程度的 shuffle  最後得出兩個資料集，大小分別為 10w 以及 66w   benchmark 本身的 code 是使用 Node.js 配合 Prisma  operator class 分別指定 gist_trgm_ops 與 gin_trgm_ops 即可  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 datasource db {     provider     = \"postgresql\"     url          = \"postgresql://admin:admin@localhost:5555/benchmark\"     extensions   = [pg_trgm] }  generator client {     provider        = \"prisma-client-js\"     previewFeatures = [\"postgresqlExtensions\"] }  model unique {     id    Int    @id @default(autoincrement())     name  String     index String     gist  String     gin   String      @@index([index])     @@index([gist(ops: raw(\"gist_trgm_ops\"))], type: Gist)     @@index([gin(ops: raw(\"gin_trgm_ops\"))], type: Gin) }      雖然我看 Enable PostgreSQL extensions for native database functions  他是說只要在 prisma 裡面啟用 pg_tgrm 再做 migration 就可以了  但我怎麼試都沒辦法    最後要注意的事情就是他的時間，畢竟要做 benchmark  他的 timer 要是高精度的，幸好 Node.js 有提供到 nanoseconds  1 2 3 4 5 process.hrtime()  const tons = (timestamp: number[]): number =&gt; {   return timestamp[0] * 1e9 + timestamp[1]; };   process.hrtime() 回傳值為一個 number array, 分別為 seconds 與 nanoseconds  這邊用一個簡單的 helper 把它全部轉成 nanoseconds   Incorrect Benchmark Testing Code  我一開始 benchmark code 是這樣寫的  1 2 3 4 5 return await conn.$queryRawUnsafe(`--sql     SELECT id     FROM \"unique\"     WHERE similarity(${field}, '${name}') &gt; 0 `)  但我後來怎麼測試都發現，GIN, GiST 跑起來卻跟沒加 index 差不多  很明顯這樣是有問題的，與 documentation 描述的不符合  後來下 EXPLAIN 下去看問題在哪   1 2 Seq Scan on \"unique\"  (cost=0.00..2344.72 rows=34149 width=4)   Filter: (similarity(gin, 'abc'::text) &gt; '0'::double precision)   問題在哪？  根據 F.35. pg_trgm — support for similarity of text using trigram matching      The pg_trgm module provides GiST and GIN index operator classes   that allow you to create an index over a text column for the purpose of very fast similarity searches.   These index types support the above-described similarity operators,   and additionally support trigram-based index searches for LIKE, ILIKE, ~, ~* and = queries.    僅有 similarity operator 有支援 GIN, GiST index  而 similarity 等 function 沒有，所以它跑起來都是使用 sequential scan  正確的寫法應該要是這樣   1 2 3 4 5 return await conn.$queryRawUnsafe(`--sql     SELECT id     FROM \"unique\"     WHERE ${field} % '${name}' `);   你可以看到，如此一來它就會使用 GIN index 執行 bitmap scan  1 2 3 4 Bitmap Heap Scan on \"unique\"  (cost=36.08..72.87 rows=10 width=4)   Recheck Cond: (gin % 'abc'::text)   -&gt;  Bitmap Index Scan on unique_gin_idx  (cost=0.00..36.08 rows=10 width=0)       Index Cond: (gin % 'abc'::text)   改用 pg_trgm operator 後要注意的一點是  他的 similarity threshold 是用 pg_trgm.similarity_threshold database variable 控制的  預設值為 0.3, 但如果你想要改他也可以透過以下指令進行更改  1 2 3 SET pg_trgm.similarity_threshold = 0.2; // or  SELECT set_limit(0.2);     查看當前 threshold 就是改成 SHOW pg_trgm.similarity_threshold 或 SELECT show_limit()    但是他的作用域是 per session 的，所以它會自己變回去  永久改掉預設值的方法就是使用 ALTER, 如下所示  1 ALTER DATABASE my_database SET pg_trgm.similarity_threshold = 0.2;   還記得前面講的 Trigraph 嗎？  簡單版本的理解基本上就是將兩個 trigraph array 做 bitwise operation 而已罷了  所以它才叫做 bitmap index scan   Benchmark Result                                                          Dataset Size       10w       66w                 Benchmark Result                         而當我們全部調適完成之後  可以看到這個圖是相當的漂亮   你會發現，沒有加 index 與 secondary index 在進行 fuzzy search 的情況下  基本上是沒有幫助的，而隨者資料量增大，他的 query 時間也會相應的上升($1 \\times 10^8$ 與 $2.5 \\times 10^8$)  相對的 GIN 與 GiST index 完美的符合我們對他的假設   因為 GiST index 有 false positive 的情況會發生  即使它使用自平衡樹，速度上也仍然不及 GIN 的 hashmap  當資料量大增的情況下，差距更大   在極限狀況 66w 筆的 unique 資料下，GIN 的執行速度超越 GiST 達 15 倍  比起 secondary index 差距甚至高達 52 倍      當然本次實驗僅專注在 read 的時間  因為 fuzzy search 很明顯，他是查詢的數量會明顯大於寫入數量    Implementation  實驗的相關數據以及程式碼，都可以在 ambersun1234/blog-post/postgresql-gist-gin 中找到   Benchmark Testing Array Type for GiST / GIN Index  普通型別沒問題，那麼對於像是 array 這種 type 會不會也有所幫助呢？  PostgreSQL 可以將欄位設定為 array type  並且支援任意 built-in 或者是自定義的型別，長度不須指定   array 型別並沒有直接被 GiST/GIN 支援  就算使用 btree_gin 這種的它也只有支援普通的型別(e.g. text, int … etc.)  1 2 CREATE EXTENSION IF NOT EXISTS btree_gin; CREATE EXTENSION IF NOT EXISTS btree_gist;  不過我們還是有辦法使用 tsvector and tsquery 來做到的   Benchmark Setup  關於資料集的部份，我想就沿用我們的作法  資料集大小一樣是 10 萬筆  只不過資料格式就改成 array 的型態  這次我主要想要測試的是 string array  至於 array 裡面的個數，分別測試 10 個以及 20 個好了   String Array Type  schema 的部份要注意的是 string array 並不能直接支援 GiST/GIN  須為 tsvector/tsquery 或者是 string  因此在 schema 的定義上必須直接儲存  1 2 3 4 5 6 7 8 9 10 11 model strArray {     id     Int                     @id @default(autoincrement())     origin String[]     index  Unsupported(\"tsvector\")?     gist   Unsupported(\"tsvector\")?     gin    Unsupported(\"tsvector\")?      @@index([index])     @@index([gist], type: Gist)     @@index([gin], type: Gin) }   這裡採用分段寫入的方式，因為從 csv 寫入我無法直接寫 tsvector  所以一開始需要定義為 nullable  當我將原始 string array 寫入後再使用 array_to_tsvector 寫入相對應的資料即可  1 2 3 UPDATE \"strArray\" SET index = array_to_tsvector(origin); UPDATE \"strArray\" SET gist = array_to_tsvector(origin); UPDATE \"strArray\" SET gin = array_to_tsvector(origin);   Int Array Type  補充一下如果你想要試一下 integer array  可以直接用 gist__int_ops 以及 gin__int_ops 即可  只不過需要新增套件 intarray  1 CREATE EXTENSION IF NOT EXISTS intarray;  schema 的定義如下  1 2 3 4 5 6 7 8 9 10 model intArray {     id    Int   @id @default(autoincrement())     index Int[]     gist  Int[]     gin   Int[]      @@index([index])     @@index([gist(ops: raw(\"gist__int_ops\"))], type: Gist)     @@index([gin(ops: raw(\"gin__int_ops\"))], type: Gin) }   執行 migration 的時候我遇到了一個很嚴重的效能問題  將 csv 資料寫入的時候並沒有花費太久的時間，反而是在建立 index 的時候比起預期時間還要長  上網看了一下好像不只我遇到   根據 slow index creation with gist and gist__int_ops  Eric Cimineli 網友說到     I was able to get around this by creating the index with empty values in the column   and then filling the data after,   but it still took around 20 hours with a table of only 14 million rows.    換句話說，10w 筆資料可能需要快 2 個小時才能完成寫入  事情似乎從 2009 年開始就有回報(Extremely slow intarray index creation and inserts.)  時至今日好像沒有解決辦法(或者我沒看到)   然後我試了不同大小的資料集  10w 跑超過 8 個小時, 1w 的也跑超過 4 個小時  那我真的沒有什麼時間可以等，所以這部份就還是一樣留著做紀錄   Array Mock Data  假資料的格式要稍微注意一下  根據 8.15. Arrays      To write an array value as a literal constant,   enclose the element values within curly braces and separate them by commas.   (If you know C, this is not unlike the C syntax for initializing structures.)   You can put double quotes around any element value, and must do so if it contains commas or curly braces.   (More details appear below.)    簡言之就是要長的像這樣  1 2 3 '{ val1 delim val2 delim ... }' 就是 '{\"hello\", \"world\"}'      注意到 csv 的內容，在 PostgreSQL 中雙引號必須要用 2 個才會正確讀進去  也就是說 '{\"hello\", \"world\"} 要變成 '{\"\"hello\"\", \"\"world\"\"}    Array Operators  這次測試的 operator 為 overlap(&amp;&amp;)  他的效果大概長這樣  1 2 select ARRAY[1, 2, 3, 4, 5] &amp;&amp; ARRAY[1, 2] -&gt; true SELECT ARRAY[1, 2, 3, 4, 5] &amp;&amp; ARRAY[10]   -&gt; false   而這個 operator 都有被 GiST 以及 GIN index 支援  可參考 11.2.5. GIN 以及 11.2.3. GiST   Benchmark Result of String Array                                                          Dataset Size       10w       10w                 Array Length       10       20                 Benchmark Result                            注意到上圖 y 軸單位的不同    圖上可能不太好看出來  在 array 長度為 10 的情況下，GIN index 可以有 GiST index 的 13 倍  而長度 20 的情況下更是可以擁有大約 10 倍的效能提昇  也再次驗證了我們上述所說的，GIN index 在效能上是比 GiST 還要好的   更不用提與沒加 index 的差別  長度為 10 的情況下，GiST 有接近 5 倍, GIN 可以接近 70 倍  為 20 的情況下，GiST 有 12.5 倍，而 GIN 可以高達 186 倍的效能提昇   References     12.9. GiST and GIN Index Types   F.35. pg_trgm — support for similarity of text using trigram matching   fuzzy search   Approximate string matching   Understanding Postgres GIN Indexes: The Good and the Bad   Using psql how do I list extensions installed in a database?   8.15. Arrays   9.13. Text Search Functions and Operators   Can PostgreSQL index array columns?   Add support for tsvector   operator does not exist: integer[] @@ integer[]   Postgres - Copy (Stripped Double Quotes)   59.2. Built-in Operator Classes   9.19. Array Functions and Operators   11.2. Index Types   Why error occurred while creating GIN index?   8.11. Text Search Types   F.8. btree_gin — GIN operator classes with B-tree behavior  ","categories": ["database"],
        "tags": ["postgresql","index","prisma","typescript","gin","gist","pg_trgm","fuzzy search"],
        "url": "/database/database-postgresql-index/",
        "teaser": null
      },{
        "title": "DevOps - 整合測試 Integration Test",
        "excerpt":"Introduction to Integration Test  光是擁有 unit test，其實是不夠的  因為 unit test 測試的範圍只有 function 本身  跨 function 之間的整合，是沒有涵蓋到的      有關 unit test 的部份，可以參考 DevOps - 單元測試 Unit Test | Shawn Hsu    integration test 在這方面可以很好的 solution  顧名思義，整合測試即為 整合不同的 component，一起進行測試      integration test 並沒有一定要跟資料庫一起測試  多 function 之間的整合也可以用 integration test  只不過我們時常在 integration test 裡面測試資料庫    Integration Test Scope  那麼究竟有哪些值得測試的呢？  所有的狀況都要測試嗎？   以往在寫 unit test 的時候，為了要包含所有的測試條件  我們會將 test case 盡可能的寫完整，寫的很詳盡  但是在 integration test 這裡我們不推薦這樣做  不在 integration test 做不代表我們不重視那些測試條件  我舉個例子好了   你有一個 API POST /group/{groupId}/members  作用是新增 member 到特定的 group 裡面  他的架構是這樣子的     input validation middleware 會先檢查輸入是否合法，e.g. groupId 必須要是數字   permission middleware 會先檢查使用者是否為 group 的一員，並確認他的權限能否有新增操作的權限   service layer 會準備所需的寫入格式   database layer 則負責執行寫入的操作   integration test 會需要做到 input validation 嗎？  這些是不是可以在 unit test 寫一個獨立的 validation test 呢？   因為 integration test 相比 unit test 來說是 heavy 了許多  因此我們會希望測試的內容著重在 component 之間的整合  比如說 permission middleware + service 有沒有正常運作, service + database 有沒有正常寫入   Dependency Injection  我們在 DevOps - 單元測試 Unit Test | Shawn Hsu 有提到  依賴於實作與依賴於界面的優缺點  在實作整合測試的時候，這些 pros and cons 會更大程度的影響你的測試撰寫   Parallel vs. Sequential Execution of Integration Test  講一個例子  前陣子我發現公司的專案在執行 integration test 的時候會有問題  看到最後發現是 transaction 的問題  過程大概是這樣子的   Jest 本身在執行測試的時候，為了節省執行時間提昇效率  它會使用本機所有的 core  根據 Jest 29.7 –maxWorkers 所述     Alias: -w. Specifies the maximum number of workers the worker-pool will spawn for running tests.   In single run mode, this defaults to the number of the cores available on your machine minus one for the main thread.    而正是因為這個原因，導致我們的測試出了問題  原因是我們並沒有做好 transaction 的管理，加上多個 test 同步執行  不同測試讀到其他人的結果，進而導致測試失敗  我們最後只能犧牲多核的好處，強制讓其用 serializable 的模式下去跑  現在我們的測試執行時間長達一分鐘  而相對的解決辦法就是使用 Dependency Injection  每一次 new 我們的 service 的時候，都給它一個新的 transaction connection  如此一來就可以解決上述的問題   Dependency Injection in JavaScript?  寫過一段時間的 JS 的你可能會發現  多數的 code 都是採用 functional programming 的方式，很少會使用 Class  沒有 Class 要怎麼做 Dependency Injection, 怎麼寫測試呢？   這時候就必須要用到 Test Double 裡面的 Fake Object 了  Fake Object 可以提供較為簡單版本的實作  假設你需要替換掉資料庫的界面實作，你可以透過 Fake Object 來做測試  如此一來你不需要大改你原本的實作，只需要換成 Fake Object 就可以了      有關 Test Double 的介紹可以參考 DevOps - 單元測試 Unit Test | Shawn Hsu    Docker Container  要測試與資料庫的整合，勢必要起一個 local database  我們的老朋友 Docker 就可以派上用場了   基本上使用說實在的也沒什麼特別的  僅須起個 database 放著，測試的時候呼叫對應的 ip, port 即可  就像下面這樣   1 2 3 4 5 6 7 $ docker run -d --name test \\     -p 6630:3306 \\     -v rest-mysql:/var/lib/mysql \\     -e MYSQL_DATABASE=db \\     -e MYSQL_USER=root \\     -e MYSQL_ROOT_PASSWORD=root \\ \t\tmariadb   One Container per Test?  跑整合測試理論上來說，每個測試案例需要有獨立的 container  但是這麼做會導致整體的 overhead 變得相對的大  因為每一個測試都需要重新建立一個 container   具體的取捨主要看團隊的共識  基本上因為 db 的讀寫(只要是 RDBMS) 都可以做 transaction rollback  所以不一定需要每個測試都有獨立的 container   dockertest  啟動 docker container 不一定只能手動建立執行  目前也有如 dockertest 這種第三方套件的存在  透過他你可以在程式碼內部用程式的方式啟動 docker container  取代了手動建立維護的需求   每次使用獨立的 container 我還有遇到一個問題是 port 佔用的問題  舉例來說，PostgreSQL 的 port 是 5432  每一次建立的時候都會佔用同一個 port   dockertest 在這方面有提供一個解決方案  他會自動幫你找到一個空閒的 port  比方說 PostgreSQL 預設 5432, 他可能 map 到 33203  利用內建的 GetPort function 取得該動態 port(也就是說你不需要煩惱真實的 port 是多少)  缺點是在寫測試的時候你需要提供一個 function 設定 env 讓 application 知道  因為像我們資料庫的連線資訊那些是透過環境變數設定的   基本上寫好，操作起來就跟手動建立一樣  只是 container 是透過 testing code 自動維護的  而且誠如前面所說，你不用管資料污染的問題   當然最大的缺點是，你的測試會變得很慢  以我的例子來說，我們手上的測試大概只有 10 幾隻  但是測試起來已經明顯地變慢了，最近的測試時間是 8 分鐘  以往大概 5, 6 分鐘左右就可以結束   Example  架構上跟 unit test 一樣  我們都是寫一個 describe block 然後裡面放上我們要測試的東西  就像這樣  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 describe(\"test getUsersSlow\", () =&gt; {   let conn: PrismaClient;    beforeEach(async () =&gt; {     jest.resetAllMocks();     conn = new PrismaClient();     jest.spyOn(database, \"newConnection\").mockReturnValue(conn);   });    afterEach(async () =&gt; {     await conn.$disconnect();   });    it(\"should get 2 users from page 1\", async () =&gt; {     const result = await userService.getUsersSlow(1, 2);     const expectedResult = [       {         id: 1,         username: \"fZAxGMLFJU\",         created_at: new Date(\"2024-04-26T03:39:52.000Z\"),         updated_at: new Date(\"2023-10-23T09:58:36.034Z\"),       },       {         id: 2,         username: \"LnJhEZFRlu\",         created_at: new Date(\"2025-06-07T01:21:27.000Z\"),         updated_at: new Date(\"2023-10-23T09:58:36.034Z\"),       },     ];      expect(result).toEqual(expectedResult);   }); });   原本的實作是這樣的  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 export default {   getUsersSlow: async (     pageNumber: number,     pageLimit: number   ): Promise&lt;UserResponse[]&gt; =&gt; {     let result: UserResponse[] = [];      try {       const connection = newConnection();       result = await userDB.findUsersSlow(connection, pageNumber, pageLimit);       logger.info(\"Successfully get users\");     } catch (error) {       logger.error(\"Encounter error, abort\", {         error: error,       });       throw new Error(Errors.InternalServerError);     }      return result;   } }   注意到一件事情  因為我們在這裡有跟 database 交互  為了讓每個測試有獨立的 connection，在 beforeEach 的時候手動建立一個 connection  並且使用 jest 的 spyOn 功能，將 database.newConnection 設置為 conn  如此一來在測試的時候，就會換成我們的實作了  然後每次執行的時候記得要將之前的 mock 重置 :arrow_right: jest.resetAllMocks()      呼應到上述 Dependency Injection in JavaScript? 說到的  不一定需要使用 Dependency Injection 才能反轉依賴    至於測試最後 afterEach 為什麼要手動 disconnect?  原因是我想要讓每個測試都有獨立的 connection   Jest Error  1 2 3 4 A worker process has failed to exit gracefully and has been force exited.  This is likely caused by tests leaking due to improper teardown.  Try running with --detectOpenHandles to find leaks.  Active timers can also cause this, ensure that .unref() was called on them.   在寫 mock 的時候要注意到你的 async/await 有沒有寫好  或者是說他有沒有正確的 mock 上去  如果沒有寫好要在檢查一遍你的 mock     以上的程式碼你都可以在 ambersun1234/blog-labs/cursor-based-pagination 當中找到   Difference with E2E Testing  我一開始看到 E2E test 的時候還以為他是跟 integration test 一樣的東西  這個字詞比較常在 frontend 的領域看到，不過他的概念在 backend 也同樣適用   E2E 的全名是 End 2 End，也就是端到端  這裡的端指的是 使用者端 與 服務端  因此 E2E 要測試的範圍，是從使用者的角度來使用我們的服務  換句話說，是從 API endpoint 進來到 response 的過程   Issues that I have when Writing Tests  有了上次寫單元測試的經驗之後，寫起測試確實是比較順利  但我還是遺漏了一些重要的事情      可以回顧一下 DevOps - 單元測試 Unit Test | Shawn Hsu    Time is Unreliable  假設你想要測試某個 test case 有沒有在規定的時間內跑完  第一直覺當然是直接算 time diff  得出結果   但是時間本身是一個很不精準的東西  即使跑測試的時候，每個 test 都會獨立執行不會互相影響  時間仍然是擁有許多變因的  舉例來說，context switch  如果測試要求的精準度很高，那它更不可能成為一個良好的評斷依據   Don’t Setup Test with Your Implementation  測試當然是一次測試一個 functionality  不過有時候你會需要做一些 setup test 之類的動作  比方說你想要測試 刪除 user account 的功能  那你一定會先建立 user account 然後再測試刪除的部份   問題在於那個 “先建立 user account” 的部份  要注意的是，你 不能用 API 建立，也 不能用 service  正確的作法應該是直接透過 ORM 或是 Raw SQL 直接建立   Integration test 主要在測試 “單個” service 在 component 之間的整合  考慮以下簡單版的實作   1 2 await this.userService.createUser(userData) await this.userService.deleteUser(userID)   這樣做，是錯的 :x:  過程中你實際使用了多個 function  你怎麼能保證 createUser 是正確的？  即使 createUser 有獨立的 test case 保護，在這個 case 主要是要測試 deleteUser  一個測試的 scope 應該僅限於它自己要測試的東西本身      至於 E2E  End to End 測試在於測試整個流程有沒有問題  對於 end user 來說它就是一個 黑盒子  你無法使用任何裡面的物件(i.e. services)    但是你可能會問，使用 ORM 不就違反了我們說的嗎？  我們的目的是，測試 “我們寫的軟體” 的流程、實作有沒有問題  如果 ORM 會錯，那其實責任不在我們身上，加上他是比較靠近底層的實作  所以透過它幫忙 setup test 其實是沒問題的   以我的例子來說，我就 偷懶嘛  都是使用 call api 的方式  這樣變成是一個 test case 裡面你會測試到多種不同的功能  而萬一其中一個壞掉，你有可能會發現不了  更重要的是，他是屬於 bad practice   Comparison of Testing Methodology                          Unit Test       Integration Test       E2E Test                       Scope       Function       Component       Whole Flow                 Test Data       Mock       Simulate Data       Real Data                 Speed       Fast       Slower than Unit Test       Slowest                 Execution Environment       Local       Local or Staging       Production           References  ","categories": ["devops"],
        "tags": ["integration test","mock","e2e test","docker","isolation","dependency injection"],
        "url": "/devops/devops-integration-test/",
        "teaser": null
      },{
        "title": "神奇的演算法 - Backtracking 與 Divide and Conquer",
        "excerpt":"Algorithm Brainstorming  直接看題目比較快，LeetCode 93. Restore IP Addresses  根據題目要求，給定一個只有數字的字串，找出所有合法的 ip address 的組合      Input: s = “25525511135”  Output: [“255.255.11.135”,”255.255.111.35”]    我們先列一下他的基本條件     字串的每個字元都是 0 ~ 9 的數字   ip 的每個數字，都是 0 ~ 255，而且開頭不能為 0(0 本身除外)   隱藏條件呢？     字串會被分割成 4 個部份，不能多也不能少   看到 “所有的組合”，最直覺的想法就是暴力解  是不是只要窮舉出所有可能的 ip 組合，再找到相符的條件即可  問題是要怎麼窮舉？   Divide and Conquer  舉個例子說明會比較簡單  相信從小到大大家都可能直接或間接的參與過運動相關賽事  那你一定看過這張圖      這張圖是 “賽程表”，表示了目前賽事的進程，以及隊伍的晉級情況  在比賽還沒開始之前，我們不知道第一名是誰對吧  但是是不是可以推敲出大概會是誰？   第一名只有兩個選擇，要馬 B, 要馬 C對吧？  B 是不是只會從 D, E 之間挑選？  C 是不是只會從 F, G 之間挑選？  以此類推, 是不是會得到一個公式呢?   1 2 3 4 因為 first place = winner(B, C) 又因為 B = winner(D, E), C = winner(F, G)  所以 first place = winner(winner(D, E), winner(F, G))  而此概念這正是 Divide and Conquer     Divide and Conquer 的概念是  將大的東西，切割成小部份  當我們將小的部份計算完成之後，大的部份也就很容易得出  就像上面提到的那樣, 我最終可以透過計算每場比賽的情況(winner(D, E), winner(F, G))，進而得出第一名是誰   Backtracking  窮舉的方法有了  那 backtracking 又是啥   說到暴力法最為人詬病的事情不外乎是 “較差的執行效率”  backtracking 的方法可以 提早停止無效的計算  啥意思呢？ 暴力法當中有很多的計算是沒有用的  Restore IP Addresses 這題我們剛剛有提到一個隱藏條件  複習一下叫做 字串會被分割成 4 個部份，不能多也不能少   這看似是一個廢話，但卻是有用的廢話  如果我的字串已經被分割成 5 個部份，請問它還有符合題目的要求，是一個 ip address 的樣子了嗎？  想必 6, 7, 8 個部份 都是非法的對吧，那往下算就不對了嘛  這個就是 backtracking 想要避免的東西   這張 gif 很好的展示了 backtracking 的實際流程  你可以看到他的答案會一直往回走，那就是代表 那條答案是錯誤的  Backtracking 提早終止的那些錯誤的計算       A Sudoku solved by backtracking.    Time Complexity  也因此，Backtracking 的執行效率一般來說會比純暴力解還要快  但需要注意的是，最差的情況下，依然跟暴力解一樣   Solution  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 import (     \"strconv\" )  func restoreIpAddresses(s string) []string {     return traverse(s, make([]string, 0), make([]string, 0)) }  func traverse(ip string, split []string, result []string) []string {     if len(split) == 3 {         split = append(split, ip)         for _, v := range split {             ipValue, _ := strconv.Atoi(v)             if ipValue &lt; 0 || ipValue &gt; 255 {                 return result             }             if string(v[0]) == \"0\" &amp;&amp; len(v) &gt; 1 {                 return result             }         }         return append(result, strings.Join(split, \".\"))     }      for i := 1; i &lt; len(ip); i++ {         result = traverse(ip[i:], append(split, ip[:i]), result)     }      return result }   思路就是結合了上面我們提到的各種方法論  一個 for loop 窮舉所有的分割方法，從第一個字元開始(不然左邊會是 “” 空字串)   值得注意的是中止條件 len(split) 需要等於 3 並不是 4  目前被分割出來的有 3 個部份，還需要加上還沒加到 split array 裡面的第四份，所以是 3  裡面的判斷基本上就是有沒有符合 ip address 的條件這樣  符合的就把它加到 result(因為題目說答案可以是任意順序，這裡就不用特別處理)   References     Backtracking  ","categories": ["algorithm"],
        "tags": ["backtracking","divide-and-conquer","recursion"],
        "url": "/algorithm/algorithm-backtracking/",
        "teaser": null
      },{
        "title": "浮點數 - 如何解決精度問題以及其原理",
        "excerpt":"Preface       ref: Damn you floating point binary addition, you’ve caused me tons of bugs over the years    要如何判斷是機器還是人類，用簡單的 0.1 + 0.2 就可以判斷了  不過到底為什麼它不會等於 0.3?  學過計算機的你一定能給出一個標準答案，是因為浮點數的精度問題所造成的  這時候問題來了，那 0.2 + 0.3 會是多少？      為什麼精度問題又不見了？  更重要的問題是為什麼它會有精度問題？ 以及我們該怎麼避免此類狀況發生？  本文會複習一遍這些概念以及其原理，一起看看吧   Binary Representation  讓我們先從基本開始  電腦儲存資料的方式一直都是以二進位的方式做  數字也是如此  回憶一下一個數字要怎麼以二進位的方式表示  以 5(10) 來說它可以表示成 101(2)  其含意為 $1 * 2^2 + 0 * 2^1 + 1 * 2^0$      因為電腦物理上只存在 有電 以及 沒電 的狀態，所以特別適合使用二進位配合處理    One’s Complement  一補數的存在，多數是用於計算二補數用的  由於使用一補數會出現 -0 這種奇妙的東西  因此單獨使用的機會較少   Calculation  將每個位元反轉即可  白話一點就是 0 變 1, 1 變 0   Two’s Complement  二進位要表示負數  我們需要使用 1 個 bit 紀錄其正負值  所以一個 32 bit 大小的儲存空間，它最大可以表示 $2^{31} - 1$ 到 $-2^{31}$      32 bit 大小是從 $2^{31}$ 到 $2^0$    值得注意的是，不是僅僅單純的加上正負號即可  對於二進位的數字，它必須要用特別的方式處理  即 2's complement 二補數   為什麼需要二補數而不是單純的儲存原本的數字呢？  根據 二補數      二補數系統的最大優點是可以在加法或減法處理中，  不需因為數字的正負而使用不同的計算方式。  只要一種加法電路就可以處理各種有號數加法，  而且減法可以用一個數加上另一個數的二補數來表示，  因此只要有加法電路及二補數電路即可完成各種有號數加法及減法，  在電路設計上相當方便。    Calculation  基本上計算只有兩個步驟要進行     計算 一補數   再加 1   我們假設一個 4 bit 的儲存空間  數字 5(10) 的二補數是這樣計算的  1 2 3 4 5(10) = 0101(2)  1010(2) -&gt; 一補數 1011(2) -&gt; 加 1   所以 4 bit 的空間下，數字 5 的二補數為 1011(2)   Introduction to Floating Point  從上述我們可以得知，數字是使用 Binary Representation 來表示  那麼對於小數 我們該怎麼處理？  其核心概念也是相同的  一樣將數字拆成二進位表示法  只不過是分成 兩個部份     整數部份的二進位   小數部份的二進位   IEEE 754       ref: 從 IEEE 754 標準來看為什麼浮點誤差是無法避免的    正負號的部份如同整數一樣，需要一個 bit 做紀錄   整數的部份儲存在 exponent 的區塊(需要正規化，可參考 Normalization)  我們知道，任何限定範圍內的 整數(i.e. integer) 都可以用二進位的方式完美的表示出來  在單精度浮點數的定義下，exponent 佔據了 8 個 bit   fraction(mantissa) 的部份則是儲存 正規化後 小數點之後 的數值(可參考 Normalization)   Normalization  光是算出整數以及小數的二進位表示並 不能直接塞  我們必須要對整數部份進行正規化   數字 8.5(10) 的二進位表示法是 1000.1(2)      如何轉成二進位，請參閱下表                           Integer to Binary       Fraction to Binary                       Image                               reference       Java Program for Decimal to Binary Conversion                     要讓它成為 IEEE 754 compatible，格式必須為 $\\pm 1.xxxxx \\times 2^n$     那個 leading bit 1 阿，我們不會 explicit 的存它，所以它通常被稱為 hidden bit    很顯然的 1000.1 並不符合 IEEE 754 的格式  要將小數點往左移，就是要乘上 base value  所以答案會是 $1.0001 \\times 2^3$      10 進位的情況下，小數點往左移就是乘以 $10^n$, 同理 2 進位的往左移就是 $2^n$    所以填進去會變成      ref: 從 IEEE 754 標準來看為什麼浮點誤差是無法避免的    為什麼是 exponent 的部份是 130 而不是 3, 為什麼要加 127？  我們留到下兩節 Why not use Two’s Complement 解釋   Why do we Need Normalization  我一開始很不能理解為什麼要做正規化 白話點就是為什麼不能直接塞  不過在此之前，讓我們看看做正規化有什麼好處      一致的表示規則   減少複雜度   可以比較簡單的實作處理四則運算   假設你有兩個數字要做乘法  分別為 $0.01101 \\times 2^2$ 以及 $100.01101 \\times 2^3$  他們相乘必須要寫成 $0.01101 \\times 100.01101 \\times 2^{2+3}$   很明顯的看到整體算式變得相對複雜  所以擁有一致的表示法，可以減少複雜度，並且你可以用現成的硬體完成這件事   Why not Store Two’s Complement in Exponent  IEEE 754 的 exponent 的部份為什麼不是直接塞  而是要讓 exponent 加上一定的偏移量(i.e. bias)呢？   單精度的情況下，根據定義你可以知道 exponent 有 8 個 bit  換句話說它能夠表示 256($2^8$) 種數字對吧  而且他的偏移量是 127($2^7 - 1$), 這肯定不是巧合  看到這裡你八成猜的出來它想做什麼  就是在 exponent 這裡也引入負數的情況      注意到這裡的負數跟 sign-bit 是不同的  這裡是 exponent 的正負號    所以 exponent 可以表示的範圍是 -126 到 127 也就是 $-2^7 + 2$ 到 $2^7 - 1$  你說為什麼不是 -127 到 128?  因為有一些特例要處理(e.g. INF, $\\infty$)  IEEE 754 上面有列出各種情況，放在這供參考      所以回到本節的問題  為什麼它不直接存 2’s complement 進去就好了？  換個方式，用 2’s complement 存會有什麼缺點     正負號處理難度增加   硬體成本難度增加   所以，理論上做的到，但就是 trade-off  使用 bias 的方式顯然比 2’s complement 的問題更小      注意到不論使用 Two’s Complement 或者是 bias 的方法  他們 exponent range 都不會有所改變    Overflow vs. Underflow  其實不只是浮點數，所有資料型態都要注意到他的上限與下限  避免 overflow 與 underflow 的問題出現      overflow: 超過我能表示的最大數值   underflow: 低於我能表示的最小數值   比如說單精度浮點數  它能表示的最小值為 $2^{-23} \\times 2^{-126}$(fraction 最小值乘上 exponent 最小值)      fraction bit 數等於 32 bit - 1 bit(sign) - 8 bit(exponent) = 23 bit    Precision Loss  俗話說的好，算術用浮點 遲早被人扁  這麼說肯定是有他的道理的  浮點數由於其精度問題，在需要精密計算的時候，時常造成誤差  也就有這麼一句經典 警惕後人   但回到最初我們破題的那個疑問  為什麼 0.1 + 0.2 != 0.3 但 0.2 + 0.3 = 0.5?   還記得小數的部份是怎麼計算的嗎?     與整數取餘數不同，小數的部份則是以 $\\times 2$ 的部份做計算，取他的 整數 部份，直到整體數值為 0 才結束  以 0.3 來看，他的二進位表示法長這樣  1 2 3 4 5 6 7 8 數值      運算後  整數部份 0.3 * 2 = 0.6      0 0.6 * 2 = 1.2      1 0.2 * 2 = 0.4      0 0.4 * 2 = 0.8      0 0.8 * 2 = 1.6      1 0.6 * 2 = 1.2      1 0.2 * 2 = 0.4      0   你有沒有發現，數值已經重複了  代表他是沒辦法算到 0 的  它循環了   所以這就是為什麼，0.3 用 IEEE 754 沒辦法表示完整的真實原因  我們只能 近似 該小數(你可以理解成整數之間有無數個小數，數不完)  所以 0.3(10) 的二進位是 0.0100110(2), IEEE 754 則是 $1.00110 \\times 2^{-2}$  你可以搭配 IEEE 754 Floating Point Converter 一起玩     0.5 的部份就也依樣畫葫蘆  1 2 數值      運算後  整數部份 0.5 * 2 = 1.0      1  所以 0.5(10) 的二進位是 0.1(2), IEEE 754 則是 $1 \\times 2^{-1}$  因為二進位可以完美的表示 0.5 這個數字，所以它不會有精度問題！   How to Prevent Precision Loss  我們成功的解析出，浮點數的精度問題了  也很了解其中的設計了   但精度問題仍然是個不小的問題對吧  只有特定的數字不會有精度問題，顯然不夠好  有沒有辦法可以讓小數，在計算機中，不存在有精度問題呢？   Big Number Arithmetic  回顧一下大數加法，當初學到它要解決的問題就是 大數  大數之所以是個問題是因為現有的資料型態基本上都有一個固定的大小  也因此儲存的數值是有一個範圍的，超過或小於都會造成問題  就是上面有稍微提到的 Overflow vs. Underflow 的問題   我們能不能借鑒這個想法  把它存到一個 array 的空間中，然後套用大數運算的原理去處理  顯然理論上是沒問題的   對，理論上  實務上會有什麼問題？     要怎麼處理小數點？ 它會有對齊的問題   運算速度問題會不會太慢   Introduction to Fixed Point  相較於浮點數，存在精度問題  定點數 可以完美的解決以上問題   定點，亦即某個點是固定不動的  既然我們談到小數了，那就是小數點囉  小數點不動是啥意思呢   我能不能用 Integer 表示小數?  可是我不是才說小數理論上有無限多個嗎？ 我要怎麼用整數表示小數？  先慢慢來，如果我只想表示 有限多個呢  假設我想要的資料精度，到小數點第二位  我能不能將整數位的 第 0 位 以及 第 1 位 保留起來當作小數的部份  其餘部份就是整數部份囉   我不一定要將小數點給存起來，我可以假裝它存在  假設我要存 11.32 這個小數好了，要怎麼把它塞進 int 裡面？  存 1132 行不行？  要顯示的時候我在除回去給你就好，但我儲存的時候，我假裝我有存小數點的概念進去(實際上沒有)   在程式裡面紀錄所謂的 縮放大小(scaling factor)，以本例來說就是 100 倍  而定點數的概念就是這麼的簡單   Q Format Notation  基本上就是上面講的 scaling factor  不過 Q Format Notation 比較多是在描述二進位的情況   他的表示是這樣子寫的 Qm.n  意思是說，有 m 個 bits 用來表示整數部份，n 個 bits 用來表示小數部份  所以 Q19.12 就是說，有 19 bits 用來表示整數部份，12 bits 用來表示小數部份  要把它還原，就是將整個 integer 除以 $2^{12}$(因為是二進位)      19 + 12 = 31 bits，因為有 sign bit 要算進去    Fixed Point Builtin Implementation  我一開始想要試定點數的時候，發現竟無從下手  要玩浮點數，我可以直接用 C 開 float, double 下去看  但是對於定點數我卻不知怎麼開始   原因在於多數程式語言的實作都有支援 IEEE 754  但是對於定點數的支援還比較少  就比如說 C  不過網路上還是有一些 Open Source 的 Library 可以使用   Summary                          Floating Point       Fixed Point                       Data Range       Big       Small                 Speed       Slow       Fast                 Native Support       Yes       No                 Precision Loss       Yes       No           Is Floating Point Useless?  看到這裡我想你已經有答案了  對於快速運算，並且可以犧牲一定精準度的場合下，使用浮點數並無不妥  若是對於精度有極高要求，如金融相關，那麼定點數可能更適合你   References     IEEE-754 與浮點數運算   從 IEEE 754 標準來看為什麼浮點誤差是無法避免的   一補數   How to normalize a mantissa   Single-precision floating-point format Range   算術溢位   算術下溢   Q (number format)   COSCUP 2024 - 從零開始建構 C 語言最佳化編譯器  ","categories": ["random"],
        "tags": ["fixed point","ieee754","floating point","binary","overflow","underflow"],
        "url": "/random/fixed-point/",
        "teaser": null
      },{
        "title": "資料庫 - 從 Apache Kafka 認識 Message Queue",
        "excerpt":"Preface  message queue 顧名思義他是一個 queue，用來存放 message 的  你可以用 Inter-Process Communication 的概念去思考它  基本上就是提供一個空間或是，讓兩個 process 進行通訊      有關 IPC 的相關討論可以參考 Goroutine 與 Channel 的共舞 | Shawn Hsu    不過，當然 message queue 服務的對象是 application  與傳統的 IPC 還是不同的  但問題是為什麼我們需要 message queue?      我用 HTTP, gRPC 之類的 protocol 也能進行通訊   我透過讀取共享檔案的方式，也能進行通訊      有關 gRPC 相關的討論可以參考 網頁程式設計三兩事 - gRPC | Shawn Hsu    是什麼樣的原因讓我們必須要開發一個新的方式  是這些行之有年的技術比較不能做到的？   Introduction to Message Queue  所以到底為什麼需要 message queue?  假設你是用 HTTP request 跟不同的 service 互動  你可能會遇到什麼狀況？     網路是不可靠的，application 之間可能會出於各種原因斷線，資料可能會丟失   使用傳統的方式收送資料，他的 data format 是不容易更改的   不同的 application 的吞吐能力可能不同，量大的時候相當於你在 DDOS 你的服務，他有可能掛掉，資料可能會丟失   收送資料時，client 與 server 必須同時在線   我知道有些理由有點牽強  但你大概可以有個概念說，為什麼 message queue 會被發明出來   那 message queue 大致上可以做到以下這幾件事情     允許 非同步 處理   不同的 service 可以 解耦合   讓不同 service 之間有個緩衝區(i.e. 不會被打爆)   允許個別 service scale up(i.e. 不會被打爆)            如果是 server produce 太多 message 導致 client 消化不過來       那麼我只要將 client scale up(或 scale out) 即可, server 可以不需要更改           簡化 message routing 的部份(要送去哪阿之類的)   可以讓 message 有容錯機制(亦即他有 retry 機制)   擁有 message persistence 的特性            雖說 message queue 本身是以 highly reliable 為原則設計的，但它還是有機率掛掉，掛掉之後還沒被 consume 的 message 不能不見           一致性的 message format            message queue 本身並沒有規定傳入的資料需要符合特定的資料格式(提供彈性)，對它來說都是 “資料”。           Re-enqueue Message  message queue 在某些狀況下，會需要重新將 message 放入 queue 當中  比如說，message 正確的被 consumer consume 了，但該 message 沒辦法被正確處理(i.e. 資料格式不能被 consumer 處理, 網路有問題 … etc.)  多半時候我們會選擇實作 retry 的機制   retry 基本上有兩種作法     在 consumer 內，使用 for-loop 進行 retry   或者是選擇重新將 message enqueue   先講結論，重新 enqueue 的方式會比較好  如果處理失敗，可能的原因可能是因為網路出問題，或者是遇到 rate limit 等等的問題  如果是使用 for-loop retry, 網路的問題並不會被解決(中間網路斷線，rate limit 都是)  而如果你選擇重新 enqueue, 在多個 consumer 的情況下(執行在不同機器上)，可能就解決了   另外，重新 enqueue 要實作在哪裡呢  像是 RabbitMQ 這種  如果是自己有額外包一層 helper, 那麼 consume message 通常我們都會傳一個 callback 進去 consume  有點像這樣  1 2 3 4 5 6 7 8 9 10 11 12 13 14 for {     select {         case &lt;-mq.ctx.Done():             return nil          case msg := &lt;-queue:             mq.logger.Debug(\"Received message\", zap.String(\"body\", string(msg.Body)))              if err := callback(string(msg.Body)); err != nil {                 mq.logger.Error(\"Failed to process message\", zap.Error(err))             }             msg.Ack(false)         }     }   一直從 queue 裡面將 message 讀出來  在第 10 行做 re-enqueue 的實作你覺的如何?  hmmm 不好，其實應該要在 callback 裡面自己做 re-enqueue 的機制   對 callback 是噴 error 出來沒錯  但是你沒辦法知道說它到底是什麼原因導致失敗的  如果是因為 message 本身就不合法，這個 case 算是正確的被 consume，而失敗是合理的  所以你應該要讓 caller 自己決定怎麼處理要不要 retry      message 不合法的情況，假設 mq 這裡是負責處理發送 email  但它收到的訊息可能 email 格式不對或是缺少必要的欄位(假設你 produce 的 code 改壞掉之類的)  你再怎麼 re-enqueue 都沒辦法解決問題，所以才要讓 caller 自己決定  當然你可以使用 DLQ 放著好方便查看問題    DLQ(Dead Letter Queue)  前面我們提到，message 可能會執行失敗  為了解決這個問題，可以透過 retry 的機制重試   不過，重試的成本在大流量系統中可能會出現效能瓶頸  而 DLQ ，就是將那些執行執行失敗的 message 最終要去的地方  把這些 message 儲存起來，就不會擋到原本的標準佇列，而待 IT 人員修復系統之後，就可以重新執行這些資料了     如果說你需要一直執行 message(不論它會不會失敗)  那顯然 DLQ 並不是一個最好的選擇  我們可以參考 OS 設計 Multilevel Queue Scheduling 以及 Priority Scheduling 的概念  我們可以分成     high priority queue   low priority queue   當 message 在 high priority queue 中失敗三次之後就將它移到 low priority queue  如此一來便不會造成 high priority queue 的效能瓶頸  而每個 queue 可以對應到不同的 consumer 實作做最佳化   Publisher-Subscriber Pattern  稍微複習一下 Publisher-Subscriber Pattern      或者你可以到 設計模式 101 - Observer Pattern | Shawn Hsu 複習         ref: Observer vs Pub-Sub Pattern    publisher 將 message 放入一個空間內，通常是一個 queue  然後由 subscriber 根據資料的 標籤(i.e. topic) 自行取用需要的資料  這種方式，publisher, subscriber 雙方都不會知道對方是誰  而且你 哪時候 要拿，我也不 care   Pull/Push Protocol     server 直接往 client 丟資料，然後 client 沒有要求 :arrow_right: push protocol   client 向 server 主動要資料 :arrow_right: pull protocol   push protocol 由於是 server 主動往 client 丟資料  一般來說，你會有很多個 client 同時處理資料  對於每一個 connection 都使用相同的發送速度顯然太不合理(而且也難以維護)  萬一有某個 client 它消化的速度跟不上怎麼辦？ 只好建立一個 buffer 留著，但這顯然沒意義(你都用 message queue 了)   與其這樣不如讓 client 自己決定接收資料的速度  它可以根據自己處理的狀況動態的調整  但有一個缺點是，當 queue 是空的的情況下，client 的行為就會變成 polling 了      可參考 淺談 Polling, Long Polling 以及其他即時通訊方法論 | Shawn Hsu    Protocols of Message Queue  JMS  現有的 RPC call 系統，要求 client 與 server 都同時在線才能進行通訊  並且該通訊方式是屬於 block I/O, 亦即他是非同步的  而這無疑阻礙了低耦合系統的開發   JMS - Java Messaging Service 是為了克服以上問題而開發出的一套 messaging 系統  支援一般 P2P 以及 Publisher-Subscriber Pattern 的通訊模型  它包含了以下元件     JMS provider :arrow_right: 實作了 JMS specification 的 server   JMS client   Messages   Administered Objects   要注意的是，JMS 只有在 P2P 的模式下是使用 message queue(QueueConnectionFactory)  在 Pub/Sub 模式下是使用 TopicConnectionFactory   MQTT  你可能有聽過 MQTT 這個東西，通常是在嵌入式系統的領域較為常見  MQTT 定義了訊息傳遞的標準，其標準已經來到了 MQTT Version 5.0, 就讓我們稍微看一下標準內容吧   MQTT - Message Queuing Telemetry Transport 是一種基於 Publisher-Subscriber pattern 的訊息交換協議  它可以跑在任何提供 有序的、無壓縮、雙向連接的協議之上，如 TCP/IP   與 AMQP 不同的是，MQTT 並沒有使用任何 message queue  單純的就是 client, server 的角色而已，client 發送帶有 MQTT header 的訊息至 server 上  server 根據 subscription 轉送至不同的 client  而 MQTT 也有支援 Topic 的概念，因此 client 可以根據自己感興趣的部份進行 subscribe   此外 message delivery 的方式也有分成     at most once   at least once   exactly once   從上述你可以知道，client server 的訊息交換是建立在 network connection 之上的  萬一斷線要怎麼辦？ 理論上來說，當發生 disconnect 行為的時候，不論是網路問題還是正常結束  都沒辦法再繼續傳送訊息，因此，MQTT 只能在網路有通的情況下進行訊息的交換   另外就是當 disconnect 的情況發生，如果 will flag 有設定，server 必須主動發送 will message 跟其他人說 connection closed      2611 After sending a DISCONNECT packet the sender:  2612     • MUST NOT send any more MQTT Control Packets on that Network Connection [MQTT-3.14.4-1].  2613     • MUST close the Network Connection [MQTT-3.14.4-2].  2614  2615 On receipt of DISCONNECT with a Reason Code of 0x00 (Success) the Server:  2616     • MUST discard any Will Message associated with the current Connection without publishing it  2617 [MQTT-3.14.4-3], as described in section 3.1.2.5.  2618  2619 On receipt of DISCONNECT, the receiver:  2620     • SHOULD close the Network Connection.    AMQP  AMQP(Advanced Message Queue Protocol) 是一個執行在 應用層 之上的 binary 協議  其標準內容已經被正式收錄在 ISO/IEC 19464 之中  但他是收費的，所以我們看 RabbitMQ 上的這一份 0.9.1 的 spec - AMQP - Advanced Message Queuing Protocol - Protocol Specification   AMQP 是為了解決跨系統間的整合，降低成本而提出的一套訊息傳遞的標準  而它由兩大部份 service-side services(稱為 broker) 以及 network protocol 所組成  而 server-side services 是由 AMQ Model 構成   AMQ Model  AMQ Model 全名為 The Advanced Message Queuing Model  包含了三大元件 exchange, message queue 以及 binding   exchange  exchange 主要負責執行 routing 的工作，負責將從 publisher 送過來的資料  轉送到特定的 message queue 上      所以 exchange 本身並不會儲存 message    exchange 通常會檢查一個叫做 routing key 的欄位     在 P2P 的模式下，通常為 message queue 的名字   在 pub/sub 的模式下，通常為 topic 的名字   exchange 有 5 種模式(direct, topic, fanout, headers 以及 system)，但是下面兩種是最重要的     Direct Exchange(Default Exchange)            Direct Exchange 會使用 routing key 進行綁定       exchange 的名字可以是 amqp.direct 或是 &lt;empty&gt;(空值)           Topic Exchange            Topic 就是在 Publisher-Subscriber Pattern 裡面講到的 topic       subscriber 可以根據自己有興趣的主題(topic) 進行訂閱       而它就是透過 binding 定義的 pattern 進行轉送的              既然我可以直接將 message 送到 message queue 上(default exchange)，那為什麼還要經過 exchange 呢?  考慮 fan-out 的情況，如果多個 consumer 都對同一件事情有興趣  手動控制需要丟到很多不同的 message queue 很顯然是不合理的  透過 exchange 做 routing 可以簡化這個部份，設計也更合理(i.e. topic exchange)    message queue  message queue 主要負責儲存資料，直到它被安全的被 consumer 處理掉  它可以儲存在 memory 或者是 disk 裡面   message queue 有分兩種形式  一種是 Durable 的，意味著說就算沒有 consumer, queue 一樣會繼續存在  另一種則是 Temporary, 當這個 queue 只服務特定的 consumer 的時候，然後那個 consumer 結束的時候，該 queue 就會被刪除了   binding  定義 exchange 以及 message queue 之間的關係   考慮 Topic Exchange type  它主要是透過 “類似 regex” 的方式進行匹配的  匹配的關係式長的像這樣子 user.stock  他是由多個詞語所組成的，其中每個詞都是大小寫的 a 到 z 以及數字 0 到 9 組成的  每個字詞中間是使用 . 做區隔  並且配合上 *, # 字號(* 代表只出現 1 個詞，# 代表出現 0 次以上)   比如說 binding 的條件(pattern)是 *.stock.#     user.stock 以及 eur.stock.db 這些是 match 的 :heavy_check_mark:   stock.nasdaq 而這個不會 match :x:   當 routing key 與 routing pattern 符合的時候，就將它轉送到特定的 message queue 上  注意到它可能會 match 到多個符合的 routing pattern，也就是多個 queue 都會拿到資料      其他 Exchange type 也都是透過類似的方法決定要如何 route message 的      所以整體的流程會是這樣子的  producer 將 message 送到 AMQP server 上  exchange 會根據 message 內部的 properties(i.e. binding) 決定要轉送到哪個 message queue 上  如果無法決定要送到哪個 queue 上面，AMQP server 會把它丟掉或者是原路送回(看實作而定)  message queue 會在 第一時間，想辦法將訊息送到 consumer 手上，如果 consumer 沒空，它才會選擇儲存起來  之後 consumer 在從 message queue 將訊息取走   Comparison  上述我們看了幾個比較重要的協議，讓我們整理成表格看一下好了                          JMS       MQTT       AMQP                       Pattern       P2P Publisher-Subscriber pattern       Publisher-Subscriber pattern       P2P Publisher-Subscriber pattern                 Have Message Queue       :heavy_check_mark:(only exists in P2P)       :x:       :heavy_check_mark:                 Asynchronous       :heavy_check_mark:       :x:       :heavy_check_mark:                 Multiple Language Support       :x:(Java)       :heavy_check_mark:       :heavy_check_mark:           Apache Kafka  根據 Kafka 官網，他是這麼定義自己的產品的      Apache Kafka is an open-source distributed event streaming platform   used by thousands of companies for high-performance   data pipelines, streaming analytics, data integration, and mission-critical applications.    對，Kafka 主要是用於 event streaming  Kafka 鼓勵開發者以 event 的角度去思考世界  每一次的變動都可以被視為是 “事件”，比如說使用者將某個商品放到購物車  他是一個 event(事件)   而 event 擁有幾個特性     它一定是 有序的，事件有先後順序   事件不可以被改變，因為改變 = 另一個事件   Architecture  Apache Kafka 本質上是 Publisher-Subscriber Pattern 的實現  它擁有     publisher 負責生產訊息   topic(log-like structure) 負責儲存訊息   subscriber 負責消化訊息   Data Store  其中 topic 是主要儲存訊息的資料結構(N to N 的架構，可以有多個 publisher 也可以有多個 subscriber)  它可以設定不同的名字，用以區分訊息種類，subscriber 再依據需要的主題進行監聽即可   topic 是一個 order sequence of event, 我們剛剛提到，事件是會分先後的  並且 topic 本身的資料是 durably stored 的，亦即它不會因為斷電等因素而掉資料  其中的原因為     他是儲存在 硬碟 裡面   資料會被拆成多份(partitioned)，並且擁有多個副本(replication), 可參考 Partition and Replication   儲存在硬碟裡，是可以避免掉資料的問題  但硬碟不是很慢嗎，Kafka 是如何維持高吞吐量的?  well, 他們在 4.2 Persistence 裡面有詳細說明  其中最大的優勢是在 sequential I/O(可參考 資料庫 - 初探分散式資料庫 | Shawn Hsu)   sequential I/O 根據他們的說法，相對於 Random I/O 有高達 6000 倍的效能提昇  與其自己維護 in-memory cache 增加維護難度，程式複雜度  並且 in-memory cache 如果碰上重啟，會需要有 warm up 的時間以及 cache miss 的問題  我們可以藉著 kernel 的 page cache 自動幫我們做 cache(現代 OS 會利用空閒的記憶體用作 disk cache)，搭配上 sequential I/O  使得整體的邏輯更簡單，並且效能也不會差太多      有關 cache 的討論，可參考 資料庫 - Cache Strategies 與常見的 Solutions | Shawn Hsu    將源源不斷的事件資料，透過適當的 batching 儲存  而這些原本屬於 random write 的資料，變成 linear write(因為 batching 可以讓資料緊緊相連，就在旁邊而已)  也就是可以利用上面提到的 sequential I/O   利用 page cache(disk cache) 搭配 sendfile system call，更可以減少 overhead  在傳輸資料的時候，sendfile 系統呼叫能夠減少 copy buffer 的次數  使得 kernel-space 的資料可以直接透過 NIC buffer 傳到 consumer 手上(不用經過 user-space)     另一個維持高吞吐量的重點在於儲存的資料結構  Apache Kafka 是使用 log-like 的結構(e.g. queue) 儲存資料的  你可曾想過為什麼它不使用像是 PostgreSQL 之類的資料庫嗎？   資料庫的操作，如果是 B-Tree 結構的  我們知道他的時間複雜度是 $O(LogN)$(其中 N 為樹高)  又根據我們對於 Sequential 以及 Random I/O 的認識，資料庫是屬於 Random I/O  Random I/O 必須要做 disk seek, 而這個操作幾乎不可能 parallel 執行  因此採用 B-Tree 結構會有一些 overhead   所以 Kafka 是使用 Queue 這個資料結構  它可以實現真正意義上的 $O(1)$ 寫入  而 Kafka 你可以想像成是寫入 檔案, 一個 topic 對應到一個資料夾，裡面有若干個檔案      另外就是，topic 裡面的資料不會因為已經被 consume 就把它刪掉  我們可以設定資料要被留存多久         ref: INTRODUCTION      稍早我們也提到，message queue 有自己的通用格式  Kafka 為了維持高吞吐量，也擁有自己的 binary message format  broker, producer, consumer 都共用，所以不需要額外的處理   Partition and Replication  Kafka 本身是分散式的系統  就我們目前知道的，Kafka 的 topic 是會被 partitioned，配合 replication 可以達到高可用性  每個 topic 裡面的資料都將被切分成若干個 partition(僅擁有部份 topic 的資料)  每個 topic partition 都可以使 不同的 client 在不同的 broker 上面進行同步的讀寫      針對每個 topic 的每個 partition, 其內部順序是有序的  partition 之間的順序不保證    當你的資料量大的時候，可以適度的增加 consumer 的數量  Kafka 會根據 consumer 的數量自動調整 partition 的數量  他會自動做 re-balancing，使得你可以即時處理更多的資料     Kafka 是使用 single leader replication 的機制  亦即每個 partition 只有一個 node(leader) 負責寫入，剩下的 node(follower) 或是 leader 提供讀取的功能      資料成功寫入的判斷是用 quorum 的方式進行的  而同意寫入票數的人員必須要是 ISR(In-Sync Replicas) 裡面的人員    如同教科書上對於 single leader replication 的描述一樣  Kafka 一樣要處理節點失效的問題  這部份是透過一個特殊的 node(稱為 controller)  controller 主要負責做兩件事情     定時將 metadata 更新至所有的 node            metadata 包含了所有的 topic, partition, offset 等等的資訊       簡單來說，metadata 也包含目前的 leader 是誰，你要 follow 哪個節點的 這種資訊           監控並處理已經離線的 node            透過 heartbeat，定期發送一個訊號給 controller，如果規定時間內沒有收到訊號，controller 就會將該 node 視為失效              有關 single leader replication，可參考 資料庫 - 初探分散式資料庫 | Shawn Hsu    Kafka 對於節點失效的定義有那麼一點點的不同  以下兩種都可以被視為是失效的狀態     節點無回應(i.e. 沒有接受到 heartbeat 的信號)   節點回應得很慢，並且資料已經落後一小段   所有 in-sync 的節點(非失效節點)會在 ISR(In-Sync Replicas) 名單裡面  ISR 就像是個委員會，委員會的人數是浮動的，當有人突然斷線或者是落後太多，就會被踢出委員會  如果掉線的節點是 leader，controller 會選擇 ISR 裡面的一個節點當作新的 leader   要重新加入 ISR，必須要追趕上 leader 的進度，這樣你就可以進行下一輪會長的競選！   阿如果全部的節點都掛了怎麼辦？  Kafka 目前的做法是等待 ISR 的其中一個節點恢復，將他選為 leader      從 ISR 以外的節點選 leader 會有可能造成資料的遺失       Kafka 不會處理 Byzantine Generals Problem 的問題    How to Consume Message  使用哪一種的 Pull/Push Protocol 是個好問題   以 Kafka 來說，是使用 pull based 的 protocol  client 透過 offset 指定要從哪一個位置開始讀取  因為資料的儲存方式是檔案，所以就是直接 seek 到指定位置，讀取一定數量的資料即可   再來的問題是  即使資料是保存在硬碟當中的，我們不可能無限的永久的儲存  他在某一天肯定是會被刪除的   確保資料被正確的 consume 是一件重要的事情  RabbitMQ 會透過 acknowledgement 來確保資料被正確的讀取以及處理  所以可以安全的刪除，但這會有問題     consume 了，處理了但是失敗了，同一個資料會被重複處理   一筆資料現在必須包含 ack 的欄位，用以紀錄是否已經被正確的處理   Kafka 的設計是這樣的  我一樣利用 ack 的概念，只不過我不需要每筆資料都紀錄  因為 “事件” 的概念是 Kafka 的核心，而又因為他是儲存在類似檔案的結構裡面，他是有序的  所以我只要紀錄該 consumer 的 offset 就好了(offset 以前的我就已經讀取完成了)  我只需要 maintain offset 就好了, 相比維護每筆資料的 ack 這顯然輕量多了      此外透過 offset 我也可以讀取以前的資料(當你發現已經 consume 過的資料有錯誤的時候可以再次讀取)    ZooKeeper and KRaft  我們在 Partition and Replication 裡面提到的機制，是最新的 KRaft  在早期，Kafka 是使用 ZooKeeper 來做這些事情的   具體來說，分散式系統這種東西它需要舉行選舉，監控節點狀態這些事情  在 Kafka 2.8 之前，這些事情都是由 ZooKeeper 來負責的，他沒辦法自己管理  也就是說當這些事情發生的時候，你需要依靠 ZooKeeper 來進行   Zookeeper 是一個獨立的分散式系統，負責幫 Kafka 做到以上的事情  所以你在 deploy 的時候，你需要 deploy 一個 ZooKeeper 的 cluster 來幫助 Kafka  這其實也不是什麼大事情，但問題是系統的自身狀態需要依靠額外的系統幫你管理這件事情造成問題   假設 leader 掛了  你需要做的事情是，讓 Zookeeper 重新選舉一個 leader(因為只有他知道系統的狀態)  並且將結果通知所有的節點，讓他們同步資料(follow 的人改變了，你現在要同步 B 的資料而不是 A 的)  當你的系統越來越大，這些 overhead 就不可忽視了   KRaft 是 Kafka 2.8 之後的新機制  主要是將 Raft algorithm 做在 Kafka 裡面  讓 Kafka 自己可以管理自己的狀態，不需要依靠外部系統(ZooKeeper)  所以現在 Node 自己會跟其他節點同步所謂的 metadata，並且自己進行選舉  這樣就更輕量了   Example  Prerequisite  我們可以使用 docker 將 Kafka 以及其他的資源在 local 跑起來  Kafka 本身很單純，因為已經有 KRaft 的了所以整個環境相對簡單   1 $ docker run --name kafka -d -p 9092:9092 apache/kafka:3.7.0   我們只需要將 9092 port forward 出來即可   不同於 RabbitMQ 自己帶有漂亮的 GUI 介面  Kafka 這裡我們可以使用 Redpanda  他也可以使用 Docker   1 2 3 4 5 $ docker run -d -p 8080:8080 \\ \t\t--network host \\ \t\t--name kafka-ui \\ \t\t-e KAFKA_BROKERS=localhost:9092 \\ \t\tdocker.redpanda.com/redpandadata/console:latest   主要就兩個點，一個是 Kafka broker 的位置，另一個是 GUI 的 port   Producer and Consumer  這裡使用 confluent 提供的 go client 來進行操作   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 var (     topic            = \"test\"     connectionString = \"localhost:9092\" )  func producer() {     conn, err := kafka.NewProducer(&amp;kafka.ConfigMap{         \"bootstrap.servers\": connectionString,     })     if err != nil {         panic(err)     }     defer conn.Close()      ticker := time.NewTicker(1 * time.Second)     for v := range ticker.C {         err := conn.Produce(&amp;kafka.Message{             TopicPartition: kafka.TopicPartition{             Topic: &amp;topic, Partition: kafka.PartitionAny,             },             Value: []byte(fmt.Sprintf(\"Hello Kafka %v\", v)),         }, nil)          if err == nil {             fmt.Println(\"Produce message to topic: \", topic)         } else if err.(kafka.Error).IsTimeout() {             fmt.Println(\"Timeout\")         } else {             fmt.Println(\"Producer error: \", err)         }     } }   producer 每一秒鐘都會產生一條新的資訊送到 Kafka 的 test topic 上面  這邊指定的 partition 為 any, 亦即 Kafka 會自己決定要送到哪個 partition 上面  這樣做的好處在於可以讓 Kafka 自己決定要怎麼分配資料，達到 load balance 的效果   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 func consumer() {     conn, err := kafka.NewConsumer(&amp;kafka.ConfigMap{         \"bootstrap.servers\": connectionString,         \"group.id\":          \"test_group\",         \"auto.offset.reset\": \"earliest\",     })     if err != nil {         panic(err)     }     defer conn.Close()      if err := conn.SubscribeTopics([]string{topic}, nil); err != nil {         panic(err)     }      for {         msg, err := conn.ReadMessage(time.Second)         if err != nil {             fmt.Println(\"Consumer error: \", err)             continue         }      fmt.Printf(\"Consumer(%v) message from topic(%v): %v\\n\", conn.String(), msg.TopicPartition, string(msg.Value))     } }   consumer 這裡有兩個東西滿有趣的  我們知道 Kafka 的 client 是依靠所謂的 offset 來記錄讀取的位置  那這個紀錄的位置是必須要由 client 提交的，一般來說 enable.auto.commit 預設是每 5 秒提交一次  這樣即使你關閉 consumer 重新開啟，你也會從上次的 offset 開始讀取      可以使用 auto.commit.interval.ms 來調整提交的時間    這裡你可以看到我們定義了 auto.offset.reset  指的是當沒有任何 offset 紀錄的時候，你要從哪裡開始讀取     earliest 代表從最早的 offset 開始讀取   latest 代表從最新的 offset 開始讀取      如果你在執行的五秒內退出程式，則現在不會有任何的 offset 紀錄  這時候 consumer 會根據 auto.offset.reset 來決定要從哪裡開始讀取    另一個有趣的東西是 group  我們說過，一個 topic 可以有多個 partition  每一個 group 都負責讀取一個 partition  group 裡面可以擁有多個 consumer   所以整體邏輯是，在單位時間內，只能有一個 consumer(在某一個 group 底下的)能夠讀寫一個 partition  group 裡面的 consumer 同一時間只能有一個對 partition 進行操作  我覺得好像哪裡怪怪的，group 這層的抽象的設計我沒有很懂     為什麼一個 group 裡需要有多個 consumer   為什麼需要有 group   分散式系統需要考慮的除了效能之外還有錯誤處理以及恢復  擁有多個 consumer 可以讓你在某一個 consumer 出問題的時候，可以有其他的 consumer 來接手  group 的設計則是為了方便管理多個 consumer      上述程式碼可以在 ambersun1234/blog-labs/message-queue 中找到    RabbitMQ  提到 message queue  不免俗的還是要要介紹一下 RabbitMQ   Architecture  RabbitMQ 是一套 open source 的 message broker  其實作了 AMQP, 提供了高可用性、且易於擴展的分散式 broker 架構   Data Store  與 Kafka 類似，他們都有 disk store  但 RabbitMQ 還有支援 in-memory store  速度，吞吐量上兩種方式沒有明顯的差異   因為 RabbitMQ 是一個 queue 的結構，所以其保證了資料的有序性  先進去的資料一定會先出來   但如果今天你的資料具有優先級  要怎麼區分不同的資料優先級呢？     開不同的 queue 負責處理不同優先級的資料，類似稍早提過的 DLQ   使用 priority queue(RabbitMQ 有支援)      Kafka 做不到資料優先級的區分    How to Consume Message  RabbitMQ 是採用 Pull/Push Protocol 中的 push protocol  亦即資料是由 server 主動推送至 client 的  而這些資料會需要進行 acknowledgement 的操作，所以 producer 是知道 consumer 拿資料了沒   然後 1 個 topic 通常只會有 1 個 consumer  你可以有多個 consumer, 這個情況用於資料產生的速度來不及消化，所以你選擇多個 consumer 來消化資料  要注意的是 同一份資料只會被消化一次，所以他不會重複讀取   RabbitMQ 會使用 ACK 來確保資料被正確的消化(可參考 重新認識網路 - 從基礎開始 | Shawn Hsu)  在下面的例子可以看到我們在 consume 的時候就自動使用 ACK 通知 producer 資料已經被消化  自動 ACK 很方便，但是當 consumer 直接 crash 的時候，資料就會丟失了  因為你已經自動確認消化了，producer 就會把資料刪掉      如果你 disable auto ack 要記得手動 ACK  不然東西會卡住    但是 consumer panic 可能並非你的本意，所以你可以選擇手動 ACK 避免這個問題  consumer 的 for loop 裡面，你可以選擇執行完再進行 ACK  這樣既可以確保資料被正確的消化，又可以避免 application 直接 crash 資料丟失的問題      注意到他跟我們手動 retry 的概念不太一樣  這裡手動 ACK 是怕 consumer 直接 panic(nil pointer dereference 之類的), 資料丟失的問題  retry 是因為處理失敗，所以要重新 re-enqueue(Re-enqueue Message)    Auto Reconnect  網路超級不可靠，它會一直斷斷續續的  我自身的例子來說，本地 docker 開發連線都非常的穩定  一旦上到 server 就會開始時常斷線  擁有自動重新連線的功能是非常重要的   RabbitMQ 你可以透過 NotifyClose 監聽 connection close 的事件(channel 或 connection)  寫起來大概長這樣   注意到，不能寫 case &lt;- r.conn.NotifyClose(make(chan *amqp.Error))  他有可能會接不到 notify close 的訊號  然後他也不會死掉，就是會整個無回應      建議使用 buffered channel 避免 deadlock    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 msgs, err := r.channel.Consume(key, \"\", false, false, false, false, nil) if err != nil {     panic(err) }  connectionChan := r.conn.NotifyClose(make(chan *amqp.Error, 1)) channelChan := r.channel.NotifyClose(make(chan *amqp.Error, 1))  for {     select {     case msg := &lt;-msgs:         // consume message          case &lt;-connectionChan:         fmt.Println(\"RabbitMQ connection closed, reconnecting...\")         // do reconnect      case &lt;-channelChan:         fmt.Println(\"RabbitMQ channel closed, reconnecting...\")         // do reconnect     } }   Default RabbitMQ Queue  像有一些資料庫有一個功能是它可以有預設的 table  RabbitMQ 也有一樣的東西   需要先開啟載入 definition 的設定開關  1 2 # rabbitmq.conf management.load_definitions = /etc/rabbitmq/load_definitions`.json   這裡 queue 就是你預設要建立的 queue 相關的設定  user 會需要是因為要登入才有權限可以操作  注意到 permission 還是需要寫，即使你的 application 也是使用同一組帳號   import definitions 的時候，vhost 必須要設定  不然你會遇到 exit:{error,&lt;&lt;\"Please create virtual host \\\"/\\\" prior to importing definitions.\"&gt;&gt;}      完整範例可以參考 ambersun1234/blog-labs/message-queue    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 // load_definitions.json {     \"users\": [         {             \"name\": \"rabbitmq\",             \"password\": \"rabbitmq\",             \"tags\": [\"administrator\"]         }     ],     \"queues\":[         {             \"name\": \"my_queue\",             \"vhost\":\"/\",             \"durable\":true,             \"auto_delete\":false,             \"arguments\":{}         }     ],     \"vhosts\": [         {             \"name\": \"/\"         }     ],     \"permissions\":[         {             \"user\":\"rabbitmq\",             \"vhost\":\"/\",             \"configure\":\".*\",             \"read\":\".*\",             \"write\":\".*\"}     ] }   Delay Delivery  除了支援優先級的機制，RabbitMQ 還有支援 delay delivery 的機制  也就是將資料暫存在 queue 中，等到時間到了才會被消費   可參考 資料庫 - Delayed Queue 的設計與考量 | Shawn Hsu   Example  Installation  一樣使用 docker 將服務跑起來  1 2 3 4 5 6 $ docker run -d \\     -p 5672:5672 \\     -p 15672:15672 \\     -e RABBITMQ_DEFAULT_USER=rabbitmq \\     -e RABBITMQ_DEFAULT_PASS=rabbitmq \\     rabbitmq:3.13-rc-management      rabbitmq image 是沒有帶管理介面的，記得要用有 management 的 image    container 需要使用兩個 port 5672 與 15672  其中 5672 是給 application 使用的，而 15672 則是 GUI 管理界面  使用帳號密碼登入後你應該會看到類似以下的東西        ref: Part 3: The RabbitMQ Management Interface    Hello world  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 package main  import (     \"context\"     \"fmt\"     \"log\"     \"time\"      amqp \"github.com/rabbitmq/amqp091-go\" )  func publishToMessageQueue(ch *amqp.Channel) {     ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)     defer cancel()      counter := 1     for {         body := fmt.Sprintf(\"Hello World %v!\", counter)         err := ch.PublishWithContext(ctx, \"\", \"test\", false, false, amqp.Publishing{ContentType: \"text/plain\", Body: []byte(body)})         if err != nil {             log.Panic(\"Failed to publish message\")         }         time.Sleep(1 * time.Second)         counter += 1     } }  func consumeFromMessageQueue(ch *amqp.Channel) {     queue, err := ch.Consume(\"test\", \"\", false, false, false, false, nil)     if err != nil {         log.Panic(\"Failed to consume from queue\")     }      for msg := range queue {         log.Printf(\"Received message: %s\", msg.Body)         msg.Ack(false)     } }  func main() {     conn, err := amqp.Dial(\"amqp://rabbitmq:rabbitmq@localhost:5672/\")     if err != nil {         log.Panic(\"Failed to connect to RabbitMQ\")     }     defer conn.Close()      ch, err := conn.Channel()     if err != nil {         log.Panic(\"Failed to open a channel\")     }     defer ch.Close()      _, err = ch.QueueDeclare(\"test\", false, false, false, false, nil)     if err != nil {         log.Panic(\"Failed to declare queue\")     }      go publishToMessageQueue(ch)     go consumeFromMessageQueue(ch)      select {} }   雖然說 message queue 主要是拿來用作跨服務的溝通  把它寫在同一隻檔案顯然是不正確的，不過這裡主要是展示如何使用 RabbitMQ 而已   code 主要的流程是  建立與 RabbitMQ 的連線，建立 channel 以及 queue  我很好奇一件事情，在先前的 AMQP 裡面我們並沒有提到 channel 這個東西  那他是要用來做什麼的？      Next we create a channel, which is where most of the API for getting things done resides    如果你往下看就可以發現，publish 與 consume message 都是透過 channel 所建立的  所以 channel 實際上可以算是 API 之間溝通的橋樑     1 ch.QueueDeclare(\"test\", false, false, false, false, nil)  message queue 需要手動建立，參數依序為 name, durable, delete when unused, exclusive, no wait, arguments  我們將 message queue 的名字命為 test, 其餘的都是 false      queue 的建立僅會在不存在的時候建立(i.e. idempotent)    1 2 3 4 ch.PublishWithContext(     ctx, \"\", \"test\", false, false,      amqp.Publishing{ContentType: \"text/plain\", Body: []byte(body)} )  publish data 到 queue 的方法是使用 PublishWithContext, 參數為 context, exchange name, routing key, mandatory, immediate, data  context 就是 golang 的 context 套件  比較值得注意的是 exchange 以及 routing key  AMQP - Exchange 中提到，要將訊息送往何處，是由 routing key 所決定的, 所以我們的 routing key 就是 test  但是 exchange 欄位為什麼是 empty string?   很明顯的 根據 AMQP - Exchange 以及 AMQP - Binding 所述  這裡使用的 exchange type 是 Direct Exchange 所以 exchange 的值可以為空   1 ch.Consume(\"test\", \"\", true, false, false, false, nil)  consume data 的參數為，queue, consumer, auto-ack, exclusive, no-local, no await, args  然後你可以用一個 for-loop 去取資料這樣   no await 表示不會等待 server 確認 request 並且立即開始傳送訊息   consumer 的欄位是 consumer tag，用以辨別 consumer 的 identity   auto-ack 是 acknowledge 的意思  RabbitMQ 有提供 message acknowledgement，亦即你可以確保 consumer 有正確接收到資料  這個 acknowledgement 是由 consumer 送回 server 的  當 message 沒有被正確 receive，RabbitMQ 會自動將訊息重新 enqueue 確保資料不會消失      如果 Consume 有設置 auto-ack, 你手動呼叫 msg.Ack() 會錯哦    完整原始碼可以參考 ambersun1234/blog-labs/message-queue   Differences between Kafka and RabbitMQ                          Apache Kafka       RabbitMQ                       Pattern       Publisher-Subscriber Pattern       Producer-Consumer Pattern                 Main Usage       event streaming       message proxy                 Check on Receive       :x:       :heavy_check_mark:                 Performance       Million messages per second       Thousands messages per second                 Authentication       :heavy_check_mark:       :heavy_check_mark:                 Fault Tolerance       :heavy_check_mark:       :heavy_check_mark:                 Data Persistence       :heavy_check_mark:(with delay)       :x:(delete on acknowledgement)                 Message Fetching       pull based       push based           References     Kafka 和 RabbitMQ 有何區別？   高級消息隊列協議   Advanced Message Queuing Protocol   什麼是 MQTT？   Getting Started with Java Message Service (JMS)   What is Apache Kafka?   Introduction   三種Exchange模式   Consumer Tags   9张图，Kafka为什么要放弃Zookeeper   4.7 Replication   5.5 Distribution   KRaft: Apache Kafka Without ZooKeeper   ZooKeeper   什麼是無效字母佇列 (DLQ)？   9张图，Kafka为什么要放弃Zookeeper   [kafka]kafka中的zookeeper是做什么的？   KIP-500 Early Access Release   KIP-500 Early Access Release   Offset management configuration   How to create a queue in RabbitMQ upon startup   Nuances of Boot-time Definition Import  ","categories": ["database"],
        "tags": ["distributed","cluster","message queue","publisher","consumer","producer","subscriber","event","kafka","amqp","mqtt","jms","rabbitmq","dlq","pubsub","p2p","pull protocol","push protocol"],
        "url": "/database/database-message-queue/",
        "teaser": null
      },{
        "title": "淺談 Polling, Long Polling 以及其他即時通訊方法論",
        "excerpt":"Polling  polling 輪詢是最為簡單的一種作法  其核心概念為定時的發出 request 確認      又名 short polling         ref: HTTP Short vs Long Polling vs WebSockets vs SSE    舉例來說每隔 1 分鐘，client 送 request 到 server 詢問目前的狀態  polling 的實作非常的簡單，從上圖你應該可以想像的出來他是怎麼工作的      其實你現在在使用的電腦鍵盤，也是使用 polling 的機制的  早期 PS/2 的鍵盤則是使用 interrupt 的方式(中斷) 觸發訊號給 OS  可以參考 What Is Keyboard Polling Rate and How Much Does It Matter?    WebSocket     to be continued    Long Polling       ref: HTTP Short vs Long Polling vs WebSockets vs SSE    Long Polling 跟 Polling 很像  但不太一樣  同樣都是每隔一段時間發 request 到 server  但是差別在於 server 這邊可以選擇要不要馬上回復  而判斷的標準是 狀態有沒有改變  舉一個例子可能比較好理解   以我們在使用的社群媒體如 Facebook, Line 等等的  通知這個東西，是可以使用 Long Polling 實作的  是不是每當有人按讚你的貼文，回覆你的留言的時候  你就會收到一個通知   假設 Long Polling 的 timeout 設定成 1 分鐘  那麼用人類的話來說，Long Polling 可以簡化成這句話  請告訴我，在接下來的 1 分鐘內，有沒有新的通知  因此，會有兩個狀況     如果 1 分鐘內有新的通知，就馬上告訴 client   如果超過 1 分鐘，沒有任何新的通知，則告訴 client 沒有新的通知   發現了嗎？  server 會在給定的時間內 hold 住 connection  直到新的狀態出現  寫成 code 大概會長這樣  1 2 3 4 5 6 7 8 9 10 11 // client notifications = get('/notifications/me')  // server while (timeout !== 0) {     // wait for 1 second     haveNewNotifications = db.getUserNotificationsCount()     if (haveNewNotifications) {         return db.getUserNotifications()     } }      Long Polling 的實作不需要管 client 多久 call 一次  timeout 是設定 server 要 hold 住 connection 多長的時間    Long Polling vs Polling  考慮以下例子   假設我用 Polling, client 每隔 5 秒詢問一次  跟使用 Long Polling, client 每隔 1 秒問一次，然後 server timeout 為 5 秒  這兩種方法是不是結果都一樣？   這樣是不是看起來改用 Long Polling 並沒有任何好處  當然不是   這一切取決於 client 要如何呼叫 server  如果 client 仍然每隔 1 秒就問一次 server, 那麼確實使用 Long Polling 並不會帶來任何好處  因為你呼叫的次數還是那麼多，它並不會減少     Polling 我們知道，它會 一直 詢問 server  而這個一直的過程，會導致不必要的 overhead  如果一直沒資料，每次問不會比較快   TCP handshake 的 overhead 可能不需要考慮，因為 HTTP 支援 persistent connection  但是也是有可能對於網路造成壅塞      可參考  重新認識網路 - HTTP1 與他的小夥伴們 | Shawn Hsu  重新認識網路 - 從基礎開始 | Shawn Hsu       使用 Long Polling 一樣會有握手的情況發生  但這個次數相對 Polling 來說是減少的      雖然以結果來說  兩者方法能達到的結果都是相同的  只不過如果你把 client Polling 的時間調得很長  那就不即時了吧?   timeout 設定成一分鐘的情況下  有可能在這一分鐘內就有新的資料，不過因為 client 還在 timeout 的時間內，client 端對新資料是一無所知的  只有當下一次 request 的時候才會包含 “幾秒鐘前的新資料”      Long Polling 跟 Websocket 這種真正即時的比較還是有差  不過還是比 Polling 更好    Websocket Overhead  WebSocket 一旦連線建立，就會一直維持住  在這一方面可謂完美的解決了 Polling 以及 Long Polling 建立連線的巨大成本帶來的 overhead  看似完美的解決方案，但是它會遇到另一個問題，file descriptor 數量限制      有關 socket 的介紹可參考 重新認識網路 - 從基礎開始 | Shawn Hsu    我的電腦，預設 file descriptor 上限數量為 1024  也就是說一個 process 的 file descriptor 上限就那麼多  回到我們原本的主題，WebSocket 會維持住 internet connection 對吧  是不是代表，你的伺服器在一定時間下，預設 WebSocket 連線數量就只能這麼多  想當然你沒有辦法一直無限開大機器對吧  也因此我們需要一個折衷的辦法，Long Polling 就誕生了   References     Why are TCP/IP sockets considered “open files”?   Is socket creation-deletion very expensive process?   HTTP Short vs Long Polling vs WebSockets vs SSE   Socket and file descriptors  ","categories": ["random"],
        "tags": ["polling","long polling","webhook","webrtc","websocket","tcp","file descriptor","socket"],
        "url": "/random/real-time-communication/",
        "teaser": null
      },{
        "title": "資料庫 - 新手做 Data Migration 資料遷移",
        "excerpt":"Preface  資料搬遷，在現代軟體服務當中屬於較為常見的一種需求  不論是單純的機器之間的搬資料抑或者是因應商業邏輯而需要做的資料搬遷等等  都是屬於 Data Migration   Introduction to Data Migration  雖然統稱 Data Migration 但實際上可以下分以下幾種      Schema Migration   Data Migration   Database Migration      有的時候你只需要進行一種，有時候是多種的組合    Schema Migration       ref: Hassle-Free Database Migrations with Prisma Migrate    有的時候，你可能會需要針對資料庫的某個欄位做些微的更動  比如說，增加 unique constraint 或者是設置 default value  這些，其實就是資料搬遷的一種   以 Prisma 來說  每一次的搬遷，它都會新增一筆新的 entry  針對該欄位的更新 sql 就會寫在裡面      各個語言其實都已經有不同的 Migration 工具  如 Node.js 裡的 Prisma, Python 裡的 Alembic 以及 Golang 裡的 golang-migrate    Data Migration  不過 Schema Migration 仍然是較為簡單的狀況  真實世界可複雜的多  商業邏輯的改變，資料搬遷的功會比想像中的多   比如說  我們想要仿造 Youtube 的開啟小鈴鐺的功能，使用者可以自由切換要不要開啟通知  因為我們已經有使用者正在使用我們的服務了  所以針對 舊有的使用者，我們必須讓它也可以使用這個功能  所以我們需要針對這些舊有用戶，幫他們新增預設的通知設定      新的使用者，因為初始化的時候已經做了，所以不需要包含在這次的搬遷內容裡面    Database Migration  又或者是資料庫本身的遷移，比方說從 MySQL 遷移到 PostgreSQL  兩邊 metadata 的差異，也是必須要進行處理的  這種要考慮的點與 Data Migration 不太一樣，不過大方向依然是類似的   Preparation  既然你已經知道你要針對哪一個部份做資料搬遷了  你需要做哪一些準備工作呢？   Backup  因為這種商業邏輯的資料搬遷往往伴隨著一定程度的危險  所以做好備份的工作是必要的   最壞的狀況就是，當資料搬遷出了大問題  你已經沒辦法挽回的時候，至少還有一個拯救的辦法   不過要注意的是，當系統升級完成但搬遷卻失敗  使用 backup 復原並不是一個好的辦法  因為你需要考慮到回復會不會造成系統相容性的問題等等的  有沒有 向後相容？ 它會不會造成現有服務運作異常  這個問題值得思考   Verify Business Requirement  除了技術方面，你還得要確認商業邏輯的部份  他是不是符合公司的要求   如果條件允許，也必須提及此次系統更新可能的影響  包含它是否商業上可行？ 會不會與未來的規劃有衝突等等的   How to do Data Migration?  仔細想想其實也就兩種      手動升級   自動化升級   其中手動升級是較為不推薦的作法  如果沒有適當的文件，它可能會難以維護  甚至你可能會忘記為什麼這個欄位會是這個數值   自動化升級至少你還有 code 可以查看  而自動化的部份，你可以單純寫 SQL 或者是使用類似 Prisma 這種工具幫你解決  如果遇到複雜的商業邏輯的部份，則可能要寫個小程式執行   Reversibility  在資料搬遷的過程中，你必須要考慮到它是否可以被還原  也就是說，如果搬遷失敗，你必須要能夠將資料還原到搬遷前的狀態   比如說，你新增了一個欄位(新增一個文章分類的欄位)，你也應該要考慮到他是否能夠在刪除的情況下正常運作  所以理論上你需要有兩個 script 來處理這件事情      up script            新增一個欄位以及必要的舊資料升級           down script            刪除新增的欄位           你可能會好奇為什麼要有 down script  萬一需要 rollback，整個 database 的狀態理應是 乾淨的  以這個例子來說，新增的欄位必須要被剔除   像是 alembic 裡面 migration 的實作  你可以看到說也是有兩個 script 來處理這件事情   1 2 3 4 5 6 7 8 9 def upgrade() -&gt; None:     \"\"\"Upgrade schema.\"\"\"     # ### commands auto generated by Alembic - please adjust! ###     # ### end Alembic commands ###  def downgrade() -&gt; None:     \"\"\"Downgrade schema.\"\"\"     # ### commands auto generated by Alembic - please adjust! ###     # ### end Alembic commands ###   Possible Issues  Backward Compatibility  有的時候資料升級，你會遇到無法向後相容的部分  也就是說新的資料格式沒辦法正確的套用到舊有的資料上  倒也不是你資料錯誤導致，而是 資料缺失 造成的   導致說更新完的資料會沒辦法與新版的系統正確的匹配運作  舉例來說，我目前碰到的狀況是我想要 “發文分類” 這個功能  但是早期建立的文章並沒有任何欄位可以區分(比方說 個人空間 還是 公開空間)  這樣的狀況你無從知道這些資料是屬於哪一個分類   無可避免的，這種時候各種做法都會有它的缺點     全部搬遷到 個人空間/公開空間   保留 NULL 值，更改處理資料邏輯   當然我們能做的，就是盡量將損失降到最低   Data Loss  執行資料搬遷，我們絕對不希望它更改到其他不相干的部份  但它仍然是可能會發生的，所以測試是必要的   針對你搬遷的部份，建立幾筆資料觀察它執行的結果  在上到 production 之前，可以在 dev 以及 staging 環境測試  我個人會推薦，在這些之前，也可以在本機進行測試   Idempotent  最後也是最重要的一點，你的自動化搬遷的執行檔案  它必須要滿足 Idempotent 的條件   何謂 Idempotent？ 就是你不管執行幾次，它得到的結果都要是一致的  比如說上面我們提到想要實作使用者的通知設定功能  你絕對不會希望一個使用者有多個相同設定   因此，在設計 migration script 的時候，他要執行的是 upsert  若是寫入的資料不存在，寫入，若存在，則略過或更新部份值  以 PostgreSQL 來說  你可以使用  1 2 INSERT INTO (xxx) VALUES(yyy) ON CONFLICT(zzz)  DO UPDATE SET id = EXCLUDED.id  當你寫入的資料，有比對到一模一樣的資料的時候，它就會選擇使用原本的 id  而這個比對的基礎，是寫在 ON CONFLICT 裡面   注意到，一模一樣的資料的定義是，它必須擁有 unique constraint 進行保護  有時候你要 upsert 的資料根本沒有 unique constraint  這時候其實你別無選擇，你只能先 query 有沒有該筆資料的存在，然後在寫入  當然這時候，使用 transaction 是相對比較好的選擇      有關 transaction 的討論，可以參考 資料庫 - Transaction 與 Isolation | Shawn Hsu    Long Migration Time  當搬遷的資料數量過於龐大  花超過額外預期的時間是有可能會發生的   資料庫系統的更新，因為會佔用一定的連線數量，以及一定的 I/O  系統的反應速度可能會變慢   Off-peak Time  你可以選擇在半夜這種不會有太多使用者在線上的時候，執行系統升級   Migration Checkpoint  或者是 migration 的檔案數量過多，導致執行時間需要拉長  如果遭遇頻繁的資料庫遷移，這種事情是可能發生的  一個解決方法是使用 checkpoint 的機制  我們知道，migration 是基於目前 “資料庫的狀態” 往上疊加的  所以 checkpoint 的概念是，我重設資料庫的狀態，然後以往的 migration 檔案因為狀態改變就不需要執行  這樣就可以減少 migration 的執行時間   為了系統的可用性，我們通常會希望系統的 down time 越低越好  盡可能的提高使用者體驗      Change Data Capture(CDC)  除了進一步優化 SQL 的部分，你也可以使用所謂 CDC 的機制  簡單來說，如果資料過於龐大導致遷移時間過長，系統就沒辦法正確動作   如果可以一邊搬遷一邊提供服務，不就解決這件事情了嗎？  對於有實作向後相容的資料(i.e. Backward Compatibility)來說，這個想法是可行的  如果不相容，根本跑不起來   所以實務上，會有兩台機器在運行     一台是目前正在線上服務的資料庫，稱為 source   另外一台是 “新版” 的資料庫，它會在後面慢慢的將舊版資料升級成新的資料格式，稱為 destination   當 source 資料庫有新的寫入，它會將這筆更新推播到 destination 資料庫  讓兩台資料庫的內容盡量保持一致(為什麼說盡量，因為推播沒那麼快)  這個技巧稱為 Change Data Capture   當兩台機器達到一定的同步程度，就可以把它切過去  這個 cutoff 會有極短暫的 downtime，但是相比其他方法來說，這個影響是最小的      也就是 Blue Green Deployment，可參考 Kubernetes 從零開始 - 部署策略 101 | Shawn Hsu      在 資料庫 - 初探分散式資料庫 | Shawn Hsu 裡面，我們有提到，分散式資料庫的架構下，資料的同步是相當困難的  其中你可以使用 Statement Replication 的機制來達成資料的同步  但是由於自身的機制，它無法處理 non deterministic function 如 NOW() 或是 RAND() 等等的  因此比較推薦使用 Logical Log 的機制來達成資料的同步  而 Logical Log 本身，就可以視為是 Change Data Capture 的資料來源   那這些 Event 資料屆時可以透過 Apache Kafka 以及 Kafka Connect 來同步資料      有關 Kafka 的介紹，可以參考 資料庫 - 從 Apache Kafka 認識 Message Queue | Shawn Hsu    On-premise vs. SAAS Migration  有些產品是落地的，資料並不在我們的控制之下  在這種情況下，資料升級無疑是相當困難的   SAAS 的產品，我們可以直接存取到資料庫本身  而我們很清楚服務內存在著什麼樣的資料  升級失敗復原相對容易且容易掌控  因為執行資料升級的會是開發服務本身的廠商   到了 On-premise 這裡，事情會完全不一樣  客戶並不一定擁有足夠的知識能夠處理，甚至可以說是沒有這樣的知識  支援是相對薄弱的，這時候如果升級失敗將會是一場災難  若是遇到 Backward Compatibility 的問題，無疑是雪上加霜   Data Migration In Kubernetes  如果你的資料庫是在 Kubernetes 內部運行，你可能會遇到跟我一樣的問題  我們的資料庫(i.e. PostgreSQL) 是透過 Helm subchart 來管理的  每一次 $ helm install 資料庫與應用程式會一併的進行安裝      有關 Helm Chart 可以參考 Kubernetes 從零開始 - Deployment 管理救星 Helm Chart | Shawn Hsu    這就導致說，資料升級的這個過程無從下手  什麼意思呢？ 就如同 Database migrations in Helm charts using pre-install, pre-upgrade hook 提到的  要執行 migration 需要資料庫先安裝，而且需要在應用程式啟動之前  這本質上是不可能的  因為 Helm Hook 本身並沒有那麼細緻的控制   pre-install 是完全不可行的，因為只有在 hook 完成之後 app 才會開始安裝  而 post-install 有可能 app 已經啟動完畢，但是 data migration 還沒完成，然後造成資料損毀      有關 Helm Hook 可以參考 Kubernetes 從零開始 - Deployment 管理救星 Helm Chart | Shawn Hsu    再來是，Helm Hook 本身的設計也不是要拿來讓你跑長時間的任務  即使你說，沒有啊目前就大概 3 分鐘會跑完也是不適合的，就… 他不是設計給你這樣用的  根據 Question: Long Running Hooks - Yes or No? 你也可以看到      Most of the time, a long running hook (for hours or days) isn’t a good idea.    其實官方是不推薦的，更甚至因為透過 Helm Hook 建立的 K8s Resource 本身是脫離 release 的  也就是說他的生命週期並不是跟著 helm upgrade, helm delete 一起的，你需要手動管理  在 on-premise 的環境這種做法是更糟糕的(可參考 On-premise vs. SAAS Migration)   Kubectl Wait  基本上你會有幾種選擇     利用 Kubernetes Job 獨立執行 migration(透過 Helm Hook 控制順序)   在 Deployment 中使用 initContainer 執行 migration   在應用程式內部進行 migration(啟動 web server 之前執行)   透過 CI/CD pipeline 執行 migration      有關 Job/Deployment 可以參考 Kubernetes 從零開始 - Pod 高級抽象 Workload Resources  有關 CI/CD pipeline 可以參考 DevOps - 從 GitHub Actions 初探 CI/CD    2 與 3 是一樣的，看似可以用但實際上你需要注意到 race condition 的問題  因為如果你需要 scaling, 多個 replica 會執行相同 migration script，沒處理好會導致資料不一致的問題  也同時必須考慮到升級時間過長的問題，可能會被 K8s kill 掉等問題   4 的假設是你的服務是 SAAS 的，落地的情況下根本無法實現   1 的情境其實是默認資料庫沒有跟 app 一起安裝，這種狀況用 Helm Hook 才有辦法   但以我的例子來說，他們是一起安裝的，所以以上都無法使用  好在，我們還有一招 kubectl wait  這個指令可以用於等待資源的狀態達成某種條件  也就是說，其實是可以使用 Kubernetes Job 負責執行 migration  然後 app 的 deployment 使用 kubectl wait 等待 migration Job 完成後再啟動   一開始的時候以下同時執行     app deployment 安裝   migration job 執行   database 安裝   然後流程會變成這樣     database 先完成安裝(同時 migration job 與 app 都被 initContainer 的資料庫檢查 block 住)   migration job 完成執行(app 被執行 kubectl wait 的 initContainer block 住，等待 migration job 完成)   app 完成安裝(這個時候 migration 已經成功)   K8s Job Lifecycle and ArgoCD Synchronization  雖然使用 Kubectl Wait 可以解決資料庫與應用程式同時安裝，無法進行資料升級的問題  但是這種作法會需要非常小心   如果你的 Job 有設定 ttlSecondsAfterFinished，Job 本身會在這個時間之後被刪除(不是底下的 Pod 而是 Job 本身)  這樣 Kubectl Wait 的判斷就會失效，導致你的主程式起不來(注意到並不是 deploy 才會用到判斷，如果你的 deployment 意外重啟而 Job 已經被刪除，同樣也適用)  搭配上 ArgoCD 這種作法只會更糟，因為 K8s 不允許同名的 Job，所以會無法同步  你唯一想的到的就是加上 ttl 讓 Job 自動被刪除，Argo 才能再次同步  不過問題又回到前面講的   K8s Job 預設是不會清除的  1 2 3 4 5 6 7 8 NAME                   READY   STATUS      RESTARTS   AGE pod/sleep-test-vvk98   0/1     Completed   0          4h55m  NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE service/kubernetes   ClusterIP   10.43.0.1    &lt;none       443/TCP   4h55m  NAME                   STATUS     COMPLETIONS   DURATION   AGE job.batch/sleep-test   Complete   1/1           57s        4h55m   所以怎麼辦呢？  你可以在 Kubectl Wait 中指定使用 label 判斷  至於 Job 的名稱可以用 generate name 來生成  這樣就完美繞過以上的所有問題   1 2 3 4 5 $ kubectl wait --timeout=60s --for=condition=complete --timeout=60s \\     $(\\         kubectl get jobs -l component=sync \\         --sort-by=.metadata.creationTimestamp -o name | tail -n1 \\     )      挑選 label 為 component=sync 的 Job，並且按照 creation timestamp 排序，選擇最後一個    唯一的 drawback 是你會有很大量的 Job instance 存在  透過簡單的 cronjob 去清除是不錯的選擇   這樣你就能夠解決資料庫與應用程式同時安裝，無法進行資料升級的問題了   How Netflix Perform Database Migration  Why Database Migration  Netflix 的 platform team 在 2024 的時候決議要從 Amazon RDS PostgreSQL 遷移到 Aurora PostgreSQL(i.e. Database Migration)  考慮的主要原因有以下     PostgreSQL 作為 Netflix 廣泛使用的資料庫，有超過 95% 的 workload 依賴於 PostgreSQL，遷移過去成本較低   因為 PostgreSQL 已經逐漸被廣泛使用，社群支援完整   Aurora 提供了高可用性以及分散式的特性，可以滿足 Netflix 的需求   Aurora 目前提供了豐富的功能，以及其未來的規劃滿足 large-scale 的需求，也是 Netflix 所需要的   High Level Migration Plan  既然決定要換，那具體怎麼做？  從 RDS 到 Aurora，主要有兩種作法     停止 RDS :arrow_right: 對 RDS 進行快照 :arrow_right: 從快照建立 Aurora 叢集並恢復   建立 RDS read replica 到 Aurora 的 CDC :arrow_right: 等到同步差距非常微小之後，停止主要 RDS master 暫停寫入 :arrow_right: 等到完全追上，將其一 Aurora slave 升級為 master 並恢復      注意到第二點的同步差距，它需要考慮的是一段時間內的差距趨於平穩，而不是單純某個時間點的微小差距而已    兩者最主要的差距是在於 停機時間  RDS 需要完全暫停，將全部資料複製並在 Aurora 上恢復，在此期間無法提供服務  反之，使用 CDC 機制可以做到 “幾乎” 無停機遷移  當然，CDC 的機制複雜程度會被拉高，需要額外考慮其他問題        ref: Automating RDS Postgres to Aurora Postgres Migration    最後他們選擇 CDC 的機制，以最大程度減小 downtime   Database Migration  當然，為了避免各種意外，首先一定是先做完整的備份(如同我們在 Preparation 提到的)  再來就是要先準備好連線參數這些，要注意的是，資料這層 Netflix 是透過 Data Access Layer 隔離的，所以對於 application 來說，它不需要更動任何東西  只是 DAL 內部需要更新這樣，兩個資料庫的參數並不完全長一樣，所以這是需要事前調整的理由        ref: Automating RDS Postgres to Aurora Postgres Migration    搬遷的過程就如同 Change Data Capture(CDC) 說明的一樣  具體來說是透過 WAL(Write Ahead Logs) 來同將資料同步，過程中 RDS 依然是主要的資料庫   最重要的是切換對吧  application level 會被暫停，但這樣真的足夠嗎？  對於 Netflix 來說，它必須要做到除了 platform team 以外沒有任何人能夠存取 db  因此，他們在 infra level 也進行了阻擋(避免，比如說，錯誤的設定導致連線還開著，沒有通知到維護時間或者是連線還沒斷開等等的情況)  確保說，沒有任何連線到資料庫，避免任何資料意外的沒有被同步到更甚至損毀的程度   關閉之後不能馬上切換，因為同步的過程會受到非常多因素干擾，最常見的是由於網路問題導致同步稍微變慢沒有跟上  所以會需要等待一段時間，到所有更新都同步，才能切換  怎麼判斷其實也很簡單，當 “待處理的 WAL 大小為 0 的時候” 就代表已經完成同步了  有趣的是，他們透過 metric 觀察到，三不五時還是會有 64MB 的資料沒有被同步到，如下圖        ref: Automating RDS Postgres to Aurora Postgres Migration    而原因在於，PostgreSQL 的 WAL 預設每五分鐘，就會執行 WAL switch  idle 期間沒有任何新的資料(因為你切斷所有連線了)，所以下一個五分鐘，執行 WAL switch 的時候，會寫入一個空的 WAL segment  而這個大小，恰好是 64MB，所以在 metric 上面你就會看到這樣的結果，但它本質上就是已經完成同步了  到這時候，我們就可以很確信同步已經完成，可以執行切換了      基本上你需要確認說，這個 0 :arrow_right: 64MB 的 模式 存在，才可以更有信心說同步完成       WAL switch 的間隔 5 分鐘，如果對於那些不太能忍受太長 downtime 的系統來說，可能會是個問題  可以調整他的 idle 時間，及早確認 pattern 並提早結束停機    對於 application team 來說，這個切換跟他們無關，因為所有的資料庫存取都是透過 DAL 這個 reverse proxy 來進行  連線資料那些也基本都是在 DAL，所以不論是程式碼還是資料庫參數，都不需要更動  data gateway 會自動將他們導向新的 Aurora 資料庫   Unexpected Migration Failure  即使有縝密的規劃與安排，他們還是在搬遷的過程中遇到了點小插曲   搬遷過程中，觀察 metric 發現到，”待處理的 WAL 大小” 一直偏高  這意味著，搬遷需要花費更長的時間才能完成  而造成這個的元兇是一個 inactive Logical Replication Slot   Logical Replication Slot  Write Ahead Log(WAL) 裡面紀錄的是每個 “事件”  他是一個事件記錄簿，除了當資料庫出意外的時候可以使用它恢復資料，也可以作為 Change Data Capture(CDC) 的資料來源   每個 CDC 的 consumer 都需要紀錄說，自己已經讀取到 WAL 的哪個位置  這個位置的紀錄被稱作 logical replication slot  WAL 的資料會同時被很多人讀取，所以 slot 也會有很多個   WAL 的檔案大小不可能無限大，它會需要定期清理  它哪時候該被清理？ 就是全部人都已經讀取過得時候 + checkpoint 已經寫入的時候(為方便討論，這邊僅考慮 slot 都已經讀取過)  當全部的 CDC consumer 都已經被讀取過之後，就能夠視為安全，可以被清理了   The Crisis  而恰好就是因為 Logical Replication Slot 的關係，導致了這次的搬遷卡住  出事的是一個 inactive 的 slot，由於清理機制是需要 所有 logical replication slot 都已經被讀取過 才能清理  導致說 metric 怎麼計算就是還有很多 WAL 沒有被同步，加上其實 RDS 在當下還是繼續接收資料，待處理資料量直線上升   要解決也很簡單，手動移除該 slot，之後整個搬遷過程就很順利了  也因為採取的是 CDC 的機制，所以其實對客戶來說他們沒感覺到異常   How to Migrate CDC Consumer and Replication Slot  我們知道 Change Data Capture(CDC) 是將資料庫的事件推播出去的方法  但是現在換了一個資料庫，對於這個推播會有影響嗎？   意思是說，Logical Replication Slot 是用來標記說我看到 WAL 的哪個位置，當換了一個資料庫，這個標記還有效嗎？ 還能夠銜接上去嗎？  理論上來說是不行的，所以其實 Netflix 他有一個專門的 CDC 服務稱為 datamesh      因為 consumer 是從 datamesh 接收資料的，所以對它來說 Slot 的問題不是他要考慮的    一開始，會先暫停 CDC consumer 並且移除該 Logical Replication Slot  此時 CDC consumer 一樣是穩定的不會斷線  當新的資料庫上線，因為舊的 slot 已經被移除，而且也不能繼續用因為是新的資料庫，所以會重新建立新的 Logical Replication Slot  並且為了銜接順利，會將全部資料包裝成事件，重新推播出去(稱為 refresh event)      為什麼要移除 Logical Replication Slot?  舉例來說，Unexpected Migration Failure 提到的情況，如果沒有移除 slot，清理機制就不會執行，導致說 metric 怎麼計算就是還有很多 WAL 沒有被同步    不要誤會，重新推播的目的 不是因為資料庫的資料不一致，而是要讓 事件重新推播 給下游  full backfill 可以讓下游與上游擁有 一致且乾淨的 baseline，因為推播是基於已經存在的資料做出改變，如果 baseline 不一致，後續的推播就會有問題  並且下游的 consumer 收到這些重複的事件應該要能夠處理(i.e. Idempotent)  類似於一個完整的 reset 這樣   References     Hassle-Free Database Migrations with Prisma Migrate   What is data migration?   Migrations   COSCUP 2025 - Zero‑Downtime Online Schema Migration in PostgreSQL   [資料工程]獲取資料庫所有異動記錄 — Change Data Capture(1)   Lab 4.2 - A Simple DAG Workflow: apply workflow with generate name   Automating RDS Postgres to Aurora Postgres Migration  ","categories": ["database"],
        "tags": ["data migration","sql","prisma","nodejs","idempotent","transaction","postgresql","upsert","backward compatibility","on-premise","saas","golang","alembic","reversibility","atlas","checkpoint","kafka","kafka connect","cdc","change data capture","statement replication","logical log","kubectl wait","argocd","logical replication slot","datamesh","netflix"],
        "url": "/database/database-migration/",
        "teaser": null
      },{
        "title": "神奇的演算法 - Greedy Algorithm",
        "excerpt":"Preface  還記得之前上演算法的時候，最看不懂的東西就是貪婪法了  不過其實他的核心概念很簡單，寫起來也簡單  趁著還記得細節的時候，把它紀錄起來   Introduction to Greedy Algorithm  貪婪演算法，顧名思義，這種演算法的概念是以 “貪婪” 為優先選擇  什麼意思呢   貪婪的表現是 不會考慮後果的  不考慮後果只注重當下的最佳選擇，就是貪婪演算法的核心概念  也因此，貪婪法 並不會都給出最佳解      只考慮當下最優解  換句話說就是，它不會折返重新計算有沒有更好的方法    但是找零錢的問題，是個典型貪婪法能夠提供最佳解的題目  通常來說，找錢一定從大面額到小面額  比如說，125 可以拆成 100 * 1 + 10 * 2 + 5 * 1  這個例子，你貪婪的對象就是 面額的大小  因此在這個狀況下，貪婪法可以給出最佳解      可參考 LeetCode 322. Coin Exchange    不過如果題目稍微變形一下，就不一定了  比方說題目要求，回傳的硬幣數量要是最小的，那貪婪法就不一定有用了  假設你擁有 1, 5, 10, 20 以及 25 的硬幣，那麼 41 塊可以有兩種找錢的方式     (25 * 1) + (10 * 1) + (5 * 1) + (1 * 1) :arrow_right: 4 枚硬幣   (20 * 2) + (1 * 1) :arrow_right: 3 枚硬幣   可以看到，貪婪法失效了   Is Greedy Algorithm Useless?  既然貪婪法不一定能給出全局最佳解  那它實際上的用處在哪呢？ 是不是就沒有用了   其實不然，雖然不一定能給出全域最佳解  但由於貪婪法他的實作容易而且執行速度較快  它能夠一定程度給你解法，即使它不是最好的答案   最後就是針對一些 real time 的 application 它可能需要邊執行邊做決定的那種  貪婪法可以給到 當下最優解   When to Use Greedy Algorithm  其實這個相比其他題目，貪婪法好像沒有一個準確的準則說這題一定可以用  但一個比較好的 guideline 是你不需要找到全域最佳解，一個足夠好的答案如果也可以接受的話，也可以使用  另一個通則則是區域最佳解對之後的選擇影響不大   LeetCode 55. Jump Game  這題其實還滿好玩的，如果能夠不看答案寫出來我相信你就理解貪婪法了   題目是這樣子的  給你一個 array, 每個 array[i] 都代表著能夠跳躍的最大距離  請你求出，你能不能從 起點 跳到 終點?   他的第一個範例是這樣的     Input: nums = [2,3,1,1,4]  Output: true  Explanation: Jump 1 step from index 0 to 1, then 3 steps to the last index.    我當初看到這個範例覺的它怪怪的  我們已經知道他是貪婪法了，可是為什麼他的執行過程看起來一點都不貪婪？  在第 0 個位置的時候，你明明可以跳兩步，但為什麼你只跳一步呢？   如果你嘗試用貪婪法下去模擬，你會發現你可以這樣走  index 0(2) :arrow_right: index 2(1) :arrow_right: index 3(1) :arrow_right: index 4(4)  這樣走也會到終點     所以這題要怎麼解  目標是走到終點，要思考的點是我們怎麼樣會走不到終點？  如果跳躍距離為 0，是不是就沒辦法往前走了？  所以我們要貪婪的對象，是目標的數值不能為 0  所以他的貪婪核心算法理論上應該長這樣   1 2 3 4 5 6 7 8 9 10 11 12 counter := nums[i]  for counter &gt; 0 {     if i + counter &gt;= len(nums) - 1 {         return true     }     if nums[i + counter] != 0 {         i += counter         break     }     counter -= 1 }   滿簡單的對吧  基本上就是看落點有沒有可能是 0(亦即我們走不到終點)  如果它不是 0 我們就拿到下一個落點位置 counter 了  之後就把當前位置加上 counter offset 一直走就知道結果了   當你寫完的時候，你可能會發現有問題  [3,0,8,2,0,0,1] 這個測資居然是錯的  實際走一次看看問題在哪   我們的貪婪法實際執行過程如下  index 0(3) :arrow_right: index 3(2) :arrow_right: index 5(0)  是 false 他到不了   但你如果手動走走看你會發現你是這樣走的  index 0(3) :arrow_right: index 2(8) :arrow_right: :crown:  他是可以到終點的     很明顯我們的算法有問題  只考慮非 0 的落點很顯然不夠貪婪  你要找的當前落點必須是你能夠達到的最遠距離  所以提早 break 是不行的   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 counter := nums[i]  maxJump := 0 maxJumpV := 0 for counter &gt; 0 {     if i + counter &gt;= len(nums) - 1 {         return true     }     if nums[i + counter] != 0 {         if nums[i + counter] &gt; maxJumpV {             maxJumpV = nums[i + counter]             maxJump = counter         }     }     counter -= 1 }   但這樣仍然不夠貪婪！  沒錯，上面的解法仍然會錯  [4,2,0,0,1,1,4,4,4,0,4,0] 這個測資即使你選了當前能夠跳躍最遠距離的仍然會錯  為什麼?   因為我們少考慮了原本跳躍的距離  在 index 0 的時候我們最大跳躍距離是 4, 在 index 1 到 index 4 之中  他們的跳躍距離分別是 [2, 0, 0, 1]，很明顯之中的最大距離是 2   但如果我們選擇 index 1, 總共能跳躍的距離是 1 + 2  你要先從 index 0 到 index 1, index 1 才能最多往後跳 2  但如果你選擇 index 4, 總共能跳躍的距離是 4 + 1  從 index 0 到 index 4, index 4 在往後跳 1   很明顯的，我們要考慮的當前最佳解是 當前跳躍距離 + 落點能提供的跳躍距離  回到它給的範例，其實他的算法是貪婪的，不過它貪婪的對象 不僅限於當前跳躍距離 而已  所以完整的算法應該是這個樣子的   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 counter := nums[i]  maxJV := 0 maxJ := 0 for counter &gt; 0 {     if i &gt;= size - 1 || i + counter &gt;= size - 1 {         return true     }      if nums[i + counter] != 0 &amp;&amp;          counter + nums[i + counter] &gt; maxJV {         maxJV = counter + nums[i + counter]         maxJ = counter     }     counter -= 1 }   當前跳躍距離(counter) + 落點能提供的跳躍距離(nums[i + counter]) 才是最貪婪的當前最佳解  因此只要找到最大的數值，我們就能夠知道你該跳多少了(maxJ)   See Also     LeetCode 122. Best Time to Buy and Sell Stock II   References     貪婪演算法  ","categories": ["algorithm"],
        "tags": ["greedy","greedy algorithm","optimal","leetcode","leetcode-322","leetcode-55","leetcode-222"],
        "url": "/algorithm/algorithm-greedy/",
        "teaser": null
      },{
        "title": "DevOps - 成就完美的自動化 IaC 與 CaC",
        "excerpt":"Infrastructure  當軟體開發完成之後，Infra 對於整體運作來說是很重要的  沒有基礎設施，如網路，電腦以及儲存空間，我們將沒辦法提供服務   如今已經有非常成熟的 cloud provider 如 AWS  某種程度上解決了基礎建設的難處  透過簡易的 UI 設定機器大小，網路等等 我們可以將大多數精力放在軟體開發上面了   不過這些操作都是手動處理居多，也是很麻煩的  更不用說你可能手殘設定錯誤，導致系統中斷服務(這並不是沒有發生過的)  因此我們可以將這些操作自動化以及一些設定來幫助我們  也就是 IaC - Infrastructure as Code   Programming Approach  Declarative Approach  透過撰寫所需要的系統組態設定的檔案，這樣的檔案就稱為宣告式   Imperative Approach  詳細描述具體實作步驟細節，即為命令式   Introduction to IaC - Infrastructure as Code  IaC 是個概念，而這個概念可能你我都已經接觸過不少次了  舉個簡單的 docker-compose 的例子   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 version: \"3.9\"  services:   web:     build: .     deploy:       replicas: 5       restart_policy:         condition: on-failure    nginx:     image: nginx:latest     ports:       - \"8080:80\"     volumes:       - ./nginx.conf:/etc/nginx/nginx.conf     depends_on:       - web   docker compose 這個 yaml 裡面定義了你的服務的基本組態設定  我們定義了兩個 service(web 以及 nginx)  包含了他的 replica, port, volume 以及連接方式等等的設定   這其實就是 Infrastructure as Code 的概念  想想看哦，是不是擁有了這個組態設定檔，給我一台電腦，我都能完美複製 一模一樣 的環境出來？  那 as Code 是什麼意思呢？  我們是不是可以將 docker-compose.yaml 的組態設定用上版本控制，那某種程度上它也是一種程式碼了吧   IaC Definition  根據 什麼是基礎設施即程式碼？ 所述      就像軟體程式碼描述應用程式及其運作方式，基礎設施即程式碼 (IaC) 描述系統架構及其運作方式    通常具有以下性質的特性，就可以被稱為是 IaC     可以自動化(code-driven configuration)   可以被版本控制   宣告式 描述系統組態設定   專注在硬體層面   Why IaC  透過自動化的方式建立環境可以帶來一些好處   既然他是自動化的，也就意味著它出錯的機率會大幅度的降低  每次部屬到新的環境的時候，如果漏掉了一個設定，就可能會釀成大錯  人為疏失一直是軟體開發的常態問題，透過自動化能夠解決是在好不過得了   並且，因為我們是透過設定檔的方式建立環境  這樣可以很輕鬆的在不同的地區建立相同的環境   Introduction to CaC - Configuration as Code  同樣的概念套到 Configuration as Code 就很好理解了  像我自己平常寫專案會是使用 .env 的方式來設定環境  而通常我會給一個 .env.example 的檔案來作為範例  只要複製這個檔案就可以成功套用預設的設定檔了  這其實某種程度上來說就是 Configuration as Code   當然，我的作法跟 CaC 的定義有些許落差，不過我相信你有 get 到這個概念   CaC Definition  相比 IaC Definition 主要描述硬體層面的細節  CaC 則是專注在描述軟體層面的細節   而定義方面也大同小異      可以自動化(code-driven configuration)   可以被版本控制   宣告式 描述軟體層面的組態設定   專注在軟體層面的細節   Node.js PM2  我公司的服務，是用 PM2 這個服務帶起來的  而 PM2 的設定檔 ecosystem.config.js 裡面可以包含預設的設定   我們將一些常用的設定放在這個檔案中  像是 timeout, worker node … etc.  並且在這個檔案中可以設定不同的環境所需要的設定   所以它本質上是符合 CaC 的概念的   How about Secrets?  CaC 是將 configuration 以程式碼的形式儲存在版控之中  但是我們知道密碼這類隱私的東西是萬萬不可上傳的  那麼要如何 “as code” 呢？   一個方式是是用 environment variable, 像是 etcd  在應用程式中讀取這個 environment variable 就可以了   References     什麼是基礎設施即程式碼？  ","categories": ["devops"],
        "tags": ["infrastructure as code","configuration as code","iac","cac","pm2"],
        "url": "/devops/devops-code/",
        "teaser": null
      },{
        "title": "網頁程式設計三兩事 - Logging 最佳實踐",
        "excerpt":"Introduction to Logging  log 對於現今電腦系統來說是一個至關重要的資訊檔案  救援回復以及修 bug 其實都離不開 log   簡單來說 log 就是一個檔案，紀錄著系統運行各個時間點的運行狀態  你可以把它想像成是日記的概念(有些會用 日誌 這個詞)   log 不僅僅存在於網頁程式當中  乃至系統層級都可以見到 log 的身影  比如說 linux 系統中的 dmesg 指令  dmesg 會紀錄系統運行的各種狀況        ref: How to Use the dmesg Linux Command    dmesg 裡面紀錄的資訊，像我之前看過的，包含驅動程式掛載紀錄，網卡狀態等等的  其實就是一系列系統運行的資訊   作為網頁開發者的我們，後端系統中 log 的資訊就可能會替換成 request 的資訊，以及處理過程中產出的輔助資訊   When and What to Log?  現在的問題是，如何知道要紀錄什麼資訊呢?   後端系統你可能可以紀錄     每個 request 的資訊(如 client ip, request URL 以及 request body)   每個 request 的時間點   各個 request 處理過程中的資訊   更重要的是，我需要在每個地方都安插 log 嗎？     舉個實際的例子，我跟團隊們在討論 log 的位置產生了一點分歧  我們應該要在 router, service, database layer 每個地方都做 log 呢？  還是只在特定的位置(比如說 service layer) 做 log 即可？   log 太多，無意義的資訊可能過多，甚至可能會塞滿你的硬碟空間  log 太少，當需要 debug 的時候可能會遺失一些重要的資訊  這個問題兩邊都站得住腳，只是要考慮 trade off 的問題罷了   使用 Log Level 適當過濾掉不太重要的 log 只是一個暫時的解法  最終 log 都會爆掉，那麼 Archive(Log Rotation and Archival) 行不行呢？   Log Level  如果 log 重複的資訊太多，要找的時候可能不好找  我們是不是能夠依照 log 的重要程度進行分類   我們可以把 request client ip, request URL 等等歸類在 Info 當中  顯然他的目的只是告訴開發者在某個時間點有一個 request 進來了這樣   request body 之類的可以放在 Debug level 裡面  當需要除錯的時候我們才需要看到它的內容      Trace 我覺的比較偏向 console.log 這種你單純想看的情況    當一個 request 進來的時候，如果他的參數是不合法的，比如說 email 的格式無效的時候  這時候你可以選擇     Warn level, 如果你沒有套用 validator 之類的東西，錯誤的資料仍然可以進到系統並完成操作   Error level, 錯誤的 email 會導致整個 操作 無法繼續進行下去      這裡選擇 Warn, Error 其實都可以，為什麼 Warn 可以的原因是因為  像是某些 log 框架，他的 log level 會跟第三方服務串接  如果大幅度的使用 Error 會導致你收到很多不太緊急的錯誤  這樣就失去了你做 integration 的意義，詳細可以參考 Slack and Sentry Integration    Fatal 則是當系統崩潰無法繼續服務的時候，就是使用 Fatal level        ref: App logging levels: everything you need to know    Structured logging  肉眼看 log 可以說是工程師日常  雖然說是日常，但看久了也是挺傷眼睛的   1 2 3 4 5 I1112 14:06:35.783529  328441 structured_logging.go:51] \"using InfoS\" longData={Name:long Data:Multiple lines with quite a bit of text. internal:0} I1112 14:06:35.783549  328441 structured_logging.go:52] \"using InfoS with\\nthe message across multiple lines\" int=1 stringData=\"long: Multiple\\nlines\\nwith quite a bit\\nof text.\" str=\"another value\"     ref: Contextual Logging in Kubernetes 1.24    前面提到我們 log 可以順便帶一些資訊，其中 request body 也會包含到  但如果說 body info 裡面包含一些換行資訊，那麼在 cli 上面看起來就不是很直觀了(如上圖所示)     我們把這些資訊透過結構化的方式組合起來  比方說 JSON 格式，每個資訊都會以特定的方式組合起來  這樣要找資訊的時候也會比較方便   此外你也透過一些軟體工具處理 raw log，比如說 Compact Log Format Viewer        ref: 14 Top Free and Open-source Log File Viewers    因此在下 log 的時候，我們不妨就預設使用 JSON 格式之類的 structured logging 機制  這樣也方便後續如果要接一些第三方服務的時候可以無痛轉換   Contextual Logging  使用 Structured Logging 可以大幅度的增加 debug 時的效率  但是 log 之間的狀態是沒有顯示的，什麼意思   舉例來說，你有一個流量算大的後端系統  而他的 log 的內容大概長這樣   1 2 3 4 5 6 {method: \"get\", message: \"request /api/user/{userId}/posts\", parameters: {userId: 1}} {method: \"get\", message: \"request /api/user/{userId}/posts\", parameters: {userId: 2}} {message: \"found user posts, total 15\"} {method: \"post\", message: \"request /api/user/{userId}/post\", body: {xxx}} {message: \"Error query database, invalid cursor provided\"} {message: \"Error connecting database\"}   請問，究竟是哪兩筆 request 出問題了呢？ 是 user 1 還是 user 2 的 request 回傳錯誤？  你可能會說，我只要在 log error 的時候帶一些 user 資訊就可以了  這個作法不太好，你怎麼確定兩行相同的 user id 就代表一定是同一個 request 的結果呢？  即使我們用了 JSON 定義了一定的規格，但這個例子來說仍然不夠清楚   所以 contextual logging 的重點在於，log 裡面要帶入 context(上下文)  request 的 context 我們已經有帶了(client ip, request url … etc.)  剩下就是 log 之間的 context   一個常見的作法是使用 UUID 的方式 assign 到每一個 request 身上  進來的 request，logger 會 assign 一個 UUID, 那麼只要是該次 request 所產生的 log 都會有這個識別符號  因此開發者在追 log 的時候就會簡單清楚明瞭了   Log Rotation and Archival  很明顯的，單靠 Log Level 只是一個暫時的解法  最終 log 檔案將會大到爆掉   比較直覺的作法會是將每天的 log 檔進行壓縮封存  而確實這是一個有效的方法  但封存到一個極限你的硬碟空間還是會不夠  所以是需要進行刪除的，就像行車記錄器一樣，它會一直複寫   這個過程稱之為 log rotation  不過是自動化執行的   透過類似 Logrotate 這類工具可以自動做到以上的事情   Slack and Sentry Integration  最後的最後  我們可以設定自動化 alert  當有嚴重的錯誤發生的時候，我們可以馬上知道並著手處理   以我前公司的例子來說  我們是使用 Log4j 配合 @log4js-node/slack 套件進行整合  當 Error 等級的 log 發生的時候，slack bot 會在 slack 上通知我  大概是這樣的感覺   References     When to use the different log levels   Logging Tips for Power Users: Contextual Logging   Log rotation   Logrotate - 處理log rotation的好用工具  ","categories": ["website"],
        "tags": ["logging","contextual logging","slack","sentry","integration","archive","log rotation","logrotate","log4j"],
        "url": "/website/website-log/",
        "teaser": null
      },{
        "title": "從 0 認識 Blockchain - 錢包基礎原理",
        "excerpt":"Preface  加密貨幣涉及了很多密碼學相關的知識  其中錢包這裡佔了滿多部份的  這篇文章會盡量的用簡單的方式來說明  跟我一起看看吧   Introduction to Wallet  錢包可以說是加密貨幣中至關重要的一部分  你的所有虛擬資產將會被儲存在錢包中   錢包的建立是使用數學的方法產生一組密碼學相關的資訊  比方說你的 Public Key 以及 Private Key  之後你就可以用這組資訊跟區塊鏈互動   其實產生的方法還有另外一種是用一組 master key  然後透過 master key 產生無數的 公私鑰  本文將會著重討論此種方式(Deterministic Wallet)   Wallet Mnemonic     mnemonic 讀作 nemonic, 第一個 m 不發音         ref: https://www.pinterest.com/pin/brave-wallet-backup-mnemonic-page–570549846541363404/    在創建新的錢包的時候，BitCoin 或者是 Ethereum 都會告訴你需要將 助記詞 保管好  而這個助記詞在錢包中扮演了相當重要的角色   前面提到，deterministic wallet 會產生一組 master key, 然後可以透過它產生無數 公私鑰  那麼 master key 就跟 mnemonic(助記詞) 很有關係了   Mnemonic Wordlist  助記詞你可以看到是由若干個英文單字所組成的  它本質上還是一串數字，只是為了方便人類辨識、閱讀以及記憶  所以採用文字的方式紀錄而已   而這個紀錄方式非也像是 ASCII 這種編碼而已  取而代之的是採用一種叫做 wordlist 的東西  想的簡單一點就是一個詞彙字典而已  比方說 apple 是 1, banana 是 2 等等      wordlist 不一定要是英文，你也可以製作其他語言的 wordlist  畢竟那個就只是一個詞彙字典而已    根據 BIP-39 所描述  助記詞應該要包含以下特性     要避免相似的詞彙   透過前四個字母就可以確定是哪一個詞彙   參考 BIP-39 wordlist  考慮以下助記詞，他的編碼過後的數字是多少？  1 bronze barrel chicken      bitcoin 的 wordlist 是 0-based, 所以要 -1    1 2 3 bronze  -&gt; 229 barrel  -&gt; 150 chicken -&gt; 331   那麼它編碼後的就是 0x1ca2589e(十進位轉八進位，自己算算看，懶的話可以用 Mnemonic Code Converter)   那麼這串數字！ 還不是 master key  你還必須要再透過 hash function 算個百八十遍才會得到最終的 master key   hash function 要選擇 PBKFD2 的演算法  他的重點在於要 重複 hashing 以增加安全性(i.e. 金鑰延伸)  輸入就是     助記詞   密碼(你的錢包的密碼)   salt      重複多少呢，BitCoin 是 2048 次      到這裡你就成功的做出 master key 了  看了上面的討論，就可以理解為什麼你要保管好 mnemonic 的原因  拿到 mnemonic 可以復原你的所有公私鑰，然後攻擊者就可以偷你的錢了(劃重點)   Wallet Seed(master key)       ref: BIP-32       entropy 是 mnemonic decode 之後的八進位數字  HMAC-SHA512 是 PBKFD2 的演算法    有了 master key 之後，上圖你可以看到  我們就可以產生無數的公私鑰  而這就是 deterministic wallet(hierarchical deterministic wallet) 的運作方式   只要有一把 master key 就可以管理底下全部的公私鑰  不論是備份或者是要做 restore 都挺方便的   Restore Wallet  看到這裡你應該很清楚，使用 Mnemonic 就可以一鍵還原你的所有錢包  但我就在想他是怎麼做的   別誤會，我知道只要 mnemonic 就可以復原  但問題是它怎麼知道你有多少錢包？   還記得先前我們說過你可以創建無數錢包嗎  所以理論上你就可以復原出無數錢包了  連包含你沒有產生的錢包理論上都算的出來 這顯然哪裡怪怪的   wallet restore 的時候他是會算出所有錢包的公私鑰沒錯  只不過它會多一個步驟是檢查他有沒有在鏈上  所以它才只會回復你自己建立過得錢包而已   Why Generate Wallet By Cryptography  為什麼你要用一個這麼複雜的方式產生錢包而不是單純的使用隨機數字來產生錢包呢   其中一個原因很明顯跟安全有關  隨機數字產生錢包，他的密碼安全性是不夠強的  就跟我們用 ssh key 登入伺服器一樣   另一個我覺的寫得很好的原因是為了去中心化  如果像傳統銀行一樣使用流水號的方式來產生錢包的話  先不說安全性，勢必是需要一個 中心化 的設施來發行流水號  而這與 blockchain 的概念背道而馳   References     【加密貨幣錢包】從 BIP32、BIP39、BIP44 到 Ethereum HD Wallet  ","categories": ["blockchain"],
        "tags": ["blockchain","ethereum","wallet","seed","hash","mnemonic","wordlist","bitcoin","master key","hierarchical deterministic wallet","deterministic wallet"],
        "url": "/blockchain/blockchain-wallet/",
        "teaser": null
      },{
        "title": "重新認識網路 - 從基礎開始",
        "excerpt":"DNS - Domain Name System  Domain Name System 是一個分散式的系統，用於紀錄網域名稱和 IP 位址之間的關聯  基本上現今我們在瀏覽網站的時候，多半是使用所謂的 domain name 上網的  比方說 google.com, facebook.com   對於人類來說，這是比較容易理解的  IP address 則是方便於電腦進行解析的，但卻是不容易理解  因此 DNS 的作用就是將 IP address 轉換成網域名稱   本質上 DNS 分散式架構有助於提昇整體吞吐量  包含單一節點失效，效能問題(可參考 資料庫 - 初探分散式資料庫 | Shawn Hsu)   DNS Server Categories       ref: What is a DNS root server and what do they do?    基本上分為三種類型      Root DNS Server            這些伺服器是整體網路的最高階，共有 13 個組織負責管理       Root server 不會直接回 ip address, 它會告訴你誰擁有這些資訊，然後你去找 TLD DNS Server           Top-level Domain DNS Server(TLD DNS Server)            高階網域如 com, org, edu 等都會紀錄在 Top-level Domain DNS Server 中       同樣它也不會直接回 ip address, 它會告訴你誰有這些資訊，然後你再去找 Authoritative DNS Server           Authoritative DNS Server            到這層就會真的給你 ip address 了           所以每次你的查詢都是從高階到低階  這樣很慢嘛，所以也會有 caching 的機制在裡面   CNAME and A Record  我大學的時候自架實驗室伺服器，為了更好的區分不同的功能  我將不同服務置於不同的 subdomain 底下，像是 doc.example.com, git.example.com  當時是將所有的 subdomain 都指向同一台伺服器，並且透過 nginx 來做反向代理   這中間其實就是使用了 CNAME 與 A record 來做到的  不同的 subdomain 都指向同一台伺服器(或者說同一個 ip address)   我們知道 domain name 對應到 ip address 是 DNS 的工作  這種紀錄，稱之為 A record   CNAME 另一方面則是 alias 的功用  doc.example.com 指向 example.com 再指向最終的 ip address  你可以發現到，他並不會直接指向 ip address  所以這種紀錄，稱之為 CNAME record  他是一個別名，指向另一個 domain name   套用 什麼是 DNS CNAME 記錄？ 所述     想像一下，一個尋寶遊戲，每個線索都指向另一個線索，最後的線索指向寶藏。   帶有 CNAME 記錄的域名就像一條線索，可以將您指向另一個線索（另一個具有 CNAME 記錄的域名）或寶藏（具有 A 記錄的域名）。      但是 doc.example.com 不一定要是 CNAME  你可以將它設定成 A record，直接指向 ip address  這樣你就可以將他指到到任何你想要的地方   DNS Load Balancing  現今的服務器通常都是分散式系統，這就意謂著一個 domain name 可能會對到很多台實體伺服器  那麼當你存取的時候，是哪一台伺服器需要回應你呢   DNS 通常會回應一連串的 IP address  client 通常會取第一個 ip 使用  每次 DNS 回應的時候，server 會輪流改變 ip address 的順序  所以同一台伺服器就不會太操勞   這樣就可以做到基本的 load balancing   nslookup  這個指令可以查詢網域名稱對應的 IP address  舉例來說   1 2 3 4 5 6 7 8 9 10 $ nslookup google.com Server:         127.0.0.53 Address:        127.0.0.53#53  Non-authoritative answer: Name:   google.com Address: 172.217.163.46 Name:   google.com Address: 2404:6800:4012:4::200e    不熟悉 command line 操作也可以使用類似 NsLookup.io 等網站來查詢   Socket     In UNIX, Everything is a File    事情還得要從作業系統說起  UNIX 的設計哲學是，Everything is a File  每個東西都是檔案，什麼意思？  檔案不就是檔案嗎？ 那麼網路，印表機這些也算檔案嗎？  依照作業系統的邏輯，如果要詳細區分這些東西為各種不同的類別，那麼實作起來將會無比困難  因此，就算是印表機這種東西，底層實作也會將它視為是 檔案   建立網路連線，它分成大約兩步驟     建立 socket(i.e. file descriptor)，建立好一個對接的端口   建立連線(並且透過 socket 進行資料的傳輸)   而 socket 就是那個 檔案  每個應用程式都擁有自己的 socket 接口  你可以使用 lsof 這個指令查看當前 internet file descriptor 的狀況  執行 $ sudo lsof -i 你會看到類似這張圖的輸出  可以很清楚的看到，chrome 瀏覽器開了許多的 socket      你應該還會看到其他應用程式，像我的還有 telegram, docker … etc.      socket 有沒有數量限制呢？ 或者說比較 general 一點  file descriptor 有沒有上限？  我想答案很明顯是有的   1 2 $ ulimit -n 1024      可參考 man ulimit    IP - Internet Protocol  提供 主機 之間的通訊，但不提供任何保障      不保證送達目的地   不保證會依序送達   Process Communication - A High Level View  兩個不同的 process 要透過 network 的方式進行溝通  離不開兩個觀念，送資料(Transport-layer Multiplexing)與收資料(Demultiplexing)   Transport-layer Multiplexing  將資料與 header 封裝在一起透過 Socket 往下丟給網路層  而 header 包含了一些下層需要知道的事情  比方說你要送到哪裡，要給誰  舉例來說 sender/receiver 的 ip 跟 port   換言之，如果 ip port 都一樣，那代表這兩個連線是一樣的      port 1024 以後都可以隨意用，1 ~ 1023 不能使用       為什麼要有 sender 的資料？ 因為要知道資料要送回哪裡    Demultiplexing  將網路層上來的資料分送到正確的 Socket 端口  然後在根據 header 裡面的內容，送到正確的地方   TCP - Transmission Control Protocol  TCP 是建立在 IP 之上的協議  IP 是專注在 主機 之間的通訊，而 TCP 則是專注在 行程(process) 之間的通訊      可參考 RFC 793    TCP 相比 IP 額外提供了     可靠性傳輸(保證送達以及保證順序送達)   壅塞控制(congestion control)      有關傳輸層的討論可以參考 重新認識網路 - OSI 七層模型 | Shawn Hsu    Why Handshake is Important  TCP 是可靠傳輸，所以它需要花一點時間建立可靠連線  為什麼要花時間建立連線？   試想線上會議的情況下，當你加入會議，你可能會先確保你的麥克風、喇叭是不是正常的  確定對方聽的到你的聲音，然後你也聽的到對方的，才會開始開會對吧  這就是一個確保連線成功的機制   結束會議也一樣  跟對方說你要下線了，對方也會跟你說要下線了  雙方都要認知道對方要下線，並且同意  才可以關閉連線      TCP 在 RFC 793 並沒有定義需要做 結束會議的握手  但是 RFC 6824 裡面有定義而且 tcpdump 的結果有時候有出現  就在這裡提一下    你可能會覺的什麼都要 double check 是一件很麻煩的事情  但沒有進行這個確保的情況下，你可能會漏掉對方的資訊  而這樣就不是可靠性傳輸了   對應到網路世界，這個過程稱之為握手  TCP 在以下情況會做握手     建立連線 - Three-way Handshake   結束連線 - Four-way Handshake     需要注意的是  TCP server 會需要 2 個 socket 來處理連線  為什麼呢   我們說過，一個 connection 背後代表著一個 unique 的 Socket  TCP 是可靠性傳輸協定，每一個獨立的連線傳輸都有一個 socket  也就是說會有若干個 socket   如果說每一個連線的 socket 都不一樣  那麼你需要一個統一的連接進入點來處理不同的 connection  這也就是為什麼 TCP server 會需要 2 個 socket 來處理連線   一個是主要進入點的 socket  另一個則是每個 client 獨立的 socket  所以是兩個   Three-way Handshake       ref: Three-Way Handshake    所以基本上 TCP 三方交握就是一個初始化 say hello 的流程  確認你已經準備好要收訊息了 我也準備好要發訊息了這樣         前情提要 tcpdump flag                          flag         shortened         description                       S         syn         synchronize 同步                       .         ack         acknowledge 確認                       F         fin         結束連線                       P         push         傳送資料                  用 Tcpdump 這個工具來稍微理解一下吧，不然光用講的我也聽不懂   執行以下指令開始監聽 $ sudo tcpdump -i lo port 4000  我這次測試的目標，是在本機上的 4000 port  因為是在本機上，所以是 lo(loopback) interface(可以用 $ ifconfig 查網卡名稱)  tcpdump 會監聽目標 port 上所有的流量，所以你就可以用他來觀察連線的狀態   第一個藍色框框表示 TCP 三方交握  很明顯符合 SYN(S), SYN + ACK(S.) 以及 ACK(.) 這三種狀態  細心的你會發現，三方交握裡面有兩個奇怪的 seq 數字(1708137740 以及 3857014844)  sequential number 是一個隨機的數字，方便 server/client 溝通同步用的   你可以看到第一個藍色框框，在第二階段 SYN + ACK，它回復的 ack 數字是 1708137740 + 1  這很明顯不是巧合，他是根據 client 的 sequential number 再加一回復的(seq + 1)  不過 server 回復的 sequential number 卻是不同的？ 因為 server client 他們會個別產生數字，不會共用     最後當連線都建立完成之後，你可以看到黃色框框的部份  就是在進行資料傳輸了  這時你會發現為什麼 sequential number 變成一個 pair 了？  而且它似乎擁有某種關聯，1:377 然後 377:14857  聰明的你一定猜到，它是在確保資料的 到達順序     你可能會發現，為什麼會有兩個三方交握(藍色框框)  仔細看會發現他是兩個不同的 client 分別跟 4000 port 進行連線  我雖然無法確定為什麼會有多個 client 連線，但這證明了一件事情，server 是可以多工處理不同 client 的連線的！  而且它還不會亂掉(因為 client port 不同，所以是不同的 Socket)     但藍色框框，最後 server 回 ACK 的時候, 為什麼是 1 呢？  這是 tcpdump 預設輸出相對 sequence number  可以在下指令的時候加一個 flag -S 就可以了  下圖，ACK 回的數字就是 seq + 1 了      Four-way Handshake        再次註明，在原始的 RFC 793 的定義中  並沒有指出需要四方握手以結束連線  我在測試的時候也是有時候有看到，有時候又沒看到    跟三方交握相反，四方握手是用在關閉連線的時候會用到  概念在 Why Handshake is Important 提到  基本上是一樣的   這裡的關閉連線是從 server 開始的  第一個藍色框框，使用 FIN + ACK 來表明我想要中斷連線  我想要終止 sequential number 為 20412 這段連線  並且跟你說，上一段資訊 1573 我已經收到了   所以第二段  ACK client 表示說好的我已經收到了你想要關閉 20412 的連線   第三段  既然你想要關閉連線，那我這邊也想要關閉 1573 的連線(FIN)  並且 20412 連線已經結束了，回一個 20412 + 1 的 ACK 給你   最後 server 表示，你的 1573 關閉請求已經批准！回一個 1573 + 1 的 ACK 給你     基本上就是每個人做的每件事情都一定要回  所以才會拆成四個步驟來做  說到底就是 server 想關, client 想關, server 結束, client 結束 這樣   Congestion Control  網路是會塞車的，網路頻寬是一個主要的因素  相信你有過一個狀況，家裡很多人在用 wifi 的時候，你會感覺到它變慢  這個就是網路已經塞車的現象了   說到塞車，假設連假你要出門旅遊  從連假的前一天開始就會有塞車的現象  這時候你可以選擇當個聰明用路人或者是跟著一起塞  一起塞的時候，你會發現到匝道會有管制，幾秒鐘放行幾輛車子這種  這就是 壅塞控制 的概念   避免過多的車子進入國道，使得塞車的現象變得更嚴重  我們可以使用基本的管制措施限制數量，用以緩解塞車的現象   當網路上的封包多到開始掉的時候，一個常見的方法就是減緩發送封包的頻率(congestion window :arrow_right: 給定時間內能傳多少資料)  對，TCP 是會掉封包的，因為底層是 IP 不可靠傳輸  TCP 有自己的偵錯方法，所以我們可以信任它   telnet  測試 TCP 連線可以使用 telnet  基本的用法就是   1 $ telnet localhost 8080   然後就可以開始傳輸資料了   UDP - User Datagram Protocol  UDP 的定義是在 RFC 768  相較於 TCP，UDP 是不會進行握手的  所以他是無連線的 protocol   UDP 相比 IP 額外新增了兩個服務      datagram header 中加入錯誤偵測欄位，確保資料無誤   process 之間的通訊   UDP 並沒有像 TCP 一樣提供可靠性傳輸哦   UDP Observation  UDP 在傳送資料前並不會有任何握手確認的動作  看一下實際上是不是真的這樣子      我們可以用 Tcpdump 以及 Wireshark 觀察  測試指令我們可以用 netcat 來測試       測試 UDP 的指令為 $ echo \"Hello world\" | nc -u 127.0.0.1 8083 -w 10   我們用 netcat(nc) 的指令，發送資料(Hello world) 到本機上的 8083 port  w flag 是指等待秒數，在這裡我們等 10 秒   你可以看到 tcpdump 的確有抓到我們送過去的資料(沒錯 tcpdump 也可以用來抓 udp traffic)  詭異的是它為什麼會顯示長度為 12 呢？      透過 wireshark 觀察可以得知，最後一個 byte 是 0A  參照 ASCII 表來對照就是 LF  所以實際上是 Hello world\\n, 也就是 12 個 byte 了     你可以很明顯的看到，UDP 並沒有所謂握手的動作  參照 tcpdump 的結果，他是直接傳資料過來的  就像有人突然跟你搭話，你有可能沒聽清楚蛤回去  封包就掉了   UDP Transfer Type  UDP 在傳輸資料的時候有分成三種不同的模式 Unicast, Multicast 以及 Broadcast   Unicast  在 UDP Observation 我們看到的例子就是屬於 unicast  他是點對點的傳輸，跟 TCP 很像  只是沒有握手而已   Multicast  相較於單點傳播，multicast 為多點傳播  但它跟 Broadcast 不一樣  多點傳播只會把資料傳給訂閱者，而不會把資料傳給其他人  有點類似 Observer Pattern(可參考 設計模式 101 - Observer Pattern | Shawn Hsu)   multicast 需要執行在特定的 ip 地址上  這個是因為它可以確保路由的時候更順利，因為只要送到這個地址的資料我就知道他是 multicast  這個區段從 224.0.0.0 到 239.255.255.255   訂閱 multicast server 的人，每當新訊息來的時候，就會收到  類似 mailing list  而發送訊息的本人，是不會收到自己發送的訊息的   Broadcast  廣播就是所有人都可以收到資料了  它不需要明確的加入 “群組”, 就可以收到資料   同樣的，廣播也需要一個地址 255.255.255.255  把資料丟過去，所有同個區域網路都會收到你的訊息  而這其實會造成一些問題   不是每個人都需要這個訊息  它會造成網路壅塞(c.f. Congestion Control)，這顯然不好  所以其實 broadcast 已經被廢棄了   SSL - Secure Sockets Layer(TLS)  一個常見的誤區，至少對我來說  是錯誤的認為 TCP/UDP 這些協定會自己幫我們把資料加密  這是錯的 是錯的 錯的！      至少你可以從我們做的 wireshark 小實驗中得出  我們甚至不用 decrypt 就知道我們送的資料是 Hello world   我們能在網路上肆意的遊玩，是因為我們的資料都是加密過的  得益於 SSL(i.e. TLS) 的幫助   TLS(現在比較常說 TLS) 是一種補強的 protocol  定義於 RFC 5246  其宗旨是為了提供在網路上雙方的隱私以及資料的完整性      At the lowest level, layered on top of some reliable  transport protocol (e.g., TCP [TCP]), is the TLS Record Protocol.    TLS 需要跑在可靠性的傳輸協定之上，如 TCP      隱私性是透過 對稱性加密演算法 以及 非對稱式加密演算法 所提供   資料的完整性是透過 Message Authenticate Codes(MACs) 所提供   基本上 TLS 是由兩部份組成 TLS Record Protocol 以及 TLS Handshake Protocol   TLS Record Protocol  record protocol 主要是用於封裝高階 protocol 的資料  舉例來說 HTTP(可參考 重新認識網路 - HTTP1 與他的小夥伴們 | Shawn Hsu)   怎麼封裝呢？  將 message-based 的資料包裝成一個可管理的封包  適當的壓縮資料，加上 Message Authenticate Codes(MACs) 來確保資料完整性      MACs 不能驗證來源合法性(i.e. 不確定是不是本人簽的)  如果需要驗證 1. 資料完整性 2. 來源合法性，可以考慮使用數位簽章(digital signature)    TLS Handshake Protocol  重頭戲來啦，client 跟 server 要建立一個安全的連線了！      他們會先核對基本資料(e.g. protocol version, 加密演算法 等等的)            然後嘗試做 session resumption(簡單的理解是 connection caching, 這樣就不用重新握手，很方便)           驗證彼此的合法性            交換 certificate 驗證彼此的身份(透過 CA 組織, 比如說免費的 Let’s Encrypt)           產生 master secret, premaster secret，交換 random number(避免 relay attack)   secure session 建立成功！ 可以開始享受安全的資料傳輸了！        ref: TLS Handshake    TCP/UDP Comparison                          TCP       UDP                       Reliable       :heavy_check_mark:       :x:                 Ordered       :heavy_check_mark:       :x:                 Handshake       :heavy_check_mark:       :x:                 Connection       :heavy_check_mark:       :x:                 Congestion       :heavy_check_mark:       :x:                 Packet size       Large       Small                 Used Socket       2       1           TCP/UDP Golang Example  詳細的程式碼可以參考 ambersun1234/blog-labs/echo-tcp-udp   TCP  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 func tcpHandle(conn net.Conn) {     defer conn.Close()      scann := bufio.NewScanner(conn)     for scann.Scan() {         input := scann.Text()          logger.Println(\"Receive: \", input)         if _, err := conn.Write([]byte(fmt.Sprintf(\"%s\\n\", input))); err != nil {             logger.Fatalln(err)         }     } }  func tcpServer() {     li, err := net.Listen(\"tcp\", fmt.Sprintf(\":%d\", tcpPort))     if err != nil {         logger.Fatalln(err)     }     defer li.Close()     logger.Printf(\"Listening on: localhost:%v\\n\", tcpPort)      for {         conn, err := li.Accept()         if err != nil {             logger.Fatalln(err)         }         if err := conn.SetDeadline(time.Time{}); err != nil {             logger.Fatalln(err)         }          go tcpHandle(conn)     } }  func tcpClient() {     host := os.Getenv(\"HOST\")     conn, err := net.Dial(\"tcp\", fmt.Sprintf(\"%v:%d\", host, tcpPort))     if err != nil {         logger.Fatalln(err)     }      ticker := time.NewTicker(2 * time.Second)     for {         &lt;-ticker.C          // send data         msg := \"Hello World!\"         logger.Println(\"Send: \", msg)         if _, err := conn.Write([]byte(fmt.Sprintf(\"%s\\n\", msg))); err != nil {             logger.Fatalln(err)         }          // receive data         data, err := bufio.NewReader(conn).ReadString('\\n')         if err != nil {             logger.Fatalln(err)         }         logger.Printf(\"Server ACK with: '%v'\\n\", string(data))     } }  基本上需要注意的點是     TCP 是以換行為分隔符號, 所以你送資料的時候記得加 \\n   其他就相對單純  用一個 for-loop 持續監聽連線  當新的連線進來的時候，就開啟 goroutine 來處理它(line 32)  這也是為什麼上面我們說 TCP 需要兩個 sockets   UDP Unicast  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 func udpUnicastHandle(conn *net.UDPConn) {     buf := make([]byte, 1024)      for {         n, adr, err := conn.ReadFromUDP(buf)         if err != nil {             logger.Fatalln(err)         }          data := string(buf[:n])         logger.Println(\"Receive: \", data)         if _, err := conn.WriteToUDP([]byte(fmt.Sprintf(\"%v\\n\", data)), adr); err != nil {             logger.Fatalln(err)         }     } }  func udpUnicastServer() {     address := net.UDPAddr{         Port: udpUnicastPort,         IP:   net.ParseIP(\"0.0.0.0\"),     }     li, err := net.ListenUDP(\"udp\", &amp;address)     if err != nil {         logger.Fatalln(err)     }     logger.Printf(\"Listening on: localhost:%v\\n\", udpUnicastPort)      udpUnicastHandle(li) }  func udpUnicastClient() {     host := os.Getenv(\"HOST\")     conn, err := net.Dial(\"udp\", fmt.Sprintf(\"%v:%d\", host, udpUnicastPort))     if err != nil {         logger.Fatalln(err)     }      ticker := time.NewTicker(2 * time.Second)     for {         &lt;-ticker.C          // send data         msg := \"Hello World!\"         logger.Println(\"Send: \", msg)         if _, err := conn.Write([]byte(msg)); err != nil {             logger.Fatalln(err)         }          // receive data         data, err := bufio.NewReader(conn).ReadString('\\n')         if err != nil {             logger.Fatalln(err)         }         logger.Printf(\"Server ACK with: '%v'\\n\", string(data))     } }   跟 TCP 的實作可以說是一模一樣  差在它不會預先建立連線  然後一些 function 改成使用 UDP 版本的這樣而已   UDP Multicast  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 func udpMulticastHandle(conn *net.UDPConn) {     address, err := net.ResolveUDPAddr(\"udp\", fmt.Sprintf(\"%v:%v\", udpMulticastHost, udpMulticastPort))     if err != nil {         logger.Fatalln(err)     }     buf := make([]byte, 1024)      for {         n, _, err := conn.ReadFromUDP(buf)         if err != nil {             logger.Fatalln(err)         }          data := string(buf[:n])         logger.Println(\"Receive: \", data)         if _, err := conn.WriteTo([]byte(fmt.Sprintf(\"%v\\n\", data)), address); err != nil {             logger.Fatalln(err)         }     } }  func udpMulticastServer() {     address, err := net.ResolveUDPAddr(\"udp\", fmt.Sprintf(\"%v:%v\", udpMulticastHost, udpMulticastPort))     if err != nil {         logger.Fatalln(err)     }     li, err := net.ListenMulticastUDP(\"udp\", nil, address)     if err != nil {         logger.Fatalln(err)     }     logger.Printf(\"Listening on: localhost:%v\\n\", udpMulticastPort)      udpMulticastHandle(li) }  func udpMulticastClient() {     address, err := net.ResolveUDPAddr(\"udp\", fmt.Sprintf(\"%v:%v\", udpMulticastHost, udpMulticastPort))     if err != nil {         logger.Fatalln(err)     }      conn, err := net.ListenMulticastUDP(\"udp\", nil, address)     if err != nil {         logger.Fatalln(err)     }      ticker := time.NewTicker(2 * time.Second)     for {         &lt;-ticker.C          // send data         msg := \"Hello World!\"         logger.Println(\"Send: \", msg)         if _, err := conn.WriteTo([]byte(msg), address); err != nil {             logger.Fatalln(err)         }          // receive data         data, err := bufio.NewReader(conn).ReadString('\\n')         if err != nil {             logger.Fatalln(err)         }         logger.Printf(\"Server ACK with: '%v'\\n\", string(data))     } }   UDP Multicast 就有意思的多了  首先，你必須使用 ListenMulticastUDP 這個 function  不論 client 或 server  注意到不能使用 ListenUDP   另外一個有趣的點是先前在 Multicast 提到  群播發送訊息的人不會收到自己的訊息  根據 https://pkg.go.dev/net#ListenMulticastUDP 裡面提到      Note that ListenMulticastUDP will set the IP_MULTICAST_LOOP socket option to 0 under IPPROTO_IP, to disable loopback of multicast packets.    而你在測試 Multicast 的時候就必須要使用不同的機器  才可以正確的讀取到群播的資料  並不需要手動撰寫過濾自身訊息的程式(你必須要使用 ListenMulticastUDP 這個 function)   並且，server 寫資料的時候也要注意  因為是群播，所以你的 destination 也必須要是 群播地址(第 16 行)  不要寫成 client address 不然你會讀不到資料   References     電腦網際網路(ISBN: 978-986-463-950-2)   網際網路協議套組   TCP header format   What is TCP Three-Way HandShake?   傳輸控制協定   Root Servers   Why does TCP socket programming need two sockets(one welcome socket and one connection socket) but UDP only needs one?   TCP: can two different sockets share a port?   透過 TCP/IP 進行三向交握的說明   Three-Way Handshake   Why is the ACK flag in a tcpdump represented as a period “.” instead of an “A”?   tcpdump: Learning how to read UDP packets   串流技術簡介- 什麼是 UDP, TCP, Unicast, Multicast, RTP, RTSP, RTMP?   What IP to use in order to perform a UDP broadcast?   TCP擁塞控制   How do multiple clients connect simultaneously to one port, say 80, on a server? [duplicate]   什麼是 DNS CNAME 記錄？   HTTPS uses Asymmetric or Symmetric encryption?  ","categories": ["network"],
        "tags": ["tcp","udp","ssl","tls","dns","nc","tcpdump","wireshark","broadcast","load balance","ip","telnet","multicast","unicast","cname"],
        "url": "/network/network-basics/",
        "teaser": null
      },{
        "title": "神奇的演算法 - 動態規劃 Dynamic Programming",
        "excerpt":"Preface  動態規劃一直是我覺的不容易掌握的演算法技巧，它不像其他演算法技巧有一個固定的模式，而是一種思維方式  題目的靈活性高，不太容易掌握   Starting From Fibonacci Sequence  費氏數列是資工系學生一開始接觸到的題目  費氏數列的定義是 $F(n) = F(n-1) + F(n-2)$，而 $F(0) = 0, F(1) = 1$  所以寫起來基本上長這樣   1 2 3 4 5 6 7 8 9 func fibonacci(n int) int {     if n == 0 {         return 0     }     if n == 1 {         return 1     }     return fibonacci(n-1) + fibonacci(n-2) }      改寫成 tail recursion 也可以大幅度的提升速度    你可能有聽過，這個版本的實作效率非常差  這是因為這個版本的實作會重複計算很多次相同的值  所以他的 time complexity 是 $O(2^n)$(i.e. exponential)   所以更好的解法之一是使用動態規劃  基本上我們知道 fib(n) 等於前一項加上前兩項  所以你可以用一個 for loop 過去就解決了   1 2 3 4 5 6 7 8 9 func fibonacci(n int) int {     dp := make([]int, n+1)     dp[0] = 0     dp[1] = 1     for i := 2; i &lt;= n; i++ {         dp[i] = dp[i-1] + dp[i-2]     }     return dp[n] }   我們是逐步的建構出 fib(n) 的值，並且搭配上記憶化搜索，所以 time complexity 是 $O(n)$   Introduction to Dynamic Programming  DP 的心法是將一個大問題拆解成許多小問題，並且將小問題的解答記錄下來(記憶)  在解決小問題的時候，記憶化搜索可以幫助我們避免重複計算  當你解決完所有的小問題後，你就可以得到大問題的解答，答案就出來了   只不過要如何找到解決小問題的公式，老實說這有點難  看幾個例子或許你會有所感覺   LeetCode 322. Coin Change  給定一個金額，以及一個硬幣面額的陣列，問你需要多少硬幣可以湊出這個金額，且硬幣數量要是最少的   這題用貪婪法是沒辦法解的，因為多了一個限制，硬幣數量要是最少的  用遞迴可以窮舉出所有可能性，不過會出現重複的組合需要過濾  既然在學習 DP，我們就用 DP 來解決這個問題   找零錢一般來說都是從大到小(找 24 塊會給 2 * 10 + 4 * 1，應該是不會給 24 * 1)，但硬幣數量不一定會是最少的  比方說  你有 [1, 6, 7, 9, 11], 然後找的零錢為 13     greedy: 1 * 11 + 2 * 1 :arrow_right: 3 個硬幣   dp: 1 * 6 + 1 * 7 :arrow_right: 2 個硬幣   要怎麼知道 n 金額的最少硬幣數量呢？  很明顯我們沒辦法第一時間想出來，因為這個問題太複雜了  一種方式是我們可以先從小金額開始，一步一步的推導出大金額的最少硬幣數量  這樣時間複雜度也僅僅只有 $O(n)$   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 func coin(coins []int, amount int) int {     dp := make([]int, amount + 1)     dp[0] = 0      for i := 1; i &lt;= amount; i++ {         result := int(1e9)         for _, coin := range coins {             if i - coin &gt;= 0 {                 result = min(result, dp[i - coin] + 1)             }         }          dp[i] = result     }      if dp[amount] == 1e9 {         return -1     }     return dp[amount] }   重點在 result = min(result, dp[i - coin] + 1)  0 元的時候，有 0 種方法可以湊出，所以 dp[0] = 0  但是為什麼公式的部份需要 +1 呢？   上述 iterate 全部的硬幣面額，然後我們要找出最小硬幣數量所以用 min  dp[i - coin] 表示的是 當前金額扣掉硬幣面額後的金額，所需的最小硬幣數量  換句話說，如果我們要湊出當前金額，我們需要湊出 i - coin 的金額，而 i - coin 的金額正好是目前考慮的硬幣面額  所以我們需要 +1，表示我們使用了一個硬幣   因為 dp[i] 裡面儲存的都是最小硬幣數量，所以可以保證 dp[i + 1] 經由我們的計算肯定也是最小硬幣數量   Multi-state Decision Problem  我們剛剛看到的題目都是屬於單一狀態決策問題  也就是他的變因只有一個，需要考慮的事情相對單純(i.e. 最小值)   不過 DP 難的地方，我覺的在於多狀態決策問題  他的變因超過一個以上，整個狀態的轉移變得非常複雜   LeetCode 120. Triangle  題目的要求是說，給定一個三角形，找出從頂點到底邊的最小路徑和   這個題目的解法有點像是走迷宮  我們可以很輕易的得出他的公式，也就是最小和等於當前的值加上下一層的最小值  你會想，這還不簡單，用個遞迴從上到下窮舉出所有可能性就可以了   1 2 3 4 5 6 7 func traverse(x, y int) int {     return triangle[x][y] + min(         traverse(triangle, x + 1, y),          traverse(triangle, x + 1, y + 1),     )  }   有沒有發現，這個解法你好像在哪裡看過  它跟我們上面說的 fibonacci 一樣，都是屬於 exponential 的解法   為什麼？  第一層 call，會往下算完全部的可能性；第二層 call，也會往下算完全部的可能性  它會重複計算很多次  改進的方法也是一樣的，我們可以用一個 dp 陣列記錄下來(記憶化搜索)   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 var nnum = int(-1 * 1e5)  func traverse(triangle [][]int, x, y int, dp [][]int) int {     size := len(triangle)     if x &gt;= size || y &gt;= size {         return 0     }      if dp[x][y] != nnum {         return dp[x][y]     }      dp[x][y] = triangle[x][y] + min(         traverse(triangle, x + 1, y, dp),          traverse(triangle, x + 1, y + 1, dp),     )      return dp[x][y] }   當前最小值取決於，你往左邊還是右邊走  那我們可以把當前的數值紀錄起來  我們可以透過 dp array 進行記憶化搜索，避免重複計算  當遇到重複計算的時候，我們就可以直接返回答案   LeetCode 97. Interleaving String  題目基本上就是給你三個字串，問你說能不能用前兩個字串交叉組成第三個字串  題目本身挺單純的，你只要用 pointer 去嘗試組合出最後一個字串即可   第一版的 code 是用遞迴寫的  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 func isInterleave(s1 string, s2 string, s3 string) bool {     return dp(s1, s2, s3) }  func dp(s1, s2, s3 string) bool {     if len(s3) == 0 &amp;&amp; len(s1) == 0 &amp;&amp; len(s2) == 0 {         return true     }     if len(s3) == 0 {         return false     }      target := s3[0]      result := false     if len(s1) &gt; 0 &amp;&amp; s1[0] == target {         result = result || dp(s1[1:], s2, s3[1:])     }     if len(s2) &gt; 0 &amp;&amp; s2[0] == target {         result = result || dp(s1, s2[1:], s3[1:])     }      return result }   能不能組成 s3 這個字串，取決於 s1 跟 s2 的狀態  所以要怎麼改進呢   使用二維的 DP 陣列，紀錄每個狀態的當前結果  看起來會像這樣  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 func isInterleave(s1 string, s2 string, s3 string) bool {     if len(s1) + len(s2) != len(s3) {         return false     }      arr := make([][]int, len(s1) + 1)     for i := 0; i &lt; len(s1) + 1; i++ {         arr[i] = make([]int, len(s2) + 1)     }      return solve(arr, s1, s2, s3, 0, 0, 0) }  func solve(arr [][]int, s1, s2, s3 string, i, j, k int) bool {     if k == len(s3) &amp;&amp; i == len(s1) &amp;&amp; j == len(s2) {         return true     }      if k == len(s3) {         return false     }      if arr[i][j] != 0 {         return arr[i][j] == 1     }      result := false     target := s3[k]     if i &lt; len(s1) &amp;&amp; s1[i] == target {         result = solve(arr, s1, s2, s3, i + 1, j, k + 1)     }     if !result &amp;&amp; j &lt; len(s2) &amp;&amp; s2[j] == target {         result = solve(arr, s1, s2, s3, i, j + 1, k + 1)     }      if result {         arr[i][j] = 1     } else {         arr[i][j] = -1     }      return result }   這邊初始化二維陣列，是使用 int 的類型  原因在於要區分有沒算過，而 boolean 的 true, false. 單用 false 會有兩種語意，所以要用 int 來紀錄     0: nil   -1: false   1: true   重點在 solve function 裡面   你可以看到基本的思路是一樣的  如果當前字母相同就往下算，而這有兩種情況     s1 與 s3 字首相同   s2 與 s3 字首也相同   所以你可以看到有兩個 if  另外第 34 行有一個 fast path, 因為我們只關心成功的 case, 所以這邊檢查 !result  代表第一個 case 已經成功了，所以不用檢查第二個 case(它不會影響結果)   然後 DP 的精隨是能夠提早 return 已經計算過得資料  所以在 22 行的時候我們這樣做了   另外一個小小的東西  我一開始是定義 solve 的回傳值是 int  然後在 main function 那在判斷  不過，這樣會 TLE, 改成 boolean 可以解決(不過這部份就有點 hack 了我覺的)  基本上你只要想到能用二維陣列，這個方法已經足夠   Conclusion  動態規劃的題目有趣但難度我覺的偏高  有些題目可以先從暴力解開始(i.e. recursion)  遞迴寫出來如果 TLE 代表方向應該是對的，當你有辦法構築出遞迴公式的時候  可以嘗試 identify 出公式內的變因，再轉換成動態規劃的問題會相對容易   另外寫程式的時候我們常常透過 debugger 或是 print 大法觀察變數  在這個例子中可以是 dp array  但我親身經歷後是覺的它沒有那麼容易看懂，相鄰的 array item 不一定有直接的關係(尤其是多維的 array)  會導致 debug 過程困難   See Also     LeetCode 198. House Robber   LeetCode 213. House Robber II   References     程式設計與演算法競賽入門經典(ISBN: 978-986-347-311-4)  ","categories": ["algorithm"],
        "tags": ["dynamic programming","dp","fibonacci","coin change","recursion","leetcode-322","leetcode-120","leetcode-97"],
        "url": "/algorithm/algorithm-dynamic-programming/",
        "teaser": null
      },{
        "title": "理解 Node.js 中非同步處理與 Event Loop 的關係",
        "excerpt":"Preface  1 2 3 4 5 6 7 8 9 10 (() =&gt; {   setTimeout(() =&gt; {     console.log(1)   })   Promise.resolve().then(() =&gt; {     console.log(2)   })   console.log(3) })() console.log(4)   工程師的面試，常常會遇到這種要求給出執行順序的考題  你能正確的判斷出他的順序嗎？   What is Event and Event Handler in JavaScript  JavaScript 最早是用於網頁當中的腳本語言  它主要是為了處理 user 的 action, 像是 onClick, onChange 等等的事件  而這些事件，都需要特定的處理程式來進行處理  就是 Event Handler        ref: How JavaScript works: Event loop and the rise of Async programming + 5 ways to better coding with async/await    即使你定義了 Event Handler，因為你無法預測使用者何時為動作  所以你實際上是定義了一個 callback function  當 事件 發生的時候，這個 callback 會負責處理所有相對應的動作   First Class Function  JavaScript 的 callback 其實是 first class function  簡單來說，first class function 就是 function 擁有跟 variable 一樣的地位  比如說      Function 可以被當作參數丟來丟去   Function 可以被當成 variable 被 assign   符合 Closure   Closure  Lexical Scoping  我們都知道！ local variable 是有特定的 scope 才能存取的  在 function 裡的 variable 也是有自己的 scope   1 2 3 4 5 6 7 8 9 10 11 function createGreeter(greeting) {   return function(name) {     return `${greeting}, ${name}!`;   } }  const morningGreeter = createGreeter(\"Good Morning\"); const eveningGreeter = createGreeter(\"Good Evening\");  console.log(morningGreeter(\"Alice\")); // Outputs \"Good Morning, Alice!\" console.log(eveningGreeter(\"Bob\"));  // Outputs \"Good Morning, Bob!\"   可以看到 createGreeter 裡面回傳了一個 function  但它會 access 外部的 variable greeting  這個特性稱為 Lexical Scoping     Closure 的特性是結合了 function 以及 Lexical Scoping  上述的例子，createGreeter 形成了一個 Closure  而任何 in-scope 的變數們都會存在於 Closure 之中(i.e. Lexical Scoping)  所以這就是為什麼當呼叫的時候，仍然可以存取 greeting 變數   Node.js Architecture  JavaScript 最初是用於網頁的腳本語言  由於他是 single thread 的，他要怎麼處理 user 的相關 event 呢？  單一執行緒，勢必會 block 住整個網頁，整體的使用者體驗是不好的  瀏覽器有提供非同步的執行環境，所以不會被 block 住   但 callback 其實是會造成所謂的波動拳  所以 Promise(ES6) 以及 Async/Await(ES2017) 被引入      Libuv       ref: Design overview    要把 JavaScript 搬到 server 上執行，對應的非同步 I/O 也需要一個環境  Node.js 為此也提供了一個由 libuv 實現的 non-blocking I/O 的環境  它本身是屬於 event driven 的架構   早期，在 node 0.4 的時候是透過 libev 以及 libeio 實現的  但現在改為 libuv  主要原因是因為      libev 內部是使用 select 系統呼叫，而開發團隊對於 select 的效能表現不滿意   libev 在當時對於 windows 的兼容性並不佳   libuv 本身抽象化了不同系統平台的非同步操作，並提供統一界面  比如說 Linux 是用 epoll, Mac 是用 kqueue  在每個平台上使用各自最優秀的 polling mechanism 來達到最好的效果   Handles and Requests  libuv 的兩個核心概念是 handle 和 request   基本上使用者可以透過這兩個概念來操作非同步 I/O  handles 是一個 long-lived 的物件  透過它去下 requests   requests 就是你想要做的事情  比如說開啟一個檔案之類的   之後就交給 libuv 去監控管理那些 file descriptor  當完成之後，callback 就會被呼叫   Event Loop       ref: Event loop: microtasks and macrotasks    JavaScript 的 code 是跑在 Event Loop 上的  Event Loop 本質上會先註冊 event 的 callback(同步或者是非同步)  然後會依序執行 event queue 當中的 callback  並適時註冊新的 event callback   除此之外，所有的 non-blocking asynchronous 操作都是在 Event Loop 上執行   它真的只是一個無窮迴圈，根據 libuv/src/unix/core.c  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 int uv_run(uv_loop_t* loop, uv_run_mode mode) {   int timeout;   int r;   int can_sleep;    r = uv__loop_alive(loop);   if (!r)     uv__update_time(loop);    /* Maintain backwards compatibility by processing timers before entering the    * while loop for UV_RUN_DEFAULT. Otherwise timers only need to be executed    * once, which should be done after polling in order to maintain proper    * execution order of the conceptual event loop. */   if (mode == UV_RUN_DEFAULT &amp;&amp; r != 0 &amp;&amp; loop-&gt;stop_flag == 0) {     uv__update_time(loop);     uv__run_timers(loop);   }    while (r != 0 &amp;&amp; loop-&gt;stop_flag == 0) {     can_sleep =         uv__queue_empty(&amp;loop-&gt;pending_queue) &amp;&amp;         uv__queue_empty(&amp;loop-&gt;idle_handles);      uv__run_pending(loop);     uv__run_idle(loop);     uv__run_prepare(loop);      timeout = 0;     if ((mode == UV_RUN_ONCE &amp;&amp; can_sleep) || mode == UV_RUN_DEFAULT)       timeout = uv__backend_timeout(loop);      uv__metrics_inc_loop_count(loop);      uv__io_poll(loop, timeout);      /* Process immediate callbacks (e.g. write_cb) a small fixed number of      * times to avoid loop starvation.*/     for (r = 0; r &lt; 8 &amp;&amp; !uv__queue_empty(&amp;loop-&gt;pending_queue); r++)       uv__run_pending(loop);      /* Run one final update on the provider_idle_time in case uv__io_poll      * returned because the timeout expired, but no events were received. This      * call will be ignored if the provider_entry_time was either never set (if      * the timeout == 0) or was already updated b/c an event was received.      */     uv__metrics_update_idle_time(loop);      uv__run_check(loop);     uv__run_closing_handles(loop);      uv__update_time(loop);     uv__run_timers(loop);      r = uv__loop_alive(loop);     if (mode == UV_RUN_ONCE || mode == UV_RUN_NOWAIT)       break;   }    /* The if statement lets gcc compile it to a conditional store. Avoids    * dirtying a cache line.    */   if (loop-&gt;stop_flag != 0)     loop-&gt;stop_flag = 0;    return r; }   Event Loop Phase  上述程式碼實作，它執行了滿多東西的，但重點在所謂的 phase  Event Loop 總共有 6 個 phase      timers(uv__run_timers)   pending(uv__run_pending)   idle, prepare(uv__run_idle and uv__run_prepare)   poll(uv__io_poll)   check(uv__run_check)   closing(uv__run_closing_handles)   Event Loop 初始化的時候它會註冊所有 event 與其相對應的 callback  這些 callback 會放在各個 phase 自己獨立的 FIFO queue 當中  當執行到對應的 phase 的時候，Event Loop 會讀取 queue 中的 event 並執行相對應的 callback 直到     沒有任何的 callback 需要執行   callback 執行數量已經達到上限   timeout(preemption)   注意到 Event Loop 是從 Poll phase 開始     讀取新的 I/O event, 並執行相對應的 callback(幾乎是全部的 callback, 但有例外)   執行 setImmediate 的 callback   執行 close callback   執行 setTimeout 以及 setInterval 的 callback   pending, idle, prepare      setTimeout 與 setImmediate 如果都在 main module 裡面執行  他們的執行順序是未知的，可以確定的是他們都是執行在下一個 tick       一個完整的 Event Loop 稱為一個 tick    Event Loop Tasks  因為 main thread 的職責是執行 JavaScript  當它遇到     同步的 task(作業系統底層沒有提供非同步版本)            舉例來說 DNS 或是 File System           CPU-intensive 的 task            Crypto           的時候，就會將其交由其他人(Worker Pool(Thread Pool) or Task Offloading)來執行   Event Loop 的重點在於執行 JavaScript 的程式碼，如果遇到會卡住的 task  不論是需要等待抑或著是需要執行一段時間的程式碼，這些 task 都應該交由他人執行  不要去嘗試以任何方式卡住 Event Loop  當 Event Loop 執行順利的情況下 Node.js 就可以很好的 scale      callback 也要盡量的小，Event Loop 才能公平的 schedule callback    Worker Pool(Thread Pool)  worker pool 顧名思義就是他有很多個 worker 等著執行工作  每一個 worker 都是一條 thread  你會問，可是 JavaScript 不是 single thread 的嗎？  對，但是 worker pool 這邊是 multi thread 的  只有執行 JavaScript(或者說執行 Event Loop) 的 main thread 是 single thread      為什麼不使用 child process?  因為光是維護那些 child process 的工作量就可能導致你沒時間去管理 worker pool  最終導致 fork bomb    CPU 吃重的任務會交給 worker pool 執行(比如說數學計算)  它會透過內建的 C++ API 來做  這在某種程度上會增加 overhead(因為要從 JS 轉到 C++)  比如說 binding 的 overhead 以及溝通的成本(serialize 以及 deserialize)      針對內建沒有提供的 addon, 你也可以自己撰寫 C++ addons 來做    Task Offloading  檔案的操作通常我們可以讓作業系統幫我們監控  比如說當檔案開啟完成的時候通知我      透過比如說 Linux 的 epoll, macOS 的 kqueue    當事情完成的時候，通知回 Event Loop 並執行其對應的 callback      Event Loop 儲存的實際上是 file descriptor 而不是 events  所以才收的到 完成的訊號    Task Queue  Event Loop 除了各自 phase 擁有自己的 callback queue 之外  還有另外三種 “task queue”，分別是 nextTick Queue, microtask Queue 和 macrotask Queue   我們在看 libuv 的程式碼的時候並沒有分的那麼細，甚至都沒看到他們在哪裡  libuv 本質上在處理的都是 macrotask(像是 I/O)  你在網路上看到在說 microtask queue 這種東西  在 Node.js 當中是他是由 V8 處理的(並不屬於 libuv)，所以當然在 libuv 裡面看不到           So, only libuv provides the eventloop features in nodejs?        Yes, the only exception being the microtask queue, i.e. what Promises, which is provided by V8,   but I think it would be odd to consider that an event loop or part of an event loop (in particular,   because it doesn’t actually involve events of any kind, it’s just a queue of code that should be run in the future).       ref: event loop: which eventloop is used by nodejs? eventloop of v8 or eventloop of libuv?      回到當初破題的問題，執行順序究竟為何  1 2 3 4 5 6 7 8 9 10 (() =&gt; {   setTimeout(() =&gt; {     console.log(1)   })   Promise.resolve().then(() =&gt; {     console.log(2)   })   console.log(3) })() console.log(4)   根據 Understanding setImmediate()  Event Loop 執行的順序為 nextTick queue :arrow_right: microtask queue :arrow_right: macrotask queue      nextTick queue            包含 process.nextTick           microtask queue            包含 promise           macrotask queue            包含 setTimeout, setInterval, setImmediate           所以我們可以知道答案是 3421   首先，任何同步的 code 一定是優先執行的  因為第一行的 anonymous function 在它被定義的時候就被呼叫了(第九行)  然後最外層的同步 code 也會被執行  所以我們可以得出 34 這個部份答案   再來就是裡面的 promise 以及 setTimeout  我們知道 microtask queue 一定會先被執行，所以 2 緊隨其後  最後才是 setTimeout 的 1      setTimeout 會在 Event Loop 的最後一個階段才被執行    References     JavaScript Asynchronous Programming and Callbacks   Why are functions not considered first class citizens in C   First-class Function   Closures   LXJS 2012 - Bert Belder - libuv   Design overview   The Node.js Event Loop: Not So Single Threaded   Overview of Blocking vs Non-Blocking   【python】asyncio的理解与入门，搞不明白协程？看这个视频就够了。   【python】await机制详解。再来个硬核内容，把并行和依赖背后的原理全给你讲明白   do promises get resolved between the end of eventloop phase and before the start Phase of the eventloop?   event loop: which eventloop is used by nodejs? eventloop of v8 or eventloop of libuv?  ","categories": ["random"],
        "tags": ["nodejs","javascript","event","queue","event loop","first class function","closure","callback","microtask queue","macrotask queue","libuv","libeio","lexical scoping"],
        "url": "/random/nodejs-event-loop/",
        "teaser": null
      },{
        "title": "如何寫出好的程式碼架構",
        "excerpt":"Preface  這個議題我覺的是軟體工程師的必經之路啦  本篇文章基本上就是紀錄我目前學到的一些東西以及方法   Mindset  有意識的要自己寫出良好的程式碼是個重要的事情  無論是多麼小的專案，你都必須要 “意識” 到自己在寫什麼  但是要自己意識到什麼是好的程式碼，什麼是壞的  需要一點練習   Low Coupling and High Cohesion  一個基本的概念是 耦合性 與 聚合性   剛開始寫程式的時候你可能會寫出耦合性很高的程式  意思指的是說，你寫出來的程式碼會有很多個相互依賴的東西  白話點就是，牽一髮動全身   這種程式碼他的可維護性就會大幅度的降低  因為每一次的更改你都需要花時間修改大量的程式碼   一個好的架構應該要是 低耦合性 與 高聚合性  怎麼做到呢 一起看看吧      本文將專注在程式碼層級的解耦合，事實上還有部屬層級以及服務層級的解耦合    Start with SOLID     我稍微調整了一下 SOLID 的順序，這樣講起來比較簡單    Dependency Inversion Principle  interface 是個很神奇的東西  學校教的時候你會覺的它沒啥用處  但工作上卻又用一大堆   Example  看回熟悉的例子  In Unix, everything is a file  作業系統其中一個功能就是讀寫檔案  硬體在這幾十年間發生了許多的變化  從 磁帶、軟碟、硬碟再到固態硬碟，每個硬體的物理特性都有所不同  因此你可以期待它操作檔案的方式 “肯定是會有所不同的”   這意味著，你需要使用不同的實作對吧  比方說硬碟需要 seek, 固態硬碟則不需要  檔案系統天生需要支援多種 “硬體” 的實作  但如果直接將檔案系統寫死，這將會是個很麻煩且災難的事情   每新增一個不同的硬體，你都需要新增相對應的程式碼在 “檔案系統” 的實作當中  你可以預期程式碼的數量將會暴漲，維護會越加的困難   取而代之的應該是  檔案系統 不應該需要知道你怎麼讀檔案，寫檔案，它甚至不應該關心你需不需要移動 “硬碟讀寫投”  這就是 抽象化 的概念     看完例子，你應該多少可以感受到 “抽象化” 的重要性  但在軟體工程裡面要怎麼做到抽象化？   主要就是透過 interface 來實現了  兩個實體(class, object) 都必須依賴於 interface  彼此之間不應該知道彼此的任何細節  這樣的好處是，他們兩個之間的 耦合性 會降低  我改我的，你改你的，都不會影響到彼此(只要介面沒有改變)   軟體的演進是很快速的，所以實作是容易改變的  但是 interface 不應該被改變  所以你可以放心的隨便改你的 “實作”, 只要它符合 interface 的標準都是可以的   舉個例子來說  你的 service layer 與 database layer 之間應該要有一層 interface  這層 interface 就是兩個不同的實體的公約      service layer 不會管你是怎麼 “儲存我的東西的”  你要用 database, redis cache 還是簡單的 in-memory cache 都跟我沒關係(I don’t care)  只要符合 interface 的標準，我 service layer 都能夠兼容   實務上，你也看到了  這對測試是有極大的幫助的  在測試的時候我可以替換成 mock 的實作，這樣測試就會更輕鬆了(你不需要真的連線到資料庫)      有關測試的部份可以參考 DevOps - 單元測試 Unit Test | Shawn Hsu    Open/Closed Principle  在 Dependency Inversion Principle 中我們提到  使用 interface 當成兩個實體之間的守則是個好的 practice  我們也提到，軟體開發常常會更改東西，只要改動符合 interface，我們就不需要更改太多部份的程式碼(因為低耦合)  實作很常會改動，但是 interface 不應該常常改變(應該說 modify)   其實我已經講完 Open/Closed Principle 了   他想表達的東西就如上所述  對於 \"新增\" 功能，我們是樂見的，因為軟體很常會有新的功能需要加入  但是對於 \"修改\" 功能，如果新增功能會 導致必須要修改現有的實作，我們是不樂見的   修改指的是，refactor 現有的實作以兼容新的功能  好的架構應該是你可以在原本的基礎上直接擴充功能的  但這建立在這個基礎是好的  不過適度的重構我覺的並沒有違反這個原則(當然一次改一堆的不算就是)   Interface Segregation Principle  依賴於共同的 interface 的好處我們已經了解的差不多了  只不過要小心 interface 與實體之間的必要性   簡單講就是，如果實體內部包含了一個它根本不需要的東西  我們就不應該去依賴於它   好比說一個大學生的包包裡面帶了小學英文課本  這根本是不需要的，所以你可以把它拿掉   interface 裡面包含了不必要的定義的時候  那就代表你應該定義一個新的 interface 而不是使用這個巨大的 interface   Single Responsibility Principle  一次只對一個人負責，什麼意思呢   舉個我真實遇到的例子  我要替新的 API endpoint 撰寫 middleware 進行 validation  一開始我把全部的欄位的驗證都寫在一個 function 裡面  很明顯的他是有問題的  因為它對不只一個人負責  不同的欄位應該要由不同的驗證 function 進行處理   分開寫好處當然就是好維護對吧  並且解耦合了   Liskov Substitution Principle  這個原則相對起來我覺的比較抽象  不過他的重點在於，所有衍生的子類別，都必須要符合父類別所定義的 “正確行為”   書中提到的例子是 正方形 與 長方形  正方形是長方形的子類別，但是正方形的特性是 四邊長相等  兩個不同的子類別，他們的特性是不同的  所以你不能夠將正方形當成長方形使用，因此不應該存在這樣的繼承關係   如今這個原則已經不再限訂於 class 了  interface 也同樣試用   Scalable Code  讓我直接用例子帶會比較好說明  給定一個需求，後端需要從其他服務取得資料，然後回傳給前端  為了避免前端等待，我們需要將這個工作放到背景執行  並且因應業務需求，同一時間內只能被 trigger 一次   針對同一時間只能有一個執行這點  你可以很輕易的得出使用 mutex 來達成  這樣就可以保證同一時間只有一個 request 在執行  但是這樣做並不 scalable   因為 mutex 的 scope 是在同一個 process 內  如果今天服務爆量，你會選擇 scale out 你的服務(i.e. 也就是 replica)  那這樣每個 replica 都會有自己的 mutex  是不是就會造成同一時間內有多個 request 在執行了？      有關 scale out 可以參考 資料庫 - 初探分散式資料庫 | Shawn Hsu    在寫程式的時候，你的程式碼不僅僅要正確做動，而且還要將未來的擴展性考慮進去  以本例來說，你可以考慮實作 distributed lock  用 Redis 寫一個 value 當作 lock  如果它不存在，你就可以進 critical section      有關 Redis 可以參考 資料庫 - Cache Strategies 與常見的 Solutions | Shawn Hsu    不過，你用 Redis 你還要另外考慮 recovery 的問題  因為 backend server 可能因為各種原因掛掉，包括但不限於 nil pointer, panic, etc.  回復的時候，你的 lock 可能會一直存在，但是伺服器因為被重啟過所以 background task 並沒有執行  導致你的服務狀態處於 middle stage(當然你 Redis key 給個 TTL 就可以解決這個問題)   在構思架構的時候，這些問題也都要被納入考量範圍   Avoid Anti-pattern  我自己在寫 code 的時候，有時後會踩到一些 anti-pattern  簡單的方法是，閱讀其他人寫的 code  學習他們的寫法，你可以了解基本的 best practice      自己學很容易學歪，所以看別人的程式碼很重要  參與 open source 的專案也是一個方法    像我自己有時後會誤用了某個東西  這很常是因為不足夠熟悉所造成的  除了多練習以外，你也應該要多唸書   當然 code review 也能夠及早的發現這些 anti-pattern 的存在  並且能夠即時的修正   Utilize AI Tools  2024 的今天，已經有許多 AI 可以輔助我們寫程式了  善用工具除了可以提高開發速度之外，也能夠幫助我們寫出更好的程式碼  比如說我現在就在使用 GitHub Copilot 自動補齊程式碼  也能夠利用它做基本的 code review  即使你是自己開發，也能夠有一個基本的 code review 功能   References     Clean Architecture 無瑕的程式碼(ISBN: 978-986-434-294-5)   Clean Code 無瑕的程式碼(ISBN: 978-986-201-705-0)   The Clean Coder 無瑕的程式碼(ISBN: 978-986-201-788-3)   軟體架構原理 工程方法(ISBN: 978-986-502-661-5)   Distributed lock for minIo   Handling Mutexes in Distributed Systems with Redis and Go   Distributed Locks with Redis  ","categories": ["random"],
        "tags": ["solid","architecture","couple","cohesion","scalable","ai"],
        "url": "/random/solid/",
        "teaser": null
      },{
        "title": "資料庫 - 從 Netflix 的 Tudum 系統看分散式系統中那些 Read/Write 問題",
        "excerpt":"Data Consistency  Eventually Consistent  在分散式系統中，根據 CAP Theorem 我們知道  AP 系統，沒辦法保證所有節點在收到相同的資料的時候維持一致(因為還沒同步完成)  所以這類系統提供的保證通常都是 Eventually Consistent  也就是他最終會趨於一致，只是時間不好說   也因此這個保證是非常弱的  那最強的保證是什麼？ 是 Strong Consistency  強一致性的系統(又稱 可線性化的系統)，在高階角度看下來就好像只有一個資料副本，並且所有的操作都是 Atomic 的  也就是說他並不會出現一些奇奇怪怪的狀況，例如說寫進去的資料過一段時間才出來之類的(近新保證)      一個可線性化的系統(i.e. 強一致性)，本身就會提供近新保證  單台資料庫並不一定可以線性化，要看隔離機制  比如說 snapshot isolation 不能保證 linearizability，因為 snapshot 不能讀到比他新的資料       有關 isolation 的介紹，可以參考 資料庫 - Transaction 與 Isolation | Shawn Hsu    但是，最終一致性與強一致性，跨距太遠了  我們需要有一些介於兩者之間的保證，如 Read-after-Write, Monotonic Read, Sequential Consistency, Same Prefix Read  這些都在一定程度上保證了資料的一致性   Read-after-Write Consistency  一個常見的問題是，我寫入的資料，我馬上讀取，卻讀不到  而原因在於你寫入與讀取的 replica 可能是不同台機器，資料還沒有同步這麼快  對於使用者來說這無疑是很奇怪的，我應該要能夠看到我剛剛做的改變   Read-after-Write 保證了，你寫入的資料，你馬上讀取，就會讀到  但是對於別人的資料，就無法保證   解法可以針對自己的資料，讓他讀取 leader 的 replica，這樣就保證不會有未同步的問題  缺點是當讀取自己的資料量大的時候，速度就會變慢了   Monotonic Read  更糟糕的是，如果多次查詢，返回的結果不一致，這可能比查不到還要糟糕  比如說，你查詢一個商品的庫存，第一次查詢是 10，第二次查詢是 0，第三次又是 10  那他到底是有還是沒有？ 這種 時間倒流的現象 是 Monotonic Read 想要避免的問題   問題同樣也在讀取不同的 replica 資料同步問題  解決的辦法也滿簡單的，你只要確保，該 user 的所有 request 都是由同一個 replica 處理就好了  比如說用 hash function 將特定的使用者全部導向特定的機器上  稱之為 Monotonic read      為什麼叫做單調讀取？ 因為當你讀了新的資料，就保證不會讀到舊的    問題是出在讀取不同的 replica 資料，那解法很自然就是讀取相同的 replica   Sequential Consistency  也同樣是順序，不同的是 Monotonic Read 是只保證 你 不會讀到舊的資料  而 Sequential Consistency 是保證全部的節點的資料都會有相同的順序   Same Prefix Read  假設資料有順序性或者說因果關係  讀取不同 replica 的資料，也同樣是遇到同步問題，導致使用者會看到牛頭不對馬尾的資料  比如說留言板，留言的順序是有因果關係/時間關係的  資料時間 與 資料寫入時間 的順序不一定是一樣的   一樣是因為讀取不同 replica 資料造成的  解法可以讀取相同的 replica, 或者是依靠時間戳記，但時間並不可靠，可參考 Unreliable Clock   Netflix Raw Hollow System for Tudum  Netflix 的 Tudum 網站提供了一些獨家專訪，花絮以及特別收錄的內容  讓使用者可以更深層的探索他們最喜歡的影視作品  這個系統主要的角色就是 內容編輯者 以及 檢視者(使用者)        ref: Netflix Tudum Architecture: from CQRS with Kafka to CQRS with RAW Hollow    一開始是由編輯者編輯一些有趣的內容比如說花絮照片等等  透過他發布到 CMS 系統內做儲存，同時也同步到 Ingestion 服務拆解內容並轉換成讀取優化的資料(原因在於這些內容需要根據不同使用者做客製化)，透過 Kafka 發布進一步處理，並儲存在一個獨立的高可用性的資料庫內  每當使用者查看內容的時候，Page Data Service 會讀取這些拆解過後的資料，重組成客製化的資料並呈現給使用者看  為了更快速的處理資料，internal cache 的方案被採用，為了降低從資料庫讀取的時間      有關 kafka 可以參考 資料庫 - 從 Apache Kafka 認識 Message Queue | Shawn Hsu    這個系統可以很好的工作，讀寫分離讓他可以很輕鬆的擴展  非同步的資料處理也可以很好的使系統達到高可用性  架構上屬於 Event driven   你可以看得出來，非同步的方式意味著這個系統是提供 Eventual Consistency 的保證  而 Netflix 團隊發現到，他們犧牲了能快速預覽內容的方便性，即使該系統提供了足高的可用性  Page Data Service 為了能夠更快速的取得資料，其內部擁有 near cache 的機制  這個內部的 near cache 可以在背景執行同步更新，每隔一段 refresh cycle 資料就會從 KVDAL(Key Value Data Abstraction Layer) 同步  那他造成的問題就會是，當資料量大起來的時候，使用者會看到過期的資料   也就是說目前 Event Driven 的架構是沒有近新保證的   Raw Hollow System  為了解決這種近新保證的問題，Netflix 團隊開發了 Raw Hollow(Read After Write Hollow) System   Raw Hollow 加強了 Hollow 系統  系統如其名，他擁有 Read-after-Write 的近新保證，並且允許更新 near cache 的資料      Raw Hollow 也自動滿足 Eventual Consistency 的保證(因為它提供了 Read-after-Write 的保證)    Architecture       ref: Introducing RAW Hollow: An In-Memory, Co-Located, Compressed Object Store with Opt-In Strong Consistency    架構上你可以看到，與 Hollow 系統不同的是，source of truth 不見了  意味著任何 local client 都可以透過 writer 更新 near cache 的資料   整個系統的資料包含了兩個部分     base dataset   in-flight changes(還沒被正式寫入的資料)   角色的部分     writer: 負責處理寫入資料(single leader 可參考 資料庫 - 初探分散式資料庫 | Shawn Hsu)   local client: 負責處理讀取資料   producer: 負責推播新的資料(非完整 dataset, 僅包含差異資料)   logkeeper: 儲存資料的暫存的地方，1GB 大小的 circular log   整體的操作圍繞在 base dataset 之上  每個 local client 都可以更新 base dataset 的資料  他們需要透過發送 request 到 writer 來更新   writer 也具備高可用性(使用 ZooKeeper 來協調)，同一時間只有一個 writer 負責寫入  當他意外下線，其他 hot standby writer 會接手   資料會暫時的儲存在 logkeeper 內  writer 會將資料同步至多個 logkeeper 內，直到所有 Quorum 內的 logkeeper 都擁有資料  才會視為操作成功   後續 producer 將會從 logkeeper 取得資料，計算差異化資料並推播至 local client 身上   Writer Self Healing  writer 會等待 所有 Quorum 內的 logkeeper 都擁有資料 才會視為操作成功  所以如果寫到一半, 即使 writer 掛掉，部分 logkeeper 內還是會有暫存資料對吧  所以新的 leader 上位，他需要跟所有 Quorum 內的 logkeeper 取得 in-flight changes 的資料  最大化的避免 data loss   如果所有 logkeeper 都掛了呢？  producer 每隔 30 秒就會上傳完整的 in-flight changes 至 S3  即使全掛，資料損失也降到最低，對於 Netflix 來說，這樣的損失是可以接受的   Constantly up-to-date  你說，透過 pub/sub 接收推播資料，這也有延遲不是  因此，為了能夠極大化的接收到最新 base dataset 的資料  local client 其實會偷跑   我們說 Raw Hollow 資料由 base dataset 以及 in-flight changes 組成  除了 pub/sub 過來的 base dataset 以外  local client 會嘗試透過 Long Polling 的機制將處理中的 in-flight changes 同步過來      有關 Long Polling 可以參考 淺談 Polling, Long Polling 以及其他即時通訊方法論    那如果 logkeeper Quorum 不滿足怎麼辦  Quorum 會動態調整，使得 Quorum 內的 logkeeper 始終 Strongly Consistent   Hollow System  Hollow 是一個分散式的 in-memory near cache 的系統，透過將資料壓縮至記憶體內，並允許應用程式快速的讀取  透過讀取 source of truth 的資料，透過 pub/sub 的機制同步到不同的節點上  注意到，Hollow 系統是一個 read-only 的系統，它只允許讀取資料，不允許寫入(或者說更改)資料        ref: Introducing RAW Hollow: An In-Memory, Co-Located, Compressed Object Store with Opt-In Strong Consistency    Adoption of Raw Hollow System  乍看之下 Raw Hollow 好像跟原本的 near cache 沒什麼兩樣  但要注意的是 Raw Hollow 系統內部儲存的是 “完整的資料集”(被壓縮過)  他本質上是資料庫而非 cache  而 Raw Hollow 系統帶來了以下的好處     每個節點可以儲存高達 1 億 筆資料，因為壓縮的資料使得整個資料集可以載入記憶體當中   快速的存取資料，減少 I/O 的消耗   減少了資料傳遞的等待時間   最終的 Tudum 架構如下      ref: Netflix Tudum Architecture: from CQRS with Kafka to CQRS with RAW Hollow    Read/Write Phenomena  單一節點的讀寫異常，我們在 資料庫 - Transaction 與 Isolation | Shawn Hsu 已經看了滿多的  但在分散式系統中，事情更複雜了，尤其是在 multi-leader 以及 leaderless replication 的情況下   雖然 multi leader 會增加系統的複雜度  但是在某些情況下，multi leader 會是一個不錯的選擇  single leader 因為每個 partition 只能有一個 leader，所以所有寫入都必須要經過他  這顯然在某些情況下會增加延遲，這並不是我們想看到的      可參考 資料庫 - 初探分散式資料庫 | Shawn Hsu    但要怎麼處理 multi leader 帶來的衝突問題？  以我們熟知的版本控制工具 git 來說，通常是我們要手動處理衝突  分散式系統下，不是不行啦 但我們有更好的方法   Causality and Ordering  要解決衝突的問題，需要先確定資料相互之間的關係   因果關係 與 事件的順序關係 是不同的概念  對可線性化的系統而言，兩者是等價的  不過分散式系統下會變得複雜   每個事件到達節點的速度是不同的  有可能造成明明是 A :arrow_right: B，卻因為網路延遲等問題  變成 B :arrow_right: A   這時候，節點收到的順序關係為 B :arrow_right: A  但是真正的因果關係為 A :arrow_right: B  很明顯這不對，因此可以利用 Lamport Timestamp 或者 Version Vector 來確定因果關係   Lamport Timestamp  既然用絕對時間會有誤差(事件抵達的時間不一定代表因果關係)，我能不能使用類似的方法  使用一個單調遞增的數字表示事件的先後順序，數字小的先發生，數字大的後發生(i.e. logical clock)   Lamport Timestamp 是類似的概念  他由 (node id, counter) 所組成  counter 就是那個單調遞增的數字，針對每一次對 x 的操作我都 counter++  這樣一來，藉由排序 counter 我就可以知道事件的先後順序，也知道是誰做的改變(根據 node id)   遇到衝突的時候  只要找到最小的 counter 就是最早發生的事件，就可以以它為準  也因為該 timestamp 有 node id 的資訊，你也知道資料該從哪裡拿   看似美好但執行起來會有點小問題  因為 Lamport Timestamp 是存在在廣大的叢集裡面  要知道順序，唯一的辦法只有從系統當中收集所有資訊才能判斷  也因此他所花費的時間很多，況且萬一其中一個節點掛掉，你就很難找到真正最早發生的事件   並且 Lamport Timestamp 無法確認 並發 的狀況  因為事件都是用 counter 來表示，是單調遞增的，它只能確定因果關係   Version Vector       ref: 《Designing Data-Intensive Applications》ch 5—Replication    簡單來說，就是替每個 key 維護一個版本號  藉由版本號，系統可以知道依賴關係(注意到不是順序關係)  比對版本號，你可以知道淺在的衝突，但 version vector 本身 並沒有辦法解決衝突  他只是會儲存所有已經寫入的資料，並提供一個機制讓你可以知道這些資料的因果關係      注意到他跟 vector clock 不是一個東西  vector clock 是用於確認事件的先後關係  version vector 是用於確認資料的因果關係(使用者發文前必須要有帳號)  也可以使用 logical clock(單調遞增的計數器) 來達到類似的效果    Conflict Resolutions and Prevention  Last Write Wins  把最後一筆寫入的資料當成是正確的資料是一種做法  但是分散式系統下，每一台機器的時間可能會有誤差，幾毫秒甚至幾秒，這些誤差會導致你無法判斷誰是最後一個寫入的      時間戳記不一定代表事件的先後順序，可參考 Causality and Ordering    刪除舊的資料這件事情並不會收到任何的通知  所以在別的節點看來會有部分資料神秘地消失了   除此之外，Last Write Wins 無法判斷事件的先後順序  順序寫入 與 並發(concurrent) 在時間上的表示都是類似的  所以你會需要額外的機制判斷資料的因果關係(如 Lamport Timestamp 或 Version Vector)      並發是兩個事件是獨立的，互不干擾沒有因果關係    為了避免資料被安靜的刪除  如 Cassandra 推薦每個寫入 assign 一個 UUID   MVCC(Multi-Version Concurrency Control)  如果要徹底的防範衝突，使用 Transaction 是最佳的選擇  如果多個交易對相同資料進行改動，最終只有一個會成功   Transaction 底層實現實際上就是依靠 MVCC  每個 transaction 交易期間需要查看不同時間的的資料狀態  為了因應這種需求，不同時間點的資料的狀態需要被記錄下來  而這種技術稱之為 MVCC(Multi-Version Concurrency Control)      有關 MVCC 的介紹，可以參考 資料庫 - Transaction 與 Isolation | Shawn Hsu    如果遇到同時寫入相同的資料，只有一個會成功其他失敗(i.e. optimistic locking)  如果是多個人讀取相同資料，其實不需要 lock  這樣可以最大化的提昇系統效能      有關 optimistic locking 的介紹，可以參考 資料庫 - Transaction 與 Isolation | Shawn Hsu    所以其實是依靠 Transaction 與 MVCC 共同實現的   Atomic Broadcast  MVCC 確保在寫入的狀況下可以保證一致性  這是基於被 snapshot 的資料順序是正確的  可是 MVCC 本身沒有保證資料順序是正確的阿？ 它只能解決衝突問題   順序的解法如 Causality and Ordering 提到的，可以使用 Lamport Timestamp 或 Version Vector 來確定因果關係  不過那都是事後論的，並且有些仍然自動修復   既然問題在傳遞到各個節點的延遲導致的順序不一致  那是不是     確保每個更新都可以正確的傳遞到每個節點，全部都收到或是全部都沒收到   每個更新都嚴格按照順序進行傳遞   就能夠解決 Causality and Ordering 提到的問題  而這就是 Atomic Broadcast 的概念      如果更新沒收到要怎麼辦？ 那簡單阿重試就好了  如果收到 4, 但是 5 還沒到就收到 6 的時候，你必須要等 5 收到  這樣才能保證每個更新傳遞的順序是正確的    Transaction  資料庫 - Transaction 與 Isolation | Shawn Hsu 裡面我們知道了各種 Transaction 的細節  重點在 unit of work 對吧，我希望全部的操作都是一起成功或是一起失敗  分散式系統下，你的操作不一定會在同一個資料庫下，那你要怎麼保證 unit of work?   舉例來說，新的使用者註冊服務需要     user table 新增一筆資料   permission table 也新增一筆資料   我們學過 partition 以及 replication  所以當上述的 table 不在同一個地方的時候，你要怎麼做 transaction 呢？   Two-phase Commit(2PC)  核心想法是，他依然是 transaction，只是他的 commit 在不同的機器上完成的  為了追蹤每一台機器上的 transaction 的狀況，你需要一個 coordinator(i.e. transaction manager)  他會決定要不要執行這個 transaction，具體來說他會這樣做   他會問每一台參與交易的資料庫，你能不能執行這個 transaction  為什麼會有不能執行的問題？ 可能是因為 unique constraint 這種東西或者是其他外力影響(網路斷線，硬碟滿了 … etc.)導致無法 commit  如果有一台資料庫回覆說不能執行，那這個 transaction 就會被取消      注意到，2PC 要求 所有人 都同意才能 commit    反之，如果每一台資料庫都確定可以執行，那 coordinator 會發送 commit 的指令  然後每個人都個別做 commit     所以所謂的 2 phase commit 指的是     coordinator 確認每個人都可以 commit(prepare phase)   由 coordinator 發送 commit 指令(commit phase)      coordinator 扮演著類似 transaction manager 的角色  所有節點的 commit 都會由 coordinator 控制，並 assign 一個全域的 transaction id    Issue  2PC 有一個問題，就是當 coordinator 掛掉的時候，整個 transaction 就會陷入一個自我懷疑的狀態  前面提到，2PC 要不要執行交易，是由 coordinator 決定的  也就是說，只有 coordinator 知道要不要做，參與交易的節點是被動的接收指令的  節點接收不到指令，他就不知道要不要 commit，這就是 2PC 的問題   那節點這時候會做什麼？  nothing, 等待 coordinator 的回應 如此而已  所以變成 single point of failure 對吧      對於有一些節點已經 commit 了的情況，這時候 coordinator 掛掉，這些節點就會一直等待，直到 coordinator 回來  恢復了之後，因為 coordinator 有保留之前的決策，所以 2PC 可以保證 eventual consistency    Three-phase Commit(3PC)  3PC 為了解決 coordinator 掛掉的問題，與其讓節點等待，而且不知道要等多長時間  不如讓節點擁有部分選擇的權利   所以 3PC 會多加一個 phase，叫做 prepare-to-commit  順序是 prepare phase -&gt; prepare-to-commit -&gt; commit phase  在真正 commit 之前，他有一個等待的時間，而 3PC 假設這個時間是有限的  我跟你說，我已經準備好 commit 了，但是我要等一下，等 coordinator 給我 commit 的指令  如果 coordinator 一直沒有回應，我就自己 commit(因為 prepare phase 的時候大家都同意可以執行交易)  這個等待的時間有兩個好處     維持原本 2PC 交給 coordinator 判斷的權利   避免無限等待的情況(非同步處理)   不過分散式系統中，假設擁有有限的等待時間這件事情是錯誤的  有時候單純是因為網路斷線，導致 coordinator 暫時無法回應  這時候 3PC 的缺點就顯現出來了   如果 coordinator 決定 abort  但是因為網路問題，節點 A 沒有收到指令  根據 prepare-to-commit 的結果是 commit，所以它需要等待一段時間  但是因為網路斷線，節點 A 收不到指令  所以就自行 commit   然後你的資料就會出現不一致的問題了   也就是說，前面提到 3PC 假設等待時間是有限的這句是錯誤的  它就只是突然掉個線，你就說它壞了 不對嘛  所以這個假設本質上是要求有一個 完美的故障檢測器  能夠區分暫時網路問題或者是節點真的掛了的狀況   Unstable Network  在分散式系統中，每個節點多為使用網路互相連接起來的  然而網路實際上是不可靠的      網路比你我想像的更不可靠    想想看以下這些問題     請求/回應 丟失，封包在路上掉了   節點失效，你不知道你的請求有沒有正確的被處理   節點暫時停止回應，可能因為目前 request 太多，處理的速度變慢   對於 client 來說，以上這幾種狀況是沒辦法分別的  你唯一能確定的只有，我沒收到回應  那在這種情況下你要怎麼處理？   寫過 web application 的你可能在處理 router 的時候，有寫過類似 timeout 的東西  他的目的也挺簡單的，就是當我超過多久時間沒回應的時候，我就不等了，直接回一個 error  對於分散式系統來說，這也是一個可行的方法  只不過他的 timeout 要怎麼設定？     固定 timeout 時間            有可能節點只是太忙所以導致回應時間變慢，如果它已經成功寫入，但卻被你回了一個 error，很明顯這是不對的       如果我們認為你一直 timeout 是因為節點已經失效了，把你的請求轉移到其他節點，那不是更糟嗎？           動態 timeout 時間            動態的設定 timeout 時間是不是顯得合理多了？       透過實驗或是動態測量網路 round trip 的時間，動態設定，是不是就能夠取得平衡了           Unreliable Clock  有玩過一些單機遊戲的你，可能有發現一個可以偷吃步的方法  有一些遊戲不是會有所謂的每日登入獎勵嗎？ 其實你可以透過更改手機或電腦的時間，然後拿到獎勵  這很明顯的是一個開發商的失誤，不過藉此你也可以發現，用時間來判斷一些東西顯然不是這麼的合適      有關時鐘的介紹，可以搭配 Linux Kernel - Clock | Shawn Hsu 一起參考    不同電腦的硬體時鐘可能會有些微的差異(millisecond 之類的)  但即使是在我們看來是這麼微小，每天的誤差可能越來越大，對於精準度極為要求的環境下仍然是不可忽視的      可以使用 NTP(Network Time Protocol) 來同步時間    舉例來說，multi-leader replication 的情況下允許多個節點同時寫入  那它極大可能會出現衝突的情況，上面我們有提到說可以使用 Last Write Wins 或 Version Vector 等等的解法去處理衝突   錯誤的時鐘，有可能會導致錯誤的資料被寫入，而這種錯誤是無法被感知到的   Split Brain  節點的故障有時候是無意的   比如說在 single-leader architecture 下  leader 可能會因為短暫的不可用(GC 導致 stop-the-world, 瞬間流量太大導致處理回應時間變長或是網路異常 … etc.)  而被降級成 follower  但是 old leader 可能沒有意識到它不再是 leader  這個時候就會出現 2 個 leader 的情況  這種情況稱為腦分裂(Split Brain)   有一個解法是這樣子的  透過簡單的 fencing token, 判斷你是否為合法的 leader  其他 follower 會透過這個 token 去尋找合法的 leader  因此，大家都知道誰是真正的 leader      這個 token 本質上就是一個單調遞增的數字，數字大的擁有者就是 leader    問題來了  old leader 恢復上線，並開始運作  這時候 follower 會告訴他說，根據 fencing token 你已經不再是 leader  因為 token 不一樣了，現在的 leader 是別人(token 的擁有者)  透過這樣的機制就可以避免腦分裂的問題   Byzantine Fault  節點的故障有時候是無意的 比如說在太空的環境下  輻射滿天飛，這時候你的硬體是有可能出現不如預期的情況的  這時候你要怎麼修正？   這種狀況通常是依靠特殊能抵抗輻射的硬體或者是準備多台硬體來進行故障恢復     有時候，節點會故意發送錯誤的訊息，意圖摧毀你的系統  這種時候，帶有惡意的故障，被稱為 Byzantine Fault   你說有哪些無聊的人要攻擊你的系統？  以區塊鏈來說，攻擊你的網路，我就可以把別人的錢轉走，這就很有可能了對吧？  著名的 51% Attack 就是典型的 Byzantine General Problem   故事大概是這樣子  不同的將軍想要執行一個作戰計畫  由於將軍們所在地點並不相同  訊息的傳遞都是由傳令兵完成的  如果傳令兵想要叛變，它完全可以發送虛假的消息，欺騙其他人      有關 51% Attack 可以參考 從 0 認識 Blockchain - 區塊鏈基礎 | Shawn Hsu    References     資料密集型應用系統設計(ISBN: 978-986-502-835-0)   宇航级CPU是如何做到抗辐射的？中美间有多大差距？    Introducing Netflix’s Key-Value Data Abstraction Layer   Netflix Tudum Architecture: from CQRS with Kafka to CQRS with RAW Hollow   Introducing RAW Hollow: An In-Memory, Co-Located, Compressed Object Store with Opt-In Strong Consistency   NONBLOCKING COMMIT PROTOCOL  ","categories": ["database"],
        "tags": ["database","distributed","cluster","byzantine fault","split brain","network","clock","monotonic read","vector clock","version vector","last write wins","lamport timestamp","transaction","2PC","3PC","netflix","tudum","raw hollow","hollow","read after write","eventually consistent","linearizability","zookeeper","atomic broadcast","sequential consistency","2pc","3pc","same prefix read","atomic broadcast"],
        "url": "/database/database-distributed-issue/",
        "teaser": null
      },{
        "title": "資料庫 - 最佳化 Read/Write 設計",
        "excerpt":"Preface  雖然說只要有錢都好辦事，但多數情況下我們都是沒錢的  因此學習如何最佳化是相對重要的事情  那麼有哪些是我們可以透過內部盡量去優化的呢？   Index  由於 Index 實屬過於複雜，因此我將其拉出來獨立一篇做探討  詳細可以參考 資料庫 - Index 與 Histogram 篇 | Shawn Hsu   Sharding(Table Partitioning)  資料表分割，顧名思義就是將一個 Database table 分割成若干部份  這樣做有以下好處     如果資料量很大，將 table 分割可以增加查詢效能(因為一個 子 table 裡面的資料量較少, index 數量減少，就可以變快)   針對被頻繁存取的資料，可以將其放置於存取效能較佳設備之上(e.g. SSD), 反之則可以放在速度較慢的裝置上(e.g. 磁帶)      單一節點下，Sharding 的作用可以體現於 寫入 效能提昇  針對 讀取 的部份，可以搭配 Replication 下去使用  可參考 資料庫 - 初探分散式資料庫 | Shawn Hsu         ref: System Design — Sharding / Data Partitioning    How to Split Table  分區看似簡單，實則有許多需要注意的地方  我們的最終目的是讓 每個分區都能被充分利用  不希望出現有些人很忙，有些很閒的情況(skewed)   Key Based  你可以依照 key 進行分區  舉例來說，Clustered Index  回顧 資料庫 - Index 與 Histogram 篇 | Shawn Hsu  clustered index 是 unique 的，也因此它永遠可以指到 一筆資料                          Key Based       Hashed Key Based                       Picture                               Picture Reference       Oxford English Dictionary       Consistent Hashing in Action                 Description       就像牛津英語辭典一樣，把一本大的字典，分成很多本(A,B 一本，C,D,E,F 一本)       透過將 key 雜湊一遍，可以平均的分配鍵值(i.e. consistent hashing)              使用 hashed key based 要注意的是，它必須要是一致性的雜湊演算法  意思就是給定 x 一定會得到 y, 有些演算法沒辦法達成這個，可參考 Rebalancing    雖然我們可以利用不同種方法  試圖平均的去分散資料  但總有那麼幾個時候，還是會出現不平均的狀況  所以會出現忙碌程度不一，很忙的那個節點，稱為 hot spot   Content Based  Key Based 除了會有 skewed 的情況會導致查詢效率不佳  針對 內容查詢 的部份它也沒辦法   比如說你要查詢在某某期間所發表的文章  很明顯的，這是針對 content 進行 query 的(created_at)  一般來說，為了最佳化這種查詢，你可能會加上 index(Non-Clustered Index, 可參考 資料庫 - Index 與 Histogram 篇 | Shawn Hsu)  在分區的狀況下，因為沒辦法確認到底哪台上面有符合的資料，你需要 查詢所有分區，才能確定所需的資料(稱為 Scatter-Gather Queries)   為了應付這種 content based 的 query  是不是可以預先把結果算出來？   符合，舉例來說，2022/01/01 ~ 2022/01/31 的資料有 1, 23, 77, 319 這些資料  把它儲存起來，當下次要查詢相同內容時，就找到這些算好的結果，再往下查詢即可  所以以上可以歸類為兩種                          Document Based       Term Based                       Description       針對內容建 index       針對 部份符合內容 建 index 例如: 文章標題包含 xx 字詞           常見的 content based 有可能是以下幾種     依照地區劃分，亞洲區、歐洲區 伺服器   依照存取頻率，可能會遇到 Celebrity Problem(名人通常流量都很高，這時候把它跟其他人放在一起可能不是一個很好的選擇)   另外因為是重建 index 嘛  所以 index 的部份也有區分成兩種                          Local Index       Global Index                       Picture                               Picture Reference       Chatper 6. Replication       Chatper 6. Replication                 Description       每個 node 只關心節點上的 index       建立全域的 index 在單一節點上(實務上還是會針對 index 進行分區，不然就沒意義)                 Pros       每個節點維護數量小，維護成本低       運算過的資料已經存在某個地方了，僅須查詢少量節點即可拿到答案，所以速度更快                 Cons       需要查詢所有節點，Scatter-Gather Queries       更複雜，更慢 每次資料更新，都要更新 index(而它可能散佈在不同節點上)           Rebalancing  前面提到節點可能會有 hot spot 的存在，當出現這個不平衡的狀態的時候  你應該怎麼做？      hot spot 不一定是因為你分區沒做好  有可能是 request 突然爆增，節點突然掛掉 … etc.    慶幸的是，現今資料庫系統都有成熟的解決方案可以使用  你不必真的跳下去實作，不過了解其手段還是挺有必要的      值得注意的是，自動平衡在某些情況也會造成問題  萬一我只是回應的比較慢，有可能被認為節點掛掉，然後就自動進行 rebalancing  造成的 overhead 損失會很大，因此多半採混合模式，亦即需要人工確認等操作    我們想要做的是，盡可能讓每個分區的 負載都平均  已知 總共資料大小 = 節點數量 * 節點分區數量 * 分區大小  我們可以控制     節點數量            新增一個節點，將某一些 hot spot 的分區搬到新的 node 上面(或是縮減分區大小)       這個方法可以依照 節點比例進行分區           節點分區數量            分區大小太大或太小都會有一些 trade off, 最好的狀況是視情況自動改變，使得分區大小都 維持在固定區間       如此一來，分區數量就也必須要自動調整(稱為 動態分區，反之則為 固定分區數量)           分區大小            分區太大會導致重新平衡(or 故障恢復)的過程花費太久       分區太小，overhead 會太大(e.g. 額外硬體費用，延遲高 … etc.)       固定分區大小呢，彈性又不夠             重新平衡多半需要在不同機器上搬移資料  如果你是用 cloud provider 如 GCP  不同 node 傳輸資料是 需要額外花費的   因此在 rebalance 的時候，你肯定希望搬遷資料的費用越少越好  如果是使用 hashed key based 的方法進行分區，那你必須確保 hash function 的結果會是一致的   舉例來說，你要將 10 筆資料分佈在 4 台機器上  如果用 modulo，你會得到以下結果                  Machine       Data                       0       4, 8                 1       1, 5, 9                 2       2, 6, 10                 3       3, 7           假如你選擇 scale down 變成 2 台                  Machine       Data                       0       2, 4, 6, 8, 10                 1       1, 3, 5, 7, 9           你可以發現大部分的資料都需要進行搬移(e.g. 資料 2 要從 2 搬到 0)  很明顯這樣會造成額外的開銷(不論是金錢還是系統可用性)  也因此選擇良好的一致性雜湊演算法是很重要的   Prepare Statement and Store Program  除了對 table 加 index 加快查詢速度之外，cache 也會是一個很好的選擇  對於常用的 SQL statements, server 會轉換成 internal structure 進行處理, 我們可以讓伺服器 Cache 住這些 structure(這樣在同一個 session 之內，就不用重新載入了)  注意到 cache 僅能供同一個 session 存取，不可以跨 session 存取，並且在 session 結束的時候，cache 會一併刪除      另外常見的 cache 手段包含像是 Redis 等等的可以參考 資料庫 - Cache Strategies 與常見的 Solutions | Shawn Hsu    server 會針對所謂的 Prepare Statement 以及 Store Program 進行 cache  但眾所周知，cache 會有所謂的 過期 問題, 亦即資料並不新鮮了  那麼什麼樣的情況下會造成 invalid 的情況呢？   當資料改變(新增、修改或刪除)？ NoNoNo  記得一件事情，Database 內部的 cache 並不是拿資料做 cache, 而是拿 metadata  當 metadata 改變的時候，Database 的 cache 會被視為是過期的  也就是說 create, drop, alter, rename, truncate, analyze, optimize 以及 repair 這種 Data Definition Language - DDL 的操作都會造成 metadata 的改變  前面幾個還滿好理解的，但為什麼 analyze 跟 optimize 會改變 metadata 呢？      to be continued      常見的 store program 包含像是 store procedure, function, triggers 以及 events     store procedure 由於其過時的程式語言，難以管理、部屬以及測試等問題   SQL Commands  Regex vs. Like Operator     to be continued    Join vs. Multiple Queries  先講結論，Join 的效率肯定是高於 multiple query 的  但是用 Join 一定是最佳解嗎   我們之前看過所謂的 N + 1 問題  在那個例子下，的確使用 Join 會是較好的選擇     詳細可以參考 資料庫 - SQL N + 1 問題 | Shawn Hsu    有什麼樣的情況下你會選擇使用 multiple query?  在單台機器的情況下，我想不會有人反對 Join 的速度  但是在多台機器的情況下，搭配上 sharding 的機制，Join 要如何實現？     對吧！ 不同的 table 在不同的機器上面要如何 Join?  就算可以 join table，他的效率一樣會優於 multiple query 嗎  有時候你得注意到，在不同的 context 底下，適用的方案不同   簡單的 query 如 SELECT * FROM user WHERE id = 1 可以被 cache 起來  相對的，複雜的 query 也可以被 cache 起來  差別在於，誰會比較常被存取？ 顯然是簡單的 query   multiple query 除了可以被 cache 起來之外  如果搭配上 scale out 可以平行處理不同的 query  這樣他的效能 不見得會比較差     至於該選擇哪一種，真的是 case by case  你可以透過簡單的估算，搭配現有的財力，選擇出適當的實作方式   Store Image in Database?     to be continued    Database inside Container?     to be continued    References     資料密集型應用系統設計(ISBN: 978-986-502-835-0)   內行人才知道的系統設計面試指南(ISBN: 978-986-502-885-5)   8.2.1.23 Avoiding Full Table Scans   5.11.1. 概念   13.7.3.1 ANALYZE TABLE Statement   每日MySQL之023：使用ANALYZE TABLE命令分析表的key distribution   13.7.3.4 OPTIMIZE TABLE Statement   SQL 索引欄位是否該包含 OR 比對項目？   Indexing Big Data: Global vs. Local Indexes in Distributed Databases  ","categories": ["database"],
        "tags": ["database","distributed","sharding","index","docker","table partitioning","store program","rebalance"],
        "url": "/database/database-optimization/",
        "teaser": null
      },{
        "title": "Kubernetes 從零開始 - 無痛初探 K8s!",
        "excerpt":"Preface  現今網頁服務由於 container 的興起，大幅度的改變了整個 web 的生態系，一切都圍繞著 container  雖然說 container 改變了開發者的工作流程，使得大部分得以簡化，但是仍有一些問題依然存在   比如說： 當服務有 bug 導致系統停機、當系統要更新而必須停機、當服務撐不住需要手動開機器用以應付大流量的時候  以上這些情況若以單純的 docker 可是無法順利解決的   Deployment Evolution       ref: Overview       Traditional Deployment            傳統部屬服務的方式就是在實體機器上面安裝服務對吧？比如說 $ sudo apt install xxx 之類的       這樣做的壞處是 當你的服務需要因為某些原因而進行搬遷的話，你沒有一個很方便的手段重新安裝服務，必須要從                    作業系統安裝 :arrow_right: 一些系統服務的安裝(e.g. ssh, mysql, 防火牆設定 … etc.) :arrow_right: 最後才是安裝你的服務           這樣用起來 使用者肯定都等的不耐煩了對吧                       而且還有一個很重要的問題，如果你在機器上跑 n 個 instance 服務，有可能會因為資源分配不均的情況所以導致 某幾個 instance 的 performance 會不如預期       你可能會想說 我多開幾台機器就可以了，實務上維護多台實體機器並不是一個很好的選擇，而且這樣對資源的利用度並不高           Virtualized Deployment            所以為了克服上述問題，虛擬化技術被提出       虛擬化技術的出現使得 資源利用度更好, 容易進行維護(容易增加、更新以及刪除)       較為人詬病的問題點是，由於 virtual machine 先天上的設計，他是從底層虛擬化上去的(亦即每個 vm 都擁有自己獨立的作業系統)，所以在效能上會是一大問題           Container Deployment            相比 virtual machine, container 解決了效能問題，主要是透過了 share operating system 的方式，詳細可以參考 Container 技術 - 理解 Docker Container | Shawn Hsu           Introduction to Kubernetes  container 的興起，加上逐漸從 monolithic 轉到 microservices 的趨勢  管理龐大的 container 們是一件不容易的事情，也因此 Kubernetes 得以快速發展   Kubernetes 的優勢     能夠自動進行負載平衡   根據不同負載量自動 scale out, scale in   擁有 self-healing 的機制，亦即 zero downtime   Powerful than Docker Compose  其實我一開始在寫設定檔的時候，我真的覺得他長得很像 docker-compose  最有感的就是資料庫連線的方式，都是透過名字來連線   話雖如此，K8s 也有比 docker-compose 更強大的地方  比如說 K8s 支援更好的 scaling, 可以動態調整，可以不限定於單一機器  docker-compose 通常是用在開發階段，不太適合正式環境，不如考慮 docker swarm   Producer-Consumer Example  K8s 很多東西可以玩，也很複雜，但是基本的概念是相對簡單的  讓我們來看一個例子      完整程式碼實作可參考 ambersun1234/blog-labs/k3d    我想要寫一個簡單的 producer-consumer 的服務  producer 會將訊息送到 rabbitmq 裡面，而 consumer 則會從 rabbitmq 裡面取出訊息  這個服務需要有三個, producer, consumer 以及 rabbitmq   因為 K8s 是一個 container management tool, 所以我們需要將這三個服務打包成 container  注意到 local 的 docker image 並不會被 K8s 所知道，我們有兩個選項     打包上傳到 docker hub 或者是私有的 docker registry   手動傳入 K8s 裡面(k3d)   Deployment  萬事俱備之後，我們就可以開始部屬服務了  K8s 是使用 yaml 檔描述你該怎麼部屬服務的(有點類似 IaC 但不全然一樣)  我們需要三個服務，他們被稱作 Deployment   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 apiVersion: apps/v1 kind: Deployment metadata:   name: producer spec:   replicas: 1   selector:     matchLabels:       app: producer   template:     metadata:       labels:         app: producer     spec:       initContainers:         - name: wait-for-rabbitmq           image: busybox:1.28           command:             [               \"sh\",               \"-c\",               \"until nslookup rabbitmq-service; do echo waiting for rabbitmq-service; sleep 2; done\",             ]       containers:         - name: producer           image: test-producer           imagePullPolicy: Never           envFrom:             - configMapRef:                 name: myconfig             - secretRef:                 name: application-credentials           resources:             requests:               memory: 2048Mi               cpu: 500m             limits:               memory: 4096Mi               cpu: 1000m      小試身手！ 第 4, 9, 13 行的 producer 各是什麼意思？ 可參考 Different Labels in Deployment    這是 producer 的設定檔，consumer 以及 rabbitmq 也是類似的  首先我們先從看得懂先開始  initContainer 是執行在 pod 啟動之前的 container  你可以拿來做一些初始化的工作，比如說等待服務啟動完成  這裡的 initContainer 是用來等待 rabbitmq-service 啟動完成      initContainer 通常是使用 until do done loop 搭配 nc 使用  使用 nc 記得搭配 -z 參數，這樣就不會真的連線進去  我們的目的僅僅是確認服務有沒有啟動而已    container 就是定義主要服務的地方  這裡定義了我們的 producer container，然後 image 是我們 local build 出來的所以 ImagePullPolicy 是 Never  比較有趣的是 resources 這塊，可以看到有 requests 以及 limits  根據 Resource Management for Pods and Containers 所述      When you specify the resource request for containers in a Pod,   the kube-scheduler uses this information to decide which node to place the Pod on.   When you specify a resource limit for a container, the kubelet enforces those limits so that the running container is not allowed to use more of that resource than the limit you set.   The kubelet also reserves at least the request amount of that system resource specifically for that container to use.    簡單來說     requests: 這是你的 container 需要的最小資源   limits: 這是你的 container 最多可以使用的資源   Service  我們知道了一個 deployment 要怎麼樣定義出來  container image 要用哪一個，他要怎麼跑，他資源上有哪些限制  但顯然還不夠，舉例來說，他要怎麼跟 RabbitMQ 連線？   RabbitMQ 以這個例子也是一個 deployment  兩個不相干的 container 要連線溝通，以 Docker-Compose 來說是不是讓他在同一個 network 下面就好了?  然後透過 container name 來連線(因為我們不知道實體 ip)  K8s 也是一樣的概念，只是他的實作方式不同  在這裡你會需要的是一個 Service   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion: v1 kind: Service metadata:   name: rabbitmq-service spec:   type: ClusterIP   selector:     app: rabbitmq   ports:     - name: amqp       protocol: TCP       port: 5672       targetPort: 5672     - name: gui       protocol: TCP       port: 15672       targetPort: 15672   理由也一樣，因為我們不知道 rabbitmq container 的實體 ip  K8s 採用的做法是透過 Service 來提供一個固定連線方式給 producer 來連線  要怎麼連線？ 一樣是 名字   ConfigMap and Secret  連線的方式我是透過 env variable 來設定的  然後 env variable 的值是透過 ConfigMap 以及 Secret 來設定的  一般來說 K8s 的設定檔都會獨立出來，兩個差別在於 Secret 是放機密資料的(但他不會加密)   1 2 3 4 5 6 7 # configmap apiVersion: v1 kind: ConfigMap metadata:   name: myconfig data:   MQ_CH: test   1 2 3 4 5 6 7 8 # secret apiVersion: v1 kind: Secret metadata:   name: application-credentials type: Opaque data:   MQ_URL: YW1xcDovL3Rlc3Q6dGVzdEByYWJiaXRtcS1zZXJ2aWNlOjU2NzIv   剛剛的 rabbitmq-service 就是用在這裡  然後 producer 的 configMapRef, secretRef 就是會使用上述的資料  他們也是透過設定檔的 “名字”(myconfig, application-credentials) 來指定的   MQ_URL 是 rabbitmq 的連線資訊  注意到 secret 的資料是 base64 encode 過的(也僅僅只有 encode 過而已，他是可以 decode 的)   Example Recap     我覺的 Kubernetes Crash Course for Absolute Beginners [NEW] 分享的概念淺顯易懂  因此這部份我也會參考原作的講解的方法，重新解釋一遍，另外也滿推薦可以看看原本的內容    上面的例子我們大約的看過 K8s 的基本組成元件  但容許我再用正式的定義複習一次   Deployment and Service  我們知道，Kubernetes 是負責管理龐大的容器們的工具  容器本身需要一個地方執行，不論是虛擬機或是實體機器，稱之為 Node  而 Kubernetes 不只是為了 docker 而生，為了要兼容其他的 container runtime  它做了一層抽象層，稱之為 Pod      這部份可參考 Container 技術 - runC, containerd 傻傻分不清 | Shawn Hsu    Pod 本身是一層抽象層，亦即你沒辦法真正的去操作它  而 Pod 是由 Deployment 的設定檔撰寫而成  它會定義說你要用哪一個 container image 啦，然後你的 replica 數量等等的   就如同你在使用 docker 的時候會 export port，從外部 access 進去  Kubernetes 的每個 pod 也有自己的 ip address, 提供你存取  不過要注意的是，當 pod 掛掉重啟的時候，該 ip address 也會跟著改變  這會造成不好的開發者使用體驗(i.e. 每次都要連到不同的機器)      為什麼 pod 會掛掉？  可能是 application 本身有問題、網路問題意外掉線啦或是重開等等的    為了克服此等問題，Service 應運而生  它負責執行 Pod 之間 routing 與 discovery 的工作  其中一個重點是，Service 可以定義固定的 “存取介面”  意思就是我可以透過 存取介面(i.e. name) 存取到我們的 Pod   ConfigMap and Secret  在 backend development 裡  資料庫的存取算是滿普遍的需求  以往我們在做這方面的東西的時候，通常會將 連線資訊 等等的寫在 config.yaml 或是 environment variable 裡面  Kubernetes 中也是同樣的概念稱為 ConfigMap   但是對於一些像是密碼之類的資訊  你可以把它放在另外一個地方 Secret  但是要注意的是，Kubernetes 的 Secret 是 不會做加密的  需要透過第三方的套件來加密      Secret 的資料要是 base64 encode 過的資料哦~      最後，資料庫的部份  在 docker 裡面我們會這樣寫 docker run -itd -p 3306:3306 -v mysql:/var/lib/mysql  不過 Kubernetes 通常不建議這麼做，因為它只是個容器管理工具  針對 persistent data 的部份建議是往外放   Label(name) is the Key to Connect  在 K8s 的設定檔裡面，你會注意到我一直強調利用 名字 來連線或者是做 value reference  yaml 檔之間的設定基本上都是透過這種方式操作的   當你要在 deployment 裡面拿到 configMap 的資料的時候，你會透過 configMapRef 以及 secretRef 取得特定 label 下的特定的資料  如果找不到相對應的，比如說 environment variable, 記得檢查 key, value 是不是有打錯字之類的   Different Labels in Deployment  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 apiVersion: apps/v1 kind: Deployment metadata:   name: producer # deployment name spec:   replicas: 1   selector:     matchLabels:       app: producer # which pod to manage   template:     metadata:       labels:         app: producer # pod name     spec:       initContainers:         ...   所以這些 producer label 分別代表什麼意思  最外層的 metadata.name 是這個 deployment 的名字   我們知道 deployment 實際上是由 pod 組成的  所以 spec 底下的資料都是 pod 的設定檔  template 定義了 pod 的規格，包含他要跑的 container, 他的環境變數等等  所以 spec.template.metadata.labels.app 代表的是 pod 的名字   最後 spec.selector.matchLabels.app  deployment 需要管理 pod，所以他需要知道是哪個(哪些) pod  所以名字符合 app: producer 的 pod 就會被這個 deployment 管理      其實都把他設定成同一個名字不太好    k3d  我們可以使用 k3d 在本機跑 K8s      minikube 一般來說比較慢，因為他是起 VM, k3d 則是用 container 跑    不過無論你用哪一種我都覺得挺好上手的，這裡會稍微記錄一下一些小小的坑  k3d 需要在一開始建立 cluster 的時候就指定好 server node 的數量  建立的時候可以指定數量，以這個例子來說一個就夠了   1 $ k3d cluster create mycluster --servers 1   另外我們稍早也提過，image 需要手動傳入 K8s 裡面  在 k3d 裡面是這樣做的   1 $ k3d image import -c mycluster test-producer:latest   最後當你不用測試的時候可以選擇停止 cluster(不需要刪除)   1 $ k3d cluster stop mycluster   Kubernetes Cluster Architecture       ref: Cluster Architecture    Control plane  因為 k8s 本體其實是 cluster 架構, 因此我們需要一個類似控制中樞的角色(大腦) 即 control plane  control plane 必須管理底下所有的 node 用以進行諸如 scheduling 等的決策事項  在 control plane 裡面還有若干服務     kube-apiserver            提供 Kubernetes API 用以管理整個 k8s cluster           etcd            用以儲存所有 cluster information state data           kube-scheduler            排程器，用以安排新的 pod 要跑在哪一個 node 上       排程器會自動挑選合適的 node 並且將 pod 安排在該 node 上面           kube-controller-manager            對於 k8s cluster 來說，我們需要一個 controller 用以監控 cluster 的 state(狀態)       透過 controller 這個 process 一點一點的將 cluster state 導向 desired state              有關 controller 的介紹可以參考 Kubernetes 從零開始 - 從自幹 Controller 到理解狀態管理    Node  node(節點) 是組成 cluster 的重要單位，節點可以是 virtual machine 或者是 physical machine  每一個 node 都是由 control plane 直接控制的, 而 node 裡面包含有 pod 用以運行 container   node 包含了以下的組成元件     kubelet            為一 node agent，負責管理底下 pod 的狀態(包含: pod 有沒有正常運行，pod 的生理狀態 健不健康之類的) 以及 將 node 註冊到整個 cluster 裡面           kube-proxy            前面提到 pod 是根據 kube-scheduler 進行排程安排到特定的 node 上，值得注意的是 pod 是會變動的(主要是根據 hardware/software/policy constrains, affinity, anti-affinity specifications, data locality, inter-workload 等等的影響)       所以說 pod 的 ip 是會變動的，而若是主機 ip 變動，對於客戶端的使用來說會是極度麻煩的事情, 也因此 kube-proxy 就是為了解決這件事情的       kube-proxy 是跑在每一個 node 上面的 network proxy, 為了解決動態 ip 的問題，k8s 將 set of pods 構築成 虛擬的 network service, 賦予外界一個固定訪問 pod 的通道       如此一來，即使後端的 pod 由於 scheduler 排程的關係移動到其他 node 上，對於前端來說仍然沒有影響！           container runtime            注意到一件事情，k8s 不單單只支援 docker, 事實上，它支援許多種的 container runtime       為了支援各項平台，k8s 有自己的一套 Kubernetes CRI (Container Runtime Interface) 界面，用以支援各種不同的 runtime(有關 container runtime 的介紹可以參考 Container 技術 - runC, containerd 傻傻分不清 | Shawn Hsu)           Pod  pod 是 k8s 中最小可部屬單元，注意到不是 container 哦  pod 是由一系列的 spec 定義出來的(裡面包含像是 image 資訊、metadata, ports … etc.)  pod 裡面可以包含 一個或多個 container, 所有的 container 共享 儲存空間、網路等等的      如果要 log pod 裡面的 container, 你可以透過 kubectl logs &lt;pod-name&gt; -c &lt;container-name&gt; 來取得  因為一個 pod 可能會有多個 container, 所以你需要指定 container 的名字       通常的作法會是一個 pod 裡面僅僅會包含一個 container    看到這裡其實我覺得有點疑惑，為什麼 Kubernetes 要多拉一層 pod 出來呢？  很明顯我可以直接用 container 來運行我的服務   如果你想要執行多個 container 並共享資源之類的事情，單 container 並沒有辦法做到  你會需要透過類似 docker-compose 的方式來達成共享網路，資料儲存  在 K8s 裡面，這個 環境 就是 pod   但 pod 不僅僅是環境而已，他也包含了一些資源管理，生命週期管理的功能  這樣他才是 k8s 中最小的可部屬單元     不過，我們其實不太會直接操作 pod  原因是 pod 的生命週期是很短暫的，亦即 pod 會隨著 node 的重啟而消失  比較常見的是他不會自動重啟，rolling update 等功能  所以他對於管理方面其實是不太方便的   通常來說會是使用更進階的 workload resources 來管理 pod  deployment, statefulset, daemonset 等等的  這個部份我們會在之後的文章中進行介紹   Conclusion  在撰寫 yaml 檔案的時候，請務必注意以下幾點     label, selector 之間的名字是不是一樣的   configMap 的資料 不用 base64 encode, 但是 secret 的資料要   連線資訊的部分，可以依賴 Service 定義一個固定的連線方式   掌握這些極基本的概念，你就有辦法開始使用 K8s 了  但路途還很遙遠，一起學習吧   References     nodes   pods   controllers   kubelet   kube-scheduler   service   kubernetes 简介：service 和 kube-proxy 原理   Kubernetes Crash Course for Absolute Beginners [NEW]   Docker Swarm vs Kubernetes: A Practical Comparison   Bash: Loop until command exit status equals 0   Defaulted container “container-1” out of: container-1, container-2  ","categories": ["kubernetes"],
        "tags": ["container","k3d","rabbitmq","producer consumer","service","deployment","configmap","secret"],
        "url": "/kubernetes/kubernetes-basic/",
        "teaser": null
      },{
        "title": "實際上手體驗 Golang DI Framework 之 Uber Dig",
        "excerpt":"Uber/dig  uber/dig 是一套基於 reflection 的 Dependency Injection Framework  意思是我們不需要手動指定依賴，而是透過 reflection 來幫我們自動找出依賴，依靠框架管理   讓我們直接看例子   Web Server Architecture  假設現在我們需要構建一個網頁後端伺服器  依照基本的 MVC 架構，我們會有以下幾個部分     Model 資料儲存   Controller 處理邏輯   Server 伺服器   Dependency Direction  所以我們知道目前需要以上這三個部分  他們的依賴關係是   1 Server -&gt; Controller -&gt; Model   controller 不應該需要知道底層儲存資料具體是怎麼做的，他可以是 MySQL, PostgreSQL 甚至 Redis 等等  同樣道理，server 也不應該需要知道 controller 具體是怎麼處理邏輯     如果是手動建立，可以依照上圖的方向，你可以很輕鬆的建立依賴關係  並且實作也不會太困難   How dig Handle Dependency Injection  前面提到 dig 是基於 reflection 的  並且會自己幫你找出依賴，這樣你就不需要手動指定   具體來說他是這樣幹的  手動建立依賴關係可能會長成這樣  1 2 3 func NewController(model *Model) *Controller {     return &amp;Controller{model} }   有發現什麼嗎？  我要建立 controller 我必須要傳入一個 model, 然後他的型別是 Model  輸出會是一個 Controller   dig 的思想是這樣的  既然我知道你的 輸入型別 以及 輸出型別  我是不是只要找到 哪一個 function 會輸出我要的型別  我就找到依賴了？   好這就是他能做的事情了  function 要我們手動提供給他  我們需要手動將所有 function(其實說 constructor 比較準確) 提供給 dig  container 是一個容器，負責管理所有的依賴，也就是說 function 都要在 container 註冊  此過程稱為 Provide   到這裡還沒完，因為我們只是提供了 function  你最後要的是什麼，他不知道  所以你需要告訴他，你要的是什麼，這個過程稱為 Invoke   Web Server Implementation  Provide  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 if err := container.Provide(func() *gin.Engine {     gin.SetMode(gin.DebugMode)     server := gin.New()     return server }); err != nil {     panic(err) }  if err := container.Provide(inmemory.NewInMemory, dig.As(new(storage.StorageI))); err != nil {     panic(err) }  if err := container.Provide(post.NewPostService); err != nil {     panic(err) } if err := container.Provide(user.NewUserService); err != nil {     panic(err) }  if err := container.Provide(NewController); err != nil {     panic(err) }   這裡總共提供了 5 個 function  大致分為 web server, storage, service 以及 controller  大部分的情況下可以直接傳 callback 進去，或者是 anonymous function   這個步驟就是告訴 dig 你有哪些 function 可以提供給他   Invoke  1 2 3 4 5 6 if err := container.Invoke(func(controller *Controller, server *gin.Engine) {     controller.Register()     server.Run(\":8080\") }); err != nil {     panic(err) }   我們的最終目標是建構網頁後端的伺服器  根據上述的 Dependency Direction 我們知道 server 會依賴 controller  所以這裡傳入的參數就是 controller 以及 server      有哪些參數可以用？ 就是稍早 Provide 的 function    裡面我們告訴 dig 我們要的是 controller 以及 server  然後再手動組裝起來  hmmm 為什麼？   既然他聰明到可以自己找出依賴，為什麼不直接幫我們組裝好呢？  因為 dig 不了解你的商業邏輯，他沒辦法知道你必須要要先註冊 route 才能啟動 server  他能做的是，自動建構需要的依賴，並提供給你   Parameter Objects  一般來說 Dependency Injection 的參數會寫成這樣  1 2 3 4 5 6 7 func NewController(postService post.PServiceI, usesrService user.UServiceI, engine *gin.Engine) *Controller { \treturn &amp;Controller{ \t\tpost: postService, \t\tuser: userService, \t\tapp: engine, \t} }  把需要的依賴手動傳入 function parameter 當中，但是當參數數量過多的時候，他的維護會相對變得困難 dig 使用另一種方式解決這個問題   透過定義一個 struct 來傳入參數  取代原本數量繁多的 function parameter   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 type ControllerDependency struct { \tdig.In  \tPostService post.PServiceI \tUserService user.UServiceI \tApp        *gin.Engine }  type Controller struct { \tpost post.PServiceI \tuser user.UServiceI \tapp *gin.Engine }  func NewController(dep ControllerDependency) *Controller { \treturn &amp;Controller{ \t\tpost: dep.PostService, \t\tuser: dep.UserService, \t\tapp: dep.App, \t} }   注意到實作裡，有兩個 structure, 一個是 dependency 的，另一個是 controller 的  dependency struct 必須要 embed dig.In  用以告訴 dig 這是一個 parameter object   Dependency Injection with Interface  你可能會注意到上述的例子裡面，我們依賴的東西都是 interface 而非 concrete type  這是因為使用 interface 可以讓我們更容易的進行測試  也方便進行解耦合   實務上當然也是可以使用 concrete type  只不過這樣會讓你的程式碼更難測試，以及更難維護   要注意的是，dig DI 的型別要是一致的  也就是說，當你依賴於 interface 的時候，你傳一個 concrete struct 進去會錯誤   1 2 3 4 5 6 7 panic: could not build arguments for function \"main\".main.func2 &gt; (/blog-labs/golang-dig/server.go:39): &gt; failed to build *main.Controller: could not build arguments for function \"main\".NewController &gt; (/blog-labs/golang-dig/controller.go:26): failed to build post.PServiceI: &gt; missing dependencies for function \"golang-dig/service/post\".NewPostService  &gt; (/blog-labs/golang-dig/service/post/post.go:24): &gt; missing type: storage.StorageI (did you mean *inmemory.InMemory?)     1 2 3 if err := container.Provide(inmemory.NewInMemory, dig.As(new(storage.StorageI))); err != nil {     panic(err) }   主程式有一樣很特別的東西，如上所示  他相比其他的 provide 多了一個參數 dig.As(new(storage.StorageI))   model 層我做了一層抽象，稱為 storage.StorageI  上層只要知道這層是一個儲存的介面即可，如同我們提到的 他不需要管是 MySQL, PostgreSQL 還是 Redis  但是實際運行的時候我們還是要具體的提供一個 storage 的實作   這裡，我們給了一個 inmemory.InMemory  他是一個具體的實作，並且實作了 storage.StorageI  可是 controller 期待的是一個 storage.StorageI   即使 inmemory.InMemory 實作了 storage.StorageI  最終的型態還是不一樣，所以會報錯   具體的解法就是 dig.As  他的意思是這個 constructor 實作了一個或多個 interface  以我們的例子來說是 storage.StorageI  所以這樣他就會動了   Debug Dig  如果你撰寫的程式碼發生了錯誤，在 dig 裡面 debug 我覺得是相對困難的地方  舉例來說，我們剛剛看過的例子  以下有 5 個 provide 的 function   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 if err := container.Provide(func() *gin.Engine {     gin.SetMode(gin.DebugMode)     server := gin.New()     return server }); err != nil {     panic(err) }  if err := container.Provide(inmemory.NewInMemory, dig.As(new(storage.StorageI))); err != nil {     panic(err) }  if err := container.Provide(post.NewPostService); err != nil {     panic(err) } if err := container.Provide(user.NewUserService); err != nil {     panic(err) }  if err := container.Provide(NewController); err != nil {     panic(err) }   1 2 3 4 5 6 7 panic: could not build arguments for function \"main\".main.func2 &gt; (/blog-labs/golang-dig/server.go:39): &gt; failed to build *main.Controller: could not build arguments for function \"main\".NewController &gt; (/blog-labs/golang-dig/controller.go:26): failed to build post.PServiceI: &gt; missing dependencies for function \"golang-dig/service/post\".NewPostService  &gt; (/blog-labs/golang-dig/service/post/post.go:24): &gt; missing type: storage.StorageI (did you mean *inmemory.InMemory?)   從這裡的錯物訊息來看，你可以很清楚的知道是型別出問題  但是，我自己遇到的例子是我的程式直接 crash 掉，並沒有任何的實用錯誤訊息  這時候你就會像個無頭蒼蠅一樣，不知道從何看起   我遇到的，是從 reflection 那裡開始報錯  我唯一看得出來的錯誤訊息是，他無法正確的建構我要的 service    但錯誤訊息仍然提供了一個我疏忽的的提示  1 panic: could not build arguments for function \"main\".main.func2   看到那個 func2 了嗎  這或許是個提示，但這個 2 是啥  往回看 provide 的 function，總共有 5 個  第 2 個剛好就是出錯的那個函數  一下子範圍就縮小了許多   1 2 3 if err := container.Provide(inmemory.NewInMemory, dig.As(new(storage.StorageI))); err != nil {     panic(err) }   Example  上述的例子可以在 ambersun1234/blog-labs/golang-dig 找到   References     Implementing DI and DIP in Golang: A Guide with Dig and Gin   uber/dig  ","categories": ["random"],
        "tags": ["dependency injection","dig","golang","unit test"],
        "url": "/random/golang-dig/",
        "teaser": null
      },{
        "title": "資料庫 - 大型物件儲存系統 MinIO 簡介",
        "excerpt":"Brief Large Object Storage System  檔案儲存在現今電腦服務中一直扮演著相當重要的角色  舉例來說，你的大頭貼會需要一個地方儲存  我記得我在學校學習的時候一般來說有兩種做法     上傳到伺服器當中的檔案系統內做儲存，資料庫內寫入存放路徑即可   直接以二進位的方式存入資料庫中   兩種方式都有各自的優缺點  我們可以確定檔案存儲的需求一直以來都是存在的   如今雲端系統的興起，儲存方式也需要隨著時代的變遷而變化  你可能聽過一些服務像是 AWS S3, Google Cloud Storage 等等  這些都是雲端儲存的服務   不過我很好奇，為什麼我們會需要 “雲端的存儲” 呢？  不放在資料庫裡面的原因可以理解，因為效能上會有問題  但寫入本機硬碟也是個選項吧？   事實上這也跟分散式系統有點關係  多台電腦平行處理，你的檔案勢必要同步到不同的機器上面(不然存取的時間就會過長)  如你所想，這樣系統的複雜度就會提高很多(availability, scaling issue … etc.)  太多的問題需要考慮，於是專門特化的檔案儲存系統就出現了   MinIO  Minio 是一個開源的物件儲存系統並且兼容 S3 規格  為了高可用性以及高效能，所有的分散式系統的設計基本上他都有  不過有一些不同   MinIO 為了應對高可用性以及高效能的場景  他支援多台伺服器組成 cluster 的架構  一個 cluster deployment 可以擁有多個 server pool  每個 server pool 可以擁有多個 minio server(又稱為 node) 以及 Erasure Set(儲存用)        ref: Object Versioning    Active-Active vs Active-Passive Replication  節點之間會進行資料的同步  以 MinIO 來說，他有兩種不同的同步方式   Active-Active Replication 是指兩個節點之間的資料是雙向同步的  Active-Passive Replication 則是單向同步   預設情況來說是使用雙向同步的  也就是說每個節點同時扮演著 master 以及 slave 的角色  所以 MinIO 其實是 multi-leader replication 的系統架構      有關 multi-leader replication 可以參考 資料庫 - 初探分散式資料庫 | Shawn Hsu    節點複製資料的時候除了資料的本體  所有相關的 metadata 以及設定也會被一同的被複製儲存  資料一旦被寫入，其存取的位置就都不會再改變(固定的 server pool 固定的 Erasure Set)  換句話說 MinIO 並不會做 re-balancing   因為資料的搬遷移動是一個非常耗時耗力的工作  MinIO 選擇了一個不同的方式，在眾多 erasure set 當中，他會選擇一個最空閒的 erasure set 來寫入資料  這樣做到最後資料量就會平均的分佈在各個 erasure set 上面  也就達成了一種平衡   值得注意的是，當一個 server pool 的 erasure set 們徹底掛掉的時候  儘管其他 erasure set 還活著，整個 cluster 依然會停止運作  原因在於他沒辦法確認資料的一致性  這時候 Admin 需要手動復原才可以繼續工作   Synchronous vs Asynchronous Replication  MinIO 的複製機制預設是非同步的  兩個的差別主要在於其他節點的寫入時間      MinIO 的方法跟傳統的定義上仍有點出入，可參考 資料庫 - 初探分散式資料庫 | Shawn Hsu    非同步複製會先等當前節點寫入完成之後，再將資料放入 replication queue  交給其他節點複製  好處是他不必等待所有人寫入的確認，效能上會好一點   同步複製 並不會等待其他節點寫完  這裡就不一樣囉，傳統上來說同步複製會等待所有節點都寫完之後才會 return  MinIO 一樣是先 一起開寫，但是當主節點完成之後就會 return      注意到 MinIO 仍然會維持 write quorum    所以最終的差別在於，放入 replication queue 的時間點不同     非同步 :arrow_right: 我寫完才開始同步   同步 :arrow_right: 一起同步   Versioning  儲存在 MinIO 的檔案是可以被版本控制的  也就是說所有的版本都會被保存下來  但是這樣會有問題對吧，保留的歷史越多，空間就會越大  很明顯這樣不管有多少空間都不夠用   所以 MinIO 也有提供可以設定 object 的 lifecycle  舉例來說，當 object 超過一定時間之後就會被刪除   不一定每個 object 都需要擁有多個版本，稱為 unversioned object  針對這種物件他的管理就相對簡單，要刪除可以不需要考慮直接刪除   針對 versioned object，他的管理就會複雜一點  刪除的時候是 soft delete，也就是說實際上沒有被刪除，但你沒辦法存取而已  具體的做法是新增一個 DeleteMarker 物件(0 byte)，這個物件會標記這個物件已經被刪除了  不過你也可以指定刪除特定的版本，因為他是刪除其中的一個版本，所以接下來存取的版本就會是上一個版本         多版本的物件預設會指向最新的版本  所有的版本是透過 UUID v4 來做識別的  上圖你可以看到兩個版本的 uuid, 以及最新的版本是一個 0 byte 的標記  註明說它已經被刪除了    Versioning by Namespace  1 2 3 4 databucket/object.blob databucket/blobs/object.blob blobbucket/object.blob blobbucket/blobs/object.blob   物件的版本控制是 per namespace 的  意思是說，即使上面的 object.blob 可能都是一樣的，但因為她們的 namespace 不同  所以他們都是獨立的        ref: Objects and Versioning    Quorum  對，MinIO 也有使用 quorum  基本上分散式系統為了確保資料的一致性，都會使用 quorum      A minimum number of drives that must be available to perform a task.   MinIO has one quorum for reading data and a separate quorum for writing data.     Typically, MinIO requires a higher number of available drives to maintain the ability to write objects than what is required to read objects.    MinIO 需要一定數量的節點才能夠正常的工作  而他的官網上有提到，寫入的 quorum 跟 讀取的 quorum 是不一樣的  並且寫入的要求會比讀取的要求更高      有關 quorum 的概念，可以參考 資料庫 - 初探分散式資料庫 | Shawn Hsu    Erasure Coding  資料儲存需要額外考慮的一個點是資料的正確性  Erasure Coding 是一種針對資料儲存的保護的方式，透過數學的方法計算來達成  具體來說是這樣子的   將一個資料(檔案)分割成多個部分，假設為 k  另外計算出額外的 n 個部分，總共為 k + n     k 個部分是原始資料   n 個部分是 parity，是經過數學計算出來的額外的部分   Erasure Coding 將一個資料分割成 k + n 的部分  並且可以僅透過 k 個部分來還原原始資料      其中 k 個資料任選，但至少一個資料部分需要為 parity    Erasure Set       ref: Erasure Coding    所以我們知道 Erasure Coding 會將資料切割成 k + n 個部分  以 MinIO 來說，他會將這些部分分配到不同的硬碟上面  上述的例子你可以看到，他總共切了 4 個 parity 出來   當你的部分資料出於各種原因掛掉的時候，只要還有 k 個部分存在，你就可以還原原始資料  對於物件儲存系統來說，這是一個非常重要的機制        ref: Erasure Coding    上圖的 k 等於 12  因為掛掉了 4 個所以只剩下 8 個  但是因為我有 4 個 parity，所以 k = 8 + 4(parity)  因此這個例子還是可以還原     Erasure Coding 在 MinIO 中的提供了物件等級的保護  overhead 的部分也減少了   傳統上來說，你可能會使用 RAID 來做硬碟的保護  單就最簡單的 RAID 1 來說，你需要兩倍的硬碟來做保護  Erasure Coding 不需要 100% 複製你的資料就可以做到同等的事情   當然你的 parity 數量越多代表系統的承受能力越高  取而代之的則是 overhead 會增加  不過這就是一個 trade-off 啦   Erasure Coding with Quorum?  要注意的是這兩個各自解決了不同的問題   Erasure Coding 是針對資料的正確性的保護  東西可能會因為硬碟的壞軌造成部分資料的損壞  透過 Erasure Coding 你可以還原原始資料   而 Quorum 則是提供資料的一致性  他指的是多個節點回傳的資料必須是一致的  他並不能保證資料沒有被損毀   MinIO 透過這兩個機制在各種意義上保護了你的資料  而他們的設計也是為了因應不同的狀況 不要搞混   Object Healing  MinIO 透過 Erasure Coding 用以保護你的資料  具體來說，MinIO 會自動地進行資料的修復   第一個時間點自然是當你存取資料的時候，MinIO 會檢查資料是否正確  或者是透過定期的掃描來檢查(透過 Object Scanner)  最後則是 admin 手動觸法掃描   Object Lifecycle Management  當系統內部的資料越來越多的時候，手動管理已經不太現實  MinIO 提供了兩種方式管理 object 的生命週期   Object Expiration  最簡單的方式就是設定 object 的過期時間  這些 object 會在過期之後被刪除(由 Object Scanner 自動執行)   lifecycle 的設定是綁定在 bucket level 的  但是你可以針對不同的 resource 設定不同的 rule   1 2 3 4 5 6 7 8 9 10 11 minioClient.SetBucketLifecycle(context.Background(), bucketName, &amp;lifecycle.Configuration{     Rules: []lifecycle.Rule{         {             ID:     \"expire\",             Status: \"Enabled\",             Expiration: lifecycle.Expiration{                 Days: 1,             },         },     }, })      上述我設定了 object 過期時間為 1 天  要注意到的是，lifecycle management 的設定時間單位是 day  目前好像沒辦法進行調整(所以很難進行測試…)   此外，lifecycle subsystem 是每 24 小時才會掃描一次  意思是說，當你今天 15:00 的時候設定 object 過期時間為 1 天  你有可能需要再等一天的時間，object 才會被刪除      上圖的 Expiration 是 lifecycle 的設定  你可以看到，資料是在 10-11 建立，但是排定的刪除日期卻是 10-13   後來可以看到，我的 webhook 在 10-13 的 12:37 才收到刪除的 event  並且上圖可以看到最後一次的 $ mc stat 指令已經沒有東西了         並且由於 Object Scanner 是一個 low priority 的 process  所以它並沒有辦法很精準的在時間到的時候就刪除  因為他要盡量避免影響到 client 的操作    需要注意到的是，PutObject 裡面你可以設定 Expires  但它不是同一個東西  文件上我沒有找到相對應的參數說明，我猜測他是跟 cache 類似的作用，並不會真的刪除 object  上圖你也很清楚的看到 Expires 的時間老早已經過期了，但是 object 仍然存在      lifecycle 的測試程式碼也可以參考 ambersun1234/blog-labs/minio-webhook    Object Tiering(Object Transition)  另一種方式是把 object 搬家，稱為 object tiering  Object Expiration 是直接刪除 object，用以應付資料過多的問題  而 Object Tiering 則是將 object 搬到不同的 storage class  可能是從 SSD 搬到 HDD cluster 這種的冷儲存用      比方說有一些資料需要留存個幾年的時間用於 auditing    搬家的 object 仍然可以被存取，因為他有保存相對應的 link   MinIO Webhook Event  不同系統之間的溝通對於 MinIO 來說可以透過 webhook 來達成  比如說你需要在 object 建立/刪除 的時候做一些事情  MinIO 提供除了 API webhook 的方式以外，也可以透過 RabbitMQ, Kafka 來做通知(可參考 Bucket Notifications 裡面有完整的列表)      可參考 資料庫 - 從 Apache Kafka 認識 Message Queue | Shawn Hsu    Webhook Endpoint  首先你需要啟用 webhook 功能並且設定 webhook URL  可以用 environment variable 來設定   1 2 3 environment:   MINIO_NOTIFY_WEBHOOK_ENABLE: on   MINIO_NOTIFY_WEBHOOK_ENDPOINT: http://localhost:3000/minio/event      用 $ mc admin config get myminio 來查看目前的設定    那你的 webhook server 需要什麼格式呢？  URL 並沒有特定規範，但是它會傳一個 JSON 格式的資料給你  所以唯一的限制是這個 endpoint 要是 HTTP POST 的   它回傳的資料格式可以參考 Event-Driven Architecture: MinIO Event Notification Webhooks using Flask  需要注意的是，不是每一種類型的 event 都會回傳同等的資料  比如說針對 delete object 的 event，根據 S3 的設計他是不會帶 metadata 的  可參考 s3:ObjectRemoved:Delete does not include the context of the deleted object userMetadata, contentType, eTag, …   也就是說你可能不會有足夠的資訊區分不同的 event  針對 single endpoint multiple event，你能依靠的資訊有限  這時候除了利用 user metadata(e.g. tags)，你也可以透過 prefix 以及 suffix 來幫助你   Register Notification Event  你可以透過 mc 工具來註冊 notification event   1 2 $ mc alias set myminio http://localhost:9000 minio miniominio $ mc event add myminio/mybucket arn:minio:sqs::_:webhook --event put,get,delete   其中 arn 是 Amazon Resource Name 格式  arn 的格式為 arn:partition:service:region:account-id:resource-id   其中比較會有疑問的會是 account_id  你可以透過內建的 mc 工具查看當前 sqs 的 arn 設定   1 2 $ mc alias set myminio http://localhost:9000 minio miniominio $ mc admin info myminio --json   找到 sqsARN(通常在最上面的輸出) 你就可以看到整串 arn 的數值  而那個底線就是 account_id      MinIO 是透過 PBAC 來控制權限的，而他是用 ARN 表示 resource  可以參考 網頁設計三兩事 - 基礎權限管理 RBAC, ABAC 與 PBAC | Shawn Hsu    上述在 mybucket 底下註冊了 put, get, delete 這三個事件  因此只要這三個事件發生，MinIO 就會通知到你的 webhook server   1 2 Server running on port 3000 2024-10-06T17:52:43.957Z Received webhook s3:ObjectCreated:Put event on key mybucket/chiai.jpg   當你都設定完成之後，基本上就可以收到資料了  以上的 log 是我的簡易 express server 產出的，像我只有把 event name 以及 key 印出來      詳細的實作可以參考 ambersun1234/blog-labs/minio-webhook    Event Notification with MinIO Go Client  上一節 Register Notification Event 是透過 mc 工具來註冊 notification event  在大多數情況下你要手動註冊或者是透過啟另一個 container 自動做掉都顯得不是那麼優雅  你可以透過 MinIO Golang SDK 做掉這一段就是   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 package main  import (     \"context\"     \"fmt\"      \"github.com/minio/minio-go/v7\"     \"github.com/minio/minio-go/v7/pkg/credentials\"     \"github.com/minio/minio-go/v7/pkg/notification\" )  func main() {     // connect to minio     endpoint := \"localhost:9000\"     accessKeyID := \"minio\"     secretAccess := \"miniominio\"      minioClient, err := minio.New(endpoint, &amp;minio.Options{         Creds:  credentials.NewStaticV4(accessKeyID, secretAccess, \"\"),         Secure: false,     })     if err != nil {         panic(err)     }      queueArn := notification.NewArn(\"minio\", \"sqs\", \"\", \"_\", \"webhook\")      queueConfig := notification.NewConfig(queueArn)     queueConfig.AddEvents(notification.ObjectRemovedDelete, notification.ObjectCreatedPut)      cfg := notification.Configuration{}     cfg.AddQueue(queueConfig)      bucketName := \"mybucket\"     fmt.Printf(\"creating minio bucket... %v\\n\", minioClient.MakeBucket(context.Background(), bucketName, minio.MakeBucketOptions{}))     fmt.Printf(\"registering webhook... %v\\n\", minioClient.SetBucketNotification(context.Background(), bucketName, cfg)) }   event 相關設定可以透過 Golang SDK 達成  以達到更高的彈性設置，從上述你可以看到，我指定的 event 是 ObjectRemovedDelete 以及 ObjectCreatedPut  並且是針對 minio:sqs::_:webhook 這個 ARN 所設定的   notification event 是在 bucket level 設定的  不同的 bucket 是不會套用到同一套設定的      詳細的實作可以參考 ambersun1234/blog-labs/minio-webhook    MinIO Access Control Policy  上面我們有大概提到說 MinIO 本身是使用 PBAC 的方法進行權限控管的(可以參考 網頁設計三兩事 - 基礎權限管理 RBAC, ABAC 與 PBAC | Shawn Hsu)  它內建有一些預設的 policy 如 readwrite, readonly 以及 writeonly  你也可以自己定義客製化的權限控管機制   以 bitnami/minio 來說，可以定義在 provisioning.policies 這裡  不過，實務上在設定的時候有個小東西需要注意   以下是 mybucket 的 policy  可以看到很簡單的，這個 policy 允許 mybucket 底下特定的操作，比如說上傳、下載等  只不過多了 s3:GetBucketLocation 以及 s3:HeadBucket 這兩個操作，那他們是針對 bucket 本身的操作  我自己測試的時候，少了這兩個基本上你就會得到 Access Denied 的錯誤      定義好 policy 之後要跟 user 進行綁定才會正確做動    1 2 3 4 5 6 7 8 9 10 11 12 name: mybucket-policy   statements:     - effect: \"Allow\"       resources:         - \"arn:aws:s3:::mybucket/*\"         - \"arn:aws:s3:::mybucket\"       actions:         - \"s3:GetBucketLocation\"         - \"s3:PutObject\"         - \"s3:GetObject\"         - \"s3:ListBucket\"         - \"s3:HeadBucket\"   然後你可能會需要用到 multipart 的 permission  我遇到的狀況是，使用 minio/minio-js 在執行上傳的時候  針對大檔案會出現 Access Denied 的錯誤   後來發現到好像他會自己轉成 multipart 的方式上傳  所以以下 policy 是必要的(三個都要)   1 2 3 - \"s3:ListMultipartUploadParts\" - \"s3:AbortMultipartUpload\" - \"s3:ListBucketMultipartUploads\"   MinIO on Kubernetes  How to Debug MinIO on Kubernetes  有的時後你可能會遇到一些問題，比方說無法連線之類的  在 K8s 裡，你沒辦法從 host 直接開 GUI 看 log  但用 cli 還是可行的   當你 kubectl exec 進去之後才發現 mc 的工具沒有裝  好加在 MinIO 官方有提供一個簡單的 debug 專用 pod   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 apiVersion: v1 kind: Pod metadata:   name: mc   labels:     app: mc spec:   containers:   - image: minio/mc:latest     command:       - \"sleep\"       - \"604800\"     imagePullPolicy: IfNotPresent     name: mc   restartPolicy: Always   將這個 pod 部署到你的 cluster 上面  然後 exec 進去 MinIO 的 pod(注意到不是 mc 的 pod)  你就可以透過 mc 這個工具連線進去你的 MinIO   我遇到的問題是連線連不上，因為不確定是 application config 沒讀到所以出錯  還是本身設定就有問題了，因此我的首要目的會是測試連線   1 $ mc alias set myminio http://minio-service:9000 minioadmin minioadmin   mc 這個工具除了可以連線到 MinIO, 其他 S3-compatible 的服務也可以  他的語法是，將連線資訊儲存在一個 alias 中，之後就可以直接使用  建立 alias 的時候他就會先測試連線是否正常，因此就可以做測試   最後我發現是我的 ENV 沒有正確的設定  透過以上簡單的步驟，你就可以快速的 debug 你的 MinIO 啦   MinIO Health Check  你可以透過 mc 工具來測試 MinIO 是否正常運作  指令是 $ mc ping   指令如其名，它可以測試目標主機是否已經可以連線了  你可以將它設定為 backend 的 initContainer，在 backend 起來之前保證 MinIO 已經可以連線      上圖你可以看到，mc ping 會不斷的 ping 目標主機(也可以針對集群本身 ping)  如果是為了做 initContainer, 你可以設定 --exit，當連線成功之後就會結束   1 2 3 4 5 6 7 8 $ docker run -itd \\   -p 9000:9000 -p 9001:9001 \\   -e MINIO_ROOT_USER=\"miniominio\" -e MINIO_ROOT_PASSWORD=\"miniominio\" \\   --name test-minio \\   minio/minio server /data --console-address \":9001\" $ docker exec -it test-minio bash # mc alias set myminio http://localhost:9000 minio miniominio # mc ping myminio --error-count 30 --exit     或者是單純的使用 curl 也是可以的  MinIO 有提供 /minio/health/live, /minio/health/ready 等的 public API endpoint 來做健康檢查      可參考 MinIO Healthcheck    References     Core Operational Concepts   很酷的糾刪碼(erasure code)技術   Erasure Coding   erasure coding (EC)   Requirements to Set Up Bucket Replication   Debugging MinIO Installs   Event-Driven Architecture: MinIO Event Notification Webhooks using Flask   Publish Events to Webhook   MinIO Feature Overview: Object Lifecycle Management   Object Scanner   Object Lifecycle Management   lifecycle does not work   mc ping   Access Management  ","categories": ["database"],
        "tags": ["aws s3","minio","golang","docker","storage","kubernetes","erasure set","quorum","bit rot healing","erasure coding","webhook","lifecycle management","object expiration","object tiering","object transition","object versioning","object lifecycle management","mc ping","mc admin","mc event","mc alias","mc stat","mc admin config","mc admin info","provisioning","multipart"],
        "url": "/database/database-minio/",
        "teaser": null
      },{
        "title": "網頁程式設計三兩事 - 基礎權限管理 RBAC, ABAC 與 PBAC",
        "excerpt":"Brief Permission Management  權限管理在現今的網頁系統中是個很重要的議題  拿你我都熟悉的社群軟體來說，其實你無意中已經接觸過權限管理了        ref: Private vs. Public: Who’s actually seeing your posts on Facebook?    平常在發布文章的時候，你可以選擇是公開發布還是私人發布  這就是一個基本權限的概念  透過指定的權限，來控制使用者對於資源的存取   Access Control List (ACL)  所以從上述的例子，我們可以想出一個權限的表達的方式  身為作者的自己，所擁有的權限應該要是 read, write  其他人的權限，針對私有文章，只能有 read   1 2 3 ambersun1234: READ, WRITE alice: READ bob: READ   把這些整合在一起  就可以組成所謂的 Access Control List  ACL 通常是針對特定的資源(文章)，指定特定的權限   Why ACL is Not Enough?  你不難發現，ACL 其實是依據 資源 以及 使用者 來做權限的控制  這其實會造成一些管理的問題, 比如說  使用者數量增加，每一個使用者都要有自己的權限表  即使 Alice 跟 Bob 同樣都只有 READ 的權限，但是他們的權限表還是要分開   可以想像，當使用者數量遞增，你的權限表也會變得越來越大  API 回傳的資料也會變得越來越大  最後造成效能瓶頸，這很明顯不是一個好的方向   Role-Based Access Control (RBAC)  RBAC 相比於 ACL，是一個更為彈性的權限管理方式  既然瓶頸是在於使用者數量增加，有沒有一種方法可以不要強制綁定使用者與權限的關係呢？   重點在使用者數量太多了，管理太麻煩  當我想要把私有文章轉成公開文章的時候，我需要去 一個一個的修改使用者的權限  這會對資料庫造成很大的 overhead, 等於你在 DDOS 他   所以權限之間的關係勢必還是要存在，只是需要簡化  同一個資源下，擁有同一種權限的使用者，多半具有共同的特性  我們可以把他歸類成同一群，比如說 管理員、一般使用者  透過中間插入一層中間層，抽象化 使用者 與 權限 之間的關係  使得你不必明確的指定權限關係，這層中間層就是 角色   只有特定角色的使用者，可以擁有特定的權限  這點也不是什麼新鮮的東西      學生可以進入學校，我是學生，所以我可以進入學校  就是個簡單的例子       注意到 RBAC 可以擁有多個角色    Attribute-Based Access Control (ABAC)  以上述學校的例子來說，我可能會身兼多種角色     我可以是 資工系 的學生   我可以是 總務處 的工讀生   所以一個使用者可能會擁有多個角色，在特定的情況下，我擁有這些角色的權限  當我還是學生的時候，我可以進入學校  當我是總務處的工讀生的時候，我可以進入總務處辦公室存取機密資料   這些權限都不是固定的  什麼意思？  當我不在學校，就不能使用教室的電腦  當我畢業的時候，我就不能進入學校  當我工讀生的工作結束的時候，我就不能進入總務處辦公室存取資料了   你會發現你的權限是動態的  以上述的例子來看，他會根據你的 地點 以及 時間 來做權限的控制   ABAC 的概念是，一個使用者擁有多個 屬性, 而非角色  你可以是     台大的學生   資工系的學生   總務處的工讀生   多媒體網路實驗室的研究生   xx 專案的工程師   要進入多媒體網路實驗室，你需要是     台大的學生   資工系的學生   多媒體網路實驗室的研究生   擁有相對應的屬性你就擁有相對應的權限  這就是 ABAC 的概念   ABAC 對於 顆粒度較小的權限(如特定實驗室的權限) 是比較好的選擇  如果使用 RBAC 你需要的角色數量就會變得很多，顯然不是一個好的選擇  透過修改屬性，你的權限就會相對應的啟用  所以 ABAC 適用於權限需要頻繁更動，較為動態複雜的情況      有關權限顆粒度的問題，可以參考 Permission Granularity    Policy-Based Access Control (PBAC)  PBAC 的概念其實跟 ABAC 差不多  只是把 attribute 換成 policy   上述的例子改寫一下就是 policy 了     台大的學生 :arrow_right: 只有台大在籍的學生可以進入校園      與 ABAC 一樣，PBAC 透過組合不同的 policy 來控制權限       ABAC 可以說是 PBAC 的一個實作方式    Permission Granularity  權限的粒度是很重要的  粒度的大小會直接影響到你的權限管理的複雜度  為什麼說顆粒度太小/太大會對權限設計有影響   顆粒度大的情況指的是，他的使用範圍太大  Admin 可以讀取所有 Admin 的資料  但是不同的 Admin(行政部門，銷售部門) 他們可以看到的東西理論上應該要不一樣  所以當你都把他們歸類成 Admin 的時候，就會造成權限的混亂   這時候你需要調整你的權限粒度，讓他們可以更細緻的控制權限  比方說 ABAC 的概念，你可以透過屬性來控制權限  顆粒度變小你可以設定的邊界就能夠更明確   RBAC 與 ABAC 各有優缺點  其中權限的顆粒度是最為顯而易見的重點  適用何種情況取決於系統的複雜程度以及需求   Casbin  Casbin 是一套基於 PERM Metamodel 的權限管理系統  它可以支援 RBAC 以及 ABAC 的權限管理方式   PERM Metamodel  無論 RBAC 或者是 ABAC 都是一種權限的管理方式  他們都可以使用一種方式來表達權限  在高層次的角度來看，他們其實都符合 PBAC 的概念  在 Casbin 中，稱為 PERM Metamodel   一個使用者需要存取特定的資源，他需要根據現有的 policy 判斷是否有權限  這句話構成 PERM Metamodel 的基礎   我需要有 policy 定義誰可以存取什麼資源  我需要有 request 定義為存取資源的請求  定義 matcher 來判斷 request 是否符合 policy  最後 effect 來判斷是否允許存取      注意到你可以擴展 PERM Metamodel 來符合你的需求  比如說 policy 跟 request 之間可以再做一層 role 的映射    Role Inheritance  Casbin 裡面你可以做到 role 的繼承  比如說 Admin 可以繼承 User 的權限  在撰寫 policy 的時候，你可以透過 role 的繼承來簡化 policy 的重複性   不過這個 “功能” 嘛，我個人覺的有點不太方便  雖然 policy 的撰寫會變得更簡單(i.e. 重複的 policy 只需要定義一遍)  但是這樣會讓你的 policy 變得不太直觀  你必須要稍加思考才能理解該 role 到底有沒有該 subject 的權限   然後又因為 policy 是定義在 csv 裡面  變成說寫註解也有點難度，這只能透過測試來確保了   Example  舉例來說   admin 可以 讀取 data1 的資料  這是一個 policy   admin 需要 讀取 data1  這是一個 request   要如何判斷以上 request 能不能執行？  你可以自己寫個判斷的 function  當 role 符合，動作符合以及資源符合的時候，就回傳 true  這就是 matcher      當 matcher 找到符合的 policy  他會回傳 policy result(.eft)       如果 policy 沒有定義 .eft，則預設為 allow    effect 是一個很神奇的東西  既然 matcher 已經找到符合的 policy 那為什麼還需要 effect 呢？  因為你可能會符合多個 policy  effect 裡面可以指定全部符合或部分符合   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # Request definition [request_definition] r = sub, obj, act  # Policy definition [policy_definition] p = sub, obj, act  # Policy effect [policy_effect] e = some(where (p.eft == allow)) // 只要有一個 policy 允許存取，就允許存取  # Matchers [matchers] m = r.sub == p.sub &amp;&amp; r.obj == p.obj &amp;&amp; r.act == p.act // 當請求主體相同，對象相同以及動作相同時，則允許存取     ref: How It Works    References     Access-control list   屬性存取控制（ABAC）為何？   What is RBAC vs ABAC vs PBAC?  ","categories": ["website"],
        "tags": ["permission","acl","rbac","abac","pbac","casbin","permission granularity","role inheritance","role","policy","request","matcher","effect"],
        "url": "/website/website-permission/",
        "teaser": null
      },{
        "title": "Linux Kernel - Address Space Layout Randomization",
        "excerpt":"   本篇文章是備份自我之前上 Jserv 老師的課程作業內容  並加以修改排版內容  完整內容在 2021q1 Homework1 (quiz1)    Introduction to ASLR  ASLR 是一種電腦資訊安全的技巧，避免 memory corruption 等各種資安問題。ASLR 機制隨機分配 process 的地址(stack, heap, libraries)，使得地址變得難以預測     要注意 ASLR 僅是一種針對攻擊的 mitigation，並不能避免資安問題 comment by HexRabbit    舉例來說 return-to-libc attacks 一般應用於 buffer overflow attack 中; 攻擊手法為將 stack 中的返回地址替換為其他地址。使用 ASLR 可以讓 stack 的地址變得不可預測，使得攻擊變得更加困難      ASLR 0: 關閉  ASLR 1: Conservative Randomization  ASLR 2: Full Randomization    PIE  Position-independent executable(PIE) 又稱作 PIC，通常搭配 ASLR 使用，達到 address space layout randomization   Experiment  考慮以下實驗結果  1 2 3 $ gcc -no-pie a.c $ ./a.out &amp; $ cat /proc/PID/maps   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 ASLR 1, no PIE  00400000-00401000 r--p 00000000 08:10 40308                              /home/ambersun/ASLR/a.out 00401000-00402000 r-xp 00001000 08:10 40308                              /home/ambersun/ASLR/a.out 00402000-00403000 r--p 00002000 08:10 40308                              /home/ambersun/ASLR/a.out 00403000-00404000 r--p 00002000 08:10 40308                              /home/ambersun/ASLR/a.out 00404000-00405000 rw-p 00003000 08:10 40308                              /home/ambersun/ASLR/a.out 00405000-00426000 rw-p 00000000 00:00 0                                  [heap] 7f425fac9000-7f425faee000 r--p 00000000 08:10 37132                      /usr/lib/x86_64-linux-gnu/libc-2.31.so 7f425faee000-7f425fc66000 r-xp 00025000 08:10 37132                      /usr/lib/x86_64-linux-gnu/libc-2.31.so 7f425fc66000-7f425fcb0000 r--p 0019d000 08:10 37132                      /usr/lib/x86_64-linux-gnu/libc-2.31.so 7f425fcb0000-7f425fcb1000 ---p 001e7000 08:10 37132                      /usr/lib/x86_64-linux-gnu/libc-2.31.so 7f425fcb1000-7f425fcb4000 r--p 001e7000 08:10 37132                      /usr/lib/x86_64-linux-gnu/libc-2.31.so 7f425fcb4000-7f425fcb7000 rw-p 001ea000 08:10 37132                      /usr/lib/x86_64-linux-gnu/libc-2.31.so 7f425fcb7000-7f425fcbd000 rw-p 00000000 00:00 0  7f425fcc5000-7f425fcc6000 r--p 00000000 08:10 36843                      /usr/lib/x86_64-linux-gnu/ld-2.31.so 7f425fcc6000-7f425fce9000 r-xp 00001000 08:10 36843                      /usr/lib/x86_64-linux-gnu/ld-2.31.so 7f425fce9000-7f425fcf1000 r--p 00024000 08:10 36843                      /usr/lib/x86_64-linux-gnu/ld-2.31.so 7f425fcf2000-7f425fcf3000 r--p 0002c000 08:10 36843                      /usr/lib/x86_64-linux-gnu/ld-2.31.so 7f425fcf3000-7f425fcf4000 rw-p 0002d000 08:10 36843                      /usr/lib/x86_64-linux-gnu/ld-2.31.so 7f425fcf4000-7f425fcf5000 rw-p 00000000 00:00 0  7ffe726da000-7ffe726fb000 rw-p 00000000 00:00 0                          [stack] 7ffe7271c000-7ffe7271f000 r--p 00000000 00:00 0                          [vvar] 7ffe7271f000-7ffe72721000 r-xp 00000000 00:00 0                          [vdso]   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 ASLR 1, no PIE 00400000-00401000 r--p 00000000 08:10 40308                              /home/ambersun/ASLR/a.out 00401000-00402000 r-xp 00001000 08:10 40308                              /home/ambersun/ASLR/a.out 00402000-00403000 r--p 00002000 08:10 40308                              /home/ambersun/ASLR/a.out 00403000-00404000 r--p 00002000 08:10 40308                              /home/ambersun/ASLR/a.out 00404000-00405000 rw-p 00003000 08:10 40308                              /home/ambersun/ASLR/a.out 00405000-00426000 rw-p 00000000 00:00 0                                  [heap] 7f0600b54000-7f0600b79000 r--p 00000000 08:10 37132                      /usr/lib/x86_64-linux-gnu/libc-2.31.so 7f0600b79000-7f0600cf1000 r-xp 00025000 08:10 37132                      /usr/lib/x86_64-linux-gnu/libc-2.31.so 7f0600cf1000-7f0600d3b000 r--p 0019d000 08:10 37132                      /usr/lib/x86_64-linux-gnu/libc-2.31.so 7f0600d3b000-7f0600d3c000 ---p 001e7000 08:10 37132                      /usr/lib/x86_64-linux-gnu/libc-2.31.so 7f0600d3c000-7f0600d3f000 r--p 001e7000 08:10 37132                      /usr/lib/x86_64-linux-gnu/libc-2.31.so 7f0600d3f000-7f0600d42000 rw-p 001ea000 08:10 37132                      /usr/lib/x86_64-linux-gnu/libc-2.31.so 7f0600d42000-7f0600d48000 rw-p 00000000 00:00 0  7f0600d50000-7f0600d51000 r--p 00000000 08:10 36843                      /usr/lib/x86_64-linux-gnu/ld-2.31.so 7f0600d51000-7f0600d74000 r-xp 00001000 08:10 36843                      /usr/lib/x86_64-linux-gnu/ld-2.31.so 7f0600d74000-7f0600d7c000 r--p 00024000 08:10 36843                      /usr/lib/x86_64-linux-gnu/ld-2.31.so 7f0600d7d000-7f0600d7e000 r--p 0002c000 08:10 36843                      /usr/lib/x86_64-linux-gnu/ld-2.31.so 7f0600d7e000-7f0600d7f000 rw-p 0002d000 08:10 36843                      /usr/lib/x86_64-linux-gnu/ld-2.31.so 7f0600d7f000-7f0600d80000 rw-p 00000000 00:00 0  7ffff7f2b000-7ffff7f4c000 rw-p 00000000 00:00 0                          [stack] 7ffff7f93000-7ffff7f96000 r--p 00000000 00:00 0                          [vvar] 7ffff7f96000-7ffff7f98000 r-xp 00000000 00:00 0                          [vdso]   可以看到 heap 的位置，兩者是一樣的 00405000-00426000     c.f 64 位元下，no PIE 起點為 0x00400000 c.f 32 位元下，no PIE 起點為 0x08048000    gcc 預設 PIE 是開啟的，需使用 -no-pie 關閉，參見 gcc/defaults.h      关于 Linux 下 ASLR 与 PIE 的一些理解    ASLR Entropy  參考 Performance and Entropy of Various ASLR Implementations，分析 ASLR 的 entropy，考慮 stack pointer 的位置 考慮以下程式碼  1 2 3 4 5 6 7 8 9 10 11 12 13 static volatile void* getsp(void) {     volatile void *sp;     __asm__ __volatile__ (\"movq %%rsp,%0\" : \"=r\" (sp) : /* No input */);     return sp; }  int main(int argc, const char *argv[]) { \tprintf(\"sp: %p\\n\", getsp()); \twhile (1);  \treturn 0; }   得到以下執行結果  1 sp: 0x7ffeb6b5bd10   對照 /proc/PID/maps  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 00400000-00401000 r--p 00000000 08:10 40306                              /home/ambersun/ASLR/a.out 00401000-00402000 r-xp 00001000 08:10 40306                              /home/ambersun/ASLR/a.out 00402000-00403000 r--p 00002000 08:10 40306                              /home/ambersun/ASLR/a.out 00403000-00404000 r--p 00002000 08:10 40306                              /home/ambersun/ASLR/a.out 00404000-00405000 rw-p 00003000 08:10 40306                              /home/ambersun/ASLR/a.out 00405000-00426000 rw-p 00000000 00:00 0                                  [heap] 7f14bdaaf000-7f14bdad4000 r--p 00000000 08:10 37132                      /usr/lib/x86_64-linux-gnu/libc-2.31.so 7f14bdad4000-7f14bdc4c000 r-xp 00025000 08:10 37132                      /usr/lib/x86_64-linux-gnu/libc-2.31.so 7f14bdc4c000-7f14bdc96000 r--p 0019d000 08:10 37132                      /usr/lib/x86_64-linux-gnu/libc-2.31.so 7f14bdc96000-7f14bdc97000 ---p 001e7000 08:10 37132                      /usr/lib/x86_64-linux-gnu/libc-2.31.so 7f14bdc97000-7f14bdc9a000 r--p 001e7000 08:10 37132                      /usr/lib/x86_64-linux-gnu/libc-2.31.so 7f14bdc9a000-7f14bdc9d000 rw-p 001ea000 08:10 37132                      /usr/lib/x86_64-linux-gnu/libc-2.31.so 7f14bdc9d000-7f14bdca3000 rw-p 00000000 00:00 0  7f14bdcab000-7f14bdcac000 r--p 00000000 08:10 36843                      /usr/lib/x86_64-linux-gnu/ld-2.31.so 7f14bdcac000-7f14bdccf000 r-xp 00001000 08:10 36843                      /usr/lib/x86_64-linux-gnu/ld-2.31.so 7f14bdccf000-7f14bdcd7000 r--p 00024000 08:10 36843                      /usr/lib/x86_64-linux-gnu/ld-2.31.so 7f14bdcd8000-7f14bdcd9000 r--p 0002c000 08:10 36843                      /usr/lib/x86_64-linux-gnu/ld-2.31.so 7f14bdcd9000-7f14bdcda000 rw-p 0002d000 08:10 36843                      /usr/lib/x86_64-linux-gnu/ld-2.31.so 7f14bdcda000-7f14bdcdb000 rw-p 00000000 00:00 0  7ffeb6b3c000-7ffeb6b5d000 rw-p 00000000 00:00 0                          [stack] 7ffeb6bd4000-7ffeb6bd7000 r--p 00000000 00:00 0                          [vvar] 7ffeb6bd7000-7ffeb6bd9000 r-xp 00000000 00:00 0                          [vdso]   不難發現 sp 的位置有一點誤差，我想這也是可接受的範圍內(0x7ffeb6b5d000, 0x7ffeb6b5bd10) 值得注意的是，上述 16 進位輸出共有 48 位元，而不是 64 位元 可參考 why virtual address are 48 bits not 64 bits? [duplicate]    多次實驗觀察 stack pointer 可得以下結果  1 2 3 4 5 6 sp: 0x7fff296fa310 sp: 0x7fffd5825950 sp: 0x7ffc725181d0 sp: 0x7ffe6da0dfc0 sp: 0x7ffcd8fc69d0 ...   可以發現到這些 virtual address 的 MSB 部分都是相同，而最後 4 bits 都是 0。這也就是為甚麼論文中實驗並不採用全部位元的原因     For Debian, we observed 30 bits of entropy in the stack.   This was bits 4 to 34 (least significant to most significant)       有興趣的話也可以觀察看看 libc 或相關 library 的地址在開啟 ASLR 下有多少 bits 的 entropy  comment by HexRabbit    Experiment  考慮以下測試程式碼     main.c     1 2 3 4 5 6 7 8 9 10 11 int main(int argc, const char *argv[]) {   if (fork() == 0) {       char *str[] = {\"./test\", NULL, NULL};       char *envp[] = {0};       if (execve(\"./test\", str, envp) &lt; 0) {           perror(\"error\");           exit(0);       }   }   return 0; }           test.c     1 2 3 4 5 6 int main(int argc, const char *argv[]) {   register void *p asm(\"sp\");   printf(\"%p\\n\", p);    return 0; }           為了驗證 ASLR 的 entropy，我參照論文設計了一個簡單的實驗，內容是取得 stack pointer 的位置去分析(搭配 execve 以及 fork) 根據 man execve     execve() executes the program referred to by pathname.  This causes the program that is currently being run by the calling process to be replaced with a new program, with newly initialized stack, heap, and (initialized and uninitialized) data segments.    execve 會重新 new stack, heap 以及 data 區塊，藉由分析 stack pointer 我們可以知道 ASLR 的 entropy。這次實驗總共測試 1000000(一百萬) 次，分別在 32 位元(raspberry pi zero wh(rapbian))，以及 64 位元(ubuntu 20.04 LTS) 下測試 完整程式碼可參考 linux2021q1_quiz1/ASLR   64 位元  1 2 3 4 5 6 7 8 9 10 11 12 13 地址: 重複次數 ... 0x7ffeb9c5d880: 2 0x7fff0baab970: 2 0x7ffeada07720: 2 0x7ffdcce3da80: 2 0x7ffe8c644020: 2 0x7ffec6822460: 2 0x7ffe54d41c80: 2 0x7fffe12b40a0: 2 0x7ffda39f9960: 2 0x7fff7dabd5a0: 2 0x7ffcdb205a70: 3  總共統計結果  1 2 3 4 重複次數: 有多少地址重複n次 1: 999083 2: 457 3: 1  在1百萬重複次數中，僅有 917(457*2+1*3) 個地址重複到   32 位元  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 地址: 重複次數 ... 0xbea55dd8: 544 0xbec38dd8: 545 0xbeb92dd8: 545 0xbeb4fdd8: 545 0xbeb1ddd8: 545 0xbea92dd8: 547 0xbee5cdd8: 549 0xbeaf1dd8: 551 0xbe9dedd8: 553 0xbe931dd8: 556 0xbeec9dd8: 559 0xbe927dd8: 566 0xbeb1fdd8: 570 0xbe992dd8: 572  可以看到在 32 位元架構下，重複機率很高，意味著攻擊者相較於 64 位元架構中，更容易猜到地址。   更改過的程式碼如下:  1 2 3 4 5 6 7 8 9 10 11 12 int main(int argc, char **argv) {     int *tmp = (int *)malloc(sizeof(int));     int a = (intptr_t)tmp;     srand(a);     free(tmp);      size_t count = 20;      node_t *list = NULL;     while (count--) {         list = list_make_node_t(list, random() % 1024);     }   首先 malloc 一個空間出來，將其地址轉為儲存起來，使用 intptr_t 強制轉型(將 pointer 轉型為 integer)，並且使用其作為亂數種子，使用完之後當然要把他 free 掉避免 memory leak。      TODO: 回顧 「Linux 核心設計」 課程說明 (2021年春季) 第 19 頁，嘗試學習早期 mimalloc 運用 ASLR 的手法。  comment by jserv    考慮 ASLR 以及 PIE 可以將程式改寫如下，參考 mimalloc/random.c  1 2 3 4 5 6 int main(int argc, char **argv) {     srand((uintptr_t)&amp;main);     size_t count = 20;     node_t *list = NULL;     while (count--) {list = list_make_node_t(list, random() % 1024); }   執行結果如下  1 2 3 4 5 6 7 8 $ ./a.out 568087200 NOT IN ORDER : 537 827 166 485 66 417 108 305 462 893 98 688 814 861 422 513 477 327 237 170      IN ORDER : 66 98 108 166 170 237 305 327 417 422 462 477 485 513 537 688 814 827 861 893 $ ./a.out -1303850336 NOT IN ORDER : 111 782 625 900 491 356 273 901 803 125 278 511 612 108 51 450 278 473 436 495      IN ORDER : 51 108 111 125 273 278 278 356 436 450 473 491 495 511 612 625 782 803 900 901  ","categories": ["linux-kernel"],
        "tags": ["linux","aslr","prng","pie"],
        "url": "/linux-kernel/linux-aslr/",
        "teaser": null
      },{
        "title": "Kubernetes 從零開始 - Local 開發測試好朋友 Skaffold",
        "excerpt":"Development Obstacles in Kubernetes  不知道你有沒有這種感覺，Kubernetes 對本地開發來說真的挺不友善的(至少以我來說)  除了你要架設一個本地的 Kubernetes Cluster 之外，還要不斷地手動更新 image   像我自己在寫後端的 API 的時候，往往需要不斷地測試程式碼，重新啟動 server  配合前端，資料庫等等服務，驗證自己的程式碼的正確性  雖然不像前端可以無腦的 $ npm run dev 自動更新，但這仍是屬於可以接受的範圍   換到 Kubernetes 之後，整件事情變得相對麻煩許多  因為本質上是執行 container, 而 container 是 immutable 的，你不能隨意的更改  變成你需要重新 build 一個 image，然後 push 到 registry，最後更新 deployment  雖然如今不用推到 registry 也可以直接在本地使用，但這仍是一個很麻煩的事情      registry 是一個儲存 container image 的地方，你可以把它想像成一個 docker hub    原因在 build image 以及將 image 同步到 local cluster 上面  以 Golang 來說，$ go mod download 很花時間，我網路慢到不行，每一次的更新都要重複的花費這些時間  同步進 cluster 也是相對耗費時間的一項事情  更別說手動      為什麼不要直接跑 binary 在 local 就可以了？  事情往往沒有那麼簡單老實說，當你的相依服務過多，env, config 一堆啦  與其跑個什麼 docker-compose 自己設定  你真的倒不如直接跑在 Kubernetes 上面，最少設定檔都幫你寫好了    Skaffold  根據現有的狀況，我期待至少可以解決以下幾個問題     Kubernetes 能夠自動監控本地的程式碼，並且自動 build image   簡化手動推送 image 到 registry 的流程   Skaffold 就是一個能夠解決這些問題的工具        ref: Architecture and Design    Skaffold Builder  在你初始化 Skaffold 專案的時候($ skaffold init)，他會需要你選擇 builder  這個 builder 會告訴 Skaffold 如何 build 你的 image  有一些是需要 local build 的，那可以選擇 Dockerfile  有一些則是現有的像是 postgres, redis 之類的      Disable Registry Push  Skaffold 預設是會將 image 推送到 registry 的  大多數情況下，我們是不需要的  可以在 skaffold.yaml 中加入以下設定  1 2 3 build:   local:     push: false   就可以關閉   Kube Context  像我自己平常在本地開發的時候是使用 k3d 來建立 local cluster  平常會區分工作用的以及私人開發用的 cluster  所以你的電腦上面就會有不同的 kube context   以我的電腦來說可以看到有三個，然後目前是使用 k3d-views-k3d   1 2 3 4 5 $ kubectl config get-contexts CURRENT   NAME              CLUSTER           AUTHINFO                NAMESPACE           k3d-k3s-default   k3d-k3s-default   admin@k3d-k3s-default    *         k3d-views-k3d     k3d-views-k3d     admin@k3d-views-k3d                orbstack          orbstack          orbstack      切換 kube context $ kubectl config use-context &lt;context-name&gt;    Skaffold 可以設定使用的 kube context，這樣就不用每次都要切換   1 2 deploy:   kubeContext: k3d-views-k3d   Example  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion: skaffold/v4beta11 kind: Config metadata:   name: count-page-views build:   local:     push: false   artifacts:     - image: views-service       docker:         dockerfile: Dockerfile manifests:   rawYaml:     - manifests/redis.yaml     - manifests/views.yaml deploy:   kubeContext: k3d-views-k3d   以上就是一個簡單的 skaffold 的例子  可以看到有兩個 yaml 檔，其中 views.yaml 使用的 views-service 的 image 是需要 build image 的所以寫在 build 區塊裡面  manifests 裡面就是單純的 deployment, service 之類的 yaml 檔   完整的範例可以參考 ambersun1234/count-page-views   References     管理多個 Kubernetes Cluster：建立、切換、合併 context  ","categories": ["kubernetes"],
        "tags": ["local development","yaml","skaffold","registry","container","local cluster","k3d"],
        "url": "/kubernetes/kubernetes-dev/",
        "teaser": null
      },{
        "title": "Kubernetes 從零開始 - 容器基本抽象 Pod",
        "excerpt":"Abstraction over Container  Pod 其實是為了更好的管理 Container 而生的一層抽象層  所以他同時也是最小的部署單位(注意到不是 Container)   既然他是一層管理容器的環境  他具有一定的特性如，所有同一個環境下的 Container 都擁有相同的網路環境，可以存取相同的 Volume 資料等等的  並且也可以一起被執行排程  事實上你可以把它看待成是 logical host   這也意味著他可以被一起管理，這也是為什麼我們會用 Pod 來管理 Container   Introduction to Pod  Pod 簡單的理解，就只是一個提供一個或多個容器執行的環境(當然也提供了一些進階的功能)  然後同一個環境下，他們可以一起 share Volume，網路等等的資源      多個 Container 的設計是比較少見的，一般來說 Pod 只會有一個 Container  如果要做 Scale, 正確的做法是增加 Pod 數量，而不是增加 Container 數量       scaling 可以參考 資料庫 - 初探分散式資料庫 | Shawn Hsu    How does Pod Scheduled  容器的執行，在 Kubernetes 來說是執行在 Node 之上的  Pod 是容器的抽象層，同時也是用於執行容器環境的抽象層，但是真正執行的地方是在 Node 上      Node 是一台真正的機器，可以是 VM，也可以是實體機器    要執行的時候，Kubernetes 會將 Pod schedule 到任一 “健康” 的 Node 上(這個過程稱為 binding)  節點不健康的意思是，他可能網路是壞的，硬碟掛了之類的   Kubernetes v1.31 之後，你可以指定 Pod 要執行在哪一個 Node 上  根據不同的實體機器，他的底層可以是 Linux 或是 Windows   Pod Lifecycle  Pod 被設計成是相對短命並且可以被隨時丟棄的 entity  他不是被用來執行長時間的服務的   用 Pod 然後期待有 Self Healing 是不現實的  Pod 只會 schedule 一次，注意到不是執行一次  並且，前面提到 Pod 是執行在 Node 之上的，所以當 Node 掛掉的時候，Pod 也會跟著掛掉，也不會被重新排程執行      一個 pod 一生只會在一個 node 上執行，不會跑到其他 node 上       當 pod 還沒有開始執行，node 就因為某些原因掛掉，pod 也 不會 被重新排程    當他失敗的時候，狀態上會被標註成 Failed  並且 timeout 之後會由 Controller 進行 GC(Garbage Collection)     事實上你可以重新 deploy 一個一模一樣的 pod 進 cluster  但是他只是長的一樣，並不代表他是同一個 pod  也就是說他的環境是不同的，也沒辦法存取到之前的資料  他也不一定會在一樣的 node 上執行(因為 scheduler 會選擇健康的節點)   以下圖來說，Volume 也會被刪除(如果 pod 被刪除)         ref: Pod Lifecycle    Why you Shouldn’t Directly Invoke Pod  在上面我們提到了許多關於 Pod 的基本特性  你可以發現到，直接手動去操作他不是一個好的選擇   最大的問題在於，Pod 是一個相對短命的 entity  並且失敗的時候也沒辦法自動重啟  因此，通常我們會使用不同的高階抽象(稱為 workloads)如 Deployment 來管理 Pod   Pod Template  1 2 3 4 5 6 7 8 9 10 11 12 apiVersion: v1 kind: Pod metadata:   name: my-pod spec:   containers:     - name: my-echo-app       image: busybox:1.28       command:         [           \"sh\", \"-c\", \"echo 'hello from busy box'; exit 0\"         ]   這是一個簡單的 Pod Template  注意到他跟其他 workloads 的 template 寫法不太一樣  這個例子就是一個簡單的 echo app   1 2 3 $ kubectl get pods NAME     READY   STATUS      RESTARTS     AGE my-pod   0/1     Completed   1 (1s ago)   1s   要小心一件事情是，執行你要的東西之後必須要有一個 exit 0  不然他會一直重啟(CrashLoopBackOff)   1 2 3 $ kubectl get pods NAME     READY   STATUS             RESTARTS      AGE my-pod   0/1     CrashLoopBackOff   2 (16s ago)   41s   如果是以 kind: Deployment 來說會長這樣  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 apiVersion: apps/v1 kind: Deployment metadata:   name: views-deployment   labels:     app: views-app spec:   selector:     matchLabels:       app: views-app   template:     metadata:       labels:         app: views-app     spec:       volumes:         - name: credential           configMap:             name: views-config             items:               - key: credential.json                 path: credential.json       initContainers:         - name: redis-stabilized           image: busybox:1.28           command:             [               \"sh\",               \"-c\",               'until nc -z redis-service 6379; do echo $(date \"+%Y-%m-%d %H:%M:%S\") waiting...; sleep 1; done;'             ]       containers:         - name: views           image: views-service:latest           ports:             - containerPort: 8888           imagePullPolicy: \"IfNotPresent\"           volumeMounts:             - name: credential               mountPath: \"/credential.json\"               subPath: credential.json           env:             - name: GOOGLE_APPLICATION_CREDENTIALS               value: \"/credential.json\"      完整範例可參考 ambersun1234/count-page-views    pod 有關的設定，像是你要跑的 image, 需要的環境變數等等  基本上是寫在 spec.template 底下  以這個例子你可以看到有 initContainers, containers 還有 volumes 的設定      基礎語法可參考 Kubernetes 從零開始 - 無痛初探 K8s! | Shawn Hsu    基本上語法不用硬記，你有辦法理解每一個的意思，要寫的時候再查就好了   Init Containers and Sidecar Containers  Init Containers  pod 除了主要執行的 container 之外，你可以設定所謂的 init containers 用於執行初始化相關的功能  比方說 web server 啟動之前要先確定你的 rabbitmq 已經啟動了  你可以這樣寫   1 2 3 4 5 6 7 8 9 10 11 12 13 initContainers:   - name: redis-stabilized     image: busybox:1.28     command:       [         \"sh\",         \"-c\",         'until nc -z redis-service 6379; do echo $(date \"+%Y-%m-%d %H:%M:%S\") waiting...; sleep 1; done;'       ]     resources:       limits:         memory: \"256Mi\"         cpu: \"500m\"   所以他的執行順序會是 initContainers :arrow_right: containers  然後我們說過 pod 是提供一個環境，所以理論上他們都可以共享彼此的資源  但以這個例子來說是有困難的，因為 initContainers 會在 containers 之前執行  等於說他們兩個的執行時間是沒有重疊到的  因此傳輸資料只能是 單向的(從 initContainers 到 containers)   Sidecar Containers  sidecar container 這個名詞我有點陌生  假設你的主要服務是 web server，然後你想要一個 container 來做 log 的收集  這個時候你可以開另一個 container 來做 log 的收集  所以你會有兩個 container 一起執行在同一個 pod 上, app 以及 log container   log container 就可以被稱之為 sidecar container  他是主要服務的附屬 container  因為是附屬的，所以理論上需要一起執行，也一樣，可以共享彼此的資源  有別於 init containers 只能進行單向資料傳輸，因為 sidecar containers 是一起執行的，所以他們可以進行雙向資料傳輸   sidecar container 有兩種寫法  1 2 3 4 5 6 7 8 9 10 11 containers: - name: app     image: busybox     command: ['sh', '-c', 'echo Hello Kubernetes! &amp;&amp; sleep 3600'] - name: logshipper     image: alpine:latest     restartPolicy: Always     command: ['sh', '-c', 'tail -F /opt/logs.txt']     volumeMounts:     - name: data         mountPath: /opt   很常見的做法是使用多個 container 放在一起  雖然實務上推薦一個 pod 只有一個 container  但這樣做也是可以的   另一種則是使用 Init Containers 來實作  1 2 3 4 5 6 7 8 initContainers: - name: logshipper     image: alpine:latest     restartPolicy: Always     command: ['sh', '-c', 'tail -F /opt/logs.txt']     volumeMounts:     - name: data         mountPath: /opt   Sidecar Inside Init Containers?  多個 container 的寫法會有什麼問題？  因為你無法保證他們的執行順序  所以你可能會遇到這樣的問題，log collector 先執行，但是 app 還沒有啟動  如果你使用其他 workloads 他可以自動幫你管理失敗的情況  不過他仍然是一個 workaround   initContainers 的寫法注意看，差別只在 restartPolicy: Always  對！ sidecar container 是 init container 的特例！  sidecar container 在 init container 結束之後仍會繼續執行  所以這時候 sidecar 就可以跟 app container 一起執行   要注意到的是，sidecar container 的狀態(initContainers 的寫法)是不會影響到 pod 的狀態的  也就是說 app container 仍然可以順利的結束  他們的生命週期是獨立的   References     Kubernetes 1.28 Sidecar Container 初體驗   Sidecar Containers   Pods   Pod Lifecycle   Kubernetes v1.28: Introducing native sidecar containers  ","categories": ["kubernetes"],
        "tags": ["pod","container","lifecycle","node","binding","ephemeral","scale","scheduling","workloads"],
        "url": "/kubernetes/kubernetes-pod/",
        "teaser": null
      },{
        "title": "資料庫 - 如何正確設定高可用的 Redis",
        "excerpt":"Preface  要如何提高系統的高可用性以及高效能，最常想到的應該就是分散式系統了  基本上你可以發現，不論是哪一段的系統架構，他們通常都會是使用分散式架構以達到高可用性   Redis 其實也可以做到這件事情  本文將會看看兩大 Redis 的高可用性解決方案  Redis Master-Slave Replication 以及 Redis Cluster   Distributed System Basics  這部分可以參考 資料庫 - 初探分散式資料庫 | Shawn Hsu   Synchronous vs Asynchronous Replication  Redis 在多數情況下都是使用 Asynchronous Replication  好處在於不必等待全部的人都寫完才能夠回復 client  當 master 寫完之後就可以回復 client，而 slave 則是在稍後同步      非同步複製下，master 仍然會知道 slave 目前的同步狀態  slave 會定期回報自己的同步狀態給 master，master 就可以擁有更彈性的同步策略  如 partial replication    你當然也可以用同步的模式(WAIT 指令)  不過要注意的是，即使同步模式下，系統依然不會有 100% consistency  他只會確保一定數量的節點擁有該資料而已(i.e. Quorum)  並且使用同步複製速度會變慢許多   Redis Master-Slave Replication  一個最簡單可以提高吞吐量的做法，我們已經會了  就是使用多台機器組成 cluster 分擔負載   Redis 的做法是使用一個 master 以及多個 slave 組成  其中 master 會負責寫入，而 slave 則是負責讀取(Single Leader Replication, 可參考資料庫 - 初探分散式資料庫 | Shawn Hsu)   Replication Mechanism  因為 master 是主要負責寫入的節點，其餘 slave 則是負責讀取  因此你需要將 master 的資料同步到 slave 上面   分散式系統要注意的一個點是節點之間的通訊  網路是不可靠的，因此在任一時間都有可能會斷線   如果 master slave 之間斷線又重新連線，那麼同步的資料僅有 部分資料 而已  其實這也很正常，畢竟你才斷個幾分鐘沒必要將所有的資料都同步一次  不僅浪費網路資源也浪費時間   但是如果已經掉線太久，就必須要同步所有的資料了(透過 snapshot)         可參考 Synchronous vs Asynchronous Replication    具體來說，slave 與 master 之間是用 offset 的方式來同步資料  意思是說，slave 會 PSYNC 傳一組識別符號(Replication ID + offset)  代表，針對 這個 master(replication id) 的 這個 offset 之後的資料要同步  所以同步部分資料的 “部分” 就是這個 offset 之後的資料   如果 Replication ID 找不到或者是 offset 過舊，那麼就需要透過 snapshot 來同步資料(i.e. Full Synchronization)   Full Synchronization  當 master 需要同步所有資料時  他會開一條 thread 將目前的資料寫入 RDB(可參考 資料庫 - Cache Strategies 與常見的 Solutions)  與此同時，master 會暫時停止寫入   注意到服務仍然會持續進行，他只是先把 client write 的指令放到 buffer 裡面  原因也挺明顯的，因為 master 正在寫入 RDB，如果這時候再寫入，可能會造成資料不一致  所以他會先暫停寫入，等到同步完畢之後再繼續   slave 收到 RDB 之後，會將資料寫入自己的資料庫  master 則會恢復寫入，並且將 buffer 裡面的指令一一執行  然後一樣同步給 slave         上圖可以看到是使用 CoW(Copy on Write) 的方式來實現, 可參考 資料庫 - Cache Strategies 與常見的 Solutions    Replication ID  前面提到同步其實是依靠 replication id + offset  其中 replication id 就是當前 master 的識別符號  所有 slave 都會記住 master 的 replication id，並依此來同步資料   當遇到 failover, 新的 master 被選上了之後，他會產生一個新的 replication id  然後 slave 會記住這個新的 replication id，並且重新同步資料   但問題是，master 新舊交替之際，並非所有 slave 能夠同步的這麼快  意思是有些 slave 可能還在同步舊的 master 的資料，但是 master 已經換了  為了確保同步資料可以銜接順利, 所以 master 有兩組 replication id 以及 offset     自己的 replication id + offset   上一個 master 的 replication id + offset         第二個 replication id 是空的原因是因為，沒有上一個    這樣還沒跟上的 slave 一樣可以在新的 master 上同步舊的 master 的資料  當 offset 已經同步完成，就可以無縫切換到新的 master offset 上面      新的 master 可能也沒有同步完成  因此他也可以先把舊的資料從舊的 master 同步過來    Task Offloading  slave 僅可以提供讀取的功能，所以你其實可以將一些 task offload 給 slave  僅需要讀取的 task 可以交給 slave 去處理，這樣 master 就可以專心處理寫入的 task  大量的讀取交給 slave 可以減輕 master 的負擔   Data Persistence  在 資料庫 - Cache Strategies 與常見的 Solutions | Shawn Hsu 中我們提到 Redis 會自動幫你定期備份資料  這樣即使是意外重啟，他也能以盡量新的資料重新啟動   不過寫入永久儲存在某些時候可能不是我們想要的  因為他會降低效能  透過 redis.conf 你可以關閉寫入永久儲存，僅透過定期備份至 slave 有一個保障即可  但要注意的是，這樣做會增加資料丟失的風險，尤其是重啟的時候      redis 2.8.18 之後支援 diskless replication    重啟 master 之後，因為 slave 會自動同步資料  如果你關閉了寫入永久儲存，那麼重啟後 slave 將會自動刪除所有資料(因為要跟 master 同步)   注意到一件很重要的事情  Redis 的這個 master-slave 是 不包含 高可用性的   Redis Sentinel  為什麼 Redis Master-Slave Replication 不包含高可用性呢  他不是有多個節點可以服務嗎？   高可用性其實不單只是有幾個節點活著，你要看他能不能提供正常的服務  當 master 掛掉之後，剩下的 slave 能夠做什麼？  單純的讀取功能而已，他沒辦法提供寫入的功能  因此單純的 master-slave replication 並不包含高可用性   像 Kafka 與 Zookeeper 一樣，Redis 也有一個專門的服務來提供類似的功能稱為 Redis Sentinel  sentinel 翻譯為哨兵，主要負責監控以及提供 failover 的功能   要監控一個 Redis cluster，你需要至少三個 sentinel  因為如果只有一個，那他掛了誰要監控服務本身呢？      可參考 資料庫 - 從 Apache Kafka 認識 Message Queue | Shawn Hsu    How to Perform Failover  sentinel 會定期監控以及嘗試復原 master 的狀態(slave 也會被監控 但僅僅是監控)  因為 master 在這個情況下是 single source of truth, slave 只是跟隨著 master 而已   基本上認定失效的過程如同 資料庫 - 初探分散式資料庫 | Shawn Hsu 所述  sentinel process 要有一定的數量也是因為要進行共識機制   Objectively Down  具體來說，每個 sentinel 會定期確認 master 還有沒有活著(ping 他之類的)，過程稱之為 heartbeat  當一個 sentinel 認定 master 已經掛了，稱之為 Subjectively Down(SDOWN)  單一節點認定失效(主觀認定 SDOWN)是不夠的，需要多數 sentinel 認定失效才可以(客觀認定 ODOWN)      sentinel 數量通常是奇數，因為要 “大多數” 同意(i.e. 過半數)    當一定數量的 sentinel 認定 master 掛了(ODOWN)，那麼他們就會進行 failover(由其中一個 sentinel 主導)  多數節點認定的方式是採用 Quorum 的方式，你可以設定需要幾個 sentinel 同意才能進行 failover   Failover  Objectively Down 之後，sentinel 會進行 failover  前面提到會由一個 sentinel 主導 failover  問題是誰？   sentinel 之間會舉行投票，選出一個 sentinel 當頭  但是這個部分 不是使用 Quorum  單純是採用多數決的方式   選出來的 sentinel 就會負責執行 failover     所以寫起來會長這樣   1 2 3 4 5 6 7 8 9 10 sentinel monitor mymaster 127.0.0.1 6379 2 &gt; 監控位於 127.0.0.1:6379 名為 mymaster 的 master &gt; 他需要至少 2 個 sentinel 認定失效才能進行 failover  sentinel down-after-milliseconds mymaster 60000 sentinel failover-timeout mymaster 180000 sentinel parallel-syncs mymaster 1 &gt; 當切換 master 的時候，slave 必須同時更新 metadata 到新的 master &gt; 這個數字就是同時更新的數量，同時同步的 slave 數量越多，要花的時間越多！ &gt; 因為同步的時候是沒辦法服務 client 的(i.e. blocking)   主觀認定下線的節點是不會被 sentinel 提拔為 master 的  雖然前面提到 sentinel 並不會對 slave 執行 failover, 但他仍會一定程度的監控 slave  當 slave 已經進入 ODOWN 的狀態，sentinel 預設也不會有任何動作   sentinel 選擇新的 master 是根據不同的因素決定，從高到低分別是     最後同步時間            跟 master 同步的時間越短，代表資料越新           replica 優先級            你可以根據自己的喜好訂定要優先使用哪個 slave 當作 master, 可能因為地理位置的關係選擇他會有比較低的延遲之類的           最後同步資料量            同步時間一樣不代表資料一樣新，因此也可以根據同步資料量來決定           Run id   Service Discovery  sentinel 一般來說都會 deploy 不止一個 instance 用以監控 Redis 的服務  主要的原因就是要避免單點失效   sentinel 知道 master 的位置，所以可以很輕鬆的監控他的狀態  不過當要進行共識決策的時候，sentinel 彼此之間也需要知道彼此的存在  sentinel 的 service discovery 機制是透過 Redis Pub/Sub 來達成的      可參考 設計模式 101 - Observer Pattern | Shawn Hsu    publisher/subscriber pattern 透過一個 channel/topic 傳遞訊息  所以新的 sentinel 加入的時候，他會 publish 一個訊息到 hello channel  跟其他人說 我是誰我在哪 以及現在 master 的狀態  所以 sentinel 之間可以互相更新系統的狀態   比方說，master 新舊交替之際，就是透過 sentinel 幫 slave 更新 metadata  讓他不要找錯地方要資料  當然，sentinel 可能會更新錯的 metadata, 因此 sentinel 在廣播狀態的時候會先確認自己的狀態是否正確(先拿別人同步過來的比對一下)   Example  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 version: \"3\" services:   # master-slave replication   master:     image: redis    slave:     deploy:       replicas: 2     image: redis     command: redis-server --slaveof master 6379     depends_on:       - master    # sentinel   sentinel-1:     image: redis     volumes:       - ./config:/etc/redis-config     command: redis-sentinel /etc/redis-config/sentinel-1.conf     depends_on:       - master    sentinel-2:     image: redis     volumes:       - ./config:/etc/redis-config     command: redis-sentinel /etc/redis-config/sentinel-2.conf     depends_on:       - master    sentinel-3:     image: redis     volumes:       - ./config:/etc/redis-config     command: redis-sentinel /etc/redis-config/sentinel-3.conf     depends_on:       - master    sentinel-4:     image: redis     volumes:       - ./config:/etc/redis-config     command: redis-sentinel /etc/redis-config/sentinel-4.conf     depends_on:       - master   基本上就是這樣跑起來即可  可以看到其實相當的簡單，slave 需要指定 master 的位置，然後 sentinel 也需要指定相關的設定(使用 config 檔案)      詳細的設定檔可以參考 ambersun1234/blog-labs/redis-cluster/sentinel-master-slave    Why not Deploy Replicas of Sentinel?  sentinel 的設定檔都是一樣的，那你說為什麼不直接指定 replica 就好了還要自己寫  原因在於，sentinel 區分的方式是用 id，而 mount 一樣的 config 檔案會讓 sentinel 誤以為是同一個 instance  config 裡面他會自己寫入一行類似這樣的東西 sentinel myid 782777a7a6fe0504ef3da25d2146a8ceaffd98e6  當同一份 config 檔案被 mount 進去的時候，他還是會認為是同一個 sentinel  所以 failover 只會在 quorum = 1 的時候觸發   你可以連進去 container 內部看他有沒有讀取到正確的 sentinel 數量   1 2 3 4 5 $ docker exec -it sentinel-1 sh # redis-cli -p 26379 127.0.0.1:26379&gt; sentinel master mymaster 33) \"num-other-sentinels\" 34) \"3\"      sentinel 預設的 port 是 26379    以我的例子來看，我有另外三個 sentinel，總共四個      mymaster 是設定檔裡面的 master name    Observe Sentinel Behaviour  將 Redis Master Slave 與 Sentinel 跑起來之後我們可以先觀察到，一開始的 master 是 172.20.0.2  1 2 3 4 5 $ docker exec sentinel-master-slave-sentinel-1-1 sh # redis-cli -p 26379 127.0.0.1:26379&gt; sentinel master mymaster 3) \"ip\" 4) \"172.20.0.2\"   手動停掉 master 看看 sentinel 有沒有觸發 failover      可以看到在 master 被手動停止之後，隔了 2 秒鐘之後所有 sentinel 都觸發了 SDOWN  並且由 sentinel-4 觸發了 ODOWN(Quorum 3/2)      你可以設定 down-after-milliseconds 來調整 sentinel 認定失效的時間，這裡我設定的是 2 秒       之後，根據我們所學的，是要選出一個 sentinel 負責處理 failover  以這個例子來看是 sentinel-4  你可以看到其他 sentinel 也都想要負責處理 failover，但由於 sentinel-4 先發起，所以就是他負責的  因此其他 sentinel 會顯示說 Next failover delay: I will not start a failover   其他 sentinel 就會進行投票(+vote-for-leader)  觀察上述 log 你可以看到大家都投給 88f... 這個 sentinel，而他就是 sentinel-4      所以 sentinel-4 號會負責選出適當的 slave 並且將它 promote 為 master  +selected-slave 可以看出他選擇 172.20.0.8      當整個選舉都結束之後，就會將新的資訊同步給其他 sentinel 以及 slave  可以看到每個 sentinel 都從 88f...(i.e. sentinel-4) 接收到新的 config(+config-update-from)      與此同時，被選中的 slave 也會被 promote 為 master  這時候你看到他會先 discard previous master cache(這就是為什麼我說 Redis 並沒有提供 Strong Consistency)  除此之外，他也會更新第二組的 Replication ID，並且跟你說上一組 id 到哪一個 offset 就要換了   最後再驗證一下  1 2 3 4 5 $ docker exec sentinel-master-slave-sentinel-1-1 sh # redis-cli -p 26379 127.0.0.1:26379&gt; sentinel master mymaster 3) \"ip\" 4) \"172.20.0.8\"   到這裡，我們已經完整的走過一次 Redis Sentinel 的 failover 流程了   Can’t resolve master instance hostname.  預設上來說，redis 並沒有開啟 resolve host name 的功能，需要手動啟動  在 sentinel.conf 裡面  1 sentinel resolve-hostnames yes   WARNING: Sentinel was not able to save the new configuration on disk!!!: Device or resource busy  另外就是 redis 是依靠 config 這個資料夾來存放 sentinel 的設定  所以不能直接 mount 檔案，這樣他會無法寫入  解法就是把 config 檔案放在 config 資料夾底下，mount 進去   1 2 volumes:   - ./config:/etc/redis-config   Strong Consistency over Redis Master-Slave Replication and Sentinel  儘管我們透過許多手段來提高 Redis 的高可用性  一致性仍然無法完美的保證，當 master 進行 failover 的時候還是會有掉資料的風險  client 仍然會向舊的 master 寫入資料，當他恢復的時候，他已經是 slave 了  即使寫入的資料已經被 acknowledged，為了跟上新的 master，那些後來的資料就會被刪除，造成 data loss   基於這個原因，我們可以得知 Redis 是 Last Failover Wins  因為所有的資料最終都是同步成 failover 過去的 master 上面  Redis 團隊也嘗試引入 Raft 共識機制，不過看起來沒有很積極的在開發  RedisLabs/redisraft 就提供了強一致性的解決方案   Redis Cluster  另一個解決方案就是使用 Redis Cluster  相比於 Redis Master-Slave Replication 以及 Redis Sentinel  上述的方案同一時間只有一個 master，而 Redis Cluster 可以有多個 master  在可用性會更加的高      不過要注意的是 Redis Cluster 仍然沒有提供強一致性    Example  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 version: \"3\" services:   node1:     image: redis     volumes:       - ./config/:/etc/redis/     command: redis-server /etc/redis/redis-1.conf    nodex: ...    cluster:     image: redis     command: &gt;       redis-cli --cluster create         node1:7000         node2:7000         node3:7000         node4:7000         node5:7000         node6:7000         --cluster-replicas 1 --cluster-yes     depends_on:       - node1       - node2       - node3       - node4       - node5       - node6   基本上跑一個 Redis Cluster 也是使用跟 Master-slave 類似的方法  redis.conf 主要是在描述該節點的設定，最重要的像是啟用 cluster 功能   1 2 3 4 port 7000 cluster-enabled yes cluster-config-file nodes.conf cluster-node-timeout 2000      詳細的設定檔可以參考 ambersun1234/blog-labs/redis-cluster/cluster    Dedicated Node for Redis Cluster Initialization  你會發現在上述的 compose 設定檔中，其實有七個節點  最後的那個節點是用來初始化 Redis Cluster 的  我就覺得很奇怪，為什麼你會需要額外的節點做這件事情  稍早看到的 Sentinel 初始化也不需要手動跟他講旁邊有誰，他可以自動透過 Pub/Sub 來找到其他 sentinel   事實上當你跑過上面的 compose 之後，你會發現一件很神奇的事情  你會發現最後一個 container 執行了一下之後就結束了  而且看起來是執行成功的，剩下的只有六個節點還在線上  這肯定不是意外   讓我們來看看他的 log                  Hash slot       Cluster check                                         可以看到大約是分成兩個步驟  當他進行 partition (可參考 Partition with Hash Slots)完成之後，做完必要的檢查之後就結束了  所以該節點做的事情是一次性的，不需要一直在跑   所以剩下的節點依然是依靠 gossip protocol(service discovery) 的機制進行溝通  與 Redis Sentinel 的 Service Discovery 類似，他是透過 cluster bus 進行的  cluster bus 的 port 通常是 data port + 10000, 假設 data port 是 6379, 那麼 cluster bus port 就是 16379  failover, config update 以及其他 cluster 間的溝通都是透過 cluster bus 進行的  同時 cluster bus 是使用 binary protocol 來進行通訊，這樣可以節省頻寬以及處理時間   Partition with Hash Slots  你會發現在 Example 當中我們其實沒有指定誰是 master 誰是 slave  我們提到過，Redis Cluster 可以有多個 master，但是他們之間是如何被指定的？   注意到在 compose 裡面的這一行 --cluster-replicas 1  他指的是，每個 master 都會必須要有一個 replica  既然 master replica 本質上是相同的，那就解一下簡單的一元方程式 2x = 6 求 x  所以我們有六個節點，其中三個是 master，三個是 replica      怎麼分配是由 Redis Cluster 自己來決定的    多個 master 之間他們所儲存的資料都是不同的(i.e. Single Leader Replication)  以 Redis 來說是透過 CRC16 這個 hash function 將資料分配到不同的 master 上面  計算 CRC16 % 16384 得到的數字就是 hash slot 的位置，再根據 hash slot 的位置來決定要放到哪個 master 上面   為了方便管理，Redis Cluster 將所有的 hash slot 分成 16384 個  每個 master 會負責一部分的 hash slot，這樣就可以確保每個 master 都有自己的資料      為什麼是 16384? 可參考 why redis-cluster use 16384 slots?    Cluster-aware Client  Redis 與其他分散式系統較為不同的是，他的 client 需要是 cluster-aware 的  意思是說，client 需要知道 cluster 裡面有哪些節點，以及他們的位置  這在其他分散式系統中是挺反人類的一件事情我覺得   1 2 3 4 5 6 7 8 9 10 11 12 $ docker exec -it cluster-node1-1 sh # redis-cli -p 7000 127.0.0.1:7000&gt; set hello world OK 127.0.0.1:7000&gt;  #  $ docker exec -it cluster-node4-1 sh # redis-cli -p 7000 127.0.0.1:7000&gt; get hello (error) MOVED 866 172.20.0.6:7000 127.0.0.1:7000&gt;   當你存取到錯的節點，他會回傳 MOVED error  告訴你說，你要求的 hash slot 並不是由這個節點所負責的  所以你會需要做額外的 request 到正確的節點上，進而導致 latency 會增加(i.e. 速度變慢)   一個解法是你在連線的時候加 -c 的參數  1 2 3 4 5 6 $ docker exec -it cluster-node4-1 sh # redis-cli -p 7000 -c 127.0.0.1:7000&gt; get hello -&gt; Redirected to slot [866] located at 172.20.0.6:7000 \"world\" 172.20.0.6:7000&gt;        ref: 4.3 Redis Cluster and Client Libraries    基本上這個 hash slot 與節點之間的對應，client 會儲存一個備份在本地  只有當遇到節點更新(比如 MOVED error)的時候，client 才會更新這個對應表  你當然可以不要記錄這個表，每次都是要求 hash slot 的位置，但這樣會增加 latency  Redis doc 說，client 是沒有一定需要保存這個表，但強烈建議就是      你也可以使用 CLUSTER SHARDS 更新整個 cluster 的狀態    Consistent Hashing Function  要注意到一件事情，CRC16 並不是 consistent hashing function  consistent hashing function 會將範圍分成一個圓環，然後將節點放在圓環上面  當新增刪除或是搬遷節點的時候，只有少量的資料需要重新分配   Resharding  既然 Redis Cluster 不是使用 consistent hashing  也就是說，大量的資料會需要重新分配到不同的節點上  無論是手動搬遷，或者是新增刪除節點，都會需要進行 re-sharding  我們期待這樣大幅度的搬動資料會造成系統會有一定程度的不可用  但是 Redis doc 說，整個 re-sharding 的過程是不需要停機的， 這顯然跟我們的認知有點出入   具體來說，當 cluster 需要做 re-sharding(migration) 的時候  其實就只是將 key 從 hash slot A 移動到 hash slot B  migrate 的過程中，我們不能讓 client 存取到錯誤的資料，也就是說在他看來，整個過程要是 atomic 的  Redis 會分別設定 hash slot 的狀態，像這樣   1 2 We send A: CLUSTER SETSLOT 8 MIGRATING B We send B: CLUSTER SETSLOT 8 IMPORTING A      意思是說我們想要將 hash slot 8 從 A 移動到 B       狀態為 MIGRATING 的 slot            依然會繼續處理 request, 只不過如果該 key 已經被 migrate 過去了，他會把 request 轉給 migrate 的 target node(使用 ASK redirect)           狀態為 IMPORTING 的 slot            他也依然會接收 request, 只不過該 request 只能是 ASKING command, 剩下的一樣會丟 MOVED error              ASK redirect 會傳 ASKING command 到正確的節點上面，這段是由 client 自動做掉的    整個搬遷的過程是使用 MIGRATE 指令  基本上它是由 DUMP + RESTORE + DEL 所組成   source node 會將資料 serialize 之後(DUMP)，透過 socket 傳給 target node  收到資料之後，target node 會將資料復原(RESTORE)，然後刪除原節點上的資料(DEL)  所以在 client 的角度上來看，整個過程是 atomic 的  正常情況下資料只會出現在一個地方(source 或 target)，如果 migrate timeout 了，則可能兩邊都有資料   這整個操作過程，是 blocking 的(可參考 redis/src/cluster.c)  也就是說，不會有 downtime 是假議題，針對大資料的 migration 依然有可能造成服務不可用  即使 migrate 指令已經經過大幅度的優化，保證在絕大多數情況下都能很快完成  但是遇到大資料仍然會造成服務不可用  只不過是因為 Redis 會進行 failover 操作，才讓你覺得他沒有 downtime      延伸閱讀 [BUG] Migrate hashes with million of keys timeout, and causes failover    How to Deal with Blocking  其實 Redis 團隊從 2015 開始就有一些關於 migrate 要怎麼做比較好的討論  可參考 “CLUSTER MIGRATE slot node” command 以及 Atomic slot migration HLD   本質上圍繞在如何盡可能的減少 blocking 的時間  forking a child 的機制是個選項，但是 CoW(Copy on Write) 以及初始化 fork 的 overhead 都是需要考慮的  blocking 的手段來說，你可以選擇開一條 thread 下去做 serialize，維持 main thread 仍然可用  但是如果該 key 太過龐大，他仍然會造成 main thread 卡住(如果要存取該 key 的話)  至於在 main thread 做 serialize 更不可能，因為他會 block 住 event loop 造成整個 node 卡住   時至今日，針對這一點 Redis 似乎沒有好的解決方案   Failover Observation  1 2 3 4 5 6 7 8 9 10 $ docker exec -it cluster-node1-1 sh # redis-cli -p 7000 127.0.0.1:7000&gt; cluster nodes 28fe0ef3db224d0d2fd987fb8dc4228d9e5890d9 172.20.0.4:7000@17000 slave 508f6cdb25f49b6e816023d3e0836a73dc9d77b7 0 1726596366252 2 connected 508f6cdb25f49b6e816023d3e0836a73dc9d77b7 172.20.0.7:7000@17000 master - 0 1726596366051 2 connected 5461-10922 bc8ef33f59cb8181128cc49377c989b5e7cd71a5 172.20.0.3:7000@17000 master - 0 1726596366050 3 connected 10923-16383 358d18ecaee6b69b4f9627c970645802c912bd78 172.20.0.6:7000@17000 myself,master - 0 1726596365000 1 connected 0-5460 9f88acd73d5c26c1a68fcb2fc93cc0d513e6326b 172.20.0.2:7000@17000 slave 358d18ecaee6b69b4f9627c970645802c912bd78 0 1726596366051 1 connected 6a62d3bd90e4f118e6d12e06361ee2d2c019088b 172.20.0.5:7000@17000 slave bc8ef33f59cb8181128cc49377c989b5e7cd71a5 0 1726596366051 3 connected 127.0.0.1:7000&gt;    第一步首先我們要知道誰是 master，之後才可以手動 trigger failover  可以看到，我們連上的 node-1 剛好是 master(輸出旁邊有寫 myself,master)      可以看到也是有進行 failover 的  最後再來確認一下 cluster nodes   1 2 3 4 5 6 7 8 9 10 $ docker exec -it cluster-node2-1 sh # redis-cli -p 7000 127.0.0.1:7000&gt; cluster nodes 508f6cdb25f49b6e816023d3e0836a73dc9d77b7 172.20.0.7:7000@17000 myself,master - 0 1726596580000 2 connected 5461-10922 28fe0ef3db224d0d2fd987fb8dc4228d9e5890d9 172.20.0.4:7000@17000 slave 508f6cdb25f49b6e816023d3e0836a73dc9d77b7 0 1726596580034 2 connected 9f88acd73d5c26c1a68fcb2fc93cc0d513e6326b 172.20.0.2:7000@17000 master - 0 1726596580438 7 connected 0-5460 bc8ef33f59cb8181128cc49377c989b5e7cd71a5 172.20.0.3:7000@17000 master - 0 1726596580235 3 connected 10923-16383 6a62d3bd90e4f118e6d12e06361ee2d2c019088b 172.20.0.5:7000@17000 slave bc8ef33f59cb8181128cc49377c989b5e7cd71a5 0 1726596580000 3 connected 358d18ecaee6b69b4f9627c970645802c912bd78 172.20.0.6:7000@17000 master,fail - 1726596502724 1726596501694 1 connected 127.0.0.1:7000&gt;   可以看到 172.20.0.2 已經是新的 master 了   References     Redis replication   Scale with Redis Cluster   High availability with Redis Sentinel   Redis (六) - 主從複製、哨兵與叢集模式   redis-sentinel throws error: “ Can’t resolve master instance hostname.”   Consistent hashing   Cyclic redundancy check  ","categories": ["database"],
        "tags": ["redis","cluster","redis sentinel","replication","master-slave","distributed system","replication id","strong consistency","quorum","failover","sentinel","redis cluster","hash slot","partition","cluster bus","no downtime","high availability"],
        "url": "/database/database-redis/",
        "teaser": null
      },{
        "title": "DevOps - 透過 Helm Chart 建立你自己的 GitHub Action Runner",
        "excerpt":"Preface  在 DevOps - 從 GitHub Actions 初探 CI/CD | Shawn Hsu 裡面有提到，你可以使用自架的 local runner 執行你的 GitHub Action  原因不外乎是因為 private repo 沒辦法免費的使用 GitHub 提供的 runner  所以你可以選擇自己架設一個 runner，這樣就可以免費的使用了   本文會記錄如何使用 Helm Chart 建立 local runner 以及中間遇到的困難點   在開始之前  既然是要使用 Helm Chart，那就必須要確保 Helm 以及 Kubernetes 都正確的安裝  Install Helm 以及 Install k3d   Token Setup  為了使 GitHub API 能夠正確的存取 Action Runner Controller，你需要設定正確的 token   Personal Access Token  可以使用傳統的 PAT 來設定      針對 repo 等級的 runner 要給 repo 的權限   org 等級的 runner 要給 admin:org 的權限      這種做法如果是在公司使用會有難度，因為 PAT 是綁定在個人帳號上的  所以可以嘗試使用 GitHub App 來取代    GitHub App  你可以使用 GitHub App 來取代 Personal Access Token  GitHub App 可以把它想像成是一個擁有權限的機器人，你可以透過它來存取 GitHub API  以我們的例子來說，就是可以透過 GitHub App 來存取 Action Runner Controller   Setup Permission  建立一個 GitHub App 之後，找到 permission 設定頁面    設定 repository 的 administration 的權限       如果是要設定 organization 的 runner 就不用設定 repository 的權限，直接設定 organization 的權限即可    設定 organization 的 self hosted runner 的權限    Prepare Kubernetes Secret  操作介面取得以下     App ID            在 GitHub App 設定頁面，有一個 App ID 的數字           Private Key            介面上產一把 private key           App Installation ID            在 GitHub App 安裝頁面，觀察 URL 就可以得到         1  https://github.com/organizations/ORGANIZATION/settings/installations/INSTALLATION_ID                           要讓 ARC 能夠正確存取 GitHub API，你需要把這些資訊放到 Kubernetes 的 secret 裡面   1 2 3 4 5 6 $ kubectl create namespace arc-runners $ kubectl create secret generic arc-secrets \\    --namespace=arc-runners \\    --from-literal=github_app_id=123456 \\    --from-literal=github_app_installation_id=654321 \\    --from-literal=github_app_private_key='-----BEGIN RSA PRIVATE KEY-----********'   Install Local Runner  GitHub 其實幫你把這個包的很好了，你可以簡單地透過 Helm Chart 安裝  總共就兩個步驟，Action Runner Controller 跟 Action Runner 即可      雖然官方文件寫，希望你可以將 controller 以及 runner 分開到不同的 namespace  但是這個實際上會有問題，所以還是放在一起，以本文來說就是 arc-runners    Action Runner Controller  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 $ kubectl create namespace arc-runners $ helm install arc \\     --namespace \"arc-runners\" \\     oci://ghcr.io/actions/actions-runner-controller-charts/gha-runner-scale-set-controller  Pulled: ghcr.io/actions/actions-runner-controller-charts/gha-runner-scale-set-controller:0.9.3 Digest: sha256:4fda46fd8c4e08fe2e3d47340573f98d7a9eeb4c3a474b0e2b08057ab24d50a9 NAME: arc LAST DEPLOYED: Wed Oct  2 01:19:15 2024 NAMESPACE: arc-runners STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: Thank you for installing gha-runner-scale-set-controller.  Your release is named arc.      如果你要重新安裝，記得要 $ helm uninstall arc -n arc-runners 先把舊的刪掉    驗證一下 controller 有沒有正常跑起來  1 2 3 $ kubectl get pods -n arc-runners NAME                                     READY   STATUS    RESTARTS   AGE arc-gha-rs-controller-5f79dc8687-l6cbd   1/1     Running   0          48s   Action Runner  worker 的建立方式也一樣是用 Helm Chart  但是這裡有兩種方法，你可以用 Personal Access Token 或者是 GitHub App 來建立   PAT 的做法就只是將 credential 塞進去就可以了  1 2 3 4 5 6 $ kubectl create namespace arc-runner $ helm install \"self-hosted\" \\     --namespace \"arc-runner\" \\     --set githubConfigUrl=\"https://github.com/ORGANIZATION\" \\     --set githubConfigSecret.github_token=\"PERSONAL_ACCESS_TOKEN\" \\     oci://ghcr.io/actions/actions-runner-controller-charts/gha-runner-scale-set   至於說 GitHub App 的方式稍早我們有建立了一組 secret arc-secrets，所以這裡就可以直接使用  1 2 3 4 5 6 $ kubectl create namespace arc-runner $ helm install \"self-hosted\" \\     --namespace \"arc-runner\" \\     --set githubConfigUrl=\"https://github.com/ORGANIZATION\" \\     --set githubConfigSecret=\"arc-secrets\" \\     oci://ghcr.io/actions/actions-runner-controller-charts/gha-runner-scale-set   安裝好大概會長這樣  1 2 3 4 5 6 7 8 9 10 11 12 Pulled: ghcr.io/actions/actions-runner-controller-charts/gha-runner-scale-set:0.9.3 Digest: sha256:ec6acc8503fe1140e7371656b2142c42be67741d93222de7af63e167f825e531 NAME: self-hosted LAST DEPLOYED: Thu Oct  3 07:14:06 2024 NAMESPACE: arc-runner STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: Thank you for installing gha-runner-scale-set.  Your release is named self-hosted.     用 $ helm list -A 看有沒有正確安裝 release 上去  以及使用 $ kubectl 指令確認，理論上會有兩個 pod 啟動，一個是 controller 一個是 listener  注意到 listener 必須要啟動你的 action 才會成功被觸發   你也可以觀察 runner pod 的狀態，每一次 trigger 都會有一個 pod 被啟動  所以不同的 action 之間是使用獨立的 pod 來執行的      我們 deploy 的 helm chart 都是 scale set  gha-runner-scale-set-controller 以及 gha-runner-scale-set    Input ‘submodules’ not supported when falling back to download using the GitHub REST API  當你都設定完成之後，實際執行 GitHub Action 的時候，checkout 會遇到這個錯誤  1 2 Error: Input 'submodules' not supported when falling back to download using the GitHub REST API.  To create a local Git repository instead, add Git 2.18 or higher to the PATH.   根據所述 Git version must 2.18 or higher??!!      When Git 2.18 or higher is not in the path, it falls back to the REST API to download the tarball.    所以即使沒有 Git 2.18 以上，也應該可以正常運作  double check 你的上述的權限是否有正確的設定      在 repository not found 中其實一直有人反應  從 2020 年開始到文章撰寫的期間都還是一直有狀況      或者是還有一個可能性  runner 沒有讀到你的 secret 資料   在 Runner Scale Set: “No gha-rs-controller deployment found” when rendering Helm chart 裡面有人有遇到跟我一樣的問題  具體上來說都是碰到 service account 無法讀取 secret 的問題   1 2 3 4 5 6 7 8 9 10 11 12 13 14 2024-01-05T23:53:24Z ERROR Reconciler error  {     \"controller\": \"autoscalingrunnerset\",      \"controllerGroup\": \"actions.github.com\",      \"controllerKind\": \"AutoscalingRunnerSet\",      \"AutoscalingRunnerSet\": {         \"name\":\"arc-controller\",         \"namespace\":\"arc-systems\"     },      \"namespace\": \"arc-systems\",      \"name\": \"arc-controller\",      \"reconcileID\": \"84ec6789-7f67-430f-98aa-df8f35f18b7a\",      \"error\": \"failed to find GitHub config secret: secrets 'arc-controller-gha-rs-github-secret' is forbidden: User 'system:serviceaccount:arc-runners:arc-gha-rs-controller' cannot get resource 'secrets' in API group '' in the namespace 'arc-systems'\" }   根據它提供的解法，我們只需要在 values.yaml 裡面加入以下的設定即可   1 2 3 controllerServiceAccount:   namespace: arc-runners   name: arc-controller-gha-rs-controller   其中 namespace 就是 Helm Chart 安裝的 namespace，arc-controller 是 Helm Release 的名稱  也因為如此，在安裝 Action Runner 的時候，也要加上這個設定  你可以嘗試把所有東西放在 values.yaml 裡面然後在 install 會比較簡潔   1 2 3 4 5 githubConfigUrl: https://github.com/ORGANIZATION githubConfigSecret: arc-secrets controllerServiceAccount:   namespace: arc-runners   name: arc-controller-gha-rs-controller   1 2 3 4 $ helm install \"self-hosted\" \\     --namespace \"arc-runner\" \\     --values \"values.yaml\" \\     oci://ghcr.io/actions/actions-runner-controller-charts/gha-runner-scale-set   Customize Runner Image  當好不容易成功執行起來之後，我遇到了一些工具沒有安裝的問題  像是 make, ssh 之類的  好在你其實可以把 runner image 替換成你自己的 image   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 FROM mcr.microsoft.com/dotnet/runtime-deps:6.0 AS build  # Replace value with the latest runner release version # source: https://github.com/actions/runner/releases # ex: 2.303.0 ARG RUNNER_VERSION=\"2.320.0\" ARG TARGETARCH # Replace value with the latest runner-container-hooks release version # source: https://github.com/actions/runner-container-hooks/releases # ex: 0.3.1 ARG RUNNER_CONTAINER_HOOKS_VERSION=\"0.6.1\"  ARG DOCKER_VERSION=27.1.1 ARG BUILDX_VERSION=0.16.2  ENV DEBIAN_FRONTEND=noninteractive ENV RUNNER_MANUALLY_TRAP_SIG=1 ENV ACTIONS_RUNNER_PRINT_LOG_TO_STDOUT=1  WORKDIR /home/runner  RUN apt update -y &amp;&amp; apt install curl unzip -y  RUN export RUNNER_ARCH=${TARGETARCH} \\     &amp;&amp; if [ \"$RUNNER_ARCH\" = \"amd64\" ]; then export RUNNER_ARCH=x64 ; fi \\     &amp;&amp; curl -f -L -o runner.tar.gz https://github.com/actions/runner/releases/download/v${RUNNER_VERSION}/actions-runner-linux-${RUNNER_ARCH}-${RUNNER_VERSION}.tar.gz \\     &amp;&amp; tar xzf ./runner.tar.gz \\     &amp;&amp; rm runner.tar.gz  RUN curl -f -L -o runner-container-hooks.zip https://github.com/actions/runner-container-hooks/releases/download/v${RUNNER_CONTAINER_HOOKS_VERSION}/actions-runner-hooks-k8s-${RUNNER_CONTAINER_HOOKS_VERSION}.zip \\     &amp;&amp; unzip ./runner-container-hooks.zip -d ./k8s \\     &amp;&amp; rm runner-container-hooks.zip  RUN export RUNNER_ARCH=${TARGETARCH} \\     &amp;&amp; if [ \"$RUNNER_ARCH\" = \"amd64\" ]; then export DOCKER_ARCH=x86_64 ; fi \\     &amp;&amp; if [ \"$RUNNER_ARCH\" = \"arm64\" ]; then export DOCKER_ARCH=aarch64 ; fi \\     &amp;&amp; curl -fLo docker.tgz https://download.docker.com/linux/static/stable/${DOCKER_ARCH}/docker-${DOCKER_VERSION}.tgz \\     &amp;&amp; tar zxvf docker.tgz \\     &amp;&amp; rm -rf docker.tgz \\     &amp;&amp; mkdir -p /usr/local/lib/docker/cli-plugins  RUN curl -fLo /usr/local/lib/docker/cli-plugins/docker-buildx \\     \"https://github.com/docker/buildx/releases/download/v${BUILDX_VERSION}/buildx-v${BUILDX_VERSION}.linux-${TARGETARCH}\" \\     &amp;&amp; chmod +x /usr/local/lib/docker/cli-plugins/docker-buildx  FROM mcr.microsoft.com/dotnet/runtime-deps:6.0 AS final  WORKDIR /home/runner  RUN adduser --disabled-password --gecos \"\" --uid 1001 runner \\     &amp;&amp; groupadd docker --gid 123 \\     &amp;&amp; usermod -aG sudo runner \\     &amp;&amp; usermod -aG docker runner \\     &amp;&amp; echo \"%sudo   ALL=(ALL:ALL) NOPASSWD:ALL\" &gt; /etc/sudoers \\     &amp;&amp; echo \"Defaults env_keep += \\\"DEBIAN_FRONTEND\\\"\" &gt;&gt; /etc/sudoers  COPY --from=build /home/runner /home/runner COPY --from=build /usr/local/lib/docker/cli-plugins/docker-buildx /usr/local/lib/docker/cli-plugins/docker-buildx  RUN apt update -y &amp;&amp; apt install ssh build-essential git -y  RUN chown -R runner /home/runner RUN install -o root -g root -m 755 docker/* /usr/bin/ &amp;&amp; rm -rf docker  USER runner      你也可以手動 build  $ docker build -t ambersun1234/arc-runner:latest -f Dockerfile .    runner base image 有兩個大要求，就是必須要有 actions/runner 以及 actions/runner-container-hooks 兩者的 binary 在 /home/runner 底下  所以前半的 multi-stage build 就是在做這件事情   後面的部分就是安裝一些我需要的工具，像是 ssh, build-essential, git 之類的  然後要設定 runner user 用以執行   Docker in Docker  然後如果你的 action 需要執行 build image 之類的事情  你需要開啟 Docker in Docker mode      上面的 Dockerfile 我們有安裝了 docker 以及 docker-buildx    透過設定 values.yaml(往下看有完整範例)  1 2 containerMode:   type: \"dind   Install  基本上你可以參考 Creating your own runner image 進行魔改就可以了  但是呢，這篇沒跟你講怎麼用   根據 Configuring the runner image，你一樣要進行修改，在 values.yaml 裡面修改 template spec 的 image  所以你的 Action Runner 的設定就會變這樣   1 2 3 4 5 6 7 8 9 10 11 12 13 14 githubConfigUrl: https://github.com/ORGANIZATION githubConfigSecret: arc-secrets containerMode:   type: \"dind\" template:   spec:     containers:       - name: runner         image: \"ambersun1234/arc-runner\"         imagePullPolicy: Always         command: [\"/home/runner/run.sh\"] controllerServiceAccount:   namespace: arc-runners   name: arc-controller-gha-rs-controller      上述 values.yaml 啟用了 Docker in Docker mode  設定了 runner image 為 ambersun1234/arc-runner  config secret 以及 config url  你可以根據你的需要適當的進行微調    安裝也一樣  1 2 3 4 $ helm install \"self-hosted\" \\     --namespace \"arc-runner\" \\     --values \"values.yaml\" \\     oci://ghcr.io/actions/actions-runner-controller-charts/gha-runner-scale-set   針對上述的 customize image 我有編一份放在 ambersun1234/arc-runner  有 linux/amd64 跟 linux/arm64 兩個版本，你可以直接使用   References     Quickstart for Actions Runner Controller   Authenticating to the GitHub API   About creating GitHub Apps   repository not found   Git version must 2.18 or higher??!!   Runner Scale Set: “No gha-rs-controller deployment found” when rendering Helm chart   Creating your own runner image   Configuring the runner image   Using Docker-in-Docker mode  ","categories": ["devops"],
        "tags": ["github action","local runner","helm chart","kubernetes","rancher","customize runner image"],
        "url": "/devops/devops-ga-arc/",
        "teaser": null
      },{
        "title": "Kubernetes 從零開始 - Helm Controller",
        "excerpt":"Introduction to Helm Controller  如果你是使用 Helm Chart 來管理你的 Kubernetes 資源  一個常見的需求會是，你可能會需要更新你的 chart  不管是 image version 還是一些設定檔的更新   當然你可以手動 $ helm upgrade 來更新你的 chart  不過官方有提供一個更好的方式，就是使用 Helm Controller   Helm Controller 允許你用 declarative 的方式來管理 Helm Chart  並且主動監控任何 CRD(Custom Resource Definition) 的變化  因此你只需要透過更新 CRD 告訴他你想要的狀態，Helm Controller 會自己去更新相對應的 chart      可參考 Kubernetes 從零開始 - 從自幹 Controller 到理解狀態管理 | Shawn Hsu 以及 Kubernetes 從零開始 - Deployment 管理救星 Helm Chart | Shawn Hsu    Use Helm Controller  k3d(i.e. k3s) 預設就已經安裝了 Helm Controller 以及相對應的 CRD  所以不需要特別安裝就可以直接使用      HelmChart 以及 HelmChartConfig 這兩個 CRD 可以用 $ kubectl get crd 來查看  有關 CRD 可以參考 Kubernetes 從零開始 - client-go 實操 CRD | Shawn Hsu    如果是其他的 Kubernetes cluster，可以參考 k3s-io/helm-controller 給的設定檔進行操作      本文會使用 k3s-io/helm-controller  網路上也有其他的 Helm Controller 實作，例如 fluxcd/helm-controller    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 apiVersion: apps/v1 kind: Deployment metadata:   name: helm-controller   labels:     app: helm-controller spec:   replicas: 1   selector:     matchLabels:       app: helm-controller   template:     metadata:       labels:         app: helm-controller     spec:       containers:         - name: helm-controller           image: rancher/helm-controller:v0.12.1           command: [\"helm-controller\"]   Patching Helm Chart  Install Redis Helm Chart  以 bitnami/redis 為例，透過 HelmChart CRD 建立 Custom Resource(CR)   1 2 3 4 5 6 7 8 9 10 apiVersion: helm.cattle.io/v1 kind: HelmChart metadata:   name: myredis   namespace: default spec:   repo: https://charts.bitnami.com/bitnami   chart: redis   set:     replica.replicaCount: 5   以上就是一個基本的 Custom Resource 定義  我在 default namespace 底下要建立一個名為 myredis 的 HelmChart Release  並且使用 https://charts.bitnami.com/bitnami 的 redis chart   因為第三方的 Helm Chart 通常允許你更改一些參數  這裡我們想要覆蓋 replica.replicaCount, 設定成 5 個 replica      參數的部份，是對應到 bitnami/redis/values.yaml 的設定值    安裝完成你應該會看到類似以下的結果   1 2 NAME            NAMESPACE       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION myredis         default         1               2024-10-05 20:40:39.93388077 +0800 CST  deployed        redis-20.1.7                    7.4.1         Kubectl Patch  所以到現在我們成功的建立了一個 HelmChart Release  假設我想要再次更新 replica 的數量，我們可以就透過 $ kubectl patch 來達成   前面說過，Helm Controller 會監控 CRD 的變化，所以我們可以透過更改 CR 觸發 Helm Controller 來更新 Helm Chart   patch 有三種方式，strategic merge patch, JSON merge patch 以及 JSON patch      strategic merge patch 沒辦法用於 HelmChart CRD    JSON Merge Patch  JSON Merge Patch 定義於 RFC 7386  稱之為 merge patch 是因為它是將 patch 的內容合併到原本的資料上  也就是說原本不存在的會被新增，已經存在的會被更新，給 null 則會被刪除   1 2 3 spec:   set:     replica.replicaCount: 10   以上就是一個簡單的 JSON Merge Patch  然後透過 kubectl patch 執行   1 $ kubectl patch HelmChart myredis --type merge --patch-file ./patch.yaml   你可以檢查看他有沒有正確的更新成功  1 2 3 4 5 6 7 8 9 $ kubectl get HelmChart myredis -o yaml ... spec:   chart: redis   repo: https://charts.bitnami.com/bitnami   set:     replica.replicaCount: 10 status:   jobName: helm-install-myredis   JSON Patch  JSON patch 定義於 RFC 6902  跟 JSON Merge Patch 類似，只是說你要指定 patch 的操作   JSON patch 是一個 array, 每個 object 包含 op, path, value  操作識別符號透過 op 來指定，比方說你要新增還是刪除  path 這個欄位的資料要特別注意的是，每個路徑是使用 / 進行分割的  以 myredis 為例，spec.set.replica.replicaCount 這個路徑就是 /spec/set/replica.replicaCount   為什麼 relica.replicaCount 這個路徑是使用 . 分割，而不是 / 呢？  因為他是一個 key 而不是一個路徑   所以寫起來會長這樣  1 2 3 4 5 6 7 [   {     \"op\": \"replace\",     \"path\": \"/spec/set/replica.replicaCount\",     \"value\": 10   } ]   如果是遇到 array, 假設是  1 2 3 4 5 apiVersion: v1 kind: Pod spec:   containers:   - image: nginx:1.14.2   你想要換 image  他的 JSON patch 會是 /spec/containers/0/image     patch 的指令會長這樣  1 $ kubectl patch HelmChart myredis --type json --patch-file ./patch.json   如果你仔細去看它的 log, 你會發現它其實就真的只是做了 $ helm upgrade  1 + helm_v3 upgrade --set replica.replicaCount=10 myredis myredis/redis   References     k3s helm   Create a Simple Kubernetes Custom Resource and CRD with kubectl   WIll “helm upgrade” restart PODS even if they are not affected by upgrade?  ","categories": ["kubernetes"],
        "tags": ["helm chart","helm controller","crd","terratest","json patch patch","strategic merge patch","json patch","kubectl patch","helm upgrade"],
        "url": "/kubernetes/kubernetes-helm-controller/",
        "teaser": null
      },{
        "title": "Kubernetes 從零開始 - 從自幹 Controller 到理解狀態管理",
        "excerpt":"Controller Pattern  有關 Controller Pattern 及 Controller 基本概念可以參考 Kubernetes 從零開始 - Informer 架構以及 Controller Pattern | Shawn Hsu   Kubernetes Operator  既然 K8s 有內建的 controller，自己客製化 controller 的意義在哪？  為了可以更好的管理自己的 Custom Resource，我們可以透過 CRD (Custom Resource Definition) 來定義自己的 Resource  要管理這些自定義的 Resource 的狀態，我們就需要自己的 controller 來管理      有關 CRD 的介紹可以參考 Kubernetes 從零開始 - client-go 實操 CRD | Shawn Hsu    這種自定義的 controller 通常被稱為 Operator      每個 resource 一定要有 controller 嗎？  其實不用，如果你的 resource object 不需要管理狀態，那就不需要 controller    在 Kubernetes 從零開始 - Helm Controller | Shawn Hsu 我們有玩過 HelmController  HelmController 是用來管理 HelmChart 以及 HelmRelease 這兩個 resource  而這就是自己寫一個 controller 一個很好的例子   Finite State Machine(FSM)  我在撰寫 Controller 的時候發現，既然要管理狀態，那是不是可以考慮使用 有限狀態機?  每一個狀態的轉移都是有一定的規則的，即使每個 CRD 都不盡相同，但它一定可以被歸類出來  比如說，pending -&gt; running -&gt; finished 這樣的狀態轉移   這些狀態事實上是一個 DAG(Directed Acyclic Graph)  因為不太可能 finished 又回去 running 這樣的事情   善加利用 FSM 可以使你管理 CRD 狀態更容易  我的作法會是每個 object 都有一個獨立的 FSM，這樣的會是 O(n) n 為 object 數量  因為 controller 會負責管理 “所有符合的 CRD”  想當然這樣會造成一定的 overhead 但就看你的需求了      Reddit 有實作一套 controller SDK reddit/achilles-sdk  最主要就是提供了 FSM 的功能    Finalizer  object 被刪除的時候，你可能會需要執行一些清理工作  透過 finalizer 可以很優雅的處理這些事情   1 2 3 4 metadata:   ...   finalizers:   - my-finalizer      你可以指定多個 finalizer 執行    finalizers 裡面本質上就只是一堆的 key  這些 key 很類似於 annotation，他的目的在於告訴 K8s 這個 object 還有一些事情要處理     當你有設定 finalizer 的時候，K8s 並不會直接將 object 從 etcd 中刪除  它會處於一個 Terminating 的狀態，直到 finalizer 執行完畢(所以才叫做 hook)   具體來說 K8s 會更新 object 的 metadata  它會寫上 deletionTimestamp  由於 controller 會監控 object 的變化，所以這次更新會被 controller 感知到  然後執行 reconcile   1 2 3 4 5 metadata: ...   deletionTimestamp: \"2020-10-22T21:30:34Z\"   finalizers:   - my-finalizer   這時候因為你會感知到 object 變化  所以你可以去看 .metadata.deletionTimestamp 有沒有存在  有的話就根據你指定的 finalizer 去執行清理工作   這段就跟你平常在寫 reconcile 一樣      如果你指定了一個 controller 不認識的 finalizer，那麼這個 object 就會永遠留在 Terminating 狀態    當你事情做完之後，要如何觸發 K8s 刪除 object 呢？  只要把 .metadata.finalizers 刪除就可以了   整理的操作會像這樣      ref: Using Finalizers to Control Deletion    Operator SDK  雖然說 kubernetes/sample-controller 提供了一個很好的範例  實務上撰寫 controller 有其他的選擇，不一定只能拿官方的範例下去改  根據 Writing your own operator 官方有列出幾個如 Operator Framework，nolar/kopf 以及 kubebuilder   以本文，我選擇使用 Operator SDK  Operator SDK 提供了非常完整的 framework 讓你可以開發，而且他是基於 kubebuilder 的   Namespaced Scoped Operator  Kubernetes Controller 可以只監聽特定 namespace 底下的 object  如果沒有指定它會是 cluster scoped 的  這種情況會有可能造成混亂   具體來說，使用 Operator SDK 在初始化 manager 的時候就可以指定，像這樣   1 2 3 4 5 6 7 8 9 10 mgr, err := ctrl.NewManager(ctrl.GetConfigOrDie(), ctrl.Options{     Scheme:             scheme,     MetricsBindAddress: metricsAddr,     Port:               9443,     LeaderElection:     enableLeaderElection,     LeaderElectionID:   \"f1c5ece8.example.com\",     Cache: cache.Options{       DefaultNamespaces: map[string]cache.Config{\"operator-namespace\": cache.Config{}},     }, })   其中 operator-namespace 就是你要監聽的 namespace  你可能發現它其實是一個 map 的結構，亦即你可以監聽多個 namespace   Livenessprobe and Readinessprobe of Operator  1 2 3 4 5 6 7 8 if err := mgr.AddHealthzCheck(\"healthz\", healthz.Ping); err != nil {     setupLog.Error(err, \"unable to set up health check\")     os.Exit(1) } if err := mgr.AddReadyzCheck(\"readyz\", healthz.Ping); err != nil {     setupLog.Error(err, \"unable to set up ready check\")     os.Exit(1) }   Operator SDK 自動幫你生成的程式碼裡面有包含基本的 health check endpoint  healthz.Ping 是 Operator SDK 自帶的一個 health check function  透過 AddHealthzCheck 以及 AddReadyzCheck 你可以將這個 health check endpoint 加入到你的 operator 裡面  但注意到它只是註冊而已，你還是需要手動呼叫它才有用      所以你不需要自己用 http 寫一個 healthz    所以你的 livenessprobe 寫起來大概會像這樣  1 2 3 4 5 6 7 8 livenessProbe:   httpGet:     path: /healthz     port: 8081   initialDelaySeconds: 3   periodSeconds: 3   timeoutSeconds: 1   failureThreshold: 3      有關 probe 可以參考 Kubernetes 從零開始 - Self Healing 是如何運作的 | Shawn Hsu    我們註冊進去的 URL 就是 /healthz  然後那個 port 8081 就很有意思了  generate 出來的程式碼裡面，他是手動 bind health probe 的 port  你可以從這裡看到，它其實是透過參數的方式 --health-probe-bind-address=:8081 傳遞進去的(ref: manager_auth_proxy_patch.yaml)  所以這個 8081 是這裡來的   對應到 source code 會是這樣定義的  1 2 3 4 5 6 flag.StringVar(     &amp;probeAddr,      \"health-probe-bind-address\",      \":8081\",      \"The address the probe endpoint binds to.\", )      如果你不是用 kustomize 你就在 container 那邊加個 arg 傳進去就好了       operator 本身的 port 預設是 8080, 你也可以在 manager 初始化的時候指定(像這裡是 9443)  可以參考 Migrate main.go    然後測試的時候你可以用 $ kubectl port-forward 來測試  預設會回 200 OK   Event Filter with Predicate  我在測試 controller 的時候有發現一個問題，就是同樣的 CR 會被觸發兩次  但是他的狀態其實是相同的，比如說 Running -&gt; Running  這樣的情況以我的需求來說是不需要的   Operator-SDK 本身有提供所謂的 Predicate  他的作用是，可以允許你自定義 filter 過濾掉一些不必要的 event  當狀態被改變的時候我才需要觸發 reconcile   寫起來大概會像這樣  注意到，只有回傳 true 的情況下該 event 才會被觸發  那你會問為什麼我需要先做 type assertion 呢？  因為有可能 event object 不一定是你的 CRD, 有可能是其他的 object  針對其他的 object 我還是讓它觸發，只過濾掉 CRD 的 event   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 import (     \"sigs.k8s.io/controller-runtime/pkg/event\"     \"sigs.k8s.io/controller-runtime/pkg/predicate\" )  func statusFilter() predicate.Predicate {     return predicate.Funcs{         UpdateFunc: func(e event.UpdateEvent) bool {             oldCR, ok := e.ObjectOld.(*v1.MyCRD)             if !ok {                 return true             }              newCR, ok := e.ObjectNew.(*v1.MyCRD)             if !ok {                 return true             }              return oldCR.Status != newCR.Status       },     } }  func (r *MemcachedReconciler) SetupWithManager(mgr ctrl.Manager) error {     return ctrl.NewControllerManagedBy(mgr).         For(&amp;cachev1alpha1.Memcached{}).         Owns(&amp;corev1.Pod{}).         WithEventFilter(statusFilter()).         Complete(r) }   Example  遵照官方的 tutorial 其實很簡單  兩個步驟就可以完成一個 operator   1 2 3 4 5 6 7 8 9 10 $ go mod init mycontroller $ operator-sdk init \\   --domain example.com \\   --repo mycontroller $ operator-sdk create api \\   --group foo \\   --version v1 \\   --kind Foo \\   --resource \\   --controller   基本上 operator-sdk 的指令就是幫你產一個 operator 的 template  有幾個比較重要的 flag 是 domain, group 以及 version   因為我們要建立一個自定義的 resource, 而所有 cluster 的操作基本上是透過 Kubernetes API 完成的  為了更方便的管理這些 API，我們會將它分類，domain，group 以及 version 就是用來區分的  所以產完你可以發現自定義的 resource 會是 foo.example.com/v1 這樣的格式  然後我們這裡建立的 resource 叫做 Foo   其實 operator-sdk 產了很多東西  包含… 一大堆的 yaml  這些 yaml 檔案有的是 CRD 有的是 RBAC 的設定檔案  operator-sdk 也有產個 Makefile 直接安裝以上的檔案(透過 kustomize)   所以你該 care 的檔案只有兩個  ./api/v1 裡面的 CRD 定義以及 ./internal/controller 裡面的 controller   CRD 的部份我們需要新增一個欄位儲存 value  然後 status 那邊要加一個 conditions 的 array 用以儲存歷史狀態   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 // FooSpec defines the desired state of Foo type FooSpec struct {     // INSERT ADDITIONAL SPEC FIELDS - desired state of cluster     // Important: Run \"make\" to regenerate code after modifying this file      // Foo is an example field of Foo. Edit foo_types.go to remove/update     Value string `json:\"value,omitempty\"` }  // FooStatus defines the observed state of Foo type FooStatus struct {     // INSERT ADDITIONAL STATUS FIELD - define observed state of cluster     // Important: Run \"make\" to regenerate code after modifying this file      Conditions []metav1.Condition `json:\"conditions,omitempty\"` }  //+kubebuilder:object:root=true //+kubebuilder:subresource:status  // Foo is the Schema for the foos API type Foo struct {     metav1.TypeMeta   `json:\",inline\"`     metav1.ObjectMeta `json:\"metadata,omitempty\"`      Spec   FooSpec   `json:\"spec,omitempty\"`     Status FooStatus `json:\"status,omitempty\"` }   Reconcile 的實作如下  基本上為了 demo 用，所以只是簡單的檢查 spec 的 value 欄位是否為特定值  注意到，從 Indexer 取得的資料有可能為空(因為 resource 被手動刪除之類的)  所以還需要做 IsNotFound 的檢查(第 5 行)   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 logger := log.FromContext(ctx)  resource := &amp;cachev1.Memcached{} if err := r.Get(ctx, req.NamespacedName, resource); err != nil {   if errors.IsNotFound(err) {     logger.Info(\"Foo resource not found\")     return ctrl.Result{}, nil   }    logger.Error(err, \"unable to fetch Foo\")   return ctrl.Result{}, err }  if resource.Spec.Foo != \"bar\" {   logger.Info(\"Foo field is not equal to bar\")   meta.SetStatusCondition(&amp;resource.Status.Conditions, metav1.Condition{     Type:               \"Unknown\",     Status:             metav1.ConditionUnknown,     Reason:             \"FooNotBar\",     Message:            \"Foo field is not equal to bar\",     LastTransitionTime: metav1.Now(),   })   if err := r.Status().Update(ctx, resource); err != nil {     logger.Error(err, \"unable to update Foo status\")     return ctrl.Result{}, err   }    return ctrl.Result{Requeue: true}, nil }  meta.SetStatusCondition(&amp;resource.Status.Conditions, metav1.Condition{   Type:               \"Ready\",   Status:             metav1.ConditionTrue,   Reason:             \"FooIsBar\",   Message:            \"Foo field is equal to bar\",   LastTransitionTime: metav1.Now(), })  if err := r.Status().Update(ctx, resource); err != nil {   logger.Error(err, \"unable to update Foo status\")   return ctrl.Result{}, err }   Run  先建立一個新的 cluster 用來測試我們的 operator   1 2 3 $ k3d cluster create mycluster --servers 1 $ kubectl config current-context # 應該要是 k3d-mycluster   然後安裝 operator  1 2 3 $ make docker-build $ k3d image import -c mycluster controller:latest $ make deploy      docker-build 裡面的指令記得下 --no-cache  controller 的 yaml 裡面，image 記得改 imagePullPolicy: Never      開始測試！  可以使用 ./config/samples/ 底下的範例 yaml 來建立 resource  底下的 spec 記得改成相對應的欄位   1 2 3 4 5 6 7 8 9 apiVersion: foo.example.com/v1 kind: Foo metadata:   labels:     app.kubernetes.io/name: k8s-controller     app.kubernetes.io/managed-by: kustomize   name: foo-sample spec:   value: hello      controller 的 log 裡面你可以看到有正確的進行做動  它會一直檢查的原因是因為，我們有將它重新 enqueue(第 28 行)      而 describe Resource，你就可以看到我們已經將 status 更新了  只是說實作裡面並沒有一直新增 condition(縱使我們定義他是一個 list)   到這裡，你就完成了一個非常簡單的 operator 了  你可以針對這個 operator 做更多的事情，比如說加入更多的檢查，或者是加入更多的欄位      詳細的實作可以參考 ambersun1234/blog-labs/k8s-controller    Handle CRD Migration with your Controller  需要注意到的是，如果你的 CRD 同時會有多個版本在服務，你的 Controller 必須要能夠處理不同版本的 CRD   Migration 算是滿常見的需求之一，比如說新的欄位或者是舊的欄位要棄用之類的  如果支援 v2 的 Controller 碰到 v1 的 CRD 那麼有可能會出現問題  以 Kubernetes 來說，你可以透過 Conversion Webhook 進行轉換  這樣即使之前遺留的 v1 CRD 也可以被正確的轉換成 v2，被 Controller 識別   當然同時你的 Controller 也必須要做升級，支援新資料格式才行      有關 Conversion Webhook 可以參考 Kubernetes 從零開始 - client-go 實操 CRD | Shawn Hsu    注意到，以 Operator-SDK 的例子來說，你沒辦法監聽到不同版本的 CRD  1 2 3 4 5 2025-02-16T09:41:27Z    ERROR   setup   unable to create controller      {   \"controller\": \"Foo\",    \"error\": \"For(...) should only be called once, could not assign multiple objects for reconciliation\" }   How to Deploy your Controller  另一個問題是如何部署你的 controller  你可以選擇跑一個 deployment 起來就可以   不過 controller 在重新啟動(rollout restart)的時候，有可能會沒有接到 event  導致 CRD 會少監聽到一些 event  這並不是我們想要的   當然你可以選擇跑多個 replica, 但這樣另一個問題油然而生  多個執行的個體會不會互相干擾呢？ 答案是肯定的   Operator SDK 是採用 Single Leader 的方式解決  同一時間只會有一個 “leader” 來執行 reconcile，而剩餘的 replica 則會待機      有關 Single Leader Replication 可以參考 資料庫 - 初探分散式資料庫 | Shawn Hsu    Leader 的選舉機制有兩種 Leader-with-Lease 以及 Leader-for-Life  預設的機制是 Leader-with-Lease   Leader with Lease  leader 的權利是具有 時效性的，當 lease 過期的時候，leader 就會被換掉  然後其他人就會想辦法成為 leader   這種實作保證了快速的 failover, 不過，它也逃不掉 腦分裂的問題  在 client-go 的實作當中有明確指出  leaderelection 這套實作是依靠時間區間來做判斷的(RenewDeadLine 以及 LeaseDuration)  也就是說他是依靠 “時間差” 而非絕對時間決定 leader 是否該被替換掉(因為在分散式系統下，時間是不可靠的)   但如果節點的時鐘跑得比較快/慢，leaderelection 也仍然沒有辦法處理這種狀況，進而導致 腦分裂      有關腦分裂可以參考 資料庫 - 從 Netflix 的 Tudum 系統看分散式系統中那些 Read/Write 問題 | Shawn Hsu    基本上你只要將 LeaderElection 設為 true 就可以了  1 2 3 4 mgr, err := ctrl.NewManager(ctrl.GetConfigOrDie(), ctrl.Options{     LeaderElection:         enableLeaderElection,     LeaderElectionID:       \"3601af7f.example.com\", })   從下列的 log 你可以很明顯的看到，同一時間只會有一個 leader 正在執行  而其他的 replica 則是在等待  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 $ kubectl get pods -A                                                                      NAMESPACE               NAME                                                 READY   STATUS      RESTARTS   AGE k8s-controller-system   k8s-controller-controller-manager-65689fb949-85ff2   2/2     Running     0          27s k8s-controller-system   k8s-controller-controller-manager-65689fb949-98fqg   2/2     Running     0          27s k8s-controller-system   k8s-controller-controller-manager-65689fb949-dtm74   2/2     Running     0          27s  $ kubectl logs -n k8s-controller-system k8s-controller-controller-manager-65689fb949-85ff2  2025-01-20T17:55:33Z    INFO    setup   starting manager 2025-01-20T17:55:33Z    INFO    controller-runtime.metrics      Starting metrics server 2025-01-20T17:55:33Z    INFO    starting server {\"kind\": \"health probe\", \"addr\": \"[::]:8081\"} 2025-01-20T17:55:33Z    INFO    controller-runtime.metrics      Serving metrics server  {\"bindAddress\": \"127.0.0.1:8080\", \"secure\": false} I0120 17:55:33.965082       1 leaderelection.go:250] attempting to acquire leader lease k8s-controller-system/3601af7f.example.com... I0120 17:55:33.973538       1 leaderelection.go:260] successfully acquired lease k8s-controller-system/3601af7f.example.com  $ kubectl logs -n k8s-controller-system k8s-controller-controller-manager-65689fb949-98fqg  2025-01-20T17:55:34Z    INFO    setup   starting manager 2025-01-20T17:55:34Z    INFO    controller-runtime.metrics      Starting metrics server 2025-01-20T17:55:34Z    INFO    starting server {\"kind\": \"health probe\", \"addr\": \"[::]:8081\"} 2025-01-20T17:55:34Z    INFO    controller-runtime.metrics      Serving metrics server  {\"bindAddress\": \"127.0.0.1:8080\", \"secure\": false} I0120 17:55:34.148711       1 leaderelection.go:250] attempting to acquire leader lease k8s-controller-system/3601af7f.example.com...  $ kubectl logs -n k8s-controller-system k8s-controller-controller-manager-65689fb949-dtm74  2025-01-20T17:55:34Z    INFO    setup   starting manager 2025-01-20T17:55:34Z    INFO    controller-runtime.metrics      Starting metrics server 2025-01-20T17:55:34Z    INFO    starting server {\"kind\": \"health probe\", \"addr\": \"[::]:8081\"} 2025-01-20T17:55:34Z    INFO    controller-runtime.metrics      Serving metrics server  {\"bindAddress\": \"127.0.0.1:8080\", \"secure\": false} I0120 17:55:34.149087       1 leaderelection.go:250] attempting to acquire leader lease k8s-controller-system/3601af7f.example.com...   Leader for Life  相較只能坐擁王位一段時間的 leader, Leader for Life 講求的是主動退位  只有當 leader 被刪除的時候，才會進行下一任的選舉   要使用 Leader for Life 要改的 code 會比較多     新增 env POD_NAME   新增 pods, nodes 的 role(get 權限即可)   當然最重要的就是主程式這裡  1 2 3 4 5 6 7 8 import (     \"github.com/operator-framework/operator-lib/leader\" )  if err := leader.Become(context.TODO(), \"mycontroller-lock\"); err != nil {     setupLog.Error(err, \"unable to become leader\")     os.Exit(1)   }      這部份的實作可以參考 commit 5b9ac77    然後一樣看輸出結果，也是只有一個 leader 會負責執行 reconcile  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 $ kubectl get pods -A NAMESPACE               NAME                                                READY   STATUS      RESTARTS   AGE k8s-controller-system   k8s-controller-controller-manager-7f444cb5c-6xmlt   1/2     Running     0          56s k8s-controller-system   k8s-controller-controller-manager-7f444cb5c-9d9zm   1/2     Running     0          56s k8s-controller-system   k8s-controller-controller-manager-7f444cb5c-cf6sj   2/2     Running     0          56s  $ kubectl logs -n k8s-controller-system k8s-controller-controller-manager-7f444cb5c-6xmlt  2025-01-20T18:36:27Z    INFO    leader  Trying to become the leader. 2025-01-20T18:36:27Z    DEBUG   leader  Found podname   {\"Pod.Name\": \"k8s-controller-controller-manager-7f444cb5c-6xmlt\"} 2025-01-20T18:36:27Z    DEBUG   leader  Found Pod       {\"Pod.Namespace\": \"k8s-controller-system\", \"Pod.Name\": \"k8s-controller-controller-manager-7f444cb5c-6xmlt\"} 2025-01-20T18:36:27Z    INFO    leader  Found existing lock     {\"LockOwner\": \"k8s-controller-controller-manager-7f444cb5c-cf6sj\"} 2025-01-20T18:36:27Z    INFO    leader  Not the leader. Waiting.  $ kubectl logs -n k8s-controller-system k8s-controller-controller-manager-7f444cb5c-9d9zm  2025-01-20T18:36:27Z    INFO    leader  Trying to become the leader. 2025-01-20T18:36:27Z    DEBUG   leader  Found podname   {\"Pod.Name\": \"k8s-controller-controller-manager-7f444cb5c-9d9zm\"} 2025-01-20T18:36:27Z    DEBUG   leader  Found Pod       {\"Pod.Namespace\": \"k8s-controller-system\", \"Pod.Name\": \"k8s-controller-controller-manager-7f444cb5c-9d9zm\"} 2025-01-20T18:36:27Z    INFO    leader  Found existing lock     {\"LockOwner\": \"k8s-controller-controller-manager-7f444cb5c-cf6sj\"} 2025-01-20T18:36:27Z    INFO    leader  Not the leader. Waiting.  $ kubectl logs -n k8s-controller-system k8s-controller-controller-manager-7f444cb5c-cf6sj  2025-01-20T18:35:26Z    INFO    leader  Trying to become the leader. 2025-01-20T18:35:26Z    DEBUG   leader  Found podname   {\"Pod.Name\": \"k8s-controller-controller-manager-7f444cb5c-cf6sj\"} 2025-01-20T18:35:26Z    DEBUG   leader  Found Pod       {\"Pod.Namespace\": \"k8s-controller-system\", \"Pod.Name\": \"k8s-controller-controller-manager-7f444cb5c-cf6sj\"} 2025-01-20T18:35:26Z    INFO    leader  No pre-existing lock was found. 2025-01-20T18:35:26Z    INFO    leader  Became the leader. 2025-01-20T18:35:26Z    INFO    setup   starting manager   unable to decode an event from the watch stream: context canceled  我在開發 operator 的時候有一個問題，就是會遇到這種錯誤 unable to decode an event from the watch stream: context canceled   根據 Consistently Seeing Reflector Watch Errors on Controller Shutdown，這看起來是一個已知問題  似乎是跟 cache 有關係，那留言內的解法是把 cache 停掉   你可以在 client.Object 裡面指定哪些 object 不要被 cache   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 import (     ctrl \"sigs.k8s.io/controller-runtime\"     \"sigs.k8s.io/controller-runtime/pkg/client\" )  func main() {     ctrl.NewManager(ctrl.GetConfigOrDie(), ctrl.Options{         Client: client.Options{             Cache: &amp;client.CacheOptions{                 DisableFor: []client.Object{                     &amp;coreV1.Pod{},                 },             },         },     }) }   References     Controllers   Cluster Architecture   What is the difference between a Kubernetes Controller and a Kubernetes Operator?   如何编写自定义的 Kubernetes Controller   How to deploy controller into the cluster   rancher/wrangler   Efficient detection of changes   Operator pattern   Operator SDK   API Overview   Kubernetes: Finalizers in Custom Resources   Finalizers   Using Finalizers to Control Deletion   Watching resources in specific Namespaces   Leader election   Understand the cached clients   How can i ignore CRD modify event on status update of custom resource objects   Reconcile is triggered after status update   Using Predicates for Event Filtering with Operator SDK   10 Things You Should Know Before Writing a Kubernetes Controller  ","categories": ["kubernetes"],
        "tags": ["kubernetes controller","state","wrangler","kubernetes operator","reconcile","crd","control loop","controller pattern","operator pattern","self healing","operator sdk","fsm","finalizer","namespaced operator","livenessprobe","readinessprobe","health check","leader election","leader with lease","leader for life","event filter","predicate","conversion webhook","crd migration"],
        "url": "/kubernetes/kubernetes-controller/",
        "teaser": null
      },{
        "title": "設計模式 101 - 分散式交易的另一種作法 Saga Pattern",
        "excerpt":"Distributed Transaction with 2PC  在 microservices 的架構下，分散式交易是必須面對的問題  我們學過，2PC(Two-Phase Commit) 是其中一種解決方式  透過一個中心化的協調者(coordinator)與所有其他參與交易的服務進行溝通與決策   為了避免參與者無限等待 coordinator 的回應，3PC 被提出      可參考 資料庫 - 從 Netflix 的 Tudum 系統看分散式系統中那些 Read/Write 問題 | Shawn Hsu    根據 Documentation: 17: 66.4. Two-Phase Transactions - PostgreSQL PostgreSQL 是有提供 2PC 的支援  但這些算法並不是每個資料庫都有支援  要如何不依賴資料庫的支援，實現分散式交易呢？   Starting from 2PC  我們可以借鑒 2PC 的想法，分散式交易本質上就是多個服務之間的交互  那是不是可以看作 多筆小交易?   但要是 整體交易 失敗該怎麼辦？  部份小交易已經 Commit 了，這些交易要怎麼 Rollback 呢？  Rollback 是不可能的，已經 Commit 的交易 Revert 也是不可能的(資料庫不支援)     換個角度說，既然資料庫不支援 Rollback 已經 Commit 的資料，我能不能透過其他方式來補償這筆交易的失敗呢？  什麼意思？   我可以透過另一筆交易，補償 之前交易造成的 結果  比方說，我要從線上商成買東西  商品數量已經漸少，但是付款失敗  以這個例子來說，你做了兩筆 小交易   1 2 1. 減少商品數量(成功) 2. 扣款(失敗)   所以以這個例子，你要怎麼補償？  很明顯的，把商品數量加回去就好了  而這個補償的交易，這就是 Compensating Transaction   Introduction to Saga Pattern       ref: Pattern: Saga    從上圖你可以很清楚的看到，Saga Pattern 是由多個小交易組成的  這一系列的小交易，我們稱之為 Saga  Saga 的想法很簡單，而它也不一定只能套用到資料庫交易上   Saga Pattern 有兩種實現方式  其中 Orchestration-based Saga 跟 2PC 滿類似的   Choreography-based Saga 主要是讓每個小交易知道自己的下一步是什麼  比如說商品數量足夠並且扣除之後，就要進行扣款  變成是 Inventory 會知道接下來需要進行扣款(Billing)的動作  因為不同服務之間他們不會直接溝通，所以你需要借助像是 message queue 這樣的工具      有關 message queue 可以參考 資料庫 - 從 Apache Kafka 認識 Message Queue | Shawn Hsu                           Choreography-based Saga       Orchestration-based Saga                       Description       每個小交易知道自己的下一步是什麼       有一個中心化的協調者(coordinator)                 Image                         Drawbacks  概念上 Saga Pattern 看起來很美好，但是實際上有一些缺點  最明顯的莫過於你需要手動處理很多事情   假設 Local Transaction 因為莫名原因停止  那麼後續的交易怎麼辦？ 該繼續等待嗎？  針對這個問題，2PC 的解法是 3PC   如何處理 Rollback 的問題？ 如果 Rollback 失敗該怎麼處理？  這種狀況，你的 application 必須要有夠高的可靠性  比如說你可能需要加入 Retry 的機制確保 Rollback 的成功   另外，Saga Pattern 本身並沒有提供 ACID 的保證  整個交易沒有保證(但 小交易 本身有)  所以可能會造成多筆 concurrent Saga 交易會有衝突的問題  這些也都必須要你自己來處理      可參考 資料庫 - Transaction 與 Isolation | Shawn Hsu    綜合來看，Saga Pattern 提供了一個分散式交易的解決思路  但是仍然有許多的細節需要仔細思考與研究   References     Pattern: Saga  ","categories": ["design pattern"],
        "tags": ["saga","transaction","microservices","2pc","compensating transaction"],
        "url": "/design%20pattern/design-pattern-saga/",
        "teaser": null
      },{
        "title": "Kubernetes 從零開始 - 資源排隊神器 Kueue",
        "excerpt":"Introduction to Kueue  你可以在 Kubernetes 裡面塞入任一數量的 job，但這只是理論上  實務上會因為硬體資源的限制，你只可以執行有限數量的 job  Kueue 這個工具可以根據這些 限制，允許有限數量的 job 同時執行  它可以做到一些基礎的排程機制，如     Job 要不要等待，可不可以開始執行(i.e. 排隊)   Job 該不該被搶佔(i.e. preemption)   Kueue 保證了所有 Job 對於資源的使用是公平的  並且可以根據偏好的資源進行分配，如 CPU, Memory, GPU 等等   而既然音同 Queue，那麼它的核心概念就是 Queue  Kueue 本身有兩種策略     StrictFiFo: 先進先出，並且是 阻塞的(如果當前 Job 沒辦法被排程，它會卡在那擋到後面的人)   BestEffort: 先進先出，但 不是阻塞的(如果當前 Job 沒辦法被排程，它會讓位)      其實 Kueue 本身是 priority queue  它會根據 1. priority 2. creation time 來決定順序    Installation  1 $ kubectl apply --server-side -f https://github.com/kubernetes-sigs/kueue/releases/download/v0.9.0/manifests.yaml   或者是用 Helm  1 2 3 4 $ helm install kueue oci://us-central1-docker.pkg.dev/k8s-staging-images/charts/kueue \\   --version=\"v0.9.1\" \\   --create-namespace \\   --namespace=kueue-system   Affinity  Affinity 指的是親和力，在計算機裡面通常指 CPU 的親和力  由於 CPU 會 context switch, 同一個 process 可能會被排程到不同的 CPU 核心上執行  而這對於效能而言是不好的，因為 CPU cache 會被清空，所以 CPU 會重新從記憶體中讀取資料  導致效能低落   你可以透過 taskset 指令將 process 綁定到特定的 CPU 核心上  操作起來長這樣  1 $ taskset 0x1 ./hello_world      在做 benchmark 的時候，taskset 很好用  因為你可以減少變因，使得你的 benchmark 更加準確    在 Kubernetes 裡面，Affinity 通常指的是 Pod 與 Node 之間的親和力(i.e. Node Affinity)  application 會希望擁有某些特定的資源  如節點本身的 cache 抑或是節點的位置等等的  你當然可以依據自己的偏好要將你的服務執行在某些節點上  比如說，考量到地理位置，你會希望服務運行在美國的節點上(因為它可以有較低的 latency)     Taints 則是 Node 與 Pod 之間的 排斥性  舉例來說，以下的指令會將 node1 標記為 maintain，並且不允許有任何的 Pod 在上面執行   1 $ kubectl taint nodes node1 maintain=true:NoSchedule      effect 欄位共有 NoSchedule, PreferNoSchedule, NoExecute 三種    Taints 是由一個類似 map 的結構表示  你可以在一個節點上標記上多個 taints，表示這個節點上有多個限制  唯有可以 容忍這些限制 的 Pod 才能夠在這個節點上執行      換言之，只要節點上有任何限制，預設情況下 Pod 都會盡量避開這些節點      Toleration 則是 Pod 與 Node 之間的 容忍性  我可以容忍某些節點上有某些限制的時候，就可以使用 Toleration  比方說我可以容忍節點正在維護，因此我的 Pod 還是可以被排程到這個節點上，然後執行   下方的 nginx 仍然可以被排程到有 maintain taint 的節點上執行   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 apiVersion: v1 kind: Pod metadata:   name: nginx   labels:     env: test spec:   containers:   - name: nginx     image: nginx     imagePullPolicy: IfNotPresent   tolerations:   - key: \"maintain\"     operator: \"Exists\"     effect: \"NoSchedule\"   Kueue Architecture       ref: Cluster Queue    所謂的資源管理到底是怎麼個管理法  Kueue 具現化你所擁有的資源，比如說你總共有多少 CPU 多少 Memory  定義清楚之後，每一個 Job 都會從中取得資源，並且執行   你所擁有的資源都將儲存在 Cluster Queue 裡面  同一個類型的 Cluster Queue 會組成一個抽象的資源群組(Cohort)   當一個新的 Job 等待資源的時候，Local Queue 會向 Cluster Queue 請求資源  並根據 Resource Flavor 的設定，將資源分配給 Job  就可以執行，結束之後釋放資源        ref: Run A Kubernetes Job    Kueue Workload  雖然我們一直提 Job, 但實際上 Kueue 是管理所謂的 Workload  Workload 可以把它想像成是 “一件事情”，所以最直接的例子就是 Kubernetes Job  它可以是 Kubernetes 的 Job, CronJob, StatefulSet, Deployment 等等的資源   本文還是就 Kubernetes Job 進行說明與操作      Deployment 以及 StatefulSet，Kueue 是透過 pod integration 來達成的  可參考 Run Plain Pods    Resource Flavor  這裡的 Flavor 就是上面提到的 偏好  但對於 CPU, Memory 等等的設定並不是在這裡做的  所以本質上 Flavor 管理的跟原生 Kubernetes 是一致的(Taints 以及 Toleration, 可參考 Affinity)   為了能夠順利的使用 Kueue, 預設情況下還是要有一個 default-flavor(如下所示)   1 2 3 4 apiVersion: kueue.x-k8s.io/v1beta1 kind: ResourceFlavor metadata:   name: default-flavor   Cohort  你可以透過 label 定義 cohort 的隸屬關係  比如說 john 以及 alice 都是 research-team 的一部分   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 apiVersion: v1 kind: Namespace metadata:   name: john   labels:     research-cohort: research-team  ---  apiVersion: v1 kind: Namespace metadata:   name: alice   labels:     research-cohort: research-team   那麼，john 以及 alice 就會共享同一個 Cohort  也就可以存取特定 research-team 底下的 Cluster Queue   Cluster Queue  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 apiVersion: kueue.x-k8s.io/v1beta1 kind: ClusterQueue metadata:   name: cluster-q spec:   namespaceSelector:     matchLabels:         kubernetes.io/metadata.name: research-team   resourceGroups:   - coveredResources: [\"jobs\"]     flavors:     - name: \"default-flavor\"       resources:       - name: \"jobs\"         nominalQuota: 5     - name: \"maintain-flavor\"         resources:         - name: \"jobs\"         nominalQuota: 1         # borrowingLimit: 1         # lendingLimit: 1      這個 Cluster Queue 只允許 research-team 存取  namespaceSelector: {} 代表所有的 namespace 都可以存取    上述定義了一個簡單的 Cluster Queue  我可以允許有 1 個 job 可以在 maintain-flavor 的資源上執行  但大多數還是希望可以在 default-flavor 上執行，而它可以有 5 個 Job 同時執行   而前面也提到，Cluster Queue 可以不只有一個  所以你可以根據業務邏輯，拆分多種資源群組  但有時候 Cluster Queue 上的資源真的不夠，你可以有條件的向其他 Cluster Queue 請求資源     borrowingLimit: 最多拿別人多少資源   lendingLimit: 最多借給別人多少資源      只有相同 Cohort 的 Cluster Queue 才能夠互相借用資源    Local Queue  1 2 3 4 5 6 7 apiVersion: kueue.x-k8s.io/v1beta1 kind: LocalQueue metadata:   namespace: default   name: local-q spec:   clusterQueue: cluster-q   Local Queue 會指向一個 Cluster Queue  並且像它請求資源   Local Queue 本身是 per namespace 的設計  屬於該 namespace 下的 Job 會提交到 Local Queue   Run  要執行 kueue, 你需要將 Cluster Queue, Local Queue 與 Resource Flavor 部署到你的 cluster 上(缺一不可)  然後透過以下的範例 Job 觀察排隊的行為   你會需要加兩個設定     Job metadata 裡面需要新增 kueue.x-k8s.io/queue-name 的 label, 它需要指定到你的 Local Queue 的名稱   將 Job 預設的狀態設定成 suspend   為什麼要 suspend Job 呢？  原因也是很簡單，因為我們要讓 Kueue 控制 Job 的執行  如果你直接讓它執行不就沒用了  因此，所有要使用 Kueue 的 Job 預設都要讓它暫停  把控制權交給 Kueue 進行處理   這裡使用 completions 紀錄我們要有 10 個 Job 成功的次數  而 parallelism 則是同時執行的 Job 數量  根據上述 Cluster Queue 的設定，同一時間只能有 1 個 Job 在執行  又因為同時可以有 2 個 Job 在執行，所以整個完成預計要 (10 / 2) * 30s = 150s   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion: batch/v1 kind: Job metadata:   name: kjob   labels:     kueue.x-k8s.io/queue-name: local-q spec:   parallelism: 2   completions: 10   suspend: true   template:     spec:       containers:         - name: dummy-job           image: gcr.io/k8s-staging-perf-tests/sleep:v0.1.0           args: [\"30s\"]       restartPolicy: Never   Intercept Specific Resource Instance Only  目前 Kueue 的實作(v0.10.1)它會監聽所有的 Job，這在某種程度上是不合理的  理論上它只需要管理，並且監聽特定的 Job   我遇到的狀況是，起 backend 的時候有一些 one-time Job 需要被執行  這個 Job 跟 Kueue 是脫勾的，它不應該要被 Kueue 管理  但是由於 Kueue 的 MutatingAdmissionWebhook 沒有辦法過濾特定的 Job  導致這個美麗的錯誤發生      ref: Kueue mutating admission webhook should intercept specific resource instance only    Internal error occurred: failed calling webhook “myresourceflavor.kb.io”: failed to call webhook  如果你在 apply Kueue 的設定檔的時候碰到類似以下的錯誤   1 2 3 4 5 Error from server (InternalError): error when creating \"mykueue.yaml\":  Internal error occurred: failed calling webhook \"myresourceflavor.kb.io\":  failed to call webhook:  Post \"https://kueue-webhook-service.mynamespace.svc:443/mutate-kueue-x-k8s-io-v1beta1-resourceflavor?timeout=10s\": no endpoints available for service \"kueue-webhook-service\"   這是因為 Kueue 的 webhook service 還沒有起來  稍微的等它一下就可以了   你可以使用 $ kubectl wait 的指令等待  service 本身其實是 expose kueue-controller-manager 這個 deployment  另外，你也可以等待 ClusterQueue, LocalQueue 等的資源  注意到他們聽的狀態是不同的   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 $ kubectl wait \\   -n my-ns \\   --timeout=1h \\   --for='jsonpath={.status.conditions[?(@.type==\\\"Available\\\")].status}=True' \\   deployments.app/kueue-controller-manager  $ kubectl wait \\   -n my-ns \\   --timeout=1h \\   --for='jsonpath={.status.conditions[?(@.type==\\\"Active\\\")].status}=True' \\   ClusterQueue/my-cluster-queue  $ kubectl wait \\   -n my-ns \\   --timeout=1h \\   --for='jsonpath={.status.conditions[?(@.type==\\\"Active\\\")].status}=True' \\   LocalQueue/my-local-queue   References     Taints and Tolerations   Processor affinity   Run Plain Pods  ","categories": ["kubernetes"],
        "tags": ["job","queue","kueue","resource","scheduling","taints","toleration","affinity","taskset","kubectl wait","cluster queue","local queue","resource flavor","cohort"],
        "url": "/kubernetes/kubernetes-kueue/",
        "teaser": null
      },{
        "title": "Kubernetes 從零開始 - 如何測試你的 Kubernetes 應用程式？",
        "excerpt":"Kubernetes Application Testing  軟體工程裡面測試應用程式是一個很重要的環節，開發 Cloud Native 應用程式的時候也一樣  常見的就是使用 Kubernetes 進行開發，建立 Pod 跑東西之類的  所以很明顯這種邏輯也是需要進行測試覆蓋的   那問題來了  如 PostgreSQL, Redis 等等的都可以使用一些軟體工程的方法繞過  常見的就是建立個 interface 解耦，並且將其 mock 掉  但是 Kubernetes 這種東西，你要怎麼 mock？      有關測試的討論，可以參考 DevOps - 單元測試 Unit Test | Shawn Hsu    Clientset and Dynamic Client  client-go 是一套與 Kubernetes Cluster 進行溝通的 library  其中最常用到的就是 clientset 以及 dynamic client   clientset 主要是用來操作 Kubernetes 內建的 Resource，像是 Pod, Deployment, Service 等等  所有的操作都是有型別定義的，所以整體操作起來是比較安全的(因為有 type check)  但是對於非內建的 Resource，像是 Custom Resource，就沒辦法直接操作了  這時候你需要使用 dynamic client 來操作  dynamic client 是一個通用的 client，可以操作任何 Resource，但是操作起來就沒有 clientset 安全了  所有的 Resource 都必須要轉型成 unstructed object 來操作  本質上它只是 map[string]any      有關 CRD 的介紹可以參考 Kubernetes 從零開始 - client-go 實操 CRD | Shawn Hsu    Unstructured Object  如果你用 operator-sdk 建構 Custom Resource  你會擁有一個 Resource structure 可以直接操作，client 也可以直接吃這個 structure  透過 dynamic client 你沒辦法直接塞這個 structure 進去，你必須要轉換成 unstructured object      既然我有 Resource 的定義，我不能用 clientset 嗎？  沒辦法，因為你需要將這個 Resource 註冊進去，顯然是有難度的    你可以用 k8s.io/apimachinery/pkg/runtime package 來進行轉換  它提供了 ToUnstructured 以及 FromUnstructured 這兩個方法  讓你在 Resource structure 與 unstructured object 之間進行轉換   1 2 3 4 var crd fooV1.Foo runtime.DefaultUnstructuredConverter.FromUnstructured(result.Object, &amp;crd)  runtime.DefaultUnstructuredConverter.ToUnstructured(crd)   Fake Client  所以回到重點，如果要 mock 應該從 clientset 以及 dynamic client 下手  因為你需要透過這兩個 client 來操作 Kubernetes Cluster  比如說建立 Pod, Deployment 等等   fake package 是一套讓你方便進行測試的實作  clientset 以及 dynamic client 都有提供 fake client  而他們都實作了各自的 interface，所以你的 service 定義的時候就會像這樣   1 2 3 4 5 6 7 8 9 10 11 12 13 14 type Service struct { \tclientSet     kubernetes.Interface \tdynamicClient dynamic.Interface }  func NewService(     clientSet kubernetes.Interface,      dynamicClient dynamic.Interface, ) *Service { \treturn &amp;Service{ \t\tclientSet:     clientSet, \t\tdynamicClient: dynamicClient, \t} }   Example  因為我們已經使用了 clientset 與 dynamic client 提供的 interface 進行解耦  所以即使抽換掉實作，我們原本程式碼也不需要任何的改變，測試寫起來會很輕鬆，就像以下這樣      完整的實作可以參考 ambersun1234/blog-labs/k8s-test    Clientset  1 2 3 4 5 6 7 8 9 10 11 func TestService_CreateEmptyJob(t *testing.T) { \tsc := fake.NewSimpleClientset() \tdc := fakeDynamic.NewSimpleDynamicClient(runtime.NewScheme())  \ts := NewService(sc, dc) \trequire.NoError(t, s.CreateEmptyJob(\"test\"))  \tjob, err := sc.BatchV1().Jobs(\"default\").Get(context.TODO(), \"test\", metaV1.GetOptions{}) \trequire.NoError(t, err) \trequire.Equal(t, \"test\", job.Name) }   所有的操作都跟你使用真正的 clientset 一樣  這裡我測試我的 CreateEmptyJob 有沒有正確建立，並使用 fake client 驗證   Dynamic Client  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 func TestService_CreateFoo(t *testing.T) { \tsc := fake.NewSimpleClientset() \tdc := fakeDynamic.NewSimpleDynamicClient(runtime.NewScheme())  \ts := NewService(sc, dc) \trequire.NoError(t, s.CreateFoo(\"test\", \"value\"))  \tfoo, err := dc.Resource(crd.GVR).Namespace(\"default\").Get(context.TODO(), \"test\", metaV1.GetOptions{}) \trequire.NoError(t, err)  \tdata, found, err := unstructured.NestedString(foo.Object, \"spec\", \"value\") \trequire.NoError(t, err) \trequire.True(t, found) \trequire.Equal(t, \"value\", data) }   因為 CRD 並不被 Kubernetes 本身所認識，所以操作的時候需要提供 GVR  GVR 包含了 Group, Version, Resource 這三個資訊  以本例，就是     Group: foo.example.com   Version: v1   Resource: foos   GVR 需要對照到 CRD 本身的定義，所以可能需要微調      注意到 Resource 這裡需要指定複數型態    References     client-go/kubernetes   client-go/dynamic  ","categories": ["kubernetes"],
        "tags": ["dynamic client","client-go","clientset","unsturctured object","fake client","GVR"],
        "url": "/kubernetes/kubernetes-test/",
        "teaser": null
      },{
        "title": "Kubernetes 從零開始 - client-go 實操 CRD",
        "excerpt":"Extend Kubernetes Resource  Kubernetes 有許多內建的 Resource，像是 Pod, Deployment, Service 等等  但開發者的需求總是不斷的增加，有時候內建的 Resource 並不能滿足商業需求  假設你需要表達一個很複雜的資源，現有的其實寫起來會很複雜   這時候，你可以使用 Custom Resource (CR) 來擴充 Kubernetes 的 Resource  擴充這件事情本質上還是圍繞在 Kubernetes 是一個 container orchestration system 的基礎  所有的操作都是跟 container 有直接或間接相關的，比如說 ConfigMap 允許動態的載入設定檔，不需要重新編譯 container image   CR 本身也符合這些特性，它不一定要跑 container, 它可以是一個單純的資源  可以讓你寫入或讀取結構化的資料   Why not just use ConfigMap or Secret?  既然它可以單純的表示一個可以讀寫的單元，很明顯內建的 ConfigMap 以及 Secret 也可以做到  的確大多數情況下，設定檔這種東西實在沒必要用 CR 自找麻煩   Another Abstraction Layer to Kubernetes Resource?  如果你的 Custom Resource 要執行 container  一個問題油然而生，我要怎麼跑？   所以本質上，Custom Resource 又再封裝了一層 Kubernetes Resource  實際上在執行的，可以是 Job, Deployment 等等內建的 Resource  這個 Custom Resource 的設計邏輯會更貼近你的 業務邏輯，讓你可以更好的操作與理解   Introduction to Custom Resource  Custom Resource 是 Kubernetes API 的 Extension  它允許你根據不同的需求(i.e. 業務邏輯) 客製化專屬的 Resource  這個 Resource 可以是一個單純的資料結構，也可以是一個需要執行 container 的複雜資源      注意到你不應該用 Custom Resource 當成是 data storage  它並不是要讓你這樣用的，對效能上會有影響    你可以直接透過 $ kubectl 的指令來操作 Custom Resource  並且 Custom Resource 可以被動態的創建，更新以及刪除，如同內建的 Resource 一樣方便   State Management with Kubernetes Operator  Custom Resource 說到底還是一個 Kubernetes Resource  因此，其 Kubernetes Object 也擁有所謂的狀態  這些狀態的控制是透過 Kubernetes Operator 進行的   Custom Resource 可以自定義他的理想狀態  常見的就是成功失敗之類的，或其實你可以定義當某個欄位變成某個數值的時候，做某件事情   有關 controller(i.e. operator) 可以參考 Kubernetes 從零開始 - 從自幹 Controller 到理解狀態管理 | Shawn Hsu   Creating Custom Resource  為了能夠在 Kubernetes 中透過 $ kubectl 指令來操作 Custom Resource  CR 本身要被定義在 Kubernetes API Server 中  預設情況下，Kubernetes API Server 並不知道你的 Custom Resource 是什麼(所以它才叫做 Custom)   原本的 API Server 只知道內建的 Resource，像是 Pod, Deployment, Service 等等  所以一個作法是起另一個 API Server 然後這個 API Server 知道你的 Custom Resource  所以你有兩個 API Server，一個是原本的，一個是你自己定義的  透過 proxy 的方式將 Custom Resource 的操作轉發到你自己的 API Server  這稱作 API Aggregation   不過透過 API Aggregation 的方式會需要你懂一點 coding  相對的，使用 CustomResourceDefinition (CRD) 會比較簡單  CRD 的安裝方式並不需要額外一台 API Server 而且也不需要任何 coding 知識                          Custom Resource Definition (CRD)       API Aggregation                       需要額外的 API Server       :x:       :heavy_check_mark:                 上手難度       低       高                 後期維護難易度       低       高                 彈性       低       高           CustomResourceDefinition (CRD)  CRD 是一個內建的 Kubernetes Resource  你可以透過 CRD 定義 Custom Resource 的 Schema  它寫起來會長這樣   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 apiVersion: apiextensions.k8s.io/v1 kind: CustomResourceDefinition metadata:   name: foos.foo.example.com spec:   group: foo.example.com   names:     kind: Foo     listKind: FooList     plural: foos     singular: foo   scope: Namespaced   versions:   - name: v1     schema:       openAPIV3Schema:         description: Foo is the Schema for the foos API         properties:           apiVersion:             type: string           kind:             type: string           metadata:             type: object           spec:             description: FooSpec defines the desired state of Foo             properties:               value:                 type: string             type: object           status:             type: object       served: true       storage: true       subresources:         status: {}      可參考 ambersun1234/blog-labs/k8s-crd 以及 ambersun1234/blog-labs/k8s-controller    當你在 Kubernetes cluster 裡面建立 CRD  它其實是建立了一個新的 Resource, 你可以透過以下的 URL 取得相對應的資源  本例來說就是 /apis/foo.example.com/v1/namespaces/*/foos/...  因為要區分不同的 Resource, 你可以看到 Group, Version 以及 Resource 都呈現在 URL 中(所謂的 GVR)   這個 CRD 本身的名字是由 Resource name + Group name 組成的  需要注意的是 Resource name 需要使用 複數  而這個名字需要符合 DNS subdomain 的規則(可參考 DNS Subdomain Names)   Installation  $ kubectl apply -f crd.yaml  建立 CRD 之後你的所有 kubectl 操作都跟內建的 Resource 一樣   CRD Cluster Role  要能夠操作 CRD 你需要一定的權限  一般情況下你是 cluster admin 你不一定需要設定 rule 才可以操作 CRD  對於服務來說，你需要設定一個 ClusterRole 來讓你的服務可以操作 CRD   Kubernetes 本身是使用 RBAC 來控制權限  所以寫起來就是 你有沒有權限去操作這個 Resource，你可以做什麼操作      有關 RBAC 可以參考 網頁程式設計三兩事 - 基礎權限管理 RBAC, ABAC 與 PBAC | Shawn Hsu    當然 ClusterRole 本身需要搭配 ClusterRoleBinding 以及 ServiceAccount 來使用      ClusterRole 作用域是整個 cluster  Role 作用域是 namespace    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 kind: ClusterRole metadata:   name: foo-editor-role rules: - apiGroups:   - foo.example.com   resources:   - foos   verbs:   - create   - delete   - get   - list   - patch   - update   - watch - apiGroups:   - foo.example.com   resources:   - foos/status   verbs:   - get      你可以使用 $ kubectl auth can-i get foo 測試你有沒有權限    CRD Example with client-go  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 func (s *Service) CreateFoo(name, value string) error {     foo := &amp;crd.Foo{         TypeMeta: metaV1.TypeMeta{             Kind:       \"Foo\",             APIVersion: \"foo.example.com/v1\",         },         ObjectMeta: metaV1.ObjectMeta{             Name: name,         },         Spec: crd.FooSpec{             Value: value,         },     }      object, err := runtime.DefaultUnstructuredConverter.ToUnstructured(foo)     if err != nil {         return err     }      _, err = s.dynamicClient.Resource(crd.GVR).         Namespace(\"default\").         Create(context.TODO(), &amp;unstructured.Unstructured{Object: object}, metaV1.CreateOptions{})      return err }      可參考 ambersun1234/blog-labs/k8s-crd    client-go 在建立 CR 的時候需要使用 dynamic-client  因為 clientset 並不知道你的 Custom Resource 是什麼  所以你需要透過 dynamic-client 來操作   可以參考 Kubernetes 從零開始 - 如何測試你的 Kubernetes 應用程式？ | Shawn Hsu   Custom Resource Versioning  你知道 Custom Resource 也可以迭代的嗎？  事實上這個設計非常的合理，因為你的業務需求可能會隨著時間而改變，所以你的 Custom Resource 也可能需要跟著改變  比方說刪減欄位，更新資料結構等等   對於這種需要 Migration 的情況，Kubernetes 本身有很好的機制負責處理  要進行升級首先你會遇到的問題就是 相容性，如果你有客製化 Kubernetes Controller，你就必須要確保它能夠正確的處理新舊版本的資料  怎麼相容呢？ 你就需要使用 Conversion Webhook 來處理      注意到，你不應該去直接修改已經發布的 Custom Resource  比如說 v1 CRD 需要升級到 v2, 你應該在 CRD 裡面額外定義一個 v2 的 schema  而不是直接修改    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 apiVersion: apiextensions.k8s.io/v1 kind: CustomResourceDefinition metadata:   annotations:     controller-gen.kubebuilder.io/version: v0.14.0   name: foos.foo.example.com spec:   group: foo.example.com   versions:   - name: v1     # Each version can be enabled/disabled by Served flag.     served: true     # One and only one version must be marked as the storage version.     storage: false     # This indicates the v1alpha1 version of the custom resource is deprecated.     # API requests to this version receive a warning header in the server response.     deprecated: true     # This overrides the default warning returned to API clients making v1alpha1 API requests.     deprecationWarning: \"foo.example.com/v1 is deprecated\"     # A schema is required     schema:       openAPIV3Schema:         properties:           spec:             description: FooSpec defines the desired state of Foo             properties:               value:                 type: string             type: object   - name: v2     served: true     storage: true     schema:     openAPIV3Schema:       properties:         spec:           description: FooSpec defines the desired state of Foo           properties:             anotherValue:               type: string             value:               type: string           type: object    # The conversion section is introduced in Kubernetes 1.13+ with a default value of   # None conversion (strategy sub-field set to None).   conversion:     # None conversion assumes the same schema for all versions and only sets the apiVersion     # field of custom resources to the proper value     strategy: None     strategy: Webhook     webhook:       # conversionReviewVersions indicates what ConversionReview versions are understood/preferred by the webhook.       # The first version in the list understood by the API server is sent to the webhook.       # The webhook must respond with a ConversionReview object in the same version it received.       conversionReviewVersions: [\"v1\"]       clientConfig:         url: \"https://my-webhook.example.com:9443/my-webhook-path\"   比如說上述 CRD 裡面包含兩種版本 v1, v2  其中比較需要注意的是 served 以及 storage     served 代表這個版本是否可以被服務，如果你不想兼容這個版本，你可以將 served 設為 false            升級不支援的 CR 你可以手動更新或者是使用 Version Migrator       如果不再支援，可以考慮從 CRD 拔掉這個版本           storage 則代表在底層中(i.e. etcd)，針對這個 CR 要儲存哪個版本的格式            注意到已經存在的舊版本的 CR 並不會被轉換成新版本的格式       只有當你新建立或者更新的時候才會寫入新版本的格式       儲存的版本只能選一個              如果你是透過 Operator-SDK 建立 CRD  多版本的 CRD 定義裡面需要新增 //+kubebuilder:storageversion 這個註解  告訴 Operator-SDK 這個版本是用來儲存的    Version Convention  在設計不同版本的 CRD 的時候，版本號的規則在這裡有點不同  比方說你要從 v1 升級到 v1.1 之類的，這件事情是不被允許的   1 2 3 Invalid value: \"v1.1\": a DNS-1035 label must consist of lower case alphanumeric characters or '-',  start with an alphabetic character, and end with an alphanumeric character  (e.g. 'my-name',  or 'abc-123', regex used for validation is '[a-z]([-a-z0-9]*[a-z0-9])?')   而根據 Versions in CustomResourceDefinitions 裡面的說明  一個合法且常見的做法為 stability level + version number  也就是組成類似 v1alpha1, v1beta1, v1 這樣的格式   Conversion Webhook  注意到上面 CRD 裡面有一個 conversion 的欄位  這是讓你拿來處理新舊版本資料格式轉換的地方      conversionReviewVersions 是要被轉換的版本  為什麼不用指定 target version 呢？ 因為同一個 CR 底層只能存一種版本的資料    你可以指定不同的策略來處理轉換     None: 不轉換，那它會長怎麼樣？            具體來說，它只會更新 apiVersion 欄位，如果遇到它不認識的欄位，它會把它刪掉       實務上當然是不推薦這個作法就是           Webhook: 透過 webhook 來處理轉換   如果要轉換  我第一個問題會是，那我的 CR 哪時後會被轉換？  它跟上一節講的 storage 有關係  由於一個 CR 它底層儲存的資料格式只會是眾多版本中的一個     讀取的時候: 底層一樣存 舊版的格式，給你的是 新版的格式   寫入以及更新的時候: 寫入的就會是 新版的格式   刪除就不用考慮格式的問題了   How to Do the Conversion?  其實這比我想像的還滿簡單，就是寫一個 endpoint 負責處理轉換邏輯  這個 endpoint 需要定義成 POST 方法，然後從 body 讀取 payload   這個 payload 長這樣  其中 objects 就是需要被轉換的 CR 們  uid 為唯一識別符號  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 {   \"apiVersion\": \"apiextensions.k8s.io/v1\",   \"kind\": \"ConversionReview\",   \"request\": {     \"uid\": \"705ab4f5-6393-11e8-b7cc-42010a800002\",     \"desiredAPIVersion\": \"example.com/v1\",          \"objects\": [       {         \"kind\": \"CronTab\",         \"apiVersion\": \"example.com/v1beta1\",         \"metadata\": {           \"creationTimestamp\": \"2019-09-04T14:03:02Z\",           \"name\": \"local-crontab\",           \"namespace\": \"default\",           \"resourceVersion\": \"143\",           \"uid\": \"3415a7fc-162b-4300-b5da-fd6083580d66\"         },         \"hostPort\": \"localhost:1234\"       },       {         \"kind\": \"CronTab\",         \"apiVersion\": \"example.com/v1beta1\",         \"metadata\": {           \"creationTimestamp\": \"2019-09-03T13:02:01Z\",           \"name\": \"remote-crontab\",           \"resourceVersion\": \"12893\",           \"uid\": \"359a83ec-b575-460d-b553-d859cedde8a0\"         },         \"hostPort\": \"example.com:2345\"       }     ]   } }   轉換成功的 response 長這樣  可以看到 uid 必須要長一樣，都是 705ab4f5-6393-11e8-b7cc-42010a800002  然後必須要回傳 result object 表示成功與否   convertedObjects 就是轉換後的 CR 們  你可以看到相比原本的 CR, 不只 apiVersion 變了, 其餘欄位也被轉換了   注意到  metadata 裡面的東西基本上都是不能動的(除了 label 以及 annotation)  因為這些資料一旦遺失，Kubernetes 就不知道這個 CR 是誰了  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 {   \"apiVersion\": \"apiextensions.k8s.io/v1\",   \"kind\": \"ConversionReview\",   \"response\": {     \"uid\": \"705ab4f5-6393-11e8-b7cc-42010a800002\",     \"result\": {       \"status\": \"Success\"     },     \"convertedObjects\": [       {         \"kind\": \"CronTab\",         \"apiVersion\": \"example.com/v1\",         \"metadata\": {           \"creationTimestamp\": \"2019-09-04T14:03:02Z\",           \"name\": \"local-crontab\",           \"namespace\": \"default\",           \"resourceVersion\": \"143\",           \"uid\": \"3415a7fc-162b-4300-b5da-fd6083580d66\"         },         \"host\": \"localhost\",         \"port\": \"1234\"       },       {         \"kind\": \"CronTab\",         \"apiVersion\": \"example.com/v1\",         \"metadata\": {           \"creationTimestamp\": \"2019-09-03T13:02:01Z\",           \"name\": \"remote-crontab\",           \"resourceVersion\": \"12893\",           \"uid\": \"359a83ec-b575-460d-b553-d859cedde8a0\"         },         \"host\": \"example.com\",         \"port\": \"2345\"       }     ]   } }   如果轉換錯誤是  1 2 3 4 5 6 7 8 9 10 11 {   \"apiVersion\": \"apiextensions.k8s.io/v1\",   \"kind\": \"ConversionReview\",   \"response\": {     \"uid\": \"&lt;value from request.uid&gt;\",     \"result\": {       \"status\": \"Failed\",       \"message\": \"hostPort could not be parsed into a separate host and port\"     }   } }   Deprecation of CRD  有了多種 CRD 版本，你需要通知使用者說這個版本已經不再支援，請它不要再繼續使用  所以你可以在 CRD 裡面設定 deprecated 以及 deprecationWarning   1 2 3 4 5 6 7 8 9   versions:   - name: v1     schema:       openAPIV3Schema:         ...     served: true     storage: false     deprecated: true     deprecationWarning: foos.foo.example.com/v1 is deprecated. Use foos.foo.example.com/v2 instead.      如果你是透過 Operator-SDK 建立 CRD  需要加上 //+kubebuilder:deprecatedversion 註記這個版本被棄用    然後你在使用的時候就可以看到警告  注意到即使 deprecated 仍然可以使用，要把 served 設為 false (或刪除)才會真的不支援      當你查看寫入 v1 的 raw yaml 的時候，可以發現到它其實是被轉換成 v2 的格式         因為我沒有定義 conversion, 所以 anotherValue 的欄位會是空的  value 欄位因為沒有改變，所以會被保留    Conversion Webhook Example       to be continued    References     Custom Resources   Extend the Kubernetes API with CustomResourceDefinitions   Kubernetes API Aggregation Layer   Set up an Extension API Server   Role and ClusterRole   Versions in CustomResourceDefinitions   What does ‘storage’ means in Kubernetes CRD?   Managing different versions of CRD in the operator-idk controller   Add deprecated information for API  ","categories": ["kubernetes"],
        "tags": ["crd","cr","dynamic client","client-go","clientset","api aggregation","custom resource definition","custom resource","crd migration","crd versioning","conversion webhook","clusterrole","role","tls","webhook","conversion","migration"],
        "url": "/kubernetes/kubernetes-crd/",
        "teaser": null
      },{
        "title": "Kubernetes 從零開始 - 透過 Argo Workflows 管理 CRD 執行順序",
        "excerpt":"Preface  之前我們看過可以使用 Kueue 這類工具達成某種程度的控制，但是它比較是資源傾向的  像是 docker 有提供 $ docker pause 之類的指令，能夠允許你做到 pause and resume 的操作  更進階的需求就會是控制 container 的執行順序      有關 Kueue 的介紹可以參考 Kubernetes 從零開始 - 資源排隊神器 Kueue | Shawn Hsu    另外你也可以使用 client-go  把排程的部份寫在後端系統裡面，然後手動控制哪時候要啟 Pod  但是這樣的方式會讓你的程式碼變得更複雜，而且不好維護   Pod 被 scheduler 排程後你無法控制它被啟動的順序  kubectl wait 某種程度上可以做到執行順序的控制，這裡我起了兩個 job  其中第二個 job 會等到第一個 job 執行完畢後才會開始執行  wait 無法判斷 pod completed, 只能使用 job  雖然它能做到順序控制，但顯然不太彈性   1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion: batch/v1 kind: Job metadata:   name: dummy-first spec:   completions: 1   template:     spec:       containers:         - name: dummy-job           image: gcr.io/k8s-staging-perf-tests/sleep:v0.1.0           args: [\"10s\"]       restartPolicy: Never   1 2 3 4 5 6 $ kubectl apply -f ./job.yaml $ kubectl wait --timeout=60s --for=condition=complete job/dummy-first $ kubectl apply -f ./job2.yaml job.batch/dummy-first created job.batch/dummy-first condition met job.batch/dummy-later created   Introduction to Argo Workflows  幸好，Argo Workflows 提供了一個方式讓你可以控制 Kubernetes Resource 的執行順序  比如說 CI/CD pipeline 或者是 machine learning pipeline 這種工作就很適合使用 Argo Workflows  你可以透過 DAG(Directed Acyclic Graph) 的方式來定義你的工作流程  讓 Argo Workflows 自動幫你管理這些工作流程      目前 Argo Workflows 也有 proposal 與 Kueue 進行整合  可參考 Integration with Kueue    Installation  1 2 $ kubectl create namespace argo $ kubectl apply -n argo -f https://github.com/argoproj/argo-workflows/releases/download/v3.6.0/install.yaml   Argo CLI  1 2 3 4 5 6 7 8 # ARGO_OS=\"darwin\" ARGO_OS=\"linux\"  curl -sLO \"https://github.com/argoproj/argo-workflows/releases/download/v3.6.0/argo-$ARGO_OS-amd64.gz\" gunzip \"argo-$ARGO_OS-amd64.gz\" chmod +x \"argo-$ARGO_OS-amd64\" sudo mv \"./argo-$ARGO_OS-amd64\" /usr/local/bin/argo argo version   為了啟用 autocompletion  在 zshrc 裡面加入以下設定檔  1 source &lt;(argo completion zsh)   How to Schedule Kubernetes Resource into Argo Workflows  Argo Workflows 提供了兩種定義相依關係的方式，Steps 以及 DAG  所有的 Workflow 定義都有一個進入點 entrypoint  從這個進入點開始，之後就會根據你定義的順序執行   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 apiVersion: argoproj.io/v1alpha1 kind: Workflow metadata:   generateName: dag-diamond- spec:   entrypoint: diamond   templates:   - name: diamond     dag:       tasks:       - name: A         template: echo       - name: B         dependencies: [A]         template: echo       - name: C         dependencies: [A]         template: echo       - name: D         dependencies: [B, C]         template: echo   以上是一個 Argo Workflows 簡化版的 DAG 範例  可以看到進入點是 diamond，然後底下的 task 都是一個一個的 Kubernetes Resource  透過簡單 dependencies 你可以很輕鬆的定義出相依關係   就上圖來說，A 會是 root  之後 B 與 C 會平行處理  最後才是 D   Service Account for Workflows Kind  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 apiVersion: v1 kind: ServiceAccount metadata:   name: argo-workflow-account   namespace: default --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata:   name: argo-workflow-admin   namespace: default rules:   - apiGroups: [\"argoproj.io\"]     resources: [\"workflows\", \"workflowtemplates\", \"workflowtasksets\", \"workflowtaskresults\", \"workfloweventbindings\", \"workflowartifactgctasks\"]     verbs: [\"*\"] --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata:   name: argo-workflow-clusterrolebinding   namespace: default roleRef:   apiGroup: rbac.authorization.k8s.io   kind: ClusterRole   name: argo-workflow-admin subjects:   - kind: ServiceAccount     name: argo-workflow-account     namespace: default   為了能夠允許 argo 可以操作 K8s 的資源，必要的權限設定是必須的  上述為了簡單化，這裡把 argo 所有資源的權限都開放了  然後這個 Service Account(argo-workflow-account) 會被用來執行 argo 的工作      官網提到的 quick-start-minimal.yaml 有包含 role 在裡面所以執行起來沒問題  但是它不建議被用在 production 環境  可參考 quick start fails out of the box due to RBAC error    Sharing Data Between Steps   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 apiVersion: argoproj.io/v1alpha1 kind: Workflow metadata:   generateName: producer-consumer- spec:   entrypoint: producer-consumer   serviceAccountName: argo-workflow-account   templates:   - name: producer-consumer     dag:       tasks:       - name: producer-message         template: producer       - name: consumer-message         dependencies: [producer-message]         template: consumer         arguments:           parameters:           - name: message             value: \"{{tasks.producer-message.outputs.parameters.message}}\"    - name: producer     container:       image: busybox       command: [sh, -c]       args: [\"echo -n hello world &gt; /tmp/hello_world.txt\"]     outputs:       parameters:       - name: message         valueFrom:           path: /tmp/hello_world.txt    - name: consumer     inputs:       parameters:       - name: message     container:       image: busybox       command: [echo]       args: [\"{{inputs.parameters.message}}\"]      注意到需要指定 service account 避免沒權限  可參考 Service Account for Workflows Kind    不同的 task 之間可以透過 inputs 以及 outputs 來共享資料  上述是一個簡單的 producer consumer 的例子  producer 定義了一個 outputs 的輸出，然後因為 DAG 指令 consumer 必須等 producer 完成後才能執行  然後 consumer 在從 producer 那邊取得資料後，輸出到 console   整體執行起來會長這樣  透過 argo cli 執行你的 workflow  1 $ argo submit ./argo.yaml      看他的 log 就可以發現有正確做動  1 $ argo logs producer-consumer-xxxxx      What Kubernetes Resource Can Be Scheduled  上面的 DAG 範例中，我們使用了 echo 這個 template  這個 template 實際上是一個 Kubernetes Resource(也可以是一個簡單的 container)  也就是說，Argo Workflows 允許你管理任一種 Kubernetes Resource(包含 CRD)   Argo Workflows 針對任一 resource 的寫法必須要塞 raw manifest 進去  基本上就是把你的 yaml 檔案直接塞進去  然後要注意的是，以這種方式建立的 resource 並非 Argo Workflows 管轄的  也就是說當 workflows 結束 cleanup 的時候，這些 resource 並不會被刪除  因此你需要 setOwnerReference: true 這個設定   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion: argoproj.io/v1alpha1 kind: Workflow metadata:   generateName: k8s-patch- spec:   entrypoint: cront-tmpl   templates:   - name: cront-tmpl     resource:       action: create       setOwnerReference: true       manifest: |         apiVersion: \"stable.example.com/v1\"         kind: CronTab         spec:           cronSpec: \"* * * * */10\"           image: my-awesome-cron-image      有關 CRD 可以參考 Kubernetes 從零開始 - client-go 實操 CRD | Shawn Hsu    Determine Workflows State  Conditional Execution   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 apiVersion: argoproj.io/v1alpha1 kind: Workflow metadata:   generateName: condition- spec:   entrypoint: entry   serviceAccountName: argo-workflow-account   templates:   - name: entry     dag:       tasks:       - name: producer-message         template: producer       - name: consumer-execute         dependencies: [producer-message]         template: consumer         arguments:           parameters:           - name: message             value: \"{{tasks.producer-message.outputs.result}}\"         when: \"{{tasks.producer-message.outputs.result}} == hello\"       - name: consumer-noexecute         dependencies: [producer-message]         template: consumer         arguments:           parameters:           - name: message             value: \"{{tasks.producer-message.outputs.result}}\"         when: \"{{tasks.producer-message.outputs.result}} != hello\"    - name: producer     container:       image: busybox       command: [sh, -c]       args: [\"echo hello\"]    - name: consumer     inputs:       parameters:       - name: message     container:       image: busybox       command: [echo]       args: [\"{{inputs.parameters.message}}\"]   有時候你需要根據某些條件來判斷是否要執行某個 task  Argo Workflows 裡面你可以透過 when 這個條件來判斷  比如說上面的例子，當 producer 的結果是 hello 時，才會執行 consumer-execute  而 consumer-noexecute 則不會執行      outputs.result 是 container 的 stdout    可以看到，consumer-noexecute 因為 when validate 不通過，所以沒有執行         How to Define Success and Failure Conditions  我們已經知道如何根據不同的條件執行不同的 task  但是你要怎麼決定一個 task 是成功還是失敗呢?   Argo Workflows 提供了兩個欄位(successCondition 以及 failureCondition)  你可以將自定義的條件定義在以上欄位，然後 Argo Workflows 就會根據這些條件來判斷 task 的狀態  要注意的是，自定義 condition 僅能使用在 resource 裡面(也就是說上面 container 的方式是沒辦法的)      條件的寫法是採用 JsonPath 的語法    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 apiVersion: argoproj.io/v1alpha1 kind: Workflow metadata:   generateName: k8s-patch- spec:   entrypoint: cront-tmpl   templates:   - name: cront-tmpl     resource:       action: create       setOwnerReference: true       successCondition: status.succeeded &gt; 0       failureCondition: status.failed &gt; 0       manifest: |         apiVersion: \"stable.example.com/v1\"         kind: CronTab         spec:           cronSpec: \"* * * * */10\"           image: my-awesome-cron-image   Argo Go Client Example  Argo Workflows 也有提供 golang client API 讓你可以透過程式碼的方式來操作      官方也有提供一個 client-go 的範例 argo-workflows/examples/example-golang    一樣最基礎的把 clientset new 出來  1 2 3 4 5 6 7 8 9 10 11 12 13 kubeconfig := flag.String(     \"kubeconfig\",     filepath.Join(homedir.HomeDir(), \".kube\", \"config\"),     \"(optional) absolute path to the kubeconfig file\", ) flag.Parse()  config, err := clientcmd.BuildConfigFromFlags(\"\", *kubeconfig) if err != nil {   panic(err) }  wfClient := wfclientset.NewForConfigOrDie(config).ArgoprojV1alpha1().Workflows(namespace)   然後建立一個 DAG workflow  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 wfv1.Workflow{     ObjectMeta: metav1.ObjectMeta{         GenerateName: \"dag-\",     },     Spec: wfv1.WorkflowSpec{         ServiceAccountName: \"argo-workflow-account\",         Entrypoint: \"root\",         Templates: []wfv1.Template{             {                 Name: \"root\",                 DAG: &amp;wfv1.DAGTemplate{                     Tasks: []wfv1.DAGTask{                         {                             Name:     \"A\",                             Template: \"job\",                             Arguments: wfv1.Arguments{                                 Parameters: []wfv1.Parameter{                                     {                                         Name: \"message\", Value: wfv1.AnyStringPtr(\"my name is A\"),                                     },                                 },                             },                         },                         {                             Name:         \"B\",                             Template:     \"job\",                             Dependencies: []string{\"A\"},                             Arguments: wfv1.Arguments{                                 Parameters: []wfv1.Parameter{                                     {                                         Name: \"message\", Value: wfv1.AnyStringPtr(\"my name is B\"),                                     },                                 },                             },                         },                     },                 },             },             {                 Name: \"job\",                 Inputs: wfv1.Inputs{                     Parameters: []wfv1.Parameter{                         {Name: \"message\"},                     },                 },                 Resource: &amp;wfv1.ResourceTemplate{                     Action:            \"create\",                     SetOwnerReference: true,                     Manifest:          job,                 },             },         },     }, }      因為我們要建立 Job, 所以一樣需要設定相對應的 role  要記得給 Job 的 role 權限(可參考 Service Account for Workflows Kind)    基本上，這個 DAG 類似於 Sharing Data Between Steps 裡面的範例  不過這裡是使用 Kubernetes Resource 建立出來的(因為 CRD 也是同樣的寫法，為了方便 demo 這裡用 Job)   然後那個 Job yaml 目前來說它沒辦法使用 client-go 裡面的 corev1.Job  就算你用 gopkg.in/yaml.v2 轉成 yaml string 也是不行的      然後需要注意 indentation，它要是空白，不能是 tab    並且 job yaml 的名字必須要是使用 generate 的(因為我們 2 個 workflows 都是用同一個 template)   總之，執行順利你會得到類似底下的結果      然後他們相對應的 log 你就可以看到參數有被正確傳遞          完整的程式碼範例可以參考 ambersun1234/blog-labs/argo-workflow    Conclusion  Argo Workflows 除了上面我們提到的基本功能之外，它還有像是  支援遞迴呼叫，重複執行等等的功能   然後它官方的 GitHub 也有提供很多的 範例 可以參考  搭配範例可以更好的理解 Argo Workflows 的運作方式   References     How do I hold a request on the k8s until pods is ready?   Is there a way to specify order of containers to execute in a Pod?   Attach Handlers to Container Lifecycle Events  ","categories": ["kubernetes"],
        "tags": ["argo-workflows","crd","kubectl wait","kueue","client-go","dag","service account","conditional execution"],
        "url": "/kubernetes/kubernetes-argo-workflow/",
        "teaser": null
      },{
        "title": "Kubernetes 從零開始 - 如何在 MicroService 架構下，跨服務找出 API 效能瓶頸",
        "excerpt":"Define Observable System  服務其實不太可能是完全穩定的，總會有一些問題發生  不論是 application 自身的 bug 還是因為 cloud provider 主機異常掉線  這些問題都會對服務造成影響   為了要能夠及時的發現問題，我們需要一個監控系統  不論是硬體狀態還是軟體狀態  透過這些狀態我們才能夠正確的判斷服務的狀態   通常來說我們要怎麼觀察服務的狀態呢？  通常使用者可能會最先發現問題，可能是因為它沒辦法連線到購物網站  或者是服務回應時間過長   但這通常已經來不及了  更進階的一個步驟會是直接透過 log message 觀察服務的狀態   Is Logging Enough?  直接看 log message 你可以大略的知道服務的狀態  request 數量少的時候是可以使用肉眼觀察法的，對於大流量的系統來說可能就不是那麼適合了   並且 log 只能告訴你服務的狀態，但是無法告訴你服務的效能表現  比如說，回應時間很慢，有可能是資料庫已經撐不住了，並不是你 application 本身造成的問題  這時候你反覆去查 application log 也是無濟於事的   What Kind of Data Should I Care  所以，一個好的監控系統，它理論上應該要能夠提供 Logs, Metrics 以及 Traces 這三種資料  服務需要以某種形式提供以下這些資料，稱之為 Telemetry Data(遙測資料)   Logs       ref: What is log monitoring?    application log 可以提供你額外的資訊，針對特定的 event 比如說 request 的 payload, ip  但是 log message 沒辦法給予你相對應的 context, 比如說他是從哪呼叫的，這個 request 是怎麼走的  不過對於服務的基本狀態還是有一定的幫助的  假設你想要往下仔細找問題，這些 log message 就會是你的好幫手      有關 logging 的部份可以參考 網頁程式設計三兩事 - Logging 最佳實踐 | Shawn Hsu    Metrics       ref: Introduction to Performance Monitoring Metrics    Metrics 所收集的資料通常是 raw data, 比如說 CPU 使用率，request 數量，request 數量以及回應時間等等  透過這些資料可以更直觀的看到服務的狀態，並且依據這些資料來做出決策或者是往下追蹤問題(透過 Logs 以及 Traces)   Traces  透過 Logs 你可以觀察到單一 request 的完整生命週期(前提是你 log 的足夠詳細)  那根據這些 log message 你可以知道這個 request 是怎麼走的，也可以知道這個 request 在各個 service 之間是怎麼傳遞的  比如說要看耗時也是可以做到的   Introduction to OpenTelemetry  OpenTelemetry 是一個開源的觀測工具  它提供了一個標準化的方式來收集 Logs, Metrics 以及 Traces 這些 Telemetry Data   OpenTelemetry Architecture  OpenTelemetry 抽象化了這些資料的收集方式，且透過它提供的 SDK 來將這些資料送到你的監控系統(稱為 observability backend)  這個監控系統可以是任一形式的，比如說 Prometheus   等於說 OpenTelemetry 只是一個中間層  你的 application 可以是任何語言實作的，透過 OpenTelemetry 提供的 SDK 收集資料(稱為 collector)  然後透過 exporter 送到監控系統   當然全部的 trace 都塞進去顯示可能會造成一些壓力  所以 OpenTelemetry 提供了一個 sampling 的機制，可以讓你選擇要送多少資料到監控系統      collector 所收集的資料格式會針對不同的 observability backend 有不同的格式    你發現沒有，OpenTelemetry 既沒有視覺化的 GUI 也沒有提供儲存資料的地方  也因此，你需要自己去選擇你的工具     Observability Backend            Prometheus       Grafana       Jaeger       uptrace/uptrace           Data Storage            Elasticsearch       InfluxDB       Cassandra       ClickHouse           Span       Distributed traces    上圖是一個完整的 Trace，其中包含了若干個 Span  可以看到 Client Span 是最上層的，往下 API Span 做了三件事情，分別是 驗證身份 以及 金流操作 等等的 unit of work      Span 的顆粒度到底要細緻到哪種程度取決於你的需求    一個 Span 可以理解為一件事情，亦即 unit of work  注意到 Span 之間的關係是 parent-child 關係，它表示的是 從屬關係  也就是說上圖 API Span 並不是在 Client Span 之後發生的，而是在 Client Span 之中發生的   此外，Span 也可以透過 Span Links 定義所謂的 因果關係   Span Data  一個 Span 為了能夠正確的傳遞足夠的訊息以便追蹤監控，通常包含了以下的資訊  前面提到，Traces 由一或多個 Span 組成，要能夠區分從屬關係，所以你需要紀錄 parent_id 以及 trace_id  如果 parent_id 為空，則表示該 Span 為 root Span   為了要能夠 debug 特定的情境，一些額外的資訊是必要的，比如說在特定 input 的情況下速度會變慢之類的  上述圖片，考慮 金流操作 這個 Span  你可能會需要紀錄，金流的 transaction id, 金流的金額等等  這些東西都需要紀錄於 Span Attributes 中(key-value pair)   此外，Log 的資訊也可以被紀錄在 Span 裡面，稱為 Span Events  Span Events 通常是紀錄 事件，發生於某個時間點的資料   雖然說 Span Events 有可能也會帶有一些額外的資訊，與 Span Attributes 的區別在於  Attributes 所表示的資料是整個 Span 的 metadata，而 Events 所表示的資料是某個時間點的資訊，並不一定適用於整個 Span   Span Links  注意到，在同一個 Traces 中，Span 之間的關係只能是 parent-child 的關係   同一個 Trace 只應該存在 parent-child 關係  不同 Trace 的 Span 才有可能存在因果關係   一樣考慮電商平台買東西的情境  圖片裡的 Trace 基本上只包含了購買的部份  我們是不是也應該要考慮 物流 的部份   很明顯，物流這塊它應該要是一個獨立的 Trace  當客戶都完成所有的訂購流程之後，我們才會開始處理物流的部份  而這兩個 Trace(購買 以及 物流) 之間是有因果關係的  因此你可以使用 Span Links 來連結這兩個 Trace 的 Span   Distributed Tracing  Trace 表示 “一個 request 的完整生命週期”，而在微服務的架構下，他是有可能呼叫一個以上的服務的  不同的服務的 Span，雖然他是隸屬同一個 request，但是這些資料是無法直接關聯起來的  透過 Context Propagation 你可以輕易的將其連結起來   Context Propagation  一個服務裡面，你的 context 可以直接透過參數的方式往下傳遞給多個 Span  如果遇到分散式的服務，你的 context 也需要透過某種方式傳遞給其他服務  這樣你才可以將這些不同的 Span 關聯到同一個 Trace 上   比如說你可以將 context 的資料放在 HTTP header 裡面，這樣你的服務就可以透過 HTTP request 來傳遞 context  或者是使用第三方的套件如 opentelemetry/instrumentation-http      propagation 主要是 instrumentation library 會幫你做掉  instrument 指的是將你的程式碼加入一些額外的資訊，比如說 log message, metrics 以及 trace    通常來說，只有在極少數狀況下會需要手動設定 trace_id 以及 span_id  根據 Propagation 的說明      It is only in rare cases that you will need to propagate context manually.    如果你跨服務追蹤需要的不是 HTTP, gRPC 等等可以做到的，可以使用 TextMapPropagator 之類的東西   Baggage the Additional Information  我們知道 Span 裡面可以儲存額外的資訊(i.e. Span Attributes)  不過這些資訊僅限於 Span 內部，如果我希望整個 Trace 都能夠存取到這些資訊呢？  你就會需要用到所謂的 Baggage   Baggage 一樣是一個 key-value store 的資料結構，他的生命週期是跟在 context 上面的  也就是說你可以在 Trace 一開始的時候就初始化 Baggage 並且在裡面塞入一些全域的資訊(比如說 userId)  這樣在整個 Trace 的生命週期中，你都可以存取到這些資訊      同樣都是 key-value store, Baggage 與 Span Attributes 並不共享  要使用 Baggage 資料你需要手動讀取並且寫入       其實不只是單個 Trace，Baggage 也可以在不同 Traces, Logs 以及 Metrics 之間傳遞         ref: Baggage    Distributed Traces and Logs with Uptrace  本文我會使用 uptrace/uptrace 當作我的 observability backend  而 uptrace 本人會需要使用 ClickHouse 來 telemetry 的資料，以及 PostgreSQL 來儲存相關的 metadata   相比 Jaeger 只能收 Trace 以及 Logs，uptrace 提供了更多的功能  uptrace 可以收 OpenTelemetry protocol (OTLP) 的資料，也就是說 Logs, Traces 以及 Metrics 都可以透過 uptrace 來收集   uptrace 引入的所謂的 system 的概念，類似於一個 namespace  只要 Span 裡面有某些特定的 attributes 出現，這個 Span 就會被歸類到某個 system 底下  以下圖來說，我有 funcs, db:postgresql 等等的 system      可參考 Semantic Attributes 以及 Grouping similar spans and events together       相同 system 的 Span 就會顯示在這裡  而根據不同的 Trace 它也有做區隔, 以不同的 Trace id 區分     然後所有相關的 Trace 資料都可以看到這邊  可以看到 parent span 以及 child span 之間的關係以及他們的耗時  甚至，因為 Log 也有連結在一起，所以你也可以看到相對應的 log message       Uptrace UI  uptrace 的設定檔裡面有一個比較值得提及的部份，一個 uptrace 系統分為多個 project  你可以設定很多個專案同時監控(對於 microservice 來說還是設定一個 project 比較好)   uptrace 本人會需要接資料，基本的 authentication 是必要的  token 的資料在設定 DSN 的時候會需要用到   然後就是接資料的 port 也是透過 yaml 設定的  預設情況下 uptrace 會接收 gRPC 以及 HTTP 的資料，分別是 14317 以及 14318      14318 port 同時也是 uptrace UI 的位置    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 projects:   # Conventionally, the first project is used to monitor Uptrace itself.   - id: 1     name: Uptrace     # Token grants write access to the project. Keep a secret.     token: project1_secret_token     pinned_attrs:       - service       - host_name       - deployment_environment     # Group spans by deployment.environment attribute.     group_by_env: false     # Group funcs spans by service.name attribute.     group_funcs_by_service: false     # Enable prom_compat if you want to use the project as a Prometheus datasource in Grafana.     prom_compat: true    # Other projects can be used to monitor your applications.   # To monitor micro-services or multiple related services, use a single project.   - id: 2     name: My project     token: my_project_secret_token     # Group funcs spans by service.name attribute.     group_funcs_by_service: true     prom_compat: true  ## ## Addresses on which Uptrace receives gRPC and HTTP requests. ## listen:   # OTLP/gRPC API.   grpc:     addr: ':14317'    # OTLP/HTTP API and Uptrace API with UI.   http:     addr: ':14318'    # tls:   #   cert_file: config/tls/uptrace.crt   #   key_file: config/tls/uptrace.key   Golang Example  完整的程式碼可以參考 ambersun1234/blog-labs/uptrace-slog   Integrate with Slog   1 2 3 4 5 6 7 8 9 10 11 12 import (     \"log/slog\"      \"go.opentelemetry.io/contrib/bridges/otelslog\"     slogmulti \"github.com/samber/slog-multi\" )  otelLogger := otelslog.NewHandler(\"mytrace\") consoleLogger := slog.NewTextHandler(os.Stdout, &amp;slog.HandlerOptions{AddSource: true}) logger := slog.New(slogmulti.Fanout(otelLogger, consoleLogger))  logger.InfoContext(ctx, \"inserting data\", slog.Any(\"req\", req))   OpenTelemetry 有針對不同的 logging package 提供橋接的功能  以本例來說是 slog, 但是 zap/logger 以及 logrus 也都有相對應的橋接   不過很快我就遇到一個問題是，otelslog 本身的實作沒辦法也同步到 console  所以這裡我使用了 slog-multi 將不同的 handler 串接在一起，然後 fanout 到不同的 handler 上面      而 Golang 官方也有人提及相關的 issue, proposal: log/slog: add multiple handlers support for logger    真正 logging 的部份就跟原本 slog 一樣，只是你需要將 context 傳入 logger 裡面   Uptrace Collector  基本上 uptrace 簡化了許多的步驟，初始化的時候有幾點要注意     你需要指定你要把 telemetry data 送到哪裡            基本上是要送到 uptrace 的 collector 上面(你可以在 collector 預處理你的資料)，但是你也可以不過 collection 直接到 observability backend。這裡我是直接送到 uptrace           指定 Resources            Resource 主要的目的是為了帶入一些 metadata，比如說你的服務名稱，服務版本等等       之後如果你發現某個服務有問題，你可以透過這些 metadata 來找到問題所在(比如說哪個 container 有問題)              可以參考 Start sending data    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import (     \"context\"      sdkTrace \"go.opentelemetry.io/otel/sdk/trace\"     \"go.opentelemetry.io/otel/attribute\"     \"github.com/uptrace/uptrace-go/uptrace\" )  ctx := context.Background() uptrace.ConfigureOpentelemetry(     uptrace.WithDSN(\"http://my_project_secret_token@localhost:14317/2\"),     uptrace.WithTracingEnabled(true),     uptrace.WithLoggingEnabled(true),     uptrace.WithTraceSampler(sdkTrace.AlwaysSample()),     uptrace.WithResourceAttributes(         attribute.String(\"service.name\", \"mytrace-service\"),     ), ) defer uptrace.Shutdown(ctx)   Gin with Span  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import (     \"go.opentelemetry.io/contrib/instrumentation/github.com/gin-gonic/gin/otelgin\"     \"github.com/uptrace/opentelemetry-go-extra/otelgorm\"     \"go.opentelemetry.io/otel\"     \"go.opentelemetry.io/otel/trace\" )  var tracer = otel.Tracer(\"myapp\")  router.Use(otelgin.Middleware(\"server\")) router.POST(\"/\", func(c *gin.Context) {     ctx := c.Request.Context()     ctx, span := tracer.Start(ctx, \"kv-service\", trace.WithSpanKind(trace.SpanKindServer))     defer span.End() })      注意到，生一個新的 Span 不是 trace.SpanFromContext(ctx)  這個是從 context 中取得目前的 Span，而不是生一個新的 Span    所有的 Span 都是從 tracer 的 instance 分出來的  每個 Span 會自動去管理 parent-child 的關係，因此你不需要特別指定誰是 root span  生命週期全部都是紀錄在 context 裡面，所以任何需要被追蹤紀錄的地方都需要透過 context 傳遞   針對 gin-gonic/gin，他有提供一個 middleware 負責基本 http 的 span  你仔細進去看它其實也是做了一樣的事情，一樣建一個新的 Span 並且將它放入 context 中(額外多一些 attributes)   HTTP Context Propagation with Gin  在 Context Propagation 裡面我們提到過，你可以透過 http header 來傳遞 context  實際上你是透過 propagator 來做這件事情的   otelgin middleware 裡面他是使用 text map propagator 從 http header 裡面取得 trace 的 id，然後儲存在 context 裡面供後續使用   1 2 3 4 5 6 7 import (     \"go.opentelemetry.io/otel\"     \"go.opentelemetry.io/otel/propagation\" )  propagator := otel.GetTextMapPropagator() ctx := propagator.Extract(savedCtx, propagation.HeaderCarrier(c.Request.Header))   我想要來實驗一下，如果我模擬 microservice 建立兩個服務，client 先打到 proxy server，然後再轉發到另一個 server  然後在 context 裡面塞 Trace 相關的資料，他是不是能夠正確的追蹤到這個 request 呢？   實作起來也很簡單，我們只要做相反的事情就好了  也就是把將 context 塞入 http header 裡面，像這樣   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 router.POST(\"/\", func(c *gin.Context) {     ctx := c.Request.Context()     ctx, span := tracer.Start(ctx, \"gateway-server\", trace.WithSpanKind(trace.SpanKindServer))     defer span.End()      data := []byte(`{\"key\":\"xyz\",\"value\":\"xyz\"}`)      req, err := http.NewRequest( \"POST\", \"http://localhost:9999/\", bytes.NewReader(data))     if err != nil {         logger.ErrorContext(ctx, \"failed to create request\", slog.Any(\"error\", err))         c.JSON(http.StatusInternalServerError, gin.H{\"error\": err.Error()})         return     }     req.Header.Set(\"Content-Type\", \"application/json\")     otel.GetTextMapPropagator().Inject(ctx, propagation.HeaderCarrier(req.Header))     logger.InfoContext(ctx, \"sending request\", slog.Any(\"header\", req.Header))      client := &amp;http.Client{}     resp, err := client.Do(req)     if err != nil {         logger.ErrorContext(ctx, \"failed to send request\", slog.Any(\"error\", err))         c.JSON(http.StatusInternalServerError, gin.H{\"error\": err.Error()})         return     }      c.JSON(http.StatusOK, gin.H{\"status\": resp.Status}) })   這邊我建立了一個 http proxy server, 使用者會先發送 request 到這個 server  然後我在幫你轉發到另一個 server 上面  途中幫你將 context 塞入 http header 裡面   重點在 otel.GetTextMapPropagator().Inject(ctx, propagation.HeaderCarrier(req.Header))  就是這行程式碼將 context 塞入 http header 裡面(具體來說會是 traceparent 這個 header)  並且在擋在後面的 server 也必須要使用 propagator 將 header 取出來放進去 context 裡面才會動      如果不想手動，其實官方也有 otelhttp 可以使用       然後在介面上它就會歸類在同一個 Trace 底下了    如此一來你就可以做到跨服務的追蹤了   References     初嚐 OpenTelemetry Go Log Beta   remychantenay/slog-otel   What is OpenTelemetry?   Introduction to Performance Monitoring Metrics   OpenTelemetry API vs SDK   Monitoring Flask with OpenTelemetry and Uptrace   Context propagation   Propagating trace context   OpenTelemetry Go Tracing API   Resources  ","categories": ["kubernetes"],
        "tags": ["telemetry","opentelemetry","trace","log","metric","uptrace","slog","golang","gin","slog-multi","otelgin","distributed trace","context propagation","baggage","span","span attribute","span link","trace id","trace parent","sampling","textmappropagator"],
        "url": "/kubernetes/kubernetes-monitor/",
        "teaser": null
      },{
        "title": "從 0 認識 Blockchain - 一手掌握 Web3 資料，以 Uniswap 為例",
        "excerpt":"How to Retrieve Blockchain Data  區塊鏈上的所有資料都是公開透明的，你可以透過第三方服務如 Alchemy、Infura 來存取區塊鏈資料  或者是，像我聽過有些公司是自行架設節點，然後修改裡面的程式碼做 cache 之類的  以我自己來說，之前碰過類似的需求，我是使用 Alchemy 監聽特定某個 contract 的事件，然後再做後處理   對於更進階的資料需求，像是查詢區段資料等等的需求  Alchemy 雖然有提供一些 API 但是它並沒有辦法滿足所有需求  而且自行架設節點也不是一個太好的方案，問題在於，你需要     自行維護硬體設備   違反區塊鏈的去中心化精神   查詢資料需要花時間同步   The Graph 提出了一個解決方案，讓開發者可以更方便的存取區塊鏈資料   The Graph  具體來說它跟我做的事情很像，它直接把 監聽特定 contract 的事件，然後做後處理 這件事情做掉  這樣你的應用程式就不需要自己去 handle 這些事情，你的 application 邏輯可以被簡化   The Graph 是一個分散式的協議，它提供了一個簡單且快速的方式來存取區塊鏈資料  這些資料，並不是原始的區塊鏈資料(raw data)，而是經過處理過的資料(aggregated data)      注意到 The Graph 並不是 Blockchain，雖然它使用了很多區塊鏈技術，但它並不是一個區塊鏈    The Graph 旨在提供快速的資料查詢，比如說你需要 日交易量, 日交易互數 這種聚合資料  它會直接儲存這些 “已經計算過得資料”，也因此這樣的查詢速度會比較快   Subgraph  監聽特定的事件，聚合相關的資料這件事情  你需要清楚的定義在所謂的 Subgraph 中，這樣 The Graph 才能幫你處理這些事情  因為你需要讓它知道，你要監聽哪些事件，然後要如何處理這些事件   當你把 Subgraph 定義完成並成功的部署到 The Graph 上之後，你就可以透過 GraphQL 查詢你所需要的資料了      你可以在 Graph Explorer 上找到各式各樣的 Subgraph    Network Participants       ref: Tokenomics of The Graph Network    為了建構如此龐大的資料庫，The Graph 有幾個不同的角色，他們各司其職，共同維護整個網路  Indexer 負責產生資料供快速查詢  為了能夠讓 Indexer 知道有哪些 Subgraph 急需被處理，Curator 會質押特定的 Subgraph 來告知 Indexer  Delegator 則是可以幫助 Indexer 進行質押，類似於一個投資者的角色，贊助該 Indexer 使其可以用這些金錢擴充算力並最終括容整個網路   Indexer  你的資料會需要某個人幫你處理，這個人就是 Indexer  Indexer 需要質押 100k 的 GRT(The Graph 的代幣)   Indexer 會根據 Subgraph 的定義，去監聽區塊鏈上特定的事件，然後把這些資料處理好  儲存在 PostgreSQL 資料庫裡面，然後當 request 進來的時候，就可以直接從資料庫裡面查詢   那… 他要如何決定要處理哪些 Subgraph 呢？  一個常見的參考點是，看這個 Subgraph 有多少人在使用，有多少人在查詢(看歷史查詢數量)  當這個 Subgraph 被使用的越多，你越可以在上面分到一杯羹是吧  或者是你可以看看這個 Subgraph 的質押量，質押量越高，代表需求越大，你就可以分到更多的獎勵      可參考 Uniswap-V3/0xddaaed8b88ac0ccfdbfabdceba1c619391760f7f    Proof of Indexing(POI)  跟傳統的區塊鏈很像，The Graph 也有一個類似的機制，叫做 Proof of Indexing(POI)  基本上就是一個證明，證明該 block 是由該 Indexer 處理的   Curator  基本上，因為每個人都可以自己定義 Subgraph  你可以作為一個 Curator，質押一些 GRT 來告知 Indexer，你想要這個 Subgraph 被處理  通常，部署 Subgraph 的開發者就是第一個 Curator, 因為你會希望你的 Subgraph 被處理嘛      可參考 Uniswap-V3/0xddaaed8b88ac0ccfdbfabdceba1c619391760f7f    Delegator  當 Indexer 或 Curator 基本上你必須要擁有一定的知識，甚至是硬體設備  Delegator 這個角色你不需要這麼多的知識，你只需要質押一些 GRT 給 Indexer  當然，投資給 Indexer 它也會回報給你   Indexer 會將賺到的收益，以一定的比例分給 Delegator(通常是 9-12%)  這裡的收益指的是，Indexer 透過 query fee 賺到的錢   這裡的概念就跟股票很像了，如果股息發的越多越高，投資者就會比較願意投資這個 Indexer  那 Indexer 賺到錢，也會回饋給 Delegator   Uniswap V3 Subgraph  在 Graph Explorer 上面你可以找到很多各式各樣的 subgraph  舉例來說，Uniswap/v3-subgraph 就是其中一個例子  The Graph 上面所有的 Subgraph 都是公開且透明的，你可以找到你需要的 Subgraph 並查詢你所需要的資料   Subgraph Components  Subgraph 需要包含至少以下部份     schema.graphql 定義了你的資料模型   subgraph.yaml 定義了你的 Subgraph 的基本資訊   mappings 資料處理的邏輯   Data Store  aggregate 過的資料可以透過 GraphQL 查詢  能夠查詢的資料，他的資料模型的定義是透過 schema.graphql 來定義的   在 Assemblyscript Mapping 裡面你會使用到這些資料模型  把它儲存到資料庫等等的操作  這個定義沒辦法直接被 Assemblyscript 使用，所以這部份是透過 codegen 來產生的      在 package.json 裡面可以看到 codegen 的指令    以下就是 GraphQL 的資料模型定義，可參考 schema.graphql  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 type Factory @entity {   # factory address   id: ID!    # amount of pools created   poolCount: BigInt!    # amoutn of transactions all time   txCount: BigInt!    # total volume all time in derived USD   totalVolumeUSD: BigDecimal!    # total volume all time in derived ETH   totalVolumeETH: BigDecimal!    # total swap fees all time in USD   totalFeesUSD: BigDecimal!    # total swap fees all time in USD   totalFeesETH: BigDecimal!    # all volume even through less reliable USD values   untrackedVolumeUSD: BigDecimal!    # TVL derived in USD   totalValueLockedUSD: BigDecimal!    # TVL derived in ETH   totalValueLockedETH: BigDecimal!    # TVL derived in USD untracked   totalValueLockedUSDUntracked: BigDecimal!    # TVL derived in ETH untracked   totalValueLockedETHUntracked: BigDecimal!    # current owner of the factory   owner: ID! }   Subgraph Definition  所有的 Subgraph 都需要一個 subgraph.yaml 來定義，像下面這樣   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 specVersion: 0.0.4 description: Uniswap is a decentralized protocol for automated token exchange on Ethereum. repository: https://github.com/Uniswap/uniswap-v3-subgraph schema:   file: ./schema.graphql features:   - nonFatalErrors   - grafting dataSources:   - kind: ethereum/contract     name: Factory     network: sepolia     source:       address: \"0x0227628f3F023bb0B980b67D528571c95c6DaC1c\"       abi: Factory       startBlock: 3518270     mapping:       kind: ethereum/events       apiVersion: 0.0.7       language: wasm/assemblyscript       file: ./src/mappings/factory.ts       entities:         - Pool         - Token       abis:         - name: Factory           file: ./abis/factory.json         - name: ERC20           file: ./abis/ERC20.json         - name: ERC20SymbolBytes           file: ./abis/ERC20SymbolBytes.json         - name: ERC20NameBytes           file: ./abis/ERC20NameBytes.json         - name: Pool           file: ./abis/pool.json       eventHandlers:         - event: PoolCreated(indexed address,indexed address,indexed uint24,int24,address)           handler: handlePoolCreated   Subgraph 本身是從鏈上 aggregate 資料，所以你需要定義所謂的 dataSources  像上述的例子，就是監聽 sepolia/0x022 的 contract  然後這裡你可以設定說你要從哪時候開始處理資料(i.e. startBlock 以及 endBlock)   針對不同的事件，你可以定義不同的 Handlers  這邊只有監聽了 PoolCreated(indexed address,indexed address,indexed uint24,int24,address) 這個事件  然後它會使用 ./src/mapping/factory.ts 的 handlePoolCreated 來處理這個事件   注意到這邊你會需要定義若干個 ABI(Application Binary Interface)  這是因為要讓 Subgraph 知道如何解析從鏈上取得的資料   AssemblyScript Mapping and Handlers  在 Subgraph Definition 你可以設定多種不同的 handler  總共會有三種不同的 handler，他們被 trigger 的順序會是     Event Handlers   Call Handlers   Block Handlers   因為一個 block 裡面包含不同的 transaction, 所以 Block Handlers 會是最後才被執行的  而 Event Handlers 則是在 Call Handlers 之前被執行  至於相同種類但不同 handler 則會依照在 subgraph.yaml 裡面的定義順序來執行     Handler 的實作就是看到底要如何聚合這些資料  你也可以直接把收到的數值直接塞進去資料庫裡面，也不是不行   對於 Handler 的實作，有兩點需要符合     function 名字必須與定義一致，並且要是 export 的   所有 Handler 的參數必須接受一個 event 參數，它可以是任何形式的 event(比如說 ethereum.Event, ethereum.Call)   1 2 3 4 5 6 7 8 9 10 11 12 export function loadTransaction(event: ethereum.Event): Transaction {   let transaction = Transaction.load(event.transaction.hash.toHexString())   if (transaction === null) {     transaction = new Transaction(event.transaction.hash.toHexString())   }   transaction.blockNumber = event.block.number   transaction.timestamp = event.block.timestamp   transaction.gasUsed = BigInt.zero() //needs to be moved to transaction receipt   transaction.gasPrice = event.transaction.gasPrice   transaction.save()   return transaction as Transaction }   以上是 Uniswap 裡面的一個 transaction helper function(定義於 src/utils/index.ts)  你可以看到它其實就是把 event 裡面的資料塞進去資料庫裡面  注意到這裡的參數是 ethereum.Event，這是因為 caller 傳進來的有可能是 child class  這裡為了能夠兼容不同的 event，所以直接使用 parent class   而以上實作，就是一個簡單的 Assemblyscript Mapping  啥？ 它不是 TypeScript 嗎？  Assemblyscript 是 TypeScript 的子集，但是它是被編譯成 WebAssembly 的  編成 WASM 的好處是執行速度可以快到飛起，同時它也繼承了相同 TypeScript 的語法  所以在 subgraph.yaml 裡面你會發現他是定義成 wasm/assemblyscript 這個東西   References     他們說這是Web3的Google？   uniswap/v3-subgraph   The Graph   Tokenomics of The Graph Network   Writing AssemblyScript Mappings  ","categories": ["blockchain"],
        "tags": ["uniswap","the-graph","web3","alchemy","infura","indexer","curator","delegator","subgraph","assemblyscript","ethereum","typescript","graphql","aggregate"],
        "url": "/blockchain/blockchain-graph/",
        "teaser": null
      },{
        "title": "Kubernetes 從零開始 - Self Healing 是如何運作的",
        "excerpt":"Application Status  在 Kubernetes 中，一個應用程式的狀態是非常重要的  當一個應用程式不可用，造成的損失是巨大的   應用程式可能因為多種原因而失效，比如說程式碼的 bug, 硬體故障等等  當應用程式失效的時候，我們希望可以透過某種方式知道它目前的狀態  進而採取必要的措施來修復它   How Kubernetes Knows Application is Down  Kubernetes 有一個強大的機制是可以自動檢測並修復應用程式，這個機制稱之為 Self-Healing  但 Kubernetes 沒辦法主動知道你的應用程式狀態，因為每個 app 的狀態都不盡相同  它沒辦法用一個通用的方式來判斷一個 app 是否正常運作   以 web application 來說，我們可以利用 HTTP request 作為驗證狀態的一種方式  比方說我可以設定一個 /healthz 的 endpoint，當 user 有辦法訪問以及他有回傳結果的時候  我們就可以認為這個 app 是正常運作的   那 /healthz 這個 endpoint 具體來說需要做什麼？  基本上，取決於不同的應用程式 對於正常運作的定義 是什麼  一般來說，只要回傳 200 OK 就可以了   Introduction to Probes        ref: Grain Probe Sampler Corn Sampler Broomcorn Sampler Reusable Grain Sampler    在 Kubernetes 中，我們可以透過 Probe 來檢測應用程式的狀態  如上圖，Probe 就是一個探測棒。因為整個應用程式的範圍太大了，你沒辦法完全驗證全部的狀態  透過玉米探測棒，將它插進去存放穀物的糧倉桶內，透過檢視部份狀態來推算整體穀物的好壞   如上面我們討論的部份  我們可以設計 /healthz 作為簡易探測棒，用以推測整個應用程式的狀態   Types of Probes  探針擁有不同的類型，針對不同的場合使用   Liveness Probe  我們最常用的就是 Liveness Probe  判斷一個應用程式是不是 還活著，如果 liveness 失敗 Kubernetes 會嘗試重新啟動它   注意到如果你沒有將它設定好，你可能會遇到不可預期的行為  比方說，我之前在 debug 的時候會發現 pod 會沒有原因的重啟，原因在於 Liveness Probe 的 endpoint 寫錯  而這個重啟看起來是有規律的  看 log 你是看不出來有什麼問題，但是在 $ kubectl describe pod 裡面就可以看到詳細原因   Readiness Probe  與 Liveness Probe 不同，Readiness Probe 是用來判斷一個應用程式是否 ready  應用程式可能會有一些 bootstrap 的 task 需要執行  比如說我最近做的專案中，在 service 啟動之後我必須要檢查 在上一次退出，執行到一半的任務  如果有，必須要將它完成   針對這種情況，如果它必須要等待才能開始服務，那麼 Readiness Probe 就是一個很好的選擇   Startup Probe  有時候 container 就是會啟的很慢，如果有設定 Liveness Probe 或 Readiness Probe，Kubernetes 會一直重啟  這並不是我們想要的   Startup Probe 可以確保 container 啟動之後再去檢查 liveness 或 readiness  就可以避免服務一直被重啟的問題，確保服務可以正常運作      注意到 Startup Probe 僅僅只是 “暫停” Liveness/Readiness Probe 的檢查直到啟動成功                     Types of Probe       Goal       Periodically Check                       Liveness Probe       是否存活       :heavy_check_mark:                 Readiness Probe       是否準備完成       :heavy_check_mark:                 Startup Probe       是否啟動       :x:           Probe Besides HTTP  /healthz 對於基本的 web application 來說是足夠的  而且官方也說明，針對 Liveness Probe 以及 Readiness Probe 這種探針  撰寫 low cost 的 endpoint 是比較好的 practice  兩種 probe 可以共用同一個 endpoint，不過設計上記得 Readiness 他的 failureThreshold 需要比較高(避免被直接 kill 掉)   但是針對比如說 gRPC 這種非 HTTP 的應用程式，你就沒辦法使用 HTTP request 來檢測了  因此，Kubernetes 也有提供 gRPC, TCP 以及 EXEC 的探測方式   gRPC Probe  gRPC 的 probe, 與 HTTP probe 類似，你一樣需要定義一個 endpoint 來檢測(可以參考 GRPC Health Checking Protocol)  他的定義會是  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 syntax = \"proto3\";  package grpc.health.v1;  message HealthCheckRequest {   string service = 1; }  message HealthCheckResponse {   enum ServingStatus {     UNKNOWN = 0;     SERVING = 1;     NOT_SERVING = 2;     SERVICE_UNKNOWN = 3;  // Used only by the Watch method.   }   ServingStatus status = 1; }  service Health {   rpc Check(HealthCheckRequest) returns (HealthCheckResponse);    rpc Watch(HealthCheckRequest) returns (stream HealthCheckResponse); }      有關 gRPC 的介紹可以參考 網頁程式設計三兩事 - gRPC 與 JSON-RPC | Shawn Hsu    注意到這個格式必須嚴格遵守，也就是說你沒辦法客製化 endpoint, request, response 等等的  不過它仍然保持著一定的彈性，比如說你可以設定 request 裡面的 service 欄位來檢測不同的服務  Kubernetes 是建議你將 service name 以及 probe type 結合  就會變成例如說 myservice-liveness 這樣的名稱   然後你在實作的時候就可以解析這個名稱，分別回應相對應的狀態  好處在於說你可以僅使用一個 port 就可以針對不同的服務進行探測   gRPC 判斷好壞的方式是透過 ServingStatus 來判斷  只有 SERVING 才會被視為成功   TCP Probe  針對 TCP 的 probe, Kubernetes 會嘗試在該 port 開一個 socket 連線，如果可以就代表成功   1 2 3 4 5 livenessProbe:   tcpSocket:     port: 8080   initialDelaySeconds: 15   periodSeconds: 10   Exec Probe  你也可以執行一段指令，然後看他的回傳值來判定   1 2 3 4 5 6 7 livenessProbe:   exec:     command:     - cat     - /tmp/healthy   initialDelaySeconds: 5   periodSeconds: 5   比如說你可以將狀態寫進去 /tmp/healthy，然後透過 cat 來讀取  如果回傳 0 就代表成功，否則就是失敗   執行 exec probe 需要特別注意的是，他的每次執行都是 fork process 來執行的  如果定期執行的間隔過短會額外增加系統負擔   How to Write and Define a Probe  前面我們說過，你可以簡單定義一個 /healthz endpoint 來作為探測棒  但你有沒有想過，Kubernetes 是如何判斷這個 endpoint 是成功還是失敗的？   根據 Liveness/Readiness Probes should treat any 2XX status as healthy 提到，所有 2XX 的 status code 都會被視為成功  也就是不一定要是 200 OK, 204 No Content 也是可以的   不過有意思的是，3XX 系列也是會被視為成功的  根據 kubernetes/pkg/probe/http/http.go  1 2 3 4 5 6 7 8 if res.StatusCode &gt;= http.StatusOK &amp;&amp; res.StatusCode &lt; http.StatusBadRequest {     if res.StatusCode &gt;= http.StatusMultipleChoices { // Redirect         klog.V(4).Infof(\"Probe terminated redirects for %s, Response: %v\", url.String(), *res)         return probe.Warning, fmt.Sprintf(\"Probe terminated redirects, Response body: %v\", body), nil     }     klog.V(4).Infof(\"Probe succeeded for %s, Response: %v\", url.String(), *res)     return probe.Success, body, nil }  可以看到只是會 warning 而已，問題不大   然後你就可以在，比方說 deployment 裡面定義  1 2 3 4 5 6 7 8 livenessProbe:   httpGet:     path: /healthz     port: 8081   initialDelaySeconds: 3   periodSeconds: 3   timeoutSeconds: 1   failureThreshold: 3   Can Probe Detect Hardware Failure?  是可以的，因為硬體故障通常會導致應用程式無法正常運作  即使是網路故障導致節點沒有回應，Kubernetes 依然會自動修復，將 application schedule 到可以正常運作的節點上   Should you Define a Probe?  原則上還是會建議讓 Kubernetes 處理自動修復的部份  盡量減少人為地介入可以避免一些不必要的問題   那這就會引到一個問題，如果我沒有定義 Probe 會怎樣？  其實也不會怎麼樣，Kubernetes 並不會因此就瘋狂重啟你的 pod  他的預設就都會是 success   What will Happen if Probe Failed?                  Probe Type       Action                       Liveness Probe       根據 RestartPolicy 決定是否重新啟動 container                 Readiness Probe       把流量導到其他 pod                 Startup Probe       根據 RestartPolicy 決定是否重新啟動 container           Liveness Probe Failed  當一個 container 已經不健康的時候(使用 Liveness Probe 來判斷)  kubelet 就會 根據 RestartPolicy, 重新啟動這個 container(or not)   Readiness Probe Failed  那如果 Readiness Probe 失敗呢？  Readiness 表示是否準備好了對吧，對於 Kubernetes 來說如果還沒準備好它就不能被 route 到上面對吧  也就是說，Load Balancer 並不會將流量導到這個 pod   但假設你需要把 pod 關掉，停掉所有的 incoming traffic  你不見得會需要 readiness probe，因為當它被停止的時候狀態會是 unready  注意到這時候 pod 還是在運行的狀態，只是不會被 route 到上面(將 pod ip 從 service 的 endpoint 中移除)  pod 會等到 container 內部完全停止之後才會被刪除   Startup Probe Failed  會根據 RestartPolicy 決定是否重新啟動 container  Startup Probe 在成功之前，Liveness Probe 以及 Readiness Probe 都不會開始執行   Who does the Self Healing?  Kubelet  撇除 replica 這種需要 Kubernetes Controller 介入的情況，Kubernetes 本身是依靠 Kubelet 來做到自動修復的  kubelet 可以把他想像成節點的管理者，負責管理比如說，節點的狀態，pod 的狀態等等的   Kubernetes Controller  有了 Probe 其實還不夠，因為它只是通知你說 pod 掛掉之類的  針對 deployment 這種需要保持一定數量的 pod 運作的情況，我們需要一個 controller 來幫我們自動修復   早期 Kubernetes 是透過 ReplicationController 來做到動態調整 pod 數量這件事情  將 Probe 與 Controller 相結合，就可以做到自動修復的功能      有關 Controller 可以參考 Kubernetes 從零開始 - 從自幹 Controller 到理解狀態管理 | Shawn Hsu    Requests and Limits  另一種跟 self healing 稍微有關的機制是 Requests and Limits  Requests and Limits 是 Kubernetes 用來管理資源分配的機制  你可以設定一個 Pod 能 最少應該要使用多少 的 CPU, Memory 等等的資源  這部份會定義在 Pod 的 spec.containers[].resources.requests 中   kube-scheduler 會根據 requests 來決定要分配到哪一個 node 上  當然它也可以用超過 requests 的資源，但是這樣可能會導致其他 pod 沒有足夠的資源可以運作  所以 spec.containers[].resources.limits 就是用來限制一個 Pod 最多可以使用的資源   那問題來了，如果 pod 的資源使用量超過 limits 會怎樣？     超過 CPU limit: 會被降速(Throttling)   超過 Memory limit: 有可能會被 terminate   兩個的機制是不同的，對於 CPU 來說是 hard limit，對於 Memory 來說是 soft limit  如果你記憶體用量超出 limits 的設定，不一定會被馬上 kill 掉，除非 kernel 遇到 memory pressure   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 apiVersion: v1 kind: Pod metadata:   name: frontend spec:   containers:   - name: app     image: images.my-company.example/app:v4     resources:       requests:         memory: \"64Mi\"         cpu: \"250m\"       limits:         memory: \"128Mi\"         cpu: \"500m\"   References     ReplicationController   Liveness, Readiness, and Startup Probes   Configure Liveness, Readiness and Startup Probes   Liveness/Readiness Probes should treat any 2XX status as healthy   Container probes   Resource Management for Pods and Containers  ","categories": ["kubernetes"],
        "tags": ["probe","liveness probe","readiness probe","startup probe","gRPC","TCP","EXEC","health check","healthz","controller","operator","self healing","self-healing","kubelet","k8s requests","k8s limits"],
        "url": "/kubernetes/kubernetes-self-healing/",
        "teaser": null
      },{
        "title": "DevOps - 詳解 Mock 概念以及如何 Mock HTTP Request",
        "excerpt":"Test Double  雖然常常講要 mock 這個 mock 那個  不過人家的正式名稱是 Test double(測試替身)   Type     Test Double 以功能性分為兩派 State Verification 以及 Behaviour Verification   Verification Type  State Verification  狀態，指的是系統內的狀態  軟體工程裡系統的狀態通常是 variable, object properties 等等   通俗點說，你的變數狀態在經過一系列的操作之後，必須要符合某種狀態  比如說一個計算器，當前數值為 10  當我進行加法 +1 的時候，它應該要變成 11  這就是狀態驗證   而 Stub 類型多以模擬狀態(資料)為主   Behaviour Verification  這裡的行為就指的是，你的運行過程，狀態遷移的 過程 合不合理  像是他有沒有跟對的 component 互動   符合這個類型的，歸類在 Mock 類型裡面，以模擬行為為主     Test Double 內部又分五個種類      Dummy            用於填充目標物件(i.e. 參數)，僅僅是為了不讓測試掛掉的作用           Fake Object            較為 簡單版本 的實作       比如說用 in-memory database 取代原本的 MySQL 之類的           Stub            根據不同的輸入，給定相對應的輸出           Spy(Partial Mock)            原本的定義是用以監看，各種被呼叫的實作的各項數據(被 call 了幾次, 誰被 call) :arrow_right: 跟間諜一樣       有時候也指 Partial Mock, 不同的是，只有實作中的 部份內容 被替代           Mock            跟 Stub 一樣，此外還包含了 Behaviour Verification           整理成表格的話就如下                  Object Type       Have Implementation       Verification Type                       Dummy       :x:       State Verification                 Fake Object       :heavy_check_mark:       State Verification or Behaviour Verification                 Stub       :x:       State Verification                 Spy       :heavy_check_mark:       Behaviour Verification                 Mock       :heavy_check_mark:       State Verification or Behaviour Verification              Dummy 為什麼可以做狀態驗證？  它沒有在 check 輸出阿？  事實上狀態驗證也包含了驗證參數數量這種，即使 Dummy 只有填充物件的用途，它仍然可以做驗證       Fake Object 可以驗證狀態或行為的原因在於  他是簡單版本的實作，同時因為他是實作，代表它能驗證輸出是否符合預期  更重要的是實作本身可以驗證行為(i.e. 確保執行順序像是 A :arrow_right: B :arrow_right: C)    Manually Create Mock Implementation  那你要怎麼建立 mock 的實作呢？  通常來講是建議使用第三方的 library 如 mockery 自動產生  但你能不能自己寫呢？當然可以 但不建議      考慮以上的關係圖，要建立 mock 的實作，你只要將所有的 interface 實作一遍就好  但是這個實作是需要非常小心的   什麼意思？  mock 的實作 應該最小化商業邏輯，甚至不應該有  舉例來說你要實作一個 function GetUser(userID string): User 的 mock  他不應該真的去查詢什麼東西，確認他存在/不存在 再回傳  這樣的實作是不對的，因為這樣的實作會讓你的測試變得複雜，且不容易維護   取而代之的是，他要回應一個固定的數值  或者是 根據不同的輸入，給定相對應的輸出  在調用的時候就會類似像這樣   1 2 3 4 5 userMock.On(\"GetUser\", \"myuserid\").Return(User{     ID: \"myuserid\",     Name: \"myname\",     Age: 18, })   當參數為 myuserid 的時候，回傳一個固定的 User 物件  不過如今都有現代化的 testing framework 提供了這樣的功能  就不需要自己寫了   Mocking HTTP Request  像我最近有遇到一個狀況是，我的 service 需要去呼叫外部的 API  而這個 service 是屬於比較低階的實作，它會直接用 http.Get 來呼叫  針對這種狀況，你就很難的去把它 mock 掉   這個 function 你當然也可以不去寫測試，因為它已經是貼近底層的實作了  就像我個人針對資料庫的實作，針對 unit test 通常是不寫的，我會在 integration test 驗證這部份      有關 integration test 可以參考 DevOps - 整合測試 Integration Test | Shawn Hsu    我們有說過， integration 這裡你就必須要連到真實的環境  跑 container 不是問題，問題是有些服務可能是付費的，也沒有提供測試環境  等於說你必須要自己架一個環境起來      你當然可以連線到真正的 production 環境，但你要想，有些服務是只有在內網的  如果要上到 CI/CD，這些服務可能是無法連線的    Why Mock HTTP Request?  你可能會好奇，為什麼我需要跟第三方的 API 一起測試？  一個常見的原因是，我們必須要確保我們的實作跟第三方的 API 有正確的串接  換言之，如果第三方的 API 有變動，我們的實作也必須要跟著變動  透過持續測試你就可以確保這一點   httptest  內建的函式庫 net/http/httptest 提供了一個 httptest.Server  將它執行起來，你就可以模擬一個 server 了  其概念就等同於我們用 docker 跑 PostgreSQL 一樣，連線設定好，就可以執行了   1 2 3 4 ts := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {     fmt.Fprintln(w, \"Hello, client\") })) defer ts.Close()      每個 test server 都有一個 URL，你可以透過這個 URL 來連線  這個 URL 是隨機的，你可以透過 ts.URL 來取得    上述是一個簡單版本的 test server  你可以看到它其實沒辦法針對不同的 route 做不同的處理  這時候你就需要用到 http.ServeMux 來處理   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 package main  import (     \"net/http\"     \"fmt\"     \"net/http/httptest\" )  func handler1(w http.ResponseWriter, r *http.Request) {     fmt.Fprintf(w, \"Hello from handler1\\n\") }  func handler2(w http.ResponseWriter, r *http.Request) {     fmt.Fprintf(w, \"Hello from handler2\\n\") }  func main() {     mux := http.NewServeMux()     mux.HandleFunc(\"/route1\", handler1)     mux.HandleFunc(\"/route2\", handler2)      s := httptest.NewServer(mux)     defer s.Close()     fmt.Println(s.URL)     select {} }   建立好 test server 以後，它會一直跑，不需要特定開一個 goroutine  然後你就可以透過 http.Get 之類的方法來連線了   你的程式碼裡面，API call 理論上應該是動態的組字串  所以你可以自己設定特定的 route, 特定的 host, 來模擬不同的狀況  那麼這樣你就有辦法撰寫測試了   References     mocking outbound http requests in go: you’re (probably) doing it wrong   Test Double（2）：五種替身簡介   https://go.dev/src/net/http/httptest/example_test.go  ","categories": ["devops"],
        "tags": ["mock","test double","dummy","fake object","stub","spy","mock","state verification","behaviour verification","mockery","golang","httptest","http request","mux"],
        "url": "/devops/devops-mock/",
        "teaser": null
      },{
        "title": "Kubernetes 從零開始 - Informer 架構以及 Controller Pattern",
        "excerpt":"Kubernetes Object  Kubernetes object 並不是指 Pod 或者是 Deployment 這種 Resource  複習一下，Resource 是所有你可以透過 Kubernetes 使用的物件(操作 kubectl 或Kubernetes API)  而 object 是這些 Resource 的 instance      有關 Resource 可以參考 Kubernetes 從零開始 - 高階抽象 Workload Resources | Shawn Hsu    Kubernetes Object State  所謂的狀態是儲存在 Object 裡面的  object 的 spec 以及 status 分別代表了 desired state 以及 current state  spec 的內容可以透過一個特殊的檔案指定(稱為 manifest， 格式為 json 或 yaml)  並透過操作 kubectl 或 Kubernetes API 來建立 object      其中 status 是由 Controller 更新的，不是由我們手動指定的    考慮建立一個 nginx pod  並且查看他的 yaml 檔案  你就會看到類似以下的東西，這就是 object 的 status   可以看到 pod status 從最初的 PodScheduled 一直到 PodReadyToStartContainers  同時你也可以得知內部 container 的狀態  Controller 會根據這些狀態來管理 object   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 $ kubectl run mynginx --image=nginx $ kubectl get pods mynginx -o yaml status:   conditions:   - lastProbeTime: null     lastTransitionTime: \"2024-10-25T14:14:04Z\"     status: \"True\"     type: PodReadyToStartContainers   - lastProbeTime: null     lastTransitionTime: \"2024-10-25T14:13:44Z\"     status: \"True\"     type: Initialized   - lastProbeTime: null     lastTransitionTime: \"2024-10-25T14:14:04Z\"     status: \"True\"     type: Ready   - lastProbeTime: null     lastTransitionTime: \"2024-10-25T14:14:04Z\"     status: \"True\"     type: ContainersReady   - lastProbeTime: null     lastTransitionTime: \"2024-10-25T14:13:44Z\"     status: \"True\"     type: PodScheduled   containerStatuses:   - containerID: containerd://0ab6c6781723446e4869f1fd96b1d62b78a95dea327e45d276d010a5236f9ac8     image: docker.io/library/nginx:latest     imageID: docker.io/library/nginx@sha256:28402db69fec7c17e179ea87882667f1e054391138f77ffaf0c3eb388efc3ffb     lastState: {}     name: mynginx     ready: true     restartCount: 0     started: true     state:       running:         startedAt: \"2024-10-25T14:14:04Z\"   Imperative vs. Declarative     Imperative Management: 手把手教學，告訴 K8s 怎麼做   Declarative Management: 告訴 K8s 我們想要什麼，K8s 會幫我們達成   Kubernetes 大多數的操作都是透過 declarative 的方式  比如說你指定 deployment replica 的數量就是告訴 desired state  然後 Kubernetes 就會幫你達成這個狀態  注意到很可能 cluster 永遠沒辦法達到你想要的狀態，但它會盡力達到   當然你也可以透過 imperative 的方式來操作，但是這樣的話，你就要自己手動管理狀態了      ref: kubectl apply vs kubectl create?    Introduction to Kubernetes Controller and State Management  在 Kubernetes 從零開始 - 無痛初探 K8s! | Shawn Hsu 中有提到  K8s 是透過 Controller 管理 cluster 狀態的  我們告訴 K8s 我們想要的狀態，然後 K8s 會幫我們達成這個狀態(i.e. Declaration Management)   Controller 並不會直接操作 pod，而是透過 Kubernetes API Server 來管理 cluster 狀態  比如說，建立或刪除 pod，甚至是更新 object 的狀態(e.g. Finished)      不會直接操作 pod 但透過 API Server 建立/刪除？  舉例來說如果 replica 5 的 deployment 少了一個，那麼的確是要建立一個新的對吧？  間接的操作 pod ，是這個意思    Controller Pattern       ref: client-go under the hood    以上架構圖就是官方給的 Controller 的架構   具體來說，Reflector 會監聽 object 的變化  將 event 放到 delta-FIFO queue 裡面  並透過 Informer 處理這些 event，將 object 儲存到 thread-safe store 裡面  並且同時將 object 的 key dispatch 到 workqueue 裡面  然後你的 Custom Controller 從 workqueue 裡面拿到 reference 並使用 Lister 來取得完整的 object 資訊(查詢 thread-safe store)   為了一次只處理固定數量的 work，所有 component 之間的溝通都是透過 workqueue 來做的  有了 queue 擋在中間，可以保證同一個 item 不會同時被多個 Controller 處理      有關 queue 的討論，可以參考 資料庫 - 從 Apache Kafka 認識 Message Queue | Shawn Hsu    Reflector  Reflector 會聽 object 的變化並將變化的 object 放到 delta-FIFO queue 裡面  問題來了 他要怎麼聽這些所謂的變化呢？   Kubernetes API Server 提供了一個方式讓你監聽特定的 object，稱為 Watch  每個 Kubernetes Object 都會有一個 resourceVersion 的欄位，你可以把它想像成是一個 unique 的 ID  這個 ID 是對應到底層的 storage 的識別符號，這個 ID 會隨著 object 的變化而變化 所以不是固定值      Watch 會持續監聽，List 則不會    你可以用這個 ID 來監聽 object 的變化  有點類似 linked list 的概念，你只要知道開頭，就可以知道後續的資料位置  所以監聽的概念也是一樣的，只要知道某個 object 目前的 resourceVersion，你就可以知道後續的位置，進而監聽它   Example  啟動一個 proxy 到 Kubernetes API Server  1 $ kubectl proxy --port 8888      port 可以隨意指定    取得目前的 resourceVersion  1 2 $ curl http://localhost:8888/api/v1/namespaces/default/pods | grep resourceVersion \"resourceVersion\": \"135966\"   然後你就可以持續監聽後續 object 的變化(RV 135966 以後的資料)  1 $ curl http://localhost:8888/api/v1/namespaces/default/pods\\?watch=1\\&amp;resourceVersion=135966      curl 使用的時候記得跳脫特殊字元    為了可以觀察到變化，你可以嘗試建幾個 pod 玩一下好方便觀察  1 $ kubectl run mynginx --image=nginx      所有的監聽歷史資料都是儲存在 etcd 裡面  想當然空間不會是無限的，預設只會保留 5 分鐘的資料    List and Watch  根據 KEP 3157  kube-apiserver 是非常脆弱的，它很容易受到記憶體壓力的影響導致服務中斷  而這股記憶體壓力來自於所謂的 LIST request(也就是 Reflector 之前的行為)  你不需要很多的 LIST request 就可以讓 kube-apiserver 過載(大概 16 個 LIST request 足以)  而它會間接導致整個 node 裡面的服務都中斷，包含 kubelet   他們發現，LIST request 的作法需要從 etcd 裡面拿到資料  並經過一系列的處理才能送回給 client(包含 unmarshal, convert, prepare response)  記憶體的用量是無法預估的(因為會受到 page size, filter 等等的影響)  而這些記憶體連 Golang 本身的 GC 都無法處理   為了要讓記憶體的用量變得可控  於是提出了使用 streaming 的方式，從 $O(watchers \\times pageSize \\times objectSize \\times 5)$ 降到 $O(watchers \\times constant)$  並且為了減少 etcd 的壓力，資料來源會從 Watch Cache 拿  只有在必要的時候與 etcd 同步資料      注意到 LIST request 並沒有要移除    所以改進的方法就明顯了  從 Watch Cache 拿資料，但一樣用 LIST request 嗎？  但如果繼續用 List Request，問題並不會解決，記憶體的問題只會從 etcd 轉移到 Watch Cache 而已  所以是用 WATCH request 搭配上 streaming 的方式來處理(模擬 LIST request 的行為)   所以你可以看到，reflector 的部分預設是用 streaming  然後有一個 fallback 的機制   tools/cache/reflector.go  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 // ListAndWatchWithContext first lists all items and get the resource version at the moment of call, // and then use the resource version to watch. // It returns error if ListAndWatchWithContext didn't even try to initialize watch. func (r *Reflector) ListAndWatchWithContext(ctx context.Context) error {     logger := klog.FromContext(ctx)     logger.V(3).Info(\"Listing and watching\", \"type\", r.typeDescription, \"reflector\", r.name)     var err error     var w watch.Interface     fallbackToList := !r.useWatchList      defer func() {         if w != nil {             w.Stop()         }     }()      if r.useWatchList {         w, err = r.watchList(ctx)         if w == nil &amp;&amp; err == nil {             // stopCh was closed             return nil         }         if err != nil {             logger.Error(err, \"The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking\")             fallbackToList = true             // ensure that we won't accidentally pass some garbage down the watch.             w = nil         }     }      if fallbackToList {         err = r.list(ctx)         if err != nil {             return err         }     }      logger.V(2).Info(\"Caches populated\", \"type\", r.typeDescription, \"reflector\", r.name)     return r.watchWithResync(ctx, w) }   為了模擬 LIST request 的行為，WATCH request 會使用所謂的 BOOKMARK event  bookmark event 是一種特殊的 event，他用來表示目前的資料版本已經跟你提供的 resourceVersion(RV) 一致  在 watchList 裡面扮演著重要的角色   還記得我們說用 WATCH request 模擬 LIST request 的行為嗎？  主要的流程還是沒變，我需要先拿到歷史資料，拿完之後再繼續監聽新資料  當你接收到 bookmark event 的時候，表示所有歷史資料已經拿完了(最新的 RV 可以從 etcd 拿到，確保是 up-to-date 的)，你的 Watch Cache 已經完全跟上了  最後，因為我們是使用 WATCH request，所以後續的 event 就會是新資料  沒有新的 API call   Resync  在 ListAndWatchWithContext 中，你會發現 Watch 不單只是 watch，還有 Resync 的機制   1 return r.watchWithResync(ctx, w)   我百思不得其解 Resync 的意義在哪  是為了處理斷線資料遺失的問題嗎？ WATCH request 可以從上次的斷點繼續拉資料(i.e. RV) 所以不是  回答這題之前，要先了解 edge-based 與 level-based 的差異      edge-based: 專注於 “事件發生” 本身   level-based: 專注於 “狀態” 本身   What does edge-based and level-based mean? 留言的例子滿精準的  如果你想要知道有多少的 Pod 是 READY 狀態     edge-based 的做法會是當 Pod 變成 READY 的時候計算一次   level-based 則是從 etcd 拿到所有的 Pod 資料，然後計算一次   edge-based 的缺點在於如果你漏掉一兩個事件，那結果會不正確  level-based 則是你不知道哪時候所有的 Pod 會完成，所以你可能會需要處理很多次，比如說 10 秒檢查一次之類的  對回去 Controller Pattern 的實作，其實你會發現他是 edge-based 也是 level-based 的   edge-based 我們已經看過 WATCH request 如何避免網路中斷等等事故的應對方法  而 level-based 的方法就是 Controller 負責 Reconciliation，將 current state 往 desired state 推進(可參考 Control Loop)   但終究是人寫的，Controller 可能有疏漏，導致 state 處理不妥當導致失敗  那 level-based 的處理方法就是我再重新執行一次 Reconciliation  Controller 本身理論上要是 idempotent 的，所以沒問題      The resync period does more to compensate for problems in the controller code than to compensate for missed watch event.  We have had very few “I missed an event bug” (I can think of one in recent memory), but we have had many controllers which use the resync as a big “try again” button.  ref: What’s the right resync period value for informers? By David Eads    重新執行並不是 Controller 內部重新 enqueue 那種，我們說的是你寫錯的那種  Resync 迫使你重新對 Resource 進行 Reconciliation  將 thread-safe store 的資料重新塞入 delta-FIFO queue 裡面，允許失敗的事件能有再一次處理的機會      注意到 Resync 並不會重新拉所有的資料，只有 thread-safe store 內部的資料    Informer  Informer 本質上在做的事情是包含 Reflector 的功能(應該說 Reflector 是一個 tool 然後 Informer 才是真正使用的)  當接收到 event 的時候(從 delta-FIFO queue 拿)，處理完 dispatch 到 workqueue 裡面  並且交給 Controller 進行處理   Indexer  從 Informer 傳遞給 Custom Controller 的資料只是單純的 object key  你大概猜得到為什麼不丟整個 object，多半是因為效能問題  寫入的部份是透過 Indexer 來做的   但只有 key 是不足以做 Reconciliation 的(因為資訊不足嘛)  Indexer 的作用就像是資料庫的 Index 一樣，可以快速的找到 object  所有相關的資料都是儲存在 thread-safe 的 store 裡面   存取的部份是透過 Lister(cache/lister) 實現的  所以這邊算是一個隱藏的 component   Control Loop       ref: 如何编写自定义的 Kubernetes Controller    具體來說，Controller 是怎麼做 狀態管理 的呢？  前面提到 Kubernetes Object 裡面有存 desired state 以及 current state  所以 Controller 就會不斷的監控 object 的狀態，並且根據 desired state 來更新 current state  要做到不斷的監控，最簡單的方式就是一個迴圈，稱為 control loop   這種不斷監控並更新狀態的方式，就是所謂的 Reconciliation      理論上每個 Controller 都是獨立的 process, 但是為了方便管理，K8s 會將他們打包在一起    Controller Types  K8s 裡面，controller 其實不只有一種  針對不同的 Resource，K8s 會有不同的內建的 Controller  deployment 有自己的 Deployment Controller，job 也有自己的 Job Controller 等等的      有關 Resource 可以參考 Kubernetes 從零開始 - 高階抽象 Workload Resources | Shawn Hsu         ref: 如何编写自定义的 Kubernetes Controller    Controller Conflict on Objects?  內建的 Controller 會監控特定的 object  但是有一些 Controller 他們看的 object 是同一種的   比如說 deployment 跟 job 都會監控 pod  會不會有一種可能他們的 Controller 會互相衝突呢？  事實上不會，Controller 會根據 object 的 label 來區分   References     Difference between Kubernetes Objects and Resources   Objects, Resources and Controllers in Kubernetes   Objects In Kubernetes   Kubernetes Object Management   Kubernetes Controller 机制详解（一）   What k8s bookmark solves?   Watch bookmarks   KEP 3157   [提问]Informer 中为什么需要引入 Resync 机制？   Informer, Cache and Queue | Kubernetes Informers vs Watch | Basics of client-go Kubernetes Part - 4   kubernetes infomer 中的 resync   深入源码分析kubernetes informer机制（三）Resync   What’s the right resync period value for informers?   What does edge-based and level-based mean?   What’s the right resync period value for informers?  ","categories": ["kubernetes"],
        "tags": ["kubernetes controller","state","wrangler","kubernetes operator","kubernetes resource","kubernetes object","reconcile","crd","control loop","controller pattern","operator pattern","informer","indexer","reflector","workqueue","lister","etcd","kubernetes api server","resync","client-go","bookmark event"],
        "url": "/kubernetes/kubernetes-controller-concept/",
        "teaser": null
      },{
        "title": "神奇的演算法 - 為什麼你的 Priority Queue 那麼慢！",
        "excerpt":"Introduction to Priority Queue  針對需要存取一個陣列內，最大或最小值的方法，常見的第一直覺是 sorting  但每次存取每次排序顯然不好，於是有了 Priority Queue 這個資料結構   與其每次都排序，不如我維護一個 有序的資料結構，就是 priority queue 的基本思想  它不一定要是依照大小排列，你可以自定義排序方式  比如說你可以做到，奇數 index 的數值小到大，偶數 index 的數值大到小  這類奇特的排序需求   Implementation  實作上有分為以下幾種方式  Array 與 Linked List 是最直覺的方式  他們的時間複雜度是     寫入/刪除: $O(n)$   查詢: $O(1)$   即使複雜度一樣，但是實作起來不同的語言會有不同的差異  以實作方便程度來說，Array 會比 Linked List 簡單很多  但是 Array 需要考慮重新配置記憶體的問題(i.e. realloc)   Heap 則是速度最快的實作  其寫入/刪除的時間複雜度可以被縮減為 $O(\\log n)$      $O(\\log n)$ 是代表每次運算可以篩掉一半的數量    Array  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 func insert(q []int, value int) []int {     var index int      for index = 0; index &lt; len(q); index++ {         if q[index] &gt; value {             break         }     }      first := append([]int{}, q[:index]...)     second := append([]int{}, q[index:]...)      return append(append(first, value), second...) }  func pop(q []int, index int) []int {     q = append([]int{-1}, q...)     index += 1      first := append([]int{}, q[:index]...)     second := append([]int{}, q[index + 1:]...)      return append(first, second...)[1:] }  func main() {     priorityQueue := make([]int, 0)      for _, num := range nums {         priorityQueue = insert(priorityQueue, num)     }      priorityQueue = pop(priorityQueue, 0) }   實作上就如上所示，找到合適的位置插入/刪除即可  透過 Golang 的 slice 可以不需要手動計算所需的記憶體大小，寫起來也比較方便   你可能會好奇，為什麼不論是 insert 或者是 pop 我都重新配置一塊記憶體呢(i.e. append([]int{}))？  原因在於說，如果直接拿原始陣列 append 起來，會去更改到原本的記憶體內容，造成資料不一致的行為   Linked List  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 type node struct {     value int     next *node }  type pq struct {     head *node }  func (q *pq) add(root *node) {     sentinel := &amp;node{next: q.head}     previous := sentinel     current := q.head      for current != nil &amp;&amp; current.value &lt; root.value {         previous = current         current = current.next     }      previous.next = root     root.next = current      q.head = sentinel.next }  func (q *pq) pop(index int) {     sentinel := &amp;node{next: q.head}     previous := sentinel     current := q.head      count := 0     for current != nil &amp;&amp; count != index {         previous = current         current = current.next         count += 1     }      previous.next = current.next     q.head = sentinel.next }  func main() {     q := &amp;pq{head: nil}      for _, num := range nums {         q.add(&amp;node{value: num, next: nil})     }     q.pop(0) }   Linked List 的實作也相對直覺  透過一個 for-loop 遍歷整個 List 尋找適合插入/刪除的位置(這點跟 Array 的實作如出一徹)  注意到，為了更優雅的處理邊界條件(也就是 previous 是 nil 的時候)  使用 if 判斷是一個合理的選項，但不優雅，所以這邊透過一個 dummy node 來處理(時間換空間)   Heap  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 func swap(h []uint64, i, j int) []uint64 {     tmp := h[i]     h[i] = h[j]     h[j] = tmp      return h }  func insert(h []uint64, num uint64) []uint64 {     h = append(h, num)     i := len(h) - 1      for i &gt; 0 {         parent := (i - 1) / 2         if h[i] &lt; h[parent] {             h = swap(h, i, parent)             i = parent         } else {             return h         }     }      return h }  func pop(h []uint64, index int) []uint64 {     last := len(h) - 1      h[0] = h[last]     h = h[:last]     size := len(h)      for index &lt; size {         left := 2 * index + 1         right := 2 * index + 2          smallest := index         if left &lt; size &amp;&amp; h[smallest] &gt; h[left] {             smallest = left         }         if right &lt; size &amp;&amp; h[smallest] &gt; h[right] {             smallest = right         }          if smallest != index {             h = swap(h, index, smallest)             index = smallest         } else {             return h         }     }      return h }  func main() {     h := make([]uint64, 0)      h = insert(h, uint64(7))     h = insert(h, uint64(23))     h = insert(h, uint64(4))     h = insert(h, uint64(12)) }      本例 Min Heap 是使用 Array 來實作(當然也可以用 Linked List)    Heap 本質上是一個 完全二元樹(complete binary tree)  注意到它跟 binary search tree 是不同的，Heap 同一層的數值沒有大小之間的關係  也就是說，只有不同 level 之間的數值才有分大小      完全二元樹，節點之間不會有空缺    同一層的數值沒有大小區分 :arrow_right: 這件事情其實很有趣  如果你把 Heap 的數值照順序畫出來，你會發現到你並沒有辦法得到一個有序的陣列  這代表 Heap 沒辦法給定排序的結果嗎？ 其實不然   Heap 是透過寫入/刪除的 過程，確保這個 “排序” 依然有效   Update Process  既然 Heap 借鑒了二元樹的想法，那麼更新的方式也是類似  只不過是  寫入的時候，我們是從 最後一個節點 開始往上更新  刪除的時候，我們是從 root 開始往下更新   這個條件，依據 Min Heap 或 Max Heap 來決定     Min Heap : 父節點的數值小於子節點   Max Heap : 父節點的數值大於子節點      不同的 Heap 他的根節點會是最大或最小的數值      不過如果在更新的過程中，兩邊 child 都可以選擇的時候該怎麼辦？  考慮以下刪除的例子(100 被換到 root 的位置然後開始往下更新)  1 2 3 4 5        100      /     \\     30      10    / \\     /  \\   40  50  70  20   如果你選擇 30 那 Heap 會變成這樣  1 2 3 4 5        30      /    \\     100     10     / \\     /  \\   40  50  70  20   很明顯的，這不是一個 Min Heap  雖然我們提到過，同一階層的節點之間，並沒有絕對的大小之分  儘管如此，在更新的時候你 依然要選擇最大/最小的節點 來更新  確保後續的節點也是符合 Heap 的規則   Index Calculation  1 2 3 4 5        0      /   \\     1     2    / \\   /   3   4 5     透過上圖你可以很輕易的推導出，Parent, Left, Right 的關係     Parent = int(i / 2)   Left   = 2 * i + 1   Right  = 2 * i + 2   LeetCode 3066. Minimum Operations to Exceed Threshold Value II  題目本身相當單純，請你計算總共需要多少的步驟，才能讓陣列中的數值都大於等於 threshold  而每一次的操作都要從陣列裡面取出 最大 與 次大 的數值，然後將他們計算後放回陣列   很明顯使用 Priority Queue 是一個明顯的選擇  只不過當你完成之後會發現 TLE 的問題  即使測資的大小是 $2 * 10^5$，但是因為你需要重複的取出 寫入  Priority Queue 的效能就會變得很差  所以使用 Heap 才是正確的選擇  ","categories": ["algorithm"],
        "tags": ["priority queue","heap","min heap","max heap","array","linked list","realloc","golang"],
        "url": "/algorithm/alogrithm-priority-queue/",
        "teaser": null
      },{
        "title": "Kubernetes 從零開始 - Deployment 管理救星 Helm Chart",
        "excerpt":"Preface  你應該有發現，Kubernetes 的佈署過程中你需要撰寫一定數量的 yaml 設定檔  不外乎是 application 的 deployment, 設定檔的 configmap, secret 等等  每次更新這些設定檔的時候，你都需要手動的去修改這些 yaml 檔案  這樣的過程是非常繁瑣且容易出錯的   Issue with Manifests in Kubernetes Deployment  更甚至，主流的佈署流程通常會有 dev, staging, production 三個環境  每一個環境所需要的設定檔可能不盡相同，Manifests 的撰寫方式並沒有考慮到這一點  導致你沒有辦法重複利用這些現有的設定檔   每一次的更新佈署都會耗費大量的精力與時間，顯然這是可以被改善的   Introduction to Helm Chart  針對日益複雜的 application, 傳統的 Manifests 撰寫方式已經無法滿足需求  Helm 提供了一個解決方案，讓你可以更有效率的管理你的 Kubernetes 佈署   具體來說他可以做到     透過模板化的方式來管理你的 Kubernetes 設定檔   重複利用你的，甚至不只你的設定檔   一鍵佈署你的 application   版本化控制你的 application   雖然它官方說明是一個 package manager  但我覺的他的重點功能更著重在於模板化的設定檔      其實有點類似於 Kustomize  但 Helm 提供了更多的功能，例如版本控制，依賴管理等等    Helm 是使用 Golang 撰寫而成的，其包含兩大組成元件     Helm Client :arrow_right: 我們使用的 CLI 工具   Helm Library :arrow_right: 抽象化了 Kubernetes API 的函式庫，負責底層的操作   Installation  1 2 3 4 $ curl -fsSL -o get_helm.sh \\     https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 $ chmod 700 get_helm.sh $ ./get_helm.sh   What is a Chart?  Helm 本身還是會需要寫 yaml 設定檔，它並沒有丟棄 Manifests 的概念  你的 application 通常會需要不只一個 Manifests, 如 deployment, service, configmap 等等  這些設定檔會被打包成一個 Chart   前面說到的一鍵佈署，就可以透過 Chart 來實現  1 $ helm install my-release ./my-chart   安裝成功之後，它就稱之為 Helm Release      可以透過 helm list 來查看所有的 Release    Subcharts  既然一個 application 可以以一個 Chart 來表示與管理  它裡面可能也會包含其他的 application, 最簡單的例子就是 database  這時候你就需要使用 Subcharts   Helm Chart 裡面你可以定義其他的 Chart 作為你應用程式的依賴(e.g. database, redis)  而這些 Subcharts 同樣可以被管理與佈署(因為你可以指定特定版本)   Chart 本身是可以公開的  它並沒有使用到任何的 source code, 只是一個打包好的設定檔  你可以在 Artifact Hub 上找到許多的 Chart   你也可以公開自己的 Chart, 讓他人可以使用(可參考 Publish your Charts for Reusability)      類似 GitHub Action 大家都會寫自己的 Action  並且公開給他人使用    注意到 Artifact Hub 本身並不是 Helm Repository  Repository 是 Chart 存放的地方，它可以包含很多的 Chart  而 Artifact Hub 只是一個方便你找到 Chart 的地方   Helm Chart Structure  一個 Chart 擁有特定的資料結構，具體來說長這樣  1 2 3 4 5 6 7 8 9 10 11 wordpress/   Chart.yaml          # A YAML file containing information about the chart   LICENSE             # OPTIONAL: A plain text file containing the license for the chart   README.md           # OPTIONAL: A human-readable README file   values.yaml         # The default configuration values for this chart   values.schema.json  # OPTIONAL: A JSON Schema for imposing a structure on the values.yaml file   charts/             # A directory containing any charts upon which this chart depends.   crds/               # Custom Resource Definitions   templates/          # A directory of templates that, when combined with values,                       # will generate valid Kubernetes manifest files.   templates/NOTES.txt # OPTIONAL: A plain text file containing short usage notes   需要定義的資料夾有     templates: 所有設定檔的模板(透過 values.yaml 來動態的設定數值)   charts: 依賴的 Chart 的檔案            你可能會好奇，所有的依賴不都是定義於 Chart.yaml 裡面嗎？ 有點類似 node_modules 的概念，它需要把依賴下載下來 所以就放在這           crds: 所有 Custom Resource Definitions 的定義需要放在這   檔案的部份     Chart.yaml: 關於 Chart 的資訊(需不需要有 dependencies 以及其資訊)   values.yaml: 所有的設定檔的數值   Subchart Declaration  1 2 3 4 5 6 7 apiVersion: v2 name: count-page-views version: 1.0.0 dependencies:   - name: redis     version: \"20.6.2\"     repository: oci://registry-1.docker.io/bitnamicharts  subchart 的版本號你可以看他的定義 以這個例子來說會是 20.6.2  可以在 Chart 的定義裡面找到 bitnami/redis      Helm 3 支援使用 OCI 的寫法，換言之你可以把 chart 放在類似 docker hub 這種地方  ref: Use OCI-based registries    Overriding Values from a Parent Chart  當你使用 Subchart 的時候，你可以透過 values.yaml 來覆蓋 Subchart 的設定檔  比方說以我的例子來說，我想要設定 redis replica 的數量  根據 bitnami/redis 可以找到是 replica.replicaCount  然後你就可以   1 2 3 redis:   replica:     replicaCount: 3   注意到這裡的 redis 是 subchart 的名字，也就是在 dependencies 裡面的名字  所以在撰寫設定檔的時候你可以適當的將一些設定抽出來方便更換     Helm Chart Upgrade CRD  Kubernetes 本身是支援 CRD Versioning 的，也就是說你可以升級 CRD(可參考 Kubernetes 從零開始 - client-go 實操 CRD | Shawn Hsu)  但是對於 Helm 來說就不是這麼一回事了   Helm 對於 CRD 的處理比較謹慎  Helm 不允許升級或是刪除已經存在的 CRD  即使你的 yaml 裡面包含了若干不同版本的資料，Helm 也不會去處理他們   主要是為了避免升級造成資料遺失的問題  目前社群並沒有一個很好的解決方案，所以 Helm 把它擱置了   Workarounds  CRD 主要會放在 crds/ 資料夾裡面  只要 Helm 主觀認定所有東西都已經被安裝之後，他就不會再去處理他們  你也可以學 kubernetes/kueue 他是擺在 templates 裡面  如此一來每次安裝 Helm 都會處理到      注意到 Helm 並沒有禁止把 CRD 擺在 templates 裡面的寫法    但如果你是用 templates 的方法，會需要額外處理安裝順序的問題  CRD 永遠會是最優先安裝的東西，因為要確保在裝 templates 的時候 CRD 已經存在  寫在 templates 裡面等於說你自己要去處理他的生命週期(可參考 Helm Chart Hooks)   Helm Chart Hooks  Helm Chart 提供了一個叫做 Hooks 的功能  Hooks 可以在特定的事件發生時執行特定的操作  比如說 post-install, post-upgrade 等等的   前面提到的 CRD 升級問題，如果是寫在 templates 裡面  也可以考慮使用 pre-install hook 處理  它可以保證，該 yaml 會在所有 templates 之前被安裝   Hooks  Chart Hook 總共有 9 種 Hook ，其實可以大略分成 4 + 1 種  分別對應到不同的 Helm 指令(install, upgrade, rollback, uninstall, test)      test 只在 $ helm test 時執行  helm test 可以幫助你驗證你的 yaml 東西有沒有正確(渲染過後的 yaml)    每種 Hook 都有 pre 跟 post 兩種     pre: 在 事件之前 執行，比如說，安裝之前、升級之前            Helm 對於事件之前的定義是，當 templates 已經完全渲染完成但還沒安裝進去之前           post: 在 事件之後 執行，比如說，刪除之後、復原之後   Hook Weight  範例可以參考 Example      hook-weight 是個字串    hook-weight 用來定義執行的順序，數字越小越早執行(可為負值)  如果數字相同，會按照 Resource Kind 的順序(ASC)  如果 Resource Kind 相同，會按照 Resource Name 的順序(ASC)   權重設計，依照慣例，通常會留有一定的空間  比如說間隔 100 來設定，這樣可以確保你可以在中間插入其他的工作  傳統的 Linux 的 init scripts 也是使用類似的方法(現已被 systemd 取代)        ref: Linux Pi的奇幻旅程(16)-大改造(續)    透過檔案名稱排序，依序執行相對應的 script  為了保有一定的彈性，每個權重之間不一定是緊密相連的  允許你在之後安插不同的依賴，這樣就不需要調整權重   而這當然也會有問題，像是如果預留的空間仍然不足，手動調整空間還是必要的  這也是傳統 runlevel 實作上的一個問題   Lifecycle  Hook Lifecycle 其實相對簡單，以 install 為例      $ helm install   呼叫內部 install API   安裝 crds/ 裡面的 CRD   驗證以及渲染 templates/ 裡面的 yaml   pre-install hook(並等待完成)   正式安裝   post-install hook(並等待完成)   完成   你可能會好奇要怎麼定義 Hook 已經被執行完成與否？  針對 Job 或是 Pod 這類 resource 主要是判斷成功與否，其他資源則是寫進去就算完成   Example  基本上 Hook 就是個 annotation 而已   1 2 3 4 annotations:   \"helm.sh/hook\": post-install,post-upgrade   \"helm.sh/hook-weight\": \"5\"   \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded   你可以在一個 yaml 裡面定義多個 Hook(用逗號分隔)  hook-weight 用來定義執行的順序，可參考 Hook Weight  hook-delete-policy 用來定義 Hook 的刪除策略，有三種      before-hook-creation: 在新的 Hook 創建之前刪除先前的 Hook(預設)   hook-succeeded: 在 Hook 執行成功之後刪除   hook-failed: 在 Hook 執行失敗之後刪除   Publish your Charts for Reusability      to be continued    References     Kustomize K8S 原生的配置管理工具   Helm does not resolve local dependencies repository file path   Charts   Overriding Values from a Parent Chart   What is the difference between fullnameOverride and nameOverride in Helm?   Insert multiline json string into helm template for base64 encoding   Custom Resource Definitions   Limitations on CRDs   Chart Hooks   linux系统脚本启动顺序 /etc/rc.d/ 与/etc/rc.d/init.d  ","categories": ["kubernetes"],
        "tags": ["helm chart","kustomize","package manager","template","manifests","crd","crd upgrade","hooks","subchart","artifact hub","chart hook","pre-install hook","post-install hook","pre-upgrade hook","post-upgrade hook","pre-rollback hook","post-rollback hook","pre-uninstall hook","post-uninstall hook","test hook","hook lifecycle","hook weight","hook delete policy","publish chart","helm repository","oci","helm 3","helm release","helm install","helm list","helm upgrade","helm rollback","helm uninstall","helm test","helm dependency","helm dependency update","helm dependency build","oci-based registries"],
        "url": "/kubernetes/kubernetes-helm-chart/",
        "teaser": null
      },{
        "title": "神奇的演算法 - Binary Search 到底怎麼寫才會對？",
        "excerpt":"Introduction to Binary Search  如果說，要在一串排序過後的陣列中，找尋特定的數值，二元搜尋絕對是最快的存在  憑藉著一次可以排除一半的可能性，使得二元搜尋的複雜度為 O(log n)   線性搜尋在一般情況下不算太差，但是當數量級上升的時候，二元搜尋的優勢就會顯現出來  只不過 binary search 擁有一大前提，那就是陣列內的資料必須是排序過後的(才可以一次過濾掉一半嘛)   寫起來也挺簡單的，可參考 LeetCode 704. Binary Search  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 func search(nums []int, target int) int {     size := len(nums)      left := 0     right := size - 1      for left &lt;= right {         mid := left + (right - left) / 2                  if nums[mid] == target {             return mid         }          if nums[mid] &gt; target {             right = mid - 1         } else {             left = mid + 1         }     }      return -1 }      注意到 mid := left + (right - left) / 2，這是為了避免 overflow    Lower and Upper Bound  二元搜尋的應用不僅僅只是找尋特定的數值，還可以找尋數值的上下界  比方說，給定一個排序過後的陣列 [1,2,3,4,4,4,4,5,6,7,8,9]，我們要找尋數值 4 的上下界  你可以改寫 binary search 的實作   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 func lowerBound(nums []int, target int) int {     size := len(nums)      left := 0     right := size - 1      for left &lt; right {         mid := left + (right - left) / 2                  if nums[mid] &gt;= target {             right = mid         } else {             left = mid + 1         }     }      if left &gt; 0 &amp;&amp; nums[left - 1] == target {         return left - 1     }      return -1 }   陣列內，數字 4 的下界是 index 3  我們一樣先從中間一步一步過濾，注意到這裡 if nums[mid] &gt;= target  如果發現中間的數值是大於等於 target，那我們就要更新 上界  注意到為什麼這裡是 right = mid，因為 mid 有可能就是 target，所以 要保留 mid   如此這般，你就會找到數值 4 的下界  當然你會需要檢查一下 left 到底對不對，upperBound 的實作也是類似的      注意到這裡是 for left &lt; right，要避免無限迴圈    Correct Way to Implement Binary Search  每次在寫二元搜尋的時候我頭都很痛，因為我總是會忘記一些細節  比如說我有看過 right = mid - 1 以及 right = mid，也有看過 left &lt; right 以及 left &lt;= right  那到底哪個才是對的？   先說 for loop 的條件  其實重點在於，當 left == right 的時候，我們還要不要繼續搜尋  換言之，他是開區間還是閉區間？  舉例來說，[10]，搜尋 10 的時候顯然答案是存在的，因此你的 for-loop 條件應該是 left &lt;= right   至於說是 mid 還是 mid - 1，取決於 你需不需要考慮 mid 這個數值  如果你不需要考慮 mid 這個數值，那就是 mid - 1，反之則是 mid   LeetCode 2560. House Robber IV  本題依舊延續著 House Robber 的傳統，Robber 不會連續搶劫相鄰的房子  給定一排房子，每個房子都有一定數量的金錢可以竊取，由於上述條件的限制，Robber 會有 N 種不同的搶劫方式   每一種搶劫方式得到的金額，都是從不同的房子取出的金額加總，其中，房屋最大金額稱為 capability  舉例來說 [2,3,5,9] 的其中一種方式為 [2,9], 那他的 capability 就是 9(單次搶劫中房屋價值最高的金額)  題目要求為，給定 至少行竊 k 間房屋，在眾多不同的方式中，找到 最小的 capability   並且附帶上一條至關重要的提示，It's always possible to steal at least k houses.   Process  題目的複雜度稍微高一點，可能需要花點時間理解  乍看之下，這似乎是一個 DP 題目，因為我們需要嘗試所有可能的組合，找到最小的 capability  動態規劃，我們知道，重點在於一步一步的建構答案，但是這題會需要嘗試所有可能嗎？      有關動態規劃可以參考 神奇的演算法 - 動態規劃 Dynamic Programming | Shawn Hsu    與其 1.) 先計算搶劫 k 間房屋，得到 capability 2.) 再計算全局最小 capability  不如直接 1.) 拿一個 guess capability 2.) 驗證其 capability 是否可以搶劫 k 間房屋   這樣的思考方式就有優化的空間了，為什麼？  如果 guess capability 無法滿足 k 間房屋，那比該數值大的 capability 也不會滿足  你是不是能夠直接篩選掉一半的可能性？  然後這是不是很熟悉？ 就是二元搜尋法   Implementation  但是二元搜尋仰賴的是一個排序好的數列，這樣才能夠進行二元搜尋  本題顯然不滿足該特性   所以前面我們才說，我們需要先猜測一個 capability，然後驗證其是否可以滿足 k 間房屋  這個 guess capability 必須要是一個範圍內的數值，本例來說就是 array 的最大最小值之間   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 func minCapability(nums []int, k int) int {     size := len(nums)      minReward := 1     maxReward := 1      for _, num := range nums {         maxReward = max(maxReward, num)     }      for minReward &lt; maxReward {         midReward := minReward + (maxReward - minReward) / 2          count := 0         for i := 0; i &lt; size; i++ {             if nums[i] &lt;= midReward {                 count += 1                 i++             }         }          if count &gt;= k {             maxReward = midReward         } else {             minReward = midReward + 1         }     }      return minReward }  func max(a, b int) int {     if a &gt; b {         return a     }     return b }   當你 apply 二元搜尋下去之後，第二步驟就是驗證該 guess capability 是否可以滿足 k 間房屋  這裡的驗證方式是採用 貪婪法，當房屋價值小於等於 guess capability 時，就可以搶劫  為什麼？ 因為單次搶劫中房屋最高價值為 capability，換言之，單次搶劫內的房子價值永遠都會小於等於 capability      有關貪婪法可以參考 神奇的演算法 - Greedy Algorithm | Shawn Hsu    二來是為什麼貪婪法會動？  我們知道貪婪法的特性是，可以保證區域最佳解，但全域最佳解則不一定  在這個例子當中，我們只 care 房屋的數量，房屋可以被偷的價值不在考慮範圍內  當遇到符合條件的房屋，一定要選，這樣可以增加勝率(因為我們只考慮房屋數量)   1 2 3 4 5 if count &gt;= k {     maxReward = midReward } else {     minReward = midReward + 1 }  最後當 count &gt;= k 的時候，為什麼是調整上界？  因為我們要求的是，全域最小的 capability，所以當 count &gt;= k 的時候，我們要繼續往下找   那為什麼 return value 是 minReward 呢？  你應該有發現，我們在做 binary search 的時候，mid 是有可能不存在於 array 中的  既然這樣為什麼這樣寫還會動？  因為 It's always possible to steal at least k houses.，最小值一定存在於 array 中   LeetCode 2529. Maximum Count of Positive Integer and Negative Integer  題目的要求是，分別要求計算正數的數量以及負數的數量，求兩者的最大值  這題雖然是 easy, 但是他的 follow up 要求整體的 runtime 要 $O(\\log n)$  你當然可以線性掃過去逐一檢查，但是仔細看題目，nums is sorted in non-decreasing order  所以 binary search 派上用場了   已經排序過的數列，要怎麼用 binary search？  因為我們只想知道正數以及負數，0 不在考慮範圍內，因此可以將 0 當成是搜尋的目標  但是要注意到，數列中的數值並不保證 unique，所以這個問題會演變成 Lower and Upper Bound   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 func maximumCount(nums []int) int {     size := len(nums)      var (         left, right int         lower = size         upper = size     )      left = 0     right = size - 1     for left &lt; right {         mid := left + int((right - left) / 2)          if nums[mid] &lt; 0 {             left = mid + 1         } else if nums[mid] &gt;= 0 {             right = mid             lower = mid         }     }      left = 0     right = size - 1     for left &lt; right {         mid := left + int((right - left) / 2)          if nums[mid] &lt;= 0 {             left = mid + 1         } else if nums[mid] &gt; 0 {             right = mid             upper = mid         }     }      return max(lower, size - upper) }  func max(a, b int) int {     if a &gt; b {         return a     }     return b }  ","categories": ["algorithm"],
        "tags": ["binary search","lower bound","upper bound","sorted array","leetcode","leetcode-704","leetcode-2560","leetcode-2529"],
        "url": "/algorithm/algorithm-binary-search/",
        "teaser": null
      },{
        "title": "Webpack 如何解決 Node.js 動態載入 Cannot find module 的問題",
        "excerpt":"JavaScript Dynamic Import  我公司最近遇到了一個需求是，我們需要有辦法 import 任一 JS 檔案  將其當作動態模組載入主程式當中運行   JavaScript 本身提供了相對應的功能，稱作 dynamic import  注意到，普通的 import 是沒辦法支援動態載入的功能的      To load modules in non-module contexts, use the dynamic import syntax instead.    Use Cases  以我們的例子來說，是希望能夠支援各種不同接口  為了實現該目標，以動態的方式載入對應的 module 並使用相同的 API interface 即可   那當然也有一些比較正常的 use case，比如說     靜態載入的資源過大，會拖慢整體的運行速度，進而降低使用者體驗   載入的資源只有在 runtime 的時候才會拿到(:arrow_left: 我們的情況)   載入的資源位置只有在 runtime 的時候才會拿到(:arrow_left: 我們的情況)   只有在需要使用該資源的時候才引入，避免 side effect   執行環境是 non-module 的   Restrictions  雖然你可以使用 dynamic import，但實務上還是不建議  只有在你真的需要的時候才去使用   原因在於 static import 的方式有助於一些靜態分析工具  如果使用 dynamic import 則沒有辦法提供良好的支援  另一個重點是，並不是所有環境都支援 dynamic import  比如說 service worker 就不支援   Cannot find Module  1 2 3     var e = new Error(\"Cannot find module '\" + req + \"'\");  Error: Cannot find module 'xxx'   之後我就遇到一個奇怪的問題  明明我動態產生的路徑是正確的，檔案也是存在的  不管執行幾次仍會遇到 Cannot find module 的問題   更懸的是，如果是直接用 node 跑不會報錯  只有在 webpack bundle 完才會出現   Webpack Dynamic Import Support  Webpack 本身也支援動態載入的功能，但是有一些限制  你沒辦法完全依靠於變數，某種程度上你需要提供一些 “線索” 給 bundler   1 2 3 4 5 // imagine we had a method to get language from cookies or other storage const language = detectVisitorLanguage(); import(`./locale/${language}.json`).then((module) =&gt; {   // do something with the translations });   以上述例子來說，動態載入的檔案位置它一定在 locale 底下並且為 json 檔案  設定為 wildcard 或者完全依靠 runtime 數值決定，會導致他有可能會指向 系統內的任一位置  這顯然不太對   Webpack Chunk  動態載入的模組，會是一個獨立的區塊，稱為 chunk  由於其動態載入的特性，亦即需要的時候才會引入進來  那把它跟 application chunk 綁在一起屬實是沒什麼必要性   分開打包，需要在引入可以讓一開始的載入速度變得更快  不過同時它會造成一些問題   如果動態載入的路徑是相對路徑  那是不是 runtime 在 evaluate 的時候就會出錯了呢？  因為 webpack 在打包的時候並不知道 runtime 的路徑  而這正是造成 Cannot find module 的罪魁禍首   WebpackIgnore the Magic Comment  那有沒有辦法不要讓 webpack 解析我的 import 路徑呢？  我可以跟它掛保證說 runtime 我會自己處理，你就把它放著就好   你可以透過所謂的 Magic Comment 進行標注  以本例來說，你需要的是 webpackIgnore   1 new URL(/* webpackIgnore: true */ 'file1.css', import.meta.url);   透過簡單的類似註解的寫法註記在程式碼當中，webpack 就知道說不要嘗試去解析 import.meta.url 是什麼  到 runtime 的時候它就會變合法的路徑了(當然，執行出錯我會自己負責)  這樣做可以確保即使是使用相對路徑，node 也能夠正確的找到檔案   Magic Comment CJS Support  Magic Comment 目前有多個種類可以使用  如果你需要 CommonJS 的支援，可以在 webpack.config.js 底下新增 flag 就可以了   不過要注意的是，目前僅有 webpackIgnore 這個 attribute 支援 CJS   1 2 3 4 5 6 7 8 9 module.exports = {   module: {     parser: {       javascript: {         commonjsMagicComments: true,       },     },   }, };   References     Webpack should have a way to ignore require calls #8826   Module Methods   module.parser.javascript.commonjsMagicComments  ","categories": ["random"],
        "tags": ["webpack","dynamic import","static import","cjs","service worker","webpack chunk","magic comment","webpack magic comment","webpackignore"],
        "url": "/random/webpack/",
        "teaser": null
      },{
        "title": "Kubernetes 從零開始 - Sidecar 與 Lifecycle Hook 組合技",
        "excerpt":"Multiple Container in Pod  誠如我們之前在 Kubernetes 從零開始 - 容器基本抽象 Pod | Shawn Hsu 裡面提到的  Pod 本身其實可以執行多個 Container，只是說平常大家習慣是一個 Pod 一個 Container 而已   不過也是有 use case 是會需要使用到多個 Container 的  比方說你可能會需要一個額外的 Container 負責執行背景程序，輔助功能取向的任務  Logging 或者是監控等任務就非常適合使用多個 Container 的架構   也有可能是因為 legacy 的關係，導致多個 Container 必須共用同一個 Pod 的資源   Multiple Container Pattern  而多 Container 的架構在 Kubernetes 來說有幾個常見的 pattern                  Type       Ambassador       Adapter       Sidecar                       Description       透過 Ambassador Container 負責轉發對外請求，封裝內部邏輯       將 Container 的輸出進行轉換，類似攔截器的設計       擴充現有 Container 的功能                 Example          ref: Multi-Container Pod Design Patterns          ref: Multi-Container Pod Design Patterns          ref: Multi-Container Pod Design Patterns           其實你會發現說，每一種 Container Pattern 都長得很像  注意到，他們只是實作的方式很像，但最終要達成的目的是不同的   Introduction to Sidecar Container  Sidecar Container 是與 main application container 一起執行的 輔助容器  他們多半使用提供輔助功能如 Logging, Monitoring 等等的任務   其實不論是 Ambassador 還是 Adapter, 他們皆使用多個 Container 的架構  你可以說，Sidecar 是最常見的一種，因為 Ambassador 與 Adapter 某種程度上來說也是 擴充了原本 Container 的功能      也因此現在 Sidecar Container 有點變成是個統稱了    他要怎麼達到擴充的功能？  別忘了，在同一個 Pod 底下，所有的 Container 共享所有的資源，包含 Network, CPU/Memory 以及 Volume 等等  因此，Sidecar Container 可以透過存取其共享資源以達到擴充的目的  舉例來說，如果要擴充 Logging 的功能，你可以設定 Logger 除了顯示在 console 上，也可以額外 Fan-out 到檔案裡頭，然後透過 Sidecar Container 負責後續的處理  或是以攔截封包來說，可以額外掛一個 tcpdump 截取所有執行過程中的封包   Different Ways to Implement Sidecar  The Old Way  在 Kubernetes 1.28 之前，你能夠定義多個 container 在 spec.containers 底下  不過這樣的問題在於，假設你需要控制啟動順序，你是沒辦法透過內建的機制來達成的  這種做法只適合他們必須要一起 co-work 的場景   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 spec:   containers:   - name: nginx       image: nginx       ports:       - name: http           containerPort: 80       volumeMounts:       - name: web           mountPath: '/usr/share/nginx/html'   - name: refresh       image: alpine/git       command:       - sh       - -c       - watch -n 60 git pull       workingDir: /usr/share/nginx/html       volumeMounts:       - name: web           mountPath: '/usr/share/nginx/html'   The New Way  新版本的 Sidecar Container 則是屬於 initContainer 的 special case  與其讓他執行完就退出，Sidecar Container 引入了 container-level 的 restartPolicy 的欄位  將其設定為 Always 就可以讓他一直存活下去  其餘寫法則與一般的 initContainer 一樣   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 spec:   containers:   - name: myapp       image: alpine:latest       command: ['sh', '-c', 'while true; do echo \"logging\" &gt;&gt; /opt/logs.txt; sleep 1; done']       volumeMounts:       - name: data           mountPath: /opt   initContainers:   - name: logshipper       image: alpine:latest       restartPolicy: Always       command: ['sh', '-c', 'tail -F /opt/logs.txt']       volumeMounts:       - name: data           mountPath: /opt   volumes:   - name: data       emptyDir: {}   Sidecar Container ImagePullPolicy  我們知道，設定為 Always 的 initContainer 是 Sidecar Container  如果你沒有指定 ImagePullPolicy 會發生什麼事情？   根據 Default image pull policy 所述      if you omit the imagePullPolicy field, and you specify the digest for the container image, the imagePullPolicy is automatically set to IfNotPresent.  if you omit the imagePullPolicy field, and the tag for the container image is :latest, imagePullPolicy is automatically set to Always.  if you omit the imagePullPolicy field, and you don’t specify the tag for the container image, imagePullPolicy is automatically set to Always.  if you omit the imagePullPolicy field, and you specify a tag for the container image that isn’t :latest, the imagePullPolicy is automatically set to IfNotPresent.    簡易好讀版就是     digest :arrow_right: IfNotPresent   :latest :arrow_right: Always   沒有 tag :arrow_right: Always   指定 tag 但不是 :latest :arrow_right: IfNotPresent   如果你的 initContainer 剛好使用 :latest tag, 你又剛好沒設定 imagePullPolicy 的話  那麼 imagePullPolicy 會被自動設定為 Always  換句話說，本來你可能是拿來做啟動檢查的 initContainer 會被自動升級成 Sidecar Container   而這顯然是有問題的，所以在使用 Sidecar Container 的時候，建議你還是明確的設定 imagePullPolicy   Sidecar Container Feature Gate  不過注意到，Kubernetes 1.28 仍需要手動開啟相對應的 feature gate 才能使用      Kubernetes 1.28 adds a new restartPolicy field to init containers that is available when the SidecarContainers feature gate is enabled.    你可以透過以下指令檢查是否已經開啟  1 2 3 4 5 6 7 $ kubectl get --raw /metrics | grep kubernetes_feature_enabled | grep Sidecar kubernetes_feature_enabled{name=\"SidecarContainers\",stage=\"ALPHA\"} 0  $ kubectl version  Client Version: v1.30.1 Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3 Server Version: v1.28.15+k3s1  可以看到說這台機器是 1.28 的版本，並且 feature gate 是關閉的(0: off, 1: on)      至於說 1.29 版本，則是預設開啟的  1 2 $ kubectl get --raw /metrics | grep kubernetes_feature_enabled | grep Sidecar kubernetes_feature_enabled{name=\"SidecarContainers\",stage=\"BETA\"} 1      如果你是使用 k3d 來建立 cluster 的話，可以透過以下指令來手動開啟 feature gate  1 2 3 4 $ k3d cluster create mycluster \\     --image rancher/k3s:v1.28.15-k3s1 \\     --k3s-arg '--kube-apiserver-arg=feature-gates=SidecarContainers=true@server:*' \\     --k3s-arg '--kubelet-arg=feature-gates=SidecarContainers=true@agent:*'      如果是 K3s 只要開 --kube-apiserver-arg 即可       Sidecar Container Lifecycle  既然是作為輔助容器，很多時候你會要求     在主容器啟動之前就先啟動   在主容器退出之後才退出      比方說你要監聽所有出入封包，你肯定是不想要漏掉最前面以及最後面那幾個封包的  這時候啟動順序就很重要了    注意到 Sidecar Container 的生命週期是與 main application container 脫鉤的  Sidecar Container 擁有自己獨立的生命週期，他是可以被單獨啟動，終止甚至重啟的  他沒辦法影響到其他 initContainer 的狀態，也沒辦法影響到 main application container 的狀態  但是可以被反過來影響(可參考 Pod Termination)   Pod Initialization  針對啟動的部分，因為 Sidecar Container 作為 initContainer 的 special case  他本質上也是繼承了 initContainer 的特性，也就是說     他一定會按照 spec.initContainers 的正順序來啟動   也表示他一定會在主容器之前啟動(因為只有 initContainer 都執行完畢，主容器才會啟動)      你甚至可以將 一般 Init Container 與 Sidecar Container 混合在一起    Pod Termination  當 main application container 要被終止的時候，Kubelet 會先處理 main application container 的終止流程  直到它完成之後，才會開始處理 Sidecar Container 的終止流程   terminate 的流程剛好是 反過來的，會是 spec.initContainers 的逆順序(也就是先進後出)  一個一個的停止，直到所有的 Sidecar Container 都終止為止      Sidecar Container 的 terminate 流程並不一定能完整執行  Container 終止是有一個 deadline 的(稱為 terminationGracePeriodSeconds)  如果超過這個時間，Kubernetes 會強制終止 Container    Sidecar Container Probes  Sidecar Container 是支援 Probe 的  因為他是個 long live 的 container 嘛，所以該有的 Liveness, Readiness 都有   有關 Probe 的討論可以參考 Kubernetes 從零開始 - Self Healing 是如何運作的 | Shawn Hsu   Sidecar in Different Workloads  前面有提到說，Sidecar Container 的生命週期是與 main application container 脫鉤的  Sidecar Container 不能影響到其他人，但別人可以影響到他      可參考 Sidecar Container Lifecycle    以 Job 來說，只要 main application container 終止了，Sidecar Container 也會跟著終止  但如果是 Deployment，因為他會有重啟的機制，所以 Sidecar Container 也會跟著重啟  你的 Sidecar Container 就會執行無數次      有關不同 Workload 的特性可以參考 Kubernetes 從零開始 - 高階抽象 Workload Resources | Shawn Hsu    Container Lifecycle Hook  Lifecycle Hook 就是可以允許你額外在特定階段執行一些額外的邏輯  目前 Kubernetes 提供了兩種 Hook 的機制                  Type       Description       Time                       postStart       在 Container 啟動之後執行       與 Container 的啟動流程同步，但不保證誰先執行完                 preStop       在 Container 被終止之前執行       preStop Hook 會在 Container 終止之前執行，但不一定可以執行完              還有額外的一種 stopSignal 可以 overwrite 掉 container 的預設終止訊號  ref: Define custom stop signals    你能夠設定以下不同的 Hook Handler 在上述時間點上執行                  Type       Description       Executor                       Exec       執行一個命令       Container                 HTTP       對一個 HTTP 端點發送請求       Kubelet                 Sleep       暫停一段時間       Kubelet           Lifecycle Hook Delivery  以上兩種 lifecycle hook 的機制，保證會至少執行一次(i.e. at least once)  在某些極端情況下，hook 會被執行超過一次  也因此，在設計 hook 的時候，你需要將其設計為 idempotent 的   Example  1 2 3 4 5 6 7 8 9 10 11 12 volumeMounts:   - name: data     mountPath: /opt lifecycle:   preStop:     exec:     command:       [         \"sh\",         \"-c\",         'printf \"$(date +\"%T.%N\") stopping prestop\\n\" &gt;&gt; /opt/logs.txt',       ]   Lifecycle Hook Log  你可能會發現說，preStop 的 printf 並不會顯示在 kubectl logs 上  Kubernetes 並不會將 Hook Handler Log 接上 Pod Event  所以無論如何你都看不到執行的 Log   取而代之的是會 broadcast 相對應的 Pod Event，如果失敗     preStop 會 broadcast FailedPreStopHook event   postStart 會 broadcast FailedPostStartHook event   怎麼觸發呢？ 把指令改成壞的就可以了      Tcpdump Sidecar Container Example  Environment  1 2 3 $ k3d --version k3d version v5.7.1 k3s version v1.29.6-k3s1 (default)   Example  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 apiVersion: batch/v1 kind: Job metadata:   name: myjob spec:   template:     spec:       containers:         - name: main-container           image: curlimages/curl           command: [\"sh\", \"-c\", \"curl google.com\"]           volumeMounts:             - name: data               mountPath: /opt       initContainers:         - name: upload-logs           image: nicolaka/netshoot           restartPolicy: Always           command: [\"sh\", \"-c\", \"sleep 86400\"]           volumeMounts:             - name: data               mountPath: /opt           lifecycle:             preStop:               exec:                 command:                   [                     \"sh\",                     \"-c\",                     \"curl -X POST https://webhook.site/bee40c4c-07a5-42bb-b2af-c24e3f4ea693 --data-binary @/opt/tcpdump.pcap\",                   ]         - name: tcpdump           image: nicolaka/netshoot           restartPolicy: Always           command: [\"sh\", \"-c\", \"tcpdump -i any -w /opt/tcpdump.pcap\"]           volumeMounts:             - name: data               mountPath: /opt       restartPolicy: Never       volumes:         - name: data           emptyDir: {}      Webhook 的測試站點可以參考 Webhook.site    以上是一個簡易的 Sidecar Container 的範例  這裡我初始化了兩個 Sidecar, 一個是負責 tcpdump 的封包擷取，一個是負責將封包傳送到 Webhook 上  而主容器則是負責發出請求   注意到 upload-logs 是排第一個  前面提到 Sidecar Container 啟動以及終止的順序 是跟 spec.initContainers 的順序有關  這裡 upload-logs 是排第一個是因為我想要讓他在 terminate 的時候是 最後執行的   The Reason “Killing”     在 describe pod 的時候  奇怪？ 為什麼 upload-logs 以及 tcpdump 都有一個 Killing Event 呢？  答案其實也挺簡單的，因為 Sidecar Container 也是會被 Terminate 的(在 main container 之後)  所以他只是單純的通知 Sidecar Container 要終止了   The SIGTERM and SIGKILL Signal  在 Lifecycle Hook Log 有提到說，你是看不到任何的 log 的  你頂多只能根據 Event 來判斷 Sidecar Container 的狀態  但也只是知道有沒有失敗而已，很明顯這樣是不足以滿足我們的需求的   我就好奇啦 到底我的兩個 Sidecar 的狀態如何  $ kubectl get pod myjob-xxx -o yaml &gt; log.yaml 指令可以更詳細的看到 Pod 的狀態   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 containerStatuses:   - containerID: containerd://83d436fad827bb0568994380dcc771abdfcf406366148560a6484e787a2ad2ea     image: docker.io/curlimages/curl:latest     imageID: docker.io/curlimages/curl@sha256:d43bdb28bae0be0998f3be83199bfb2b81e0a30b034b6d7586ce7e05de34c3fd     lastState: {}     name: main-container     ready: false     restartCount: 0     started: false     state:       terminated:         containerID: containerd://83d436fad827bb0568994380dcc771abdfcf406366148560a6484e787a2ad2ea         exitCode: 0         finishedAt: \"2025-05-17T19:40:44Z\"         reason: Completed         startedAt: \"2025-05-17T19:40:44Z\"   hostIP: 192.168.97.3   hostIPs:   - ip: 192.168.97.3   initContainerStatuses:   - containerID: containerd://7d9c1ee00a12e5e078f34a5acdb4c823f8640a66285833344436b13a10ba16b7     image: docker.io/nicolaka/netshoot:latest     imageID: docker.io/nicolaka/netshoot@sha256:a20c2531bf35436ed3766cd6cfe89d352b050ccc4d7005ce6400adf97503da1b     lastState: {}     name: upload-logs     ready: false     restartCount: 0     started: false     state:       terminated:         containerID: containerd://7d9c1ee00a12e5e078f34a5acdb4c823f8640a66285833344436b13a10ba16b7         exitCode: 137         finishedAt: \"2025-05-17T19:41:14Z\"         reason: Error         startedAt: \"2025-05-17T19:40:37Z\"   - containerID: containerd://614b4873d23538f37d9f45cb26b084361d5da5a27bc943a149272ac876d58236     image: docker.io/nicolaka/netshoot:latest     imageID: docker.io/nicolaka/netshoot@sha256:a20c2531bf35436ed3766cd6cfe89d352b050ccc4d7005ce6400adf97503da1b     lastState: {}     name: tcpdump     ready: false     restartCount: 0     started: false     state:       terminated:         containerID: containerd://614b4873d23538f37d9f45cb26b084361d5da5a27bc943a149272ac876d58236         exitCode: 0         finishedAt: \"2025-05-17T19:40:44Z\"         reason: Completed         startedAt: \"2025-05-17T19:40:40Z\"   phase: Succeeded   podIP: 10.42.1.64   podIPs:   - ip: 10.42.1.64   qosClass: BestEffort   startTime: \"2025-05-17T19:40:34Z\"   你要關心的是裡面的 state  可以看到說     main-container: exitCode 0, 正常結束   upload-logs: exitCode 137, 被強制終止   tcpdump: exitCode 0, 正常結束   恩？ 恩？？？  為什麼 upload-logs 會被強制終止呢？   Pod Lifecycle  我想要延伸一下 Pod Termination 的內容  概念上還是一樣的，main container 結束之前，Sidecar Container 會繼續執行  main container 結束之後才會開始處理 Sidecar Container   Kubernetes 要終止 Container 是發送 SIGTERM 的訊號(這也解釋了為什麼你在 describe pod 的時候會看到 Killing Event)  而刪除這個動作並非是你可以慢慢做，他是有一個時間限制的(terminationGracePeriodSeconds, 預設 30 秒)   這個 terminationGracePeriodSeconds 是 所有 main container + 所有 Sidecar Container + 所有 lifecycle preStop 的終止時間總和，他們是共享的  也就是說如果 main container 花了比較久的時間執行，那剩餘的時間就不多了  如果真的執行不完怎麼辦？ Kubernetes 會強制終止(SIGKILL)   對於 Kubernetes 來說，Sidecar Container 能不能 Graceful shutdown 並沒那麼重要   所以整體流程會是     SIGTERM 通知 Container 要終止了   等待 terminationGracePeriodSeconds 的時間   如果 Container 還沒結束，則強制終止(SIGKILL)   The Exit Code  所以顯然，upload-logs 的 exit code 並不是巧合  而實際上也的確是因為他被強制終止了      注意到這裡的 exit code 是 Sidecar Container 本身的 exit code  並不是 lifecycle hook 的 exit code(他會用 Event 表示, 可參考 The Reason “Killing”)       SIGTERM 的 exit code 是 143 = SIGNAL 15(128 + 15)   SIGKILL 的 exit code 是 137 = SIGNAL 9(128 + 9)   雖然根據 Differences from application containers 裡面它有提到說      So exit codes different from 0 (0 indicates successful exit), for sidecar containers are normal on Pod termination and should be generally ignored by the external tooling.    但 沒道理啊  為什麼單純的 sleep 會被強制終止？  理應他會先收到 SIGTERM 然後如果真的超過時間，才會 SIGKILL   原因在於，sleep 86400 在 Sidecar Container 裡面被覆寫為 pid 1  pid 1 的 init process 要負責處理 SIGTERM 的訊號(以及通知 child process 終止)  很明顯，單純的 sleep 並沒有處理這個訊號  也因此他最終是被 SIGKILL 強制終止的   我們可以利用 trap 來處理特定的訊號，以這個例子來說，就是 SIGTERM  然後將 sleep 指令放到 background process 並使用 wait 來攔截訊號      放到背景執行是因為，foreground process 會阻塞訊號直到完成    所以改起來會是  1 $ trap 'exit 0' TERM; sleep 86400 &amp; wait   這樣子，所有的 Sidecar Container 都能夠正常的結束     以上，我們已經完全了解 Sidecar Container 的全部機制了   Comparison with Different Container Types                  Type       Application       Init       Sidecar                       Definition location       spec.containers       spec.initContainers       spec.containers or spec.initContainers                 Lifecycle       Independent       Dependent       Independent                 Probes       :heavy_check_mark:       :x:       :heavy_check_mark:                 Share Resources       :heavy_check_mark:       :heavy_check_mark:       :heavy_check_mark:                 Communication       Bidirectional       Unidirectional       Bidirectional           References     Multi-Container Patterns in Kubernetes: Adapter, Ambassador, Sidecar   Ambassador Container Pattern   Sidecar Container Pattern   Multi-Container Pod Design Patterns   Differences between Sidecar and Ambassador and Adapter pattern   Sidecar Containers   Container Lifecycle Hooks   Attach Handlers to Container Lifecycle Events   Pod Lifecycle   SIGTERM: Linux Graceful Termination | Exit Code 143, Signal 15   “trap … INT TERM EXIT” really necessary?   Cannot trap SIGINT and SIGTERM when using “sleep infinity” [duplicate]   shell中trap的使用   FAQ   Default image pull policy  ","categories": ["kubernetes"],
        "tags": ["sidecar","container pattern","ambassador","adapter","logging","monitoring","tcpdump","init container","liveness","readiness","probe","lifecycle hook","post start","pre stop","sigterm","sigkill","exit code","signal","trap","wait","sleep","background process","foreground process","netshoot","event","killing event","terminationGracePeriodSeconds","feature gate","image pull policy"],
        "url": "/kubernetes/kubernetes-sidecar/",
        "teaser": null
      },{
        "title": "神奇的演算法 - 分組的好朋友 Union Find",
        "excerpt":"Introduction to Union Find  Disjoint Set 是一種資料結構，用來管理一組互不相交的集合（disjoint sets）。每個集合中的元素都是唯一的，且不同集合之間沒有共同的元素。   而 Union Find 是 Disjoint Set 的另一個名稱，因為這個資料結構主要支援兩種操作：     Find: 查詢某個元素屬於哪個集合   Union: 將兩個集合合併成一個新的集合   Grouping with Hash Map  而如果只是單純分組，是不是能透過 hash map 達成就行了？  是沒錯，但是單純的 hash map 在處理不同組別合併的時候就會很麻煩   比如說 group 1 要跟 group 2 合併  你需要找出所有 group 2 的人，然後一一更改至 group 1  這部分是需要找尋所有的 element 然後更新分組號碼   每一次都需要 $O(n)$，這個效能並不好  並且多個群組的合併也需要更小心的操作  比如說 (group 0, group 1) 要跟 (group 2, group 3) 合併  這就會需要手動處理更多 corner case   How Disjoint Set Works  Disjoint Set 採取了不同的做法  既然是分組嘛，所以核心的思想是，相同組別的人，其編號必定相同  一開始每個元素都是自己一組的，每一組都有編號(可以假設 element n 是 group n)   那要怎麼合併呢？  前面提到相同組別一定擁有相同的編號  考慮以下例子，element 1 與 element 2 合併就會是  1 2 3 4 5 6 7 element 1: group 1 element 2: group 2  to  element 1: group 1 element 2: group 1   我們是不是可以把這個看做是 Tree 呢？  每一個組別都是一個巨大的樹狀結構，而編號就是 根節點  也就是說，合併的過程其實就只是將 node 新增到該樹而已   不過要注意的是，要相連的是 兩元素的根節點  並不是單純的兩元素的 group number  原因在於 group number 有可能只是 child node :heavy_check_mark:   針對多個群組合併  1 2 3 4 5 element 1: group 1 element 2: group 1  element 3: group 3 element 4: group 3   假設 element 2 跟 element 3 合併  也就是會變成全部的元素都在同一組別底下  1 2 3 4 element 1: group 1 element 2: group 1 element 3: group 1 element 4: group 3 &lt;-- child node   但是你可以發現 element 4 的組別還沒更新  這個就呼應到說 group number 有可能只是 child node 的 case 了  所以在查詢 element 4 的組別的時候依然要查詢 根節點  這部分偏向 lazy loading 啦，有用到的才更新      那如果是更新 element 4 呢？ element 3 的組別號碼是不是也會出問題？  搭配 Tree Compression 會是正確的    Tree Compression  雖然說組別內部可能會存在 child node  他最終都會指向 根節點 沒錯，但如果子節點過多會導致查詢效率低落  因此我們需要對他做一定的優化   這個方法稱為 路徑壓縮  因為我們其實不太在乎 根節點以外的節點  我只想知道組別號碼是多少而已   1 2 3 4 5 6 7 8 9 10 func set(m []int, root int) ([]int, int) {     if m[root] == root {         return m, root     }      var parent int     m, parent = set(m, m[root])     m[root] = parent     return m, parent }   透過遞迴的方式逐一尋找根節點的數值，然後一一更新回去所有路徑上的子節點  這樣可以保證說查詢的時候只需要 look up 一次就可以得到正確答案   LeetCode 1061. Lexicographically Smallest Equivalent String  這題給定你兩個字串陣列，s1 與 s2 是可以互相置換的，也就是說 s1[i] == s2[i]  然後在給定你一個字串 baseStr，要求你使用上述置換陣列轉換 baseStr 使其結果為字典序最小   因為這個置換陣列是有可能存在 chaining 的關係的  比如說 'e' == 'o' 然後 'a' == 'e'，可以得到 'a' == 'o'  也因此你可以將置換陣列的內容分組，以上述來說 'a', 'e' 以及 'o' 為同一組  然後答案要是字典序最小的，那也很容易，分組編號設定為字典序最小的即可   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 func smallestEquivalentString(s1 string, s2 string, base string) string {     m := make([]int, 26)     for i := 0; i &lt; 26; i++ {         m[i] = i     }      for i := 0; i &lt; len(s1); i++ {         c1 := int(s1[i] - 'a')         c2 := int(s2[i] - 'a')          rootC1 := get(m, c1)         rootC2 := get(m, c2)                  if rootC1 &lt; rootC2 {             m[rootC2] = rootC1             m, _ = set(m, rootC2)         } else {             m[rootC1] = rootC2             m, _ = set(m, rootC1)         }     }      result := make([]string, 0)     for i := 0; i &lt; len(base); i++ {         result = append(result, string(get(m, int(base[i] - 'a')) + 'a'))     }      return strings.Join(result, \"\") }  func get(m []int, root int) int {     if m[root] == root {         return root     }     return get(m, m[root]) }  func set(m []int, root int) ([]int, int) {     if m[root] == root {         return m, root     }      var parent int     m, parent = set(m, m[root])     m[root] = parent     return m, parent }   你可以看到說，為了找到組別內字典序最小的字母，因此在合併的時候其實是有多了一個判斷的  1 2 3 4 5 6 7 if rootC1 &lt; rootC2 {     m[rootC2] = rootC1     m, _ = set(m, rootC2) } else {     m[rootC1] = rootC2     m, _ = set(m, rootC1) }   如果今天沒有這個限制，隨便指派一個並不會出問題  也就是針對一般的 Union Find 來說，你只需要 m[rootC1] = rootC2 即可  因為是更改最上層的組別編號，所以在查詢的時候依然可以得到正確答案   References     Disjoint-set data structure   Minimum spanning tree  ","categories": ["algorithm"],
        "tags": ["union find","disjoint set","tree compression","leetcode-1061","leetcode-1584"],
        "url": "/algorithm/algorithm-union-find/",
        "teaser": null
      },{
        "title": "玩轉 Syslog 第一次就上手",
        "excerpt":"Introduction to Syslog  任何運行中的系統都會有 Log，主要是為了方便除錯、監控以及分析等等的  可以說沒有 Log 的系統是沒有辦法運作的   在 網頁程式設計三兩事 - Logging 最佳實踐 | Shawn Hsu 中我們已經知道說針對 application 怎麼設計良好的 Log 系統  而光是擁有 application log 其實是不足的  你可能會需要更底層的 Log 來幫助你監控整體系統   比方說你也需要知道 Kernel 的 Log, 更甚至是 Router 的 Log  也就是說，擁有一個統整的 Log 系統在某些時候是重要的   Syslog 的概念就是，我擁有一個統一的系統，可以處理不同來源的 Log  將它統一收集，封存(audit 相關)起來，方便後續的分析與追蹤  這個統一的系統由於需要跨網路收集，因此是屬於 client-server 的架構  Syslog Server 負責接收 Client 的 Log 並做後續處理   Background  我個人覺得 Syslog 的歷史其實滿曲折的  當初是在 University of California, BSD TCP/IP 的系統上實作的  在後來 Syslog 的標準被廣泛的系統採納並支持，逐漸成為行業的標準  話雖如此，該 “標準” 其實並沒有正式成為 IETF 的標準  RFC 3164 中僅僅只是記錄了當時的實作方式      所以你可以看到 Category 歸類為 Informational    而真正納入 IETF 標準的是 RFC 5424  不過由於 RFC 3164 是當時的實作方式，為了相容性，現今仍然兩者並存居多   RFC 3164  1 &lt;PRI&gt;TIMESTAMP HOSTNAME TAG: MESSAGE   當時為了能夠區分哪一種資訊是 Syslog，因此他們刻意了定義了上述的格式  只要看到這種格式的資料，哦你就可以確定說他是 Syslog 這樣   整個 Log 分三段，分別是 PRI, HEADER, MESSAGE     PRI 是 Priority Value，可以參考 Syslog Priority   HEADER，包含 TIMESTAMP(format: MMM DD hh:mm:ss), HOSTNAME   MESSAGE，包含 TAG(service name 或是 process id), CONTENT                  Section       Field       Description                       PRI       Priority Value       可參考 Syslog Priority                 HEADER       TIMESTAMP       格式: MMM DD hh:mm:ss                 HEADER       HOSTNAME       只能是 hostname, ipv4 或是 ipv6，不能是 domain name                 MESSAGE       TAG       服務名稱或是 process id，只能是 字母數字                 MESSAGE       CONTENT       有用的資訊           舉例來說一個合法的 RFC 3164 的 Log 是這樣  1 &lt;34&gt;Oct 11 22:14:15 mymachine su: 'su root' failed for lonvick on /dev/pts/8   可以看到說，上述的 Message Tag 為 su  因為他只能包含字母數字，所以 : 會被歸類在 content 裡面   RFC 5424  我們提到 RFC 3164 僅僅只是個紀錄，他其實不算是一個標準  非標準的壞處就是大家各自有各自的實作方式，久而久之其實就會有相容性的問題  RFC 5424 的出現旨在淘汰舊的實作方式，並提供業界一套標準的格式   除了更改格式，RFC 5424 抽離了對 transport protocol 的依賴  在 RFC 3164 中，預設是走 UDP 傳輸  而這大大的限縮了 Syslog 的應用場景 應該說不夠彈性  RFC 5424 中，並沒有規定一定要使用哪一種傳輸協議  但是為了相容性，他還是支持 UDP 的傳輸方式   因為 RFC 3164 的格式其實能夠表達的東西不夠多  改良後的格式為  1 &lt;PRI&gt; VERSION TIMESTAMP HOSTNAME APP-NAME PROCID MSGID [SD-ID * (SD-PARAM-NAME=\"SD-PARAM-VALUE\")] MSG                  Section       Field       Description       Nil Value                       PRI       Priority Value       可參考 Syslog Priority       :x:                 VERSION       Version       1 用以標示 Syslog Protocol 的版本, 5424 是 1       :x:                 TIMESTAMP       Timestamp       格式為 RFC 3339       :heavy_check_mark:                 HOSTNAME       Hostname       格式為 RFC 1034，或是部分資料如 hostname, ip 等       :heavy_check_mark:                 APP-NAME       Application Name       服務名稱或是 process id，只能是 字母數字       :heavy_check_mark:                 PROCID       Process ID       只能是 字母數字       :heavy_check_mark:                 MSGID       Message ID       只能是 字母數字       :heavy_check_mark:                 SD-ID       Structured Data ID       只能是 字母數字       :x:                 SD-PARAM-NAME       Structured Data Parameter Name       只能是 字母數字       :x:                 SD-PARAM-VALUE       Structured Data Parameter Value       只能是 字母數字       :x:                 MSG       Message       log 訊息       :heavy_check_mark:           額外的補充資料是一個 array 的結構，本身可以是空的  1 [exampleSDID@32473 iut=\"3\" eventSource=\"Application\" eventID=\"1011\"][examplePriority@32473 class=\"high\"]   比方說上述擁有兩個部分 exampleSDID@32473 以及 examplePriority@32473  分組的用意在於說他底下的 key-value pair 都是屬於該區段的，給予較高的可讀性   舉例來說一個合法的 RFC 5424 的 Log 是這樣  1 &lt;34&gt;1 2025-06-20T01:27:42Z myhostname myapp 12345 99 - [exampleSDID@32473 iut=\"1\" eventSource=\"application\" eventID=\"1011\"] Test message content   Syslog Priority  Priority Value 是由 Facility 以及 Severity 組成  這個欄位的用意是告訴你這個 Log 的優先程度，哪個服務，有多嚴重這樣  你可以透過 PRI 的數值決定你的 Syslog 要怎麼處理      針對服務的數值，如果你沒有一個可以選，他有 reserved facility 可以使用，是 16 ~ 23    問題是，兩個數字怎麼組合起來  公式是 (Facility * 8) + Severity  所以你會得到一個 0 ~ 191 的數值  這個數值就是 PRI 的值                  Facility       Numerical Code               Severity       Numerical Code                       0       kernel messages               0       emergency                 1       user-level messages               1       alert                 2       mail system               2       critical                 3       system daemons               3       error                 4       security/authorization messages               4       warning                 5       messages generated internally by syslogd               5       notice                 6       line printer subsystem               6       informational                 7       network news subsystem               7       debug                 8       UUCP subsystem                                         9       clock daemon                                         10       security/authorization messages                                         11       FTP daemon                                         12       NTP subsystem                                         13       log audit                                         14       log alert                                         15       clock daemon                                         16 ~ 23       local use                                   Syslog Transport  既然他是 client-server 的架構，並且需要跨網路收集，很明顯傳輸是一項重要的事情   RFC 3164 中提到說，預設是走 UDP 傳輸  預設是 514 的 port，然後建議 server 以及 client 都使用這個 port      ref: RFC 5426(Transmission of Syslog Messages over UDP)    至於 RFC 5424 中，並沒有規定一定要使用哪一種傳輸協議  所以 TCP 或 UDP 都是可以的  不過標準提到他 必須支援 TLS-based 的傳輸方式(ref: RFC 5425)  但他只說要支援，你也可以 不要使用 TLS 的傳輸方式      ref: RFC 6587(Transmission of Syslog Messages over TCP)    Acknowledgment and Reliability  Syslog 本身是一個相對簡單的 protocol  所以它本身其實沒有任何關於 Acknowledgement 的機制  可靠性傳輸實際上是依靠上層傳輸協議的實作而定  如果不依賴 TCP 之類的，他其實是沒有辦法保證傳輸的可靠性   比如說 legacy 的 Syslog protocol 指定使用 UDP 傳輸  除此之外，數據正確性也是沒有保障的  即使擁有 Checksum 機制，由於 hash 碰撞的特性，同樣的 Checksum 也無法保證相同資料   Data Fragmentation  凡事碰到網路傳輸，我們其實最害怕的就是掉資料  Syslog 其實也會遇到這種問題，而前面提過說他實際上是依賴上層傳輸協議的實作  Syslog protocol 本身並沒有提供任何的機制來保證傳輸的可靠性   為了應對這種辦法，RFC 3164 中提到說  你應該盡量將資料縮小，建議的大小是 1024 bytes(在很古老的 Syslog 系統下小於 1024 會出問題)  雖然 UDP 理論上可以吃到 65535 bytes  不過他會遇到 Fragmentation 的問題      傳統 Syslog server/receiver 需要能夠處理 480 ~ 2048 bytes 的資料    UDP 是 IP 層以上的傳輸協議，他底層依賴於 IP 層的機制  雖然 UDP 可以一次送 65535 bytes 的資料  但是 IP 層會有 MTU 的限制，也就是說太大的資料本質上會被拆成很多小份資料傳輸  這其實會對 Syslog 有一定的影響，因為 會掉資料   還是要強調 Syslog 是很簡單的協定，他也不希望為了處理這種事情而變得複雜  不想掉資料又想要在一定程度上保持簡潔，所以他會建議每一筆的資料大小限制在 1024 bytes  這個大小是需要調整的，跨網路傳輸要設定成 網路上最小的 MTU 大小(因為要大家都能傳，所以是最小)  一次只傳一筆，一筆資料剛好是路徑上最小的 MTU 大小  這可以在最大程度上避免 Fragmentation 的問題  多筆資料也是不建議的，雖然 Syslog 本身有 Timestamp，不過如果遇到相同時間戳記仍然會有問題  也無法保證順序   那 TCP 呢？ Syslog 將所有他不想做的事情外包給 TCP 處理  事實上他也做得挺好的   UDP 是 message based 的傳輸協議，也就是傳一筆收一筆  TCP 則是 stream based 的傳輸協議，也就是傳一坨收一坨  你需要有一個辦法分別他的斷點在哪，常見的就是使用 換行 區隔  稱為 Non-transparent Framing   Non-transparent Framing  採用換行符號來區隔不同的資料實務上會有問題  因為傳統上那些特殊字元是沒有跳脫的  導致接收端在處理的時候會錯誤   比如說如果一筆資料裡面有多個換行符號  他就會被誤拆成多筆資料   Octet Counting  怎麼解決 Non-transparent Framing 的問題其實也滿簡單的  既然換行符號不嚴謹，我是不是能夠告訴你這個資料長度，然後再給你開頭  無論資料裡面是啥，都能夠正確處理？   具體來說就只是在開頭加上資料長度  而這出乎意料的好用，並且簡單   syslog-ng  接下來就讓我們使用 syslog-ng 把 Server 架設起來觀察  syslog-ng 預設的 config 是使用 default-network-drivers(可參考 syslog-ng Open Source Edition 3.16 - Release Notes)  長這樣      注意到你不需要額外撰寫 config, 這是預設的  阿如果要更改也一樣是改 /etc/syslog-ng/syslog-ng.conf 這個檔案    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 @version: 4.8 @include \"scl.conf\"  source s_local {         internal(); };  source s_network {         default-network-drivers(                 # NOTE: TLS support                 #                 # the default-network-drivers() source driver opens the TLS                 # enabled ports as well, however without an actual key/cert                 # pair they will not operate and syslog-ng would display a                 # warning at startup.                 #                 #tls(key-file(\"/path/to/ssl-private-key\") cert-file(\"/path/to/ssl-cert\"))         ); };  destination d_local {         file(\"/var/log/messages\");         file(\"/var/log/messages-kv.log\" template(\"$ISODATE $HOST $(format-welf --scope all-nv-pairs)\\n\") frac-digits(3)); };  log {         source(s_local);         source(s_network);         destination(d_local); };      要使用 default-network-drivers 需要有 @include \"scl.conf\" 這行    這個 default-network-drivers 可以接收來自以下的設定     514/tcp, 514/udp RFC 3164   601/tcp RFC 5424   6514/tls   跑起來就會是這樣      注意到同一個 port 可以 forward TCP 以及 UDP    1 2 3 4 5 6 7 $ docker run -d \\     --name syslog-ng-server \\     -p 514:514/tcp \\     -p 515:514/udp \\     -p 601:601 \\     balabit/syslog-ng:latest \\     --no-caps   Everything Compatible with RFC 3164?  測試基本的 3164 格式  1 $ echo \"&lt;34&gt;Oct 11 22:14:15 mymachine su: 'su root' failed for lonvick on /dev/pts/8\" | nc localhost 514        可以看到說他有被正確的讀取為 3164 的格式 TCP  然後各個欄位都有正確的被解析      可以看到 UDP 也是妥妥的     然後我就很好奇，如果只傳一個 hello world 過去會發生什麼事情  為了觀察沒有給定 PRI 的資料，config 要稍微更改一下  1 $ISODATE $PRI $HOST $(format-welf --scope all-nv-pairs)\\n  多加一個 \\$PRI 讓我們觀察         神奇了！ 居然會過？？？？  你可以看到說，timestamp、priority 甚至是 hostname 都有被正確的指派  這肯定是我們漏掉了什麼   其實 RFC 3164 中提到說，如果沒有給定以上三者的資料 他會自己幫你補齊     hostname 會填自己的 hostname(而 192.168.215.1 正是我的 container 的 IP)   timestamp 會填現在的時間(你可以看到資料時間其實是早於 Syslog connection accepted 的時間)   priority 會填 13(user-level + notice)      取得 container 的 IP 可以用 $ docker inspect &lt;container_id&gt;    也就是說，其實你隨便打都會過 3164 的格式  因為他會幫你自動帶入這些資料  所以其實不是不會驗格式，是他會幫你補齊   Invalid Frame Header with RFC 5424?  1 $ echo \"&lt;34&gt;1 2025-06-20T01:27:42Z myhostname myapp 12345 99 - [exampleSDID@32473 iut=\"1\" eventSource=\"application\" eventID=\"1011\"] Test message content\" | nc localhost 601      恩？ Invalid Frame Header  這其實是因為稍早我們提過的 Non-transparent Framing 的問題  所以我們要新增長度方便進行切割(i.e. Octet Counting)   上述資料長度可以透過以下指令取得  1 2 $ echo \"&lt;34&gt;1 2025-06-20T01:27:42Z myhostname myapp 12345 99 - [exampleSDID@32473 iut=\"1\" eventSource=\"application\" eventID=\"1011\"] Test message content\" | wc -c 139   所以完整的指令會是  1 $ echo -n \"139 &lt;34&gt;1 2025-06-20T01:27:42Z myhostname myapp 12345 99 - [exampleSDID@32473 iut=\"1\" eventSource=\"application\" eventID=\"1011\"] Test aessage content\" | nc localhost 601         References     What are Syslog formats?   syslog-ng Open Source Edition 3.16 - Release Notes   Syslog   Difference between host name and domain name   TCP stream vs UDP message   What is the minimum MTU of IPv4 68 bytes or 576 bytes?   Syslog 和 RFC 5424 分類的實際用途   How to get a Docker container’s IP address from the host  ","categories": ["random"],
        "tags": ["syslog","network","tcp","udp","syslog-ng","rfc","rfc3164","rfc5424","fragmentation","non-transparent framing","octet counting","priority","facility","severity","timestamp","hostname","app-name","procid","msgid","sd-id","sd-param-name","sd-param-value","msg","rfc-3339","rfc-1034","rfc-5426","rfc-6587","client-server","docker","nc","wc"],
        "url": "/random/syslog/",
        "teaser": null
      },{
        "title": "資料庫 - Delayed Queue 的設計與考量",
        "excerpt":"What is Delayed Queue?  Delayed Queue 是一種特殊的 message queue  與一般的 message queue 不同，Delayed Queue 裡面的資料並不會被立即取出  你可以對每個 message 設定一個延遲時間  只有當時間到了之後，資料才可以被 consumer 消費   CronJob and At Command  既然主要的目的是執行 “一次性的任務”，linux 的 at 指令很適合在這個場景下使用  at 本身就允許所謂的 later execution  使用者可以排定一個一次性任務並等待執行   但是對於分散式系統來說，at 並不能很好地滿足需求  原因在於它本身並沒有試錯重試的機制，失敗的會直接消失   到這裡，另一個想法是透過 CronJob 來實現  問題是我們需要的是 一次性的任務，而不是 定時的任務  你可能會說一樣設定 CronJob 但等他完成之後就刪除，這其實也是一種 anti-pattern  並且與 at 的缺點類似，你無法追蹤失敗的任務   Delayed Queue Implementation  Delayed Queue 的實作是非常看不同需求而定的  不過本質上，他們都需要一個不間斷的機制來監控資料本身(不論是主動推播還是使用 polling 的機制)      有關 polling 可以參考 淺談 Polling, Long Polling 以及其他即時通訊方法論 | Shawn Hsu                   Name       Concerns                       RabbitMQ Delayed Message Exchange Plugin       實作本身有單點失效的問題                 RabbitMQ TTL with DLX       per-message TTL 的彈性不夠, queue TTL 也同樣受限於 FIFO 的特性                 Netflix Dyno Queues       Dynomite 會使得整個系統變得相對厚重           RabbitMQ Delayed Message Exchange Plugin  RabbitMQ 官方有提供 rabbitmq-delayed-message-exchange plugin 用於實現 Delayed Queue 的功能   這個 delay 功能是做在 exchange 上面的  時間到了之後才會被往後丟到 queue 中(如果他沒辦法 route 到 queue 則會被丟棄, i.e. unroutable message)  而 delay 並非無限制的，最多大概可以到一兩天這樣，更久的就不建議  而你可以設定從 秒，分鐘，小時 等等的區間   Erlang Mnesia  套件 rabbitmq-delayed-message-exchange 是基於 Erlang Mnesia Database 實現的   Mnesia 速度快、效率高並且支援 transaction 以及 cluster replication  但是其缺點是故障恢復的機制較差   delay 的資料是儲存在 Mnesia table 之上的(Mnesia 本來是用於儲存 metadata 而非資料本身)  plugin 本身的實作是 single disk replica 的機制(注意到並非 Mnesia 本身的限制)  意味著，如果節點失效，所有 delay 的資料都會遺失  雖然 Mnesia table 對於節點重啟有良好的恢復機制  scheduled delivery 的 timer 會被重新安裝，所以在這個情況下還是會動的  只不過，單一節點失效 仍然是一個很大的問題   RabbitMQ TTL with DLX  根據 Scheduling Messages with RabbitMQ 的說法      For a while people have looked for ways of implementing delayed messaging with RabbitMQ. So far the accepted solution was to use a mix of message TTL and Dead Letter Exchanges as implemented by NServiceBus here.    在還沒有 rabbitmq-delayed-message-exchange 之前  delayed message 的實作是透過 TTL 以及 DLX 實現的   將 message 設定 TTL 放到 queue 中  不要取出，等待其到期之後由 DLX 將資料轉送到 DLQ 中  就可以達到 delayed message 的效果   TTL (Time-to-Live)  在 RabbitMQ 中，你可以設定所謂的 Time-to-Live(TTL)，顧名思義，就是 messages 可以在 queue 中存活多久  當超過 TTL 的時間，message 會被丟棄  所謂的丟棄就是訊息不會被路由到 consumer 身上      TTL 可以設定在 single queue, multiple queue 或是 per-message    至於說，哪時候會被丟棄呢？     Quorum Queue            訊息變成 Queue 的第一個元素的時候(Head of Queue)           Classic Queue            訊息變成 Queue 的第一個元素的時候(Head of Queue)       policy 設定的改變間接影響              無論是 Quorum Queue 還是 Classic Queue  他們都是 FIFO 的 queue    如果 TTL 是設定在 queue 上，那麼訊息就會依照順序被 TTL 掉  如果是在 message 上，事情就會比較複雜，因為每個 message 的 TTL 都不盡相同  比方說 E1 是 30 秒，E2 是 10 秒  即使 E2 的 TTL 比較短，他仍然需要等到 E1 被移除之後才會被丟棄   在這樣的情況下，E2 會多等 30 秒才會被丟棄  在使用 per-message TTL 的情況下需要額外注意   DLX (Dead Letter Exchange)  在 資料庫 - 從 Apache Kafka 認識 Message Queue | Shawn Hsu 中我們提到，DLQ 是將那些執行執行失敗的 message 最終要去的地方  而 RabbitMQ 內是透過 DLX 這個 exchange 將資料路由到 DLQ 的      沒有指定 exchange 會使用 default exchange  資料流會是 exchange 到 queue    一個 message 可以被 dead letter 的情況有     被 nack 掉(成功收到但是沒有辦法處理)   超過 TTL 的時間   因為超過 Queue 的長度導致 message 被丟棄   在 Quorum Queue 的情況下，message 被回傳的次數超過 delivery limit      如果是 Queue 本身 expired, 則 messages 並不會 被 dead letter    被 dead letter 的 message 會被轉送到指定的 routing key 上  如果沒有指定，就是原本的 routing key   Apache ActiveMQ  針對兩種實作 ActiveMQ Classic 以及 ActiveMQ Artemis 都支援 delayed message，只是實作方式不同   ActiveMQ Classic  ActiveMQ Classic 本身是採用 polling 的機制實現   mainloop 是一個無窮迴圈的 while loop  他並非有固定的 interval 去檢查，而是會根據資料狀態動態的調整  預設是 500ms, 但是他也會改成比如說，剩餘等待時間  既然是 polling 的機制，他有可能會 miss 掉 real time 的特性，透過動態調整 interval 可以很好的避免這個問題   1 2 long waitTime = nextExecutionTime - currentTime; this.scheduleTime.setWaitTime(waitTime);   ActiveMQ Artemis  ActiveMQ Artemis 則是使用 Java 內建的 ScheduledThreadPoolExecutor 實現  簡單來說呢，他可以排程一個 command，在     指定的時間執行一次   進行排程重複執行   當收到一個 delay message 的時候，就會計算出 delay 然後 schedule 下去  在 ScheduledDeliveryHandlerImpl.java#L190  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 private void scheduleDelivery(final long deliveryTime) {       final long now = System.currentTimeMillis();        final long delay = deliveryTime - now;        if (delay &lt; 0) {          if (logger.isTraceEnabled()) {             logger.trace(\"calling another scheduler now as deliverTime {} &lt; now={}\", deliveryTime, now);          }          // if delay == 0 we will avoid races between adding the scheduler and finishing it          ScheduledDeliveryRunnable runnable = new ScheduledDeliveryRunnable(deliveryTime);          scheduledExecutor.schedule(runnable, 0, TimeUnit.MILLISECONDS);       } else if (!runnables.containsKey(deliveryTime)) {          ScheduledDeliveryRunnable runnable = new ScheduledDeliveryRunnable(deliveryTime);           if (logger.isTraceEnabled()) {             logger.trace(\"Setting up scheduler for {} with a delay of {} as now={}\", deliveryTime, delay, now);          }           runnables.put(deliveryTime, runnable);          scheduledExecutor.schedule(runnable, delay, TimeUnit.MILLISECONDS);       } else {          if (logger.isTraceEnabled()) {             logger.trace(\"Couldn't make another scheduler as {} is already set, now is {}\", deliveryTime, now);          }       }    }   這個 scheduledExecutor 往上追     QueueImpl.java#L376   QueueFactoryImpl.java#L54   ActiveMQServerImpl.java#L3234   就是一個 ScheduledThreadPoolExecutor  相比於 ActiveMQ Classic 的 polling 機制，ActiveMQ Artemis 的實作依賴於語言本身的實作，可以避免 polling 帶來的 overhead   Netflix Dyno Queues  Netflix 的 Content Platform Engineering 也有使用 Delayed Queue 的需求  原本他們是使用 Cassandra 搭配 Zookeeper 實現的  不過他們很快發現了問題所在      Cassandra 使用 queue 的資料結構是 anti pattern   Distributed Lock 導致效能不佳(一次只能有一個 consumer，即使使用 shard，問題也只能暫時緩解)   而 Dyno Queue 的設計很好的解決了以上的問題  基於 Dynomite 搭配 Redis Sorted Set 的設計可以擁有以下特性     分散式的系統   不需要外部 lock 機制   非強制 FIFO   支援 sharding   At least once delivery      基本上 Dynomite 就是一個抽象封裝，底下可以替換不同的 storage engine  支援 multi-datacenter replication 達到高可用性    Redis Sorted Set  具體來說資料是儲存在 Sorted Set 之上的  因為我們要做 Delayed Queue 嘛，本質上就是根據時間排序的 Priority Queue  那要怎麼查詢 Delayed 的資料呢？      有關 priority queue 可以參考 神奇的演算法 - 為什麼你的 Priority Queue 那麼慢！ | Shawn Hsu    其實問題比想像中還簡單  在 Sorted Set 裡面的 key 肯定是跟時間有關  Dyno Queue 是將 時間 以及 priority 組合起來當作 key  要判斷一個資料是否 delay 就將 當前時間 與 max priority 做計算  並拿取 0 ~ score 之間的資料   為什麼是 0 ~ score 之間的資料呢？  因為你的 score 是當前時間，所以小於 score 的資料就是 delay 的資料  拿出來之後，為了避免資料遺失，所以他需要手動進行 ACK   也就是說，當你 pop 資料的時候，他會被移動到所謂的 unack set 中(並不是直接被移除)  手動 ACK 代表你已經處理完了這個資料，unack set 中的資料會被移除  如果你沒有 ACK 會發生什麼事情？   因為不確定你是否處理完這個資料，所以過一段時間之後  unack set 中的資料會被放回 Delayed Queue 當中(這是透過一個 background job 定期去檢查的)  有了這種機制，基本上就可以保證 At least once delivery 的特性   而 Redis In memory 以及 Single Thread 的特性，使得其滿足 Netflix 團隊對於 Delayed Queue 的需求      有關 Redis 可以參考 資料庫 - Cache Strategies 與常見的 Solutions | Shawn Hsu    References     Netflix 開發的 Delayed Queue   Distributed delay queues based on Dynomite   Scheduling Messages with RabbitMQ   Metadata store   Delay interval predictability   rabbitmq-delayed-message-exchange   Time-to-Live and Expiration   RabbitMQ Queue Types Explained   Dead Letter Exchanges   Differences From ActiveMQ 5  ","categories": ["database"],
        "tags": ["delayed queue","message queue","rabbitmq","redis","linux at","cronjob","linux atd","dynomite","cassandra","zookeeper","mnesia","erlang","ttl","dlq","dlx","quorum queue","classic queue","priority queue","polling","transaction","cluster replication","ack","nack","dead letter","dyno queue","sorted set","fifo","sharding","activemq","activemq classic","activemq artemis","java","scheduledthreadpoolexecutor","mainloop"],
        "url": "/database/database-delayed-queue/",
        "teaser": null
      },{
        "title": "Kubernetes 從零開始 - 部署策略 101",
        "excerpt":"Scaling Workloads  只有單台機器服務的情況下，多數是不足的  因為你需要考量到，比如說服務突然的不可用(軟體錯誤、硬體故障等等的)，或者是流量真的大到一台機器處理不來的情況  這時候你就會需要用到 “scaling” 的概念   在 資料庫 - 初探分散式資料庫 | Shawn Hsu 中，我們有大概的看過一些概念     scale out            透過將 application 分散到不同的機器上面，解決單台機器硬體資源不足的問題           scale up            單純的增加單台機器上的硬體資源如 CPU, Memory 等等           如今的系統架構，基本上都使用 scale out 的概念居多  原因在於說，單台機器的硬體資源是有上限的，你的 CPU 跟 Memory 不可能無限增加，相反的，多台機器可以解決這個問題(你可以無限增加機器數量，把它連起來就好)      Node.js PM2 的 cluster mode 也是利用了 scaling 的概念       scale out 要注意的是，你的服務最好是 stateless 的  stateful 其實也可以，只是資料一致性等等會是個滿大的問題  而 HTTP server 基本上都是 stateless 的，所以你可以很輕鬆的 scale out       還有一個很常見的誤區，monolithic 架構是可以 scale 的  不是只有 microservice 架構可以 scale    Manual Scaling  有一個真實的例子是這樣子的  行銷團隊預計在晚上的時候向使用者推播一個新消息  然後訊息內容裡面包含一個到官網的連結，告訴使用者說官網全新改版   所以其實我們可以預期說，在晚上的時候網站的流量應該會增加，是比平常還要多的那種程度  讓機器自己 scale 肯定是沒有問題的，但是我們都忽略了一個至關重要的細節  也就是雲端自己把機器建立起來，執行起來是需要花時間的  這個等待時間是致命的   因為可能第二台機器上線了，然後因為這個 delay，使用者早就看完並離開，流量又下去了  然後他什麼也沒做就又被迫下線，然後使用者體驗到的是一個卡到爆的網站  可能行銷帶來的效益就不是那麼足夠了   也因此，針對這種 已知會有流量增加的 的情況，比較好的方式是事先手動增加服務容量  換句話說，在行銷團隊要發送推播的時候，工程團隊就必須先手動調整好伺服器的設定，用以應對接下來的流量  這樣使用者體驗到的就會是流暢的服務   Auto Scaling  相比之下，auto scaling 能夠自己根據目前不同的負載自動的調整所需要的資源      如果單純的 scale workloads 還無法滿足需求，那就需要進行 node 的 scale 了  可以參考 Node Autoscaling    Horizontal Pod Autoscaler(HPA)  在 Kubernetes 中，scale out 是透過 Horizontal Pod Autoscaler(HPA) 來實現的   HPA 是透過所謂的 control loop 去不定時的監控底下的資源(預設是 15 秒掃一次，但可以用 --horizontal-pod-autoscaler-sync-period 來調整)  具體來說是，在 HorizontalPodAutoscaler CRD 裡面，指定說你要監聽的 resource 是哪一個(apiVersion, kind 跟 name)  然後就看該 resource 的 metrics 做調整      對於那種不支援 scale 的 resource 如 DemonSet 是沒辦法使用 HPA 的    更具體一點來說，他是根據 metrics 決定 replica 的數量的   \\[desiredReplicas = \\lceil currentReplicas \\times \\frac{currentMetric}{desiredMetric} \\rceil\\]  這個公式中，透過計算 metric 的比值決定你需不需要調整 replica 的數量  完美的情況下，這個比值應該為 1，所以 desiredReplicas 會等於 currentReplicas  而因為 metric 的資料是即時且不斷變化的，replica 數量會被一直調整  造成所謂的 thrashing 現象(或稱 flapping)      flapping 的問題可以透過設定 Stabilization Window 以及 Tolerance 來調整    The Scaling Behavior  要如何 scale 是個好問題  replica 的數量如果忽高忽低，對底層的 infra 來說是一種負擔，何況是對使用者  因此，在 HPA 裡面，你可以細部調整 K8s 要如何 scale  具體來說，有 Scaling Policy, Stabilization Window 以及 Tolerance 可以調整   Tolerance  tolerance 本質上就是允許一定程度的上下浮動  讓 replica 的數量不會太過敏感  預設情況下是 10% 的誤差範圍，可以透過 --horizontal-pod-autoscaler-tolerance 來調整  或是 CRD 裡面直接指定 tolerance(可以參考 HorizontalPodAutoscalerSpec)   1 2 3 behavior:   scaleUp:     tolerance: 0.05 # 5% tolerance for scale up   Stabilization Window  stabilization window 也可以解決 flapping 的問題  因為 metric 的資料不間斷變化會造成 Pod 的數量不斷的上下浮動  既然數量會一直變化，其中的一個辦法是取一段時間區間，看說這段時間內，Pod 的數量如何  HPA 的算法會在資料區間，取得 最高的 replica 數量 使用   為什麼是最高？           如果流量一會高一會低，區間內最高值基本上就會是 持平 的趨勢  也就是說 Pod 並不會突然的被刪掉又被加回來            如果流量確實在減少，那麼計算出來的 desired replica count 肯定會隨著時間越來越少  每一次的 evaluation 如果都取最高值，最終也會呈現 緩降 的趨勢  如此一來便達到穩定的目的          stabilizationWindowSeconds 也可以設定 scaleUp 的狀況  只是不太常這樣用，因為 scale up 就代表很緊急了(客訴滿天飛)，所以不需要穩定期    1 2 3 behavior:   scaleDown:     stabilizationWindowSeconds: 300      可以在 CRD 裡面直接指定 stabilizationWindowSeconds(可以參考 HorizontalPodAutoscalerSpec)    Scaling Policy  另外的 scaling policy 也同樣可以應用於 scaling 的調整  有別於 Stabilization Window 以及 Tolerance 針對 replica 數量的決策  policy 主要著墨在 “如何調整”   1 2 3 4 5 6 7 8 9 behavior:   scaleDown:     policies:     - type: Pods       value: 4       periodSeconds: 60     - type: Percent       value: 10       periodSeconds: 60      可以在 CRD 裡面直接指定 policies(可以參考 HorizontalPodAutoscalerSpec)    假設計算出來的 desired replica 是 100 但是當前只有 10  意味著 90 個 Pod 要被啟動  一次性啟動這麼多 Pod，雖然是合法的，但你其實不確定這會不會造成問題  像我自己在本機跑 cluster 的時候，如果一次建立這麼多的 Pod，我是有感覺到電腦忽然變卡  而這種穩定性問題是需要被重視的   既然不建議一次啟動這麼多 Pod，分批也是可行的選項  所以這其實就是 scaling policy 要做的事情  透過定義 “如何” 調整，穩定的 scale 才是我們想要的   上述例子中     policy[0] 代表著 60 秒內只能調整 4 個 Pod   policy[1] 代表著 60 秒內只能調整總體 10% 的 Pod   你可以針對 scaleUp 以及 scaleDown 分別指定不同的 policy  然後如果你有多個 policies，可以額外設定 .spec.behavior.scaleUp.selectPolicy 決定要怎麼挑  是要在眾多 policies 中挑選 影響最大還是最小的(Min, Max)，又或者是禁用調整(Disabled)呢？      預設 selectPolicy 會挑影響最大的，也就是 Max    Metric Data  稍早在 Horizontal Pod Autoscaler(HPA) 提到，replica 數量是透過 metric 來決定的  可是還是很抽象，究竟要監聽什麼呢？  更具體的來說，你可以使用以下這些資源      Resource            一個最基本的判斷方式就是使用 CPU, Memory 的資源去判斷需不需要 scale    每個 scale target 底層一定都是執行 Pod，每個 Pod 用的資源都不一樣，多個 Pod 的資源會被 平均起來 計算進而得出 current metric                    不過，由於他會被平均起來，所以有可能總體是足夠的，但其中某個 Pod 累得半死也是有可能的                            ContainerResource            為了避免 Pod 平均帶來的誤差，你也可以指定在 container level 計算    其他都跟 Resource 一樣，只不過是看 scale target 底下的 Pod 底下的 container    而且也都是會被平均起來算出 current metric           Pods            我們稍早都是在看 CPU, Memory 的資源，那有沒有其他資源可以使用呢？    答案也是肯定的，只不過需要客製化(可以參考 Metrics APIs)    計算方式也一樣，會被平均起來做比較           Object            這種類型比較特別，他是監聽 single Kubernetes object 的 metric    文件上的例子是 Ingress 的 hits-per-second    在建立路由的時候，你的規則是定義在 Ingress Object 內，配合 Metrics APIs 可以搭配查詢說有多少流量進來           External            這種類型比較特別，他是監聽 external resource 的 metric    外部指的就是說，這個 metric 不是 Kubernetes 內建的，而是你自行定義的    比方說，consumer 的數是由 message queue 內部的資料數量決定，而當然他也是需要 Metrics APIs 的配合              可以參考 $ kubectl explain hpa.spec.metrics    公式裡的 current metric 已經定義好了  剩下的 desired metric 就相對簡單了   \\[desiredReplicas = \\lceil currentReplicas \\times \\frac{currentMetric}{desiredMetric} \\rceil\\]  就三種類型     averageUtilization: 平均的數字(百分比)   averageValue: 平均的數字   value: 單純的數字，沒有平均過的數值(適用於 Object 這種，比方說計算 hits-per-second 的時候)      Resource 搭配 averageUtilization 會變成平均的平均嗎？  是 current metric 先平均，得到 A 這個平均值，然後跟 desired metric 的平均值 B 相比較       可以參考 $ kubectl explain hpa.spec.metrics.{metric_type}.target    Metrics APIs  我們知道了要監聽什麼，監聽的資料從哪來又是個問題  HPA controller 是透過 Metrics API 取得資料的  而以下的設定是必要的     API Aggregation Layer 需要被啟用   Metrics API 需要被啟用。基本上來說，分成三大種            resource metrics: 基本上資料會從 metrics server 取得，資料內容會是 K8s object 的資料       custom metrics: 但是如果你需要自己客製化資料，就是 custom metrics。注意到他還是使用 K8s object 的資料       external metrics: 客製化，但是是外部的資料           舉例來說 kubernetes-sigs/prometheus-adapter 裡面包含了 custom metrics 以及 external metrics 的實現   Summary  HPA 的概念稍微複雜許多，你需要搞清楚不同資源下需要的設定檔是哪一種  並且需要意識到在 scaling 的過程中可能產生的問題  東西蠻多，需要多多消化   Vertical Pod Autoscaler(VPA)  在 Kubernetes 中，scale up(調整 Pod 的資源，比如說 CPU 跟 Memory) 是透過 Vertical Pod Autoscaler(VPA) 來實現的  VPA 並非內建的，是需要額外安裝的(kubernetes/autoscaler)   VPA 會需要額外的 metrics server 取得目前的資源使用情況  使用的部分，你會需要撰寫 VerticalPodAutoscaler 的 CRD 來實現  然後再 CRD 內部，指定你要監聽的 Resource 是什麼，比如說這個例子會是 hamster   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 # This config creates a deployment with two pods, each requesting 100 millicores # and trying to utilize slightly above 500 millicores (repeatedly using CPU for # 0.5s and sleeping 0.5s). # It also creates a corresponding Vertical Pod Autoscaler that adjusts the # requests. # Note that the update mode is left unset, so it defaults to \"Auto\" mode. --- apiVersion: \"autoscaling.k8s.io/v1\" kind: VerticalPodAutoscaler metadata:   name: hamster-vpa spec:   # recommenders field can be unset when using the default recommender.   # When using an alternative recommender, the alternative recommender's name   # can be specified as the following in a list.   # recommenders:    #   - name: 'alternative'   targetRef:     apiVersion: \"apps/v1\"     kind: Deployment     name: hamster   resourcePolicy:     containerPolicies:       - containerName: '*'         minAllowed:           cpu: 100m           memory: 50Mi         maxAllowed:           cpu: 1           memory: 500Mi         controlledResources: [\"cpu\", \"memory\"]     updatePolicy:       updateMode: \"Auto\" --- apiVersion: apps/v1 kind: Deployment metadata:   name: hamster spec:   selector:     matchLabels:       app: hamster   replicas: 2   template:     metadata:       labels:         app: hamster     spec:       securityContext:         runAsNonRoot: true         runAsUser: 65534 # nobody       containers:         - name: hamster           image: registry.k8s.io/ubuntu-slim:0.1           resources:             requests:               cpu: 100m               memory: 50Mi           command: [\"/bin/sh\"]           args:             - \"-c\"             - \"while true; do timeout 0.5s yes &gt;/dev/null; sleep 0.5s; done\"   當你執行以上 Deployment 與 VPA 之後，過一陣子你會發現到，你的 Pod 的資源使用情況會被 VPA 調整  注意到是 Pod 的 resources 被調整，而非 Deployment 的 resources 被調整   VPA Operate Mode  上述的例子中你可以看到 VPA 有所謂的運作模式                  Mode       Description                       Off       VPA 不會調整 Pod 的資源                 Initial       VPA 會在 資源建立的當下 調整 Pod 的資源，之後不會調整                 Recreate       會動態的調整資源，不論是建立還是更新都是 重新建立 新的 Pod                 Auto       目前是 Recreate Operate Mode           重新建立 Pod 這個行為有可能會影響到你應用程式的執行，所以他其實沒有那麼的適合  於是 Kubernetes VPA 團隊正在整合 In-place Pod Vertical Scaling 的技術  旨在不重新建立 Pod 的情況下調整其資源      恩？ 整合？  Kubernetes 本身已經有支援 Pod 的資源的 In-place 調整(可以參考 Resize Container Resources)  也就是說其實 Pod 的資源的 In-place 調整已經支援了，只是 VPA 的整合還沒有完成    Resize Container Resources  具體來說，Pod 的資源可以不需要重新啟動就可以調整資源  事實上在 Kubernetes 早在 1.27 中引入了 InPlacePodVerticalScaling feature gate  並於 1.33 正式進入 beta 階段      然後 kubectl 也需要至少 v1.32 才能使用 --subresource=resize 這個參數    1 2 $ kubectl get --raw /metrics | grep InPlacePodVerticalScaling kubernetes_feature_enabled{name=\"InPlacePodVerticalScaling\",stage=\"ALPHA\"} 0   所以要怎麼 resize 資源呢？  1 2 $ kubectl patch pod resize-demo --subresource resize --patch \\   '{\"spec\":{\"containers\":[{\"name\":\"pause\", \"resources\":{\"requests\":{\"cpu\":\"800m\"}, \"limits\":{\"cpu\":\"800m\"}}}]}}'      或者是直接 apply -f --subresource resize --server-side 會比較直覺    同一時間你會需要所謂的 resize policy  一個新定義的欄位 spec.containers[].resizePolicy   1 2 3 4 5 resizePolicy: - resourceName: cpu   restartPolicy: NotRequired - resourceName: memory   restartPolicy: RestartContainer      以上兩個資源的 restartPolicy 不一樣的情況下  會 fallback 成 RestartContainer，一個要重啟一個不要，那當然就是重啟囉(比較嚴格的 policy)    目前來說，Kubernetes 1.34 含以前，你只能更改 cpu 以及 memory 的資源  然後指定要不要進行重啟，如果沒指定，預設會是 NotRequired  會需要重啟的原因，比如說你的 application 需要重啟才能拿到更多 memory 之類的      而這個重啟很有意思的是，如果 Pod 等級的 restartPolicy 是 Never  在 resizePolicy.restartPolicy 就 只能指定為 NotRequired  resize 要重啟但是 Pod 不重啟顯然這樣會衝突    init container 這種沒辦法重啟的是無法使用 resize 功能的(但是 sidecar 可以，可參考 Kubernetes 從零開始 - Sidecar 與 Lifecycle Hook 組合技 | Shawn Hsu)    不過這個 resize 的要求很大程度上是看目前的狀態而定  不能說你要求了 1 個 CPU，結果現在只有 0.5 個 CPU，這樣是不行的  你可以根據 Pod 回給你的狀態來確認這件事情   Pod status condition 的內容會有以下新增     PodResizePending type            kubelet 已經確認了你的要求，但不能馬上滿足，根據 message 的內容你可以確切地知道為什麼會 pending       message                    reason: Infeasible: :arrow_right: 你的要求根本不可能滿足(比方說你要求的 CPU 資源超過該節點的總量)           reason: Deferred :arrow_right: 你的要求目前不可能滿足，但我可以重試(我把某某 Pod 刪掉就可以滿足你的需求之類的)                           PodResizeInProgress type            kubelet 已經確認你的要求並且正在調動資源，如果有出什麼意外，也會在 message 中告訴你              你會發現說，其實上面的狀態也不夠，因為完成是不會有狀態的，上述只會跟你講失敗的情況  所以另外一個欄位 status.observedGeneration 被引入了(他是 PodObservedGenerationTracking feature gate，於 1.33 引入)  generation 的目的是在於紀錄每一次的改動(i.e. metadata.generation)  而 status.observedGeneration 則是紀錄 kubelet 已經確認並完成的 generation                  .metadata.generation       .status.observedGeneration 以及 .status.conditions[].observedGeneration                                         如果你更改的 resource 間接改動到 QOS class 也是不行的         簡單來說，Quality of Service(QOS) Class 會根據 requests, limits 以及 container 的數量來決定  所以說你更改 resource 可能會間接改動到 QOS class       可以用以下指令開一台 cluster 來測試    1 2 3 4 5 6 $ k3d cluster create mycluster \\     --image rancher/k3s:v1.33.4-k3s1 \\     --k3s-arg '--kube-apiserver-arg=feature-gates=PodObservedGenerationTracking=true@server:*' \\     --k3s-arg '--kube-apiserver-arg=feature-gates=InPlacePodVerticalScaling=true@server:*' \\     --k3s-arg '--kubelet-arg=feature-gates=InPlacePodVerticalScaling=true@agent:*' \\     --k3s-arg '--kubelet-arg=feature-gates=PodObservedGenerationTracking=true@agent:*'      Event Driven Autoscaling  利用 Kubernetes-based Event Driven Autoscaling 可以實現 event based 的自動擴展  比方說可以根據 message queue 內的資料量、API 請求數量或是 metrics server 提供的資訊等等所謂 Scalers 來自動擴展   KEDA 並不是用來取代 HPA 或是 VPA，而是用來補足他們的不足  這個工具能夠提供多樣化的 scaler 讓你根據不同的資源進行調節  所以 KEDA 是沒辦法自己獨立運行的   更甚至你可以利用 KEDA 做到 off-peak hour 的調整  就是比如說，在晚上的時候因為大家都在睡覺，所以上網的人可能會比較少，然後你就可以把資源降低  KEDA 的 Cron Scaler 就滿適合的      Scaler 其實有很多種，包含我有看到 Redis, Kafka, PostgreSQL 等等的  你甚至可以自己客製化 scaler    Different Deployment Strategies  現在的服務基本上講究一個 zero downtime，要給使用者不間斷的服務  而為了達成這個目標，部署策略就需要經過設計   最簡單的部署方式就是停機維護  而停機維護是相對不友善的選擇，卻也是對開發者最友善的模式  龐大的資料升級，資料庫遷移等等的，停機維護可以將所有的風險降到最低  雖然仍然有辦法達成 zero downtime，但這部分的操作會需要團隊更細心的安排與規劃      有關 migration 可以參考 資料庫 - 新手做 Data Migration 資料遷移 | Shawn Hsu    比較保險的方式會是擁有多台機器逐台更新，也就是要求你的服務具備一定 scaling 的能力  因為至少在你更新 A 機器的時候，BCD 依然可以服務使用者  多台機器的情況下事情就變得有趣了，你會有不同的策略比如說 Canary Deployment 或者 Blue Green Deployment   Recreate and Rolling Update  停機維護最主要是指你的系統沒辦法兼容兩個不同版本同時運行  因為他有可能使用的 database schema 不同，同時運行會造成資料毀損等問題  次要則是一些比如說底層作業系統更新、網路設備更新抑或是硬體設備維修更新等等  雖然通常這會比較偏向 排程維護更新 的範疇，但廣義上仍然是屬於 Recreate(因為服務對使用者不可用)   與 Recreate 會造成所謂的 downtime 不同，Rolling Update 則是會確保系統一直是可用的狀態  我們提到，多台機器逐步更新是這類操作的關鍵，因為有不同機器繼續撐著運行，在使用者的角度看來，系統依然是可用的  使用 Rolling Update 需要確保你的系統有做好 “向後相容” 或 “向前相容” 的設計  這樣才能確保在更新過程中，系統依然可以正常運行   在 Kubernetes 中，你可以透過設定 spec.strategy.type 來決定使用哪種策略     Recreate: 先砍再建   RollingUpdate: 邊建邊砍            你也可以控制新舊 Pod 的數量，目的在於確保在更新過程中，系統依然可以正常運行                    spec.strategy.rollingUpdate.maxSurge 表示最多可以 比原本多出多少 Pod(預設 25%, 至多 125% 的 Pod 會是可用的)           spec.strategy.rollingUpdate.maxUnavailable 表示最多可以有多少 Pod 不可用(預設 25%, 至少 75% 的 Pod 會是可用的)                              預設的策略會是 RollingUpdate    他執行起來會長這樣  你可以很清楚的看出他的差別，Rolling Update 的過程中，系統依然是可用的  而 Recreate 的過程中，系統是不可用的(全部都下去)      Recreate 即使下線的時間很短，仍然會造成影響，因此也視為有 downtime          Canary Deployment  基本上 Rolling Update 是相對常見的策略  逐步的更新你的服務，使得其永遠可用      Rolling Update 的 use case，比方説我公司內部開發機的更新就是直接 rolling 上去，反正掛了也是內部不可用而已    但這個可用其實是相對薄弱的  你的服務是起來的，並不代表他有好好工作  比方說，服務內部有一個 bug, 他並不會讓服務直接掛掉，但是會造成結果錯誤  這種時候如果 Liveness 以及 Readiness 沒有設定好，就會造成 silent error  這並不是我們樂見的      有關 Liveness 以及 Readiness 可以參考 Kubernetes 從零開始 - Self Healing 是如何運作的 | Shawn Hsu    我們當然不希望全部都上了之後才發現有問題  最簡單的就是分流囉，有多套版本的服務，一部分的人用新的，一部分的用舊的  少部分的人先使用新版本，如果都沒有問題，逐步開放至全部使用者 是 Canary Deployment 的精髓  所以同一時間會有不同版本的服務在線上，透過 分流機制 將部分使用者導向新版本  為了避免 silent error 這種事情，分流可以很好的限縮錯誤範圍，當發現新版本有問題，影響也會是最小的   同一時間多版本在線上服務，一個重點是要確保服務本身是 向後相容的  不然你的資料會損毀，而這是最不想發生的狀況  預先執行資料遷移是個選擇，或者是用 Blue Green Deployment 來達成      有關 data migration 可以參考 資料庫 - 新手做 Data Migration 資料遷移 | Shawn Hsu    Blue Green Deployment  他跟 Canary Deployment 類似都是使用多版本的部署策略   差別在於，Blue Green 是一瞬間切換的  他 並不是逐步更新，而是將流量瞬間切換到新版本的系統上  所以你其實沒辦法小規模測試   因為不想要有多個資料來源，所以 Blue Green 事實上是維護兩套完全獨立的系統  一套舊系統(Blue)，一套新系統(Green)，兩者獨立運行  等到你確定新版本沒問題之後，更改路由設定，將全部的流量都導入新版本     你可能會問，兩套系統運行，那資料要怎麼同步？  最終的目的都是減少 downtime, 而 Blue Green 的舊系統會繼續服務  為了新系統切換的時候資料是近新的，所以通常會用 Kafka 之類的進行資料同步      有關 Kafka 可以參考 資料庫 - 從 Apache Kafka 認識 Message Queue | Shawn Hsu       有關 data migration 可以參考 資料庫 - 新手做 Data Migration 資料遷移 | Shawn Hsu    A/B Testing  另外一種部署策略我覺得是比較特殊的  他的目的不在於發布新的系統，而是為了測試方面的部署策略   A/B Testing 的目的，是 藉由部署不同的系統版本，窺探使用者的反應  當然，還是有新版本的系統被部署，只是要注意他的本質是 測試  比方說，有新版本的 UI，你想要知道使用者對於新版本的 UI 的反應如何  所以可以將 Ingress 的流量分切，讓小部分的人體驗新功能，搭配個問卷調查之類的     8 成的人依舊是使用舊版本   而 2 成的人是使用新版本   當你測試完成之後，再把流量復原然後就可以根據測試結果決定後續   Shadow Deployment  Shadow 類似於 A/B Testing，只是他不需要分流  他也是用於測試，只不過是將 production 流量複製一份出來測試  這樣的好處就是你可以很直接的觀察新版本的表現，而 完全不會影響到舊版本  response 則會全部被丟棄   Conclusion                  Strategy       Recreate       Rolling Update       Blue Green Deployment       Canary Deployment       A/B Testing       Shadow Deployment                       Purpose       部署       部署       部署       部署       測試       測試                 Downtime       :heavy_check_mark:       :x:       :x:       :x:                                 Risk Level       High       Medium       Low       Low       Low       Low                 Rollback Difficulty       High       Medium       Low       Low       Low       Low                 Release Unit       完整服務       單台機器       兩個不同環境       部分流量       部分流量       複製流量           How Kubernetes Handle Deployment Strategy  上述我們提到的部署策略中，有一些是同時部署兩個版本然後透過分流的方式達成  在 Kubernetes 中，如果不考慮其他的工具，基本上依靠 Service 就可以做到八成像了   1 2 3 4 5 6 7 8 9 name: frontend replicas: 3 ... labels:   app: guestbook   tier: frontend   track: stable ... image: gb-frontend:v3   1 2 3 4 5 6 7 8 9 name: frontend-canary replicas: 1 ... labels:   app: guestbook   tier: frontend   track: canary ... image: gb-frontend:v4   上述兩個不同的 Deployment 分別對應到新舊版本的應用程式(v3 以及 v4)  然後在 label 的部分，你可以看到他有共享相同的 label(i.e. frontend 以及 guestbook)  在 Service 的部分就可以使用這些相同的 label 選取  就能夠達成基本的分流，以 replica 的數量來判斷就是 3:1 的流量   1 2 3 selector:    app: guestbook    tier: frontend   Argo Rollouts  就如同我們上述討論的，Kubernetes 內建的 Rollout 功能雖然強大但是仍然有許多改進空間  比如說     沒辦法控制 Rollout 的速度   Liveness 以及 Readiness 沒辦法做更深度且全面的檢查   沒辦法依靠外部 Metrics 衡量更新   基於以上考量點，實務上在正式環境內部還是有太多的隱患存在  所以 Argo Rollouts 正是為了解決以上痛點而誕生的   本質上也是透過 Kubernetes Controller 以及 CRD 來實現的  比如說 Rollout CRD 就是 Deployment 的包裝，並且額外提供了一些功能  前面提到 Blue Green Deployment 以及 Canary Deployment 都是有支援的，但是也只支援這兩個      有關 CRD 可以參考 Kubernetes 從零開始 - client-go 實操 CRD | Shawn Hsu       有關 Kubernetes Controller 可以參考 Kubernetes 從零開始 - Informer 架構以及 Controller Pattern | Shawn Hsu    Architecture       ref: Architecture    基本上如果你熟悉 Kubernetes Controller 的運作方式，就不難理解  本質上，Controller 會監聽 Rollout CRD 的任何變化，並根據其內容調整相對應的資源  前面提到，雖然說他是包裝 Deployment，但是 Controller 並不會對原生 Deployment 有任何反應      有關 Kubernetes Controller 可以參考 Kubernetes 從零開始 - Informer 架構以及 Controller Pattern | Shawn Hsu    也跟 Deployment 一樣，底層 Argo Rollouts 會使用 ReplicaSet 來管理 Pod  為了能方便管理不同的版本，一些額外的 metadata 以及 labels 會被套用上去  並且需要搭配 Service 進行分流(Controller 會指派 unique hash 確保選擇到正確的 ReplicaSet)，Service Mesh 或者 Ingress 的 solution 如 Traefik、Nginx Ingress Controller 或 Istio 等等也可以搭配使用   另外你也可以將 Metrics 整合進來，檢查 Rollout 的狀態   Rollout CRD  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 apiVersion: argoproj.io/v1alpha1 kind: Rollout metadata:   name: example-rollout-canary spec:   # Number of desired pods.   # Defaults to 1.   replicas: 5   analysis:     # limits the number of successful analysis runs and experiments to be stored in a history     # Defaults to 5.     successfulRunHistoryLimit: 10     # limits the number of unsuccessful analysis runs and experiments to be stored in a history.     # Stages for unsuccessful: \"Error\", \"Failed\", \"Inconclusive\"     # Defaults to 5.     unsuccessfulRunHistoryLimit: 10    # Label selector for pods. Existing ReplicaSets whose pods are selected by   # this will be the ones affected by this rollout. It must match the pod   # template's labels.   selector:     matchLabels:       app: guestbook    # WorkloadRef holds a references to a workload that provides Pod template   # (e.g. Deployment). If used, then do not use Rollout template property.   workloadRef:     apiVersion: apps/v1     kind: Deployment     name: rollout-ref-deployment     # Specifies if the workload (Deployment) is scaled down after migrating to Rollout.     # The possible options are:     # \"never\": the Deployment is not scaled down     # \"onsuccess\": the Deployment is scaled down after the Rollout becomes healthy     # \"progressively\": as the Rollout is scaled up the Deployment is scaled down     # If the Rollout fails the Deployment will be scaled back up.     scaleDown: never|onsuccess|progressively    # Template describes the pods that will be created. Same as deployment.   # If used, then do not use Rollout workloadRef property.   template:     spec:       containers:         - name: guestbook           image: argoproj/rollouts-demo:blue    # Minimum number of seconds for which a newly created pod should be ready   # without any of its container crashing, for it to be considered available.   # Defaults to 0 (pod will be considered available as soon as it is ready)   minReadySeconds: 30    # The number of old ReplicaSets to retain.   # Defaults to 10   revisionHistoryLimit: 3    # Pause allows a user to manually pause a rollout at any time. A rollout   # will not advance through its steps while it is manually paused, but HPA   # auto-scaling will still occur. Typically not explicitly set the manifest,   # but controlled via tools (e.g. kubectl argo rollouts pause). If true at   # initial creation of Rollout, replicas are not scaled up automatically   # from zero unless manually promoted.   paused: true    # The maximum time in seconds in which a rollout must make progress during   # an update, before it is considered to be failed. Argo Rollouts will   # continue to process failed rollouts and a condition with a   # ProgressDeadlineExceeded reason will be surfaced in the rollout status.   # Note that progress will not be estimated during the time a rollout is   # paused.   # Defaults to 600s   progressDeadlineSeconds: 600    # Whether to abort the update when ProgressDeadlineSeconds is exceeded.   # Optional and default is false.   progressDeadlineAbort: false    # UTC timestamp in which a Rollout should sequentially restart all of   # its pods. Used by the `kubectl argo rollouts restart ROLLOUT` command.   # The controller will ensure all pods have a creationTimestamp greater   # than or equal to this value.   restartAt: '2020-03-30T21:19:35Z'    # The rollback window provides a way to fast track deployments to   # previously deployed versions.   # Optional, and by default is not set.   rollbackWindow:     revisions: 3    strategy:     # Blue-green update strategy     blueGreen:       # Reference to service that the rollout modifies as the active service.       # Required.       activeService: active-service        # Pre-promotion analysis run which performs analysis before the service       # cutover. +optional       prePromotionAnalysis:         templates:           - templateName: success-rate         args:           - name: service-name             value: guestbook-svc.default.svc.cluster.local        # Post-promotion analysis run which performs analysis after the service       # cutover. +optional       postPromotionAnalysis:         templates:           - templateName: success-rate         args:           - name: service-name             value: guestbook-svc.default.svc.cluster.local        # Name of the service that the rollout modifies as the preview service.       # +optional       previewService: preview-service        # The number of replicas to run under the preview service before the       # switchover. Once the rollout is resumed the new ReplicaSet will be fully       # scaled up before the switch occurs +optional       previewReplicaCount: 1        # Indicates if the rollout should automatically promote the new ReplicaSet       # to the active service or enter a paused state. If not specified, the       # default value is true. +optional       autoPromotionEnabled: false        # Automatically promotes the current ReplicaSet to active after the       # specified pause delay in seconds after the ReplicaSet becomes ready.       # If omitted, the Rollout enters and remains in a paused state until       # manually resumed by resetting spec.Paused to false. +optional       autoPromotionSeconds: 30        # Adds a delay before scaling down the previous ReplicaSet. If omitted,       # the Rollout waits 30 seconds before scaling down the previous ReplicaSet.       # A minimum of 30 seconds is recommended to ensure IP table propagation       # across the nodes in a cluster.       scaleDownDelaySeconds: 30        # Limits the number of old RS that can run at once before getting scaled       # down. Defaults to nil       scaleDownDelayRevisionLimit: 2        # Add a delay in second before scaling down the preview replicaset       # if update is aborted. 0 means not to scale down. Default is 30 second       abortScaleDownDelaySeconds: 30        # Anti Affinity configuration between desired and previous ReplicaSet.       # Only one must be specified       antiAffinity:         requiredDuringSchedulingIgnoredDuringExecution: {}         preferredDuringSchedulingIgnoredDuringExecution:           weight: 1 # Between 1 - 100        # activeMetadata will be merged and updated in-place into the ReplicaSet's spec.template.metadata       # of the active pods. +optional       activeMetadata:         labels:           role: active        # Metadata which will be attached to the preview pods only during their preview phase.       # +optional       previewMetadata:         labels:           role: preview      # Canary update strategy     canary:       # Reference to a service which the controller will update to select       # canary pods. Required for traffic routing.       canaryService: canary-service        # Reference to a service which the controller will update to select       # stable pods. Required for traffic routing.       stableService: stable-service        # Metadata which will be attached to the canary pods. This metadata will       # only exist during an update, since there are no canary pods in a fully       # promoted rollout.       canaryMetadata:         annotations:           role: canary         labels:           role: canary        # metadata which will be attached to the stable pods       stableMetadata:         annotations:           role: stable         labels:           role: stable        # The maximum number of pods that can be unavailable during the update.       # Value can be an absolute number (ex: 5) or a percentage of total pods       # at the start of update (ex: 10%). Absolute number is calculated from       # percentage by rounding down. This can not be 0 if  MaxSurge is 0. By       # default, a fixed value of 1 is used. Example: when this is set to 30%,       # the old RC can be scaled down by 30% immediately when the rolling       # update starts. Once new pods are ready, old RC can be scaled down       # further, followed by scaling up the new RC, ensuring that at least 70%       # of original number of pods are available at all times during the       # update. +optional       maxUnavailable: 1        # The maximum number of pods that can be scheduled above the original       # number of pods. Value can be an absolute number (ex: 5) or a       # percentage of total pods at the start of the update (ex: 10%). This       # can not be 0 if MaxUnavailable is 0. Absolute number is calculated       # from percentage by rounding up. By default, a value of 1 is used.       # Example: when this is set to 30%, the new RC can be scaled up by 30%       # immediately when the rolling update starts. Once old pods have been       # killed, new RC can be scaled up further, ensuring that total number       # of pods running at any time during the update is at most 130% of       # original pods. +optional       maxSurge: '20%'        # Adds a delay before scaling down the previous ReplicaSet when the       # canary strategy is used with traffic routing (default 30 seconds).       # A delay in scaling down the previous ReplicaSet is needed after       # switching the stable service selector to point to the new ReplicaSet,       # in order to give time for traffic providers to re-target the new pods.       # This value is ignored with basic, replica-weighted canary without       # traffic routing.       scaleDownDelaySeconds: 30        # The minimum number of pods that will be requested for each ReplicaSet       # when using traffic routed canary. This is to ensure high availability       # of each ReplicaSet. Defaults to 1. +optional       minPodsPerReplicaSet: 2        # Limits the number of old RS that can run at one time before getting       # scaled down. Defaults to nil       scaleDownDelayRevisionLimit: 2        # Background analysis to run during a rollout update. Skipped upon       # initial deploy of a rollout. +optional       analysis:         templates:           - templateName: success-rate         args:           - name: service-name             value: guestbook-svc.default.svc.cluster.local            # valueFrom.podTemplateHashValue is a convenience to supply the           # rollouts-pod-template-hash value of either the Stable ReplicaSet           # or the Latest ReplicaSet           - name: stable-hash             valueFrom:               podTemplateHashValue: Stable           - name: latest-hash             valueFrom:               podTemplateHashValue: Latest            # valueFrom.fieldRef allows metadata about the rollout to be           # supplied as arguments to analysis.           - name: region             valueFrom:               fieldRef:                 fieldPath: metadata.labels['region']        # Steps define sequence of steps to take during an update of the       # canary. Skipped upon initial deploy of a rollout. +optional       steps:         # Sets the ratio of canary ReplicaSet to 20%         - setWeight: 20          # Pauses the rollout for an hour. Supported units: s, m, h         - pause:             duration: 1h          # Pauses indefinitely until manually resumed         - pause: {}          # set canary scale to an explicit count without changing traffic weight         # (supported only with trafficRouting)         - setCanaryScale:             replicas: 3          # set canary scale to spec.Replica * (setweight / maxTrafficWeight) without changing traffic weight         # if maxTrafficWeight unspecified, it defaults to 100         # (supported only with trafficRouting)         - setCanaryScale:             weight: 25          # set canary scale to match the canary traffic weight (default behavior)         - setCanaryScale:             matchTrafficWeight: true          # The percentage or number of replica pods within the applications ReplicaSet         # that are available and ready when a rollout is ready to be promoted. Useful if your application         # configured an HPA to help handle different loads of traffic, but you still want quick promotions.         # Defaults to 100% if replicaProgressThreshold is not specified.         # The 'type' field should be either \"Percent\" | \"Pod\"         # Current percentage that is checked against the input percent value is calculated by the following:         # CURRENT PERCENTAGE = available replicas / desired replicas for the current step         # +optional         - replicaProgressThreshold:             type: Percent             value: 90           # executes the configured plugin by name with the provided configuration         - plugin:             name: example             config:               key: value          # Sets header based route with specified header values         # Setting header based route will send all traffic to the canary for the requests         # with a specified header, in this case request header \"version\":\"2\"         # (supported only with trafficRouting, for Istio only at the moment)         - setHeaderRoute:             # Name of the route that will be created by argo rollouts this must also be configured             # in spec.strategy.canary.trafficRouting.managedRoutes             name: 'header-route-1'             # The matching rules for the header route, if this is missing it acts as a removal of the route.             match:               # headerName The name of the header to apply the match rules to.               - headerName: 'version'                 # headerValue must contain exactly one field of exact, regex, or prefix. Not all traffic routers support                 # all types                 headerValue:                   # Exact will only match if the header value is exactly the same                   exact: '2'                   # Will match the rule if the regular expression matches                   regex: '2.0.(.*)'                   # prefix will be a prefix match of the header value                   prefix: '2.0'            # Sets up a mirror/shadow based route with the specified match rules           # The traffic will be mirrored at the configured percentage to the canary service           # during the rollout           # (supported only with trafficRouting, for Istio only at the moment)         - setMirrorRoute:             # Name of the route that will be created by argo rollouts this must also be configured             # in spec.strategy.canary.trafficRouting.managedRoutes             name: 'header-route-1'             # The percentage of the matched traffic to mirror to the canary             percentage: 100             # The matching rules for the header route, if this is missing it acts as a removal of the route.             # All conditions inside a single match block have AND semantics, while the list of match blocks have OR semantics.             # Each type within a match (method, path, headers) must have one and only one match type (exact, regex, prefix)             # Not all match types (exact, regex, prefix) will be supported by all traffic routers.             match:               - method: # What HTTP method to match                   exact: 'GET'                   regex: 'P.*'                   prefix: 'POST'                 path: # What HTTP url paths to match.                   exact: '/test'                   regex: '/test/.*'                   prefix: '/'                 headers:                   agent-1b: # What HTTP header name to use in the match.                     exact: 'firefox'                     regex: 'firefox2(.*)'                     prefix: 'firefox'          # an inline analysis step         - analysis:             templates:               - templateName: success-rate          # an inline experiment step         - experiment:             duration: 1h             templates:               - name: baseline                 specRef: stable                 # optional, creates a service for the experiment if set                 service:                   # optional, service: {} is also acceptable if name is not included                   name: test-service               - name: canary                 specRef: canary                 # optional, set the weight of traffic routed to this version                 weight: 10             analyses:               - name: mann-whitney                 templateName: mann-whitney                 # Metadata which will be attached to the AnalysisRun.                 analysisRunMetadata:                   labels:                     app.service.io/analysisType: smoke-test                   annotations:                     link.argocd.argoproj.io/external-link: http://my-loggin-platform.com/pre-generated-link        # Anti-affinity configuration between desired and previous ReplicaSet.       # Only one must be specified.       antiAffinity:         requiredDuringSchedulingIgnoredDuringExecution: {}         preferredDuringSchedulingIgnoredDuringExecution:           weight: 1 # Between 1 - 100        # Traffic routing specifies the ingress controller or service mesh       # configuration to achieve advanced traffic splitting. If omitted,       # will achieve traffic split via a weighted replica counts between       # the canary and stable ReplicaSet.       trafficRouting:         # Supports nginx and plugins only: This lets you control the denominator or total weight of traffic.         # The total weight of traffic. If unspecified, it defaults to 100         maxTrafficWeight: 1000         # This is a list of routes that Argo Rollouts has the rights to manage it is currently only required for         # setMirrorRoute and setHeaderRoute. The order of managedRoutes array also sets the precedence of the route         # in the traffic router. Argo Rollouts will place these routes in the order specified above any routes already         # defined in the used traffic router if something exists. The names here must match the names from the         # setHeaderRoute and setMirrorRoute steps.         managedRoutes:           - name: set-header           - name: mirror-route         # Istio traffic routing configuration         istio:           # Either virtualService or virtualServices can be configured.           virtualService:             name: rollout-vsvc # required             routes:               - primary # optional if there is a single route in VirtualService, required otherwise           virtualServices:             # One or more virtualServices can be configured             - name: rollouts-vsvc1 # required               routes:                 - primary # optional if there is a single route in VirtualService, required otherwise             - name: rollouts-vsvc2 # required               routes:                 - secondary # optional if there is a single route in VirtualService, required otherwise          # NGINX Ingress Controller routing configuration         nginx:           # Either stableIngress or stableIngresses must be configured, but not both.           stableIngress: primary-ingress           stableIngresses:             - primary-ingress             - secondary-ingress             - tertiary-ingress           annotationPrefix: customingress.nginx.ingress.kubernetes.io # optional           additionalIngressAnnotations: # optional             canary-by-header: X-Canary             canary-by-header-value: iwantsit           canaryIngressAnnotations: # optional             my-custom-annotation.mygroup.com/key: value          # ALB Ingress Controller routing configuration         alb:           ingress: ingress # required           servicePort: 443 # required           annotationPrefix: custom.alb.ingress.kubernetes.io # optional          # Service Mesh Interface routing configuration         smi:           rootService: root-svc # optional           trafficSplitName: rollout-example-traffic-split # optional        # Add a delay in second before scaling down the canary pods when update       # is aborted for canary strategy with traffic routing (not applicable for basic canary).       # 0 means canary pods are not scaled down. Default is 30 seconds.       abortScaleDownDelaySeconds: 30        # Automatically reduce the number of stable pods as the number of canary pods increases       # Only available when traffic routing is used. Default value is false meaning that as more canary pods       # are created the number of stable pods stays the same.        dynamicStableScale: false  status:   pauseConditions:     - reason: StepPause       startTime: 2019-10-00T1234     - reason: BlueGreenPause       startTime: 2019-10-00T1234     - reason: AnalysisRunInconclusive       startTime: 2019-10-00T1234   Blue Green and Canary Deployment  一切都始於使用者發出指令(i.e. 更新 Rollout CRD)，要求進行上版   針對 Blue Green Deployment  activeService 以及 previewService 會是這次更新過程中的主要角色   一開始     activeService 會指向 revision 1 RS   previewService 會指向 revision 1 RS   然後開始進行更新，revision 2 RS 建立     activeService 會指向 revision 1 RS   previewService 會指向 revision 2 RS      在 Blue Green 中，previewService 是類似於 funnel traffic(漏斗流量)    當 revision 2 RS 已經準備好了  就會開始執行 prePromotionAnalysis 執行升級檢查  在正式 “promotion” 之前，你可以讓他先暫停一下(e.g. autoPromotionEnabled 以及 autoPromotionSeconds)  進到 promotion 階段就是將 activeService 指向 revision 2 RS  最後就剩下 postPromotionAnalysis 再次執行升級檢查   而 Canary Deployment 比較不一樣  因為說實在的，他並沒有一個統一的做法  所以 Argo Rollouts 針對 Canary 反而是讓你自定義他該怎麼做   1 2 3 4 5 6 7 8 9 10 11 12 13 strategy:   canary:     steps:     - setWeight: 20     - pause: {}     - setWeight: 40     - setCanaryScale:       replicas: 3     - pause: { duration: 1m }     - setWeight: 60     - pause: { duration: 1h }     - setWeight: 80     - pause: { duration: 30m }      如果沒有指定 steps，就會 fallback 到 Rolling update    透過一系列的 steps 來控制 canary 該怎麼執行  所以上述可以這樣理解     設定 canary 吃 20% 的流量   等待直到手動恢復   設定 canary 吃 40% 的流量   調整 canary replicas 的數量到 3(他並不會影響流量佔比)   … 以此類推   HPA and VPA  Rollout CRD 都可以跟現有的 HorizontalPodAutoscaler 以及 VerticalPodAutoscaler 相容   主要的原因在於 Argo Rollouts v0.3.0 有揭露 /scale 這個 subresource(跟原本的 Kubernetes Deployment 相同)  也因為如此，HPA 可以透過 /scale 讀取 current replicas(從 scale subresource 取得)並與 status.replicas 比較來決定要如何調整   以前是 HPA ➡️ replica set  現在是 HPA ➡️ Rollout ➡️ replica set   有了 Rollout 這層包裝，replica set 的操作就會落到 Rollout Controller 的身上      決定數量依然是 HPA 的責任，Argo Rollouts 則負責實際的調整    1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata:   name: demo-hpa   labels:     app: demo spec:      # The min and max number of pods the HPA can scale   minReplicas: 1   maxReplicas: 10   scaleTargetRef:       # The HPA targets the Rollout object for scaling.     apiVersion: argoproj.io/v1alpha1     kind: Rollout     name: rollout-hpa-example ...      VPA 也是可以照 HPA 的寫法，只不過官方文件並未說明是否也是因為 /scale subresource 的關係    1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion: \"autoscaling.k8s.io/v1beta2\" kind: VerticalPodAutoscaler   metadata:     name: vpa-rollout-example     namespace: test-vpa   spec:     targetRef:       apiVersion: \"argoproj.io/v1alpha1\"       kind: Rollout       name: vpa-demo-rollout     updatePolicy:       updateMode: \"Auto\" ...   References     Autoscaling Workloads   Managing Workloads   Software — 軟體版本Canary是什麼意思?   Day08 - 使用 Kubernetes 實現藍綠部屬 (Blue/Green Deployment)   從異世界歸來的第十三天 - Kubernetes Deployment Strategies - Rolling Update &amp; Recreate (二)   從異世界歸來的第十四天 - Kubernetes Deployment Strategies - Blue/Green Deployment 藍綠部署 (三)   從異世界歸來的第十五天 - Kubernetes Deployment Strategies - Canary Deployment 金絲雀部署 (四)   Resize CPU and Memory Resources assigned to Containers   Horizontal Pod Autoscaling   Node Autoscaling   Configure Quality of Service for Pods   KEDA Concepts   HorizontalPodAutoscaler Walkthrough   ⎈ Rolling Update &amp; Recreate Deployment Strategies in Kubernetes ⚙️   Day 29: Kubernetes A/B Testing   Managing Workloads   Horizontal Pod Autoscaling   BlueGreen Deployment Strategy   Canary Deployment Strategy  ","categories": ["kubernetes"],
        "tags": ["scaling","scale out","scale up","hpa","vpa","rolling update","recreate","blue green","canary","shadow","deployment","rollout","controller","argo rollouts","keda","keda concepts","horizontal pod autoscaler","vertical pod autoscaler","hpa and vpa","rollout crd","manual scaling","auto scaling","stabilization window","tolerance","scaling policy","metric data","resize container resources","observed generation","ab testing","shadow deployment"],
        "url": "/kubernetes/kubernetes-scale/",
        "teaser": null
      },{
        "title": "Kubernetes 從零開始 - 在分散式的世界實現 Zero Downtime 路由管理",
        "excerpt":"Proxy  Proxy 是一個中間人，負責處理 client 與 server 之間的溝通請求  相比於 client 直接與 server 溝通，Proxy 的優勢在於可以進行流量控制、負載平衡、安全性控制等  他可以分為兩類 Forward Proxy 以及 Reverse Proxy      Proxy(network level) 是負責全部的流量，而 middleware(application level) 只有負責該次 request/response    Forward Proxy        ref: 系統設計 - 正向代理跟反向代理    正向代理是負責處理從 “client” 發出去的流量(換言之就是處理 Outgoing 的流量)  如果遇到說你不希望 server 可以知道 client 的資訊，就可以使用正向代理  所有的 client 都會先經過 forward proxy 再轉發到 server  常見的策略是 forward proxy 會將 client 的 ip 轉換成自己的 ip，藉此隱藏 client 的資訊   Reverse Proxy        ref: 系統設計 - 正向代理跟反向代理    反向代理則是負責處理進到 “server” 的流量(換言之就是處理 Incoming 的流量)  所有的 request 都會先經過 reverse proxy 再轉發到後端  這樣的好處是可以隱藏後端 server 資訊，以及進行負載平衡(可參考 Load Balancing)  你可以決定要將 request 導向哪一個 server      使用 Nginx 實現 reverse proxy，如果你需要保留原始 request 資訊  可以利用 proxy_set_header 幫你加 Host 以及 Connection    Load Balancing       ref: Load Balancing Approach in Distributed System    負載平衡是一種常見的增加伺服器吞吐能力的手段  他的假設是你的應用程式部屬在 多台機器 上  你不會希望只有其中一台伺服器很忙而已   因此負載平衡會將大量的 request 盡量均勻的分佈在所有 worker(機器) 上面  當新的機器加入的時候(scaling)，它也能夠分攤現有的工作量，使得吞吐量得以提昇   Load Balancing Methods  基本上負載平衡的算法分為靜態以及動態兩種     靜態：根據伺服器的負載能力來分配工作量，例如 Round Robin, Weighted Round Robin, DNS Round Robin   動態：根據動態的資訊來分配工作量，例如 Least Connection，Sticky Session      為什麼 Weighted Round Robin 是靜態的？  因為你沒辦法動態調整 CPU Memory 這些資源分配，並且 load balancer 本身也沒有辦法知道伺服器的負載能力    Round Robin  Round Robin 的算法就是，每個人都有一小段時間服務  在作業系統是排程算法之一，在負載平衡的世界也有類似的身影  因為你的伺服器可能有很多個，套用 Round Robin 的概念就會是  Server A 先服務，下一個給 Server B，再來給 Server C，以此類推   Weighted Round Robin  Round Robin 的缺點是，他沒辦法根據不同伺服器的負載能力來分配工作量  比方說 Server A 擁有更好的 CPU 更多的 RAM, 理論上他要負責更多的 request  所以 Weighted Round Robin 就是為了解決這個問題而誕生的   他可以給不同的伺服器一個權重，權重高的就會被分配到更多的 request   DNS Round Robin  一個 domain name 可以指定多個 ip address  當 client 請求該 domain name 的時候，DNS 會回應一連串的 ip address  client 會隨機選擇一個 ip address 進行連線   所以透過這種方式就可以達到基本的負載平衡(DNS Round Robin)  那你也可以加 Weighted Round Robin 的概念進去      有關 DNS Round Robin  可參考 重新認識網路 - 從基礎開始 | Shawn Hsu    Least Connection  Load balancer 會根據每一台伺服器當前的連線數量判定說要不要將 request 轉發過去  對比 Weighted Round Robin 來說，他可以動態的調整伺服器的負載能力  不會只依靠單一的權重來決定，而是會根據伺服器當前的連線數量來決定   Sticky Session  執行 load balancing 如果碰到 session 這種東西可能會有一點麻煩  假設你的 session 是儲存在 server 本身的，那問題可大了   多台的機器做 load balancing 意味著你下一次 request 到後端，可能是不一樣的 server 在服務  此時，server B 並沒有你在 server A 上面註冊的 session  因此你可能會遇到一些存取的問題   這個時候你會希望，client A 永遠是由 server A 服務，並不會由其他伺服器接手  所以 sticky session 的用意就是這個   同時 Nginx 也在 Plus 的服務中提供相關服務，可參考以下文件 Enabling Session Persistence   雖然現在實務上，因為 HTTP 本身 stateless 的特性  搭配 token 驗證身份的方式，使得 sticky session 較少見   Routing in Kubernetes  Kubernetes Service Discovery  針對 cluster 內部的 Pods，你可以透過定義 Service 來讓外部進行存取  那 Service 是如何知道有哪些 Pods 的呢？   在 Service 建立之初，control plane(i.e. EndpointSlice Controller) 會在有 selector 的情況下自動幫你建立 EndpointSlice 這個 Resource  因為 Pod 本身還是有自己的 ip address，為了能夠讓 Service 能夠轉接到 Pod 上面  這些資訊是被儲存在 EndpointSlice 裡面(其他的資訊例如說，conditions, hostname, nodename 以及 zone)      EndpointSlice 的前身是 Endpoints    Pod 是 ephemeral 的，所以即使你有紀錄 Pod 的 ip address  他也可能被移除以及重新加入，然後紀錄就會失效  EndpointSlice Controller 會負責監控這些資源的變化，當相關的 Resource 有更動，controller 就會重新調整 EndpointSlice 的資訊  使得這個紀錄永遠都會是 up-to-date 的      有關 controller 可以參考 Kubernetes 從零開始 - Informer 架構以及 Controller Pattern | Shawn Hsu    這個紀錄必須讓其他的節點也知道，kube-proxy 依賴於 EndpointSlice 進行內部路由  每個節點必須積極的同步這些資料，為了確保傳遞的過程是輕量的  所以新增會優於更新，因為新增一個全新的 EndpointSlice 比更新多筆 EndpointSlice 來得更輕量  EndpointSlice 本身只會存有 100 筆資訊(可以調整)，如果超過，則會切割成多個 EndpointSlice     在執行路由的時候，如果你想要維持低延遲的特性，一個方法是 server 就在你家旁邊  這樣就不會有跨區域的網路延遲  在 Kubernetes 中，你可以在 Service 裡面設定 annotation service.kubernetes.io/topology-mode: Auto 來達成  EndpointSlice Controller 會負責在 record 旁邊附加 “hint” 來表示這個 Pod 的拓樸資訊  然後 kube-proxy 會根據這些 hint 來決定要如何路由(i.e. TAR, Topology Aware Routing)   當然也不是說設定 annotation 就有用，比方說如果 node 沒有 topology.kubernetes.io/zone 的 label，那麼 TAR 就無法使用  限制其實還滿多的，可以參考 Safeguards 以及 Constraints   Terminology Ingress vs. Egress  通常在講 network traffic 的時候，早些年 inbound/outbound 的說法比較常見  現在我們會使用 ingress/egress 來表示   根據 When (and why?) did in-/outbound become ingress/egress? 上的討論來看  bound 類的說法比較像是這是終點的意思，而 gress 類比較是 “經過” 的意思   所以     ingress = 流入的流量   egress = 流出的流量      流入哪？ 流出哪裡呢？  端看你怎麼界定那個邊界  邊界也可以是路由器、一台虛擬機或一個集群       不要將 Ingress 與 Kubernetes Ingress Resource 混為一談  Kubernetes Ingress Resource 是一個 Kubernetes Resource  用於定義不同的路由規則以轉發 HTTP/HTTPS 的流量  但 Ingress 這個單詞的意思已經演變成一種更為廣泛的概念，所以我們會以這個為準    From Service to Ingress Controller  Service 能做到的有限，比方說他沒辦法客製化路由規則  比方說我想要 /api 到 Service A，然後 /admin 到 Service B  基本上你需要額外的工具輔助達到類似的效果   Kubernetes 沒有內建這種工具，取而代之的是所謂 Ingress Controller  流入集群內部的流量統一會先經過 Ingress Controller 進行處理(所以他是一個 Reverse Proxy 的概念)  你可以自由選擇不同 vendor 的 solution, 比如說 Ingress Nginx Controller 或者 Traefik Ingress Controller  基本上都支援基礎的路由設定，更進階的 load balancing、logging、 L4 routing 以及 TLS 等等的看各家實作      有關 L4, L7 可以參考 重新認識網路 - OSI 七層模型 | Shawn Hsu    除了進階的功能以外，最基本的 L7 路由是由 Kubernetes Ingress Resource 定義  其實你可以注意到，Ingress 並不是要取代 Service，而是互補的  底層還是要依靠 Service 來進行轉發      如果要使用進階的功能，內建的 Ingress 可能不足以描述  所以不同 vendor 可能會使用其他資源甚至是 CRD，比如說 Apache APISIX 有 ApisixRoute 資源       有關 CRD 可以參考 Kubernetes 從零開始 - client-go 實操 CRD | Shawn Hsu    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 apiVersion: networking.k8s.io/v1 kind: Ingress metadata:   name: ingress-wildcard-host spec:   rules:   - host: \"foo.bar.com\"     http:       paths:       - pathType: Prefix         path: \"/bar\"         backend:           service:             name: service1             port:               number: 80   - host: \"*.foo.com\"     http:       paths:       - pathType: Prefix         path: \"/foo\"         backend:           service:             name: service2             port:               number: 80   Ingress Controllers  Ingress Nginx Controller  Ingress Nginx Controller 就是以 Nginx 為基礎的 Ingress Controller  基本上這個 controller 的主要目的是根據不同的 Resource 生成 nginx.conf 的設定檔  這些 Resource 包括 Ingress, Services, Endpoints, Secrets 以及 ConfigMaps   每一次 Resource 有變化，Controller 就會重新生成設定檔  這樣的做法會一直重複生成，雖然很耗資源，但這是必要的，因為你沒辦法知道 Resource 的變化會不會最終導致 nginx.conf 的變化  也有可能沒改變？ 那這樣就不需要更新設定檔了(不過計算還是需要的)   實作上 Ingress Nginx Controller 是使用 Informer 進行處理  註冊 EventHandler 來監聽，並且通過 Ring Buffer(RingChannel) 傳遞需要處理的資料(可以參考 controller/store/store.go#L250)  之後再由 Ingress Controller 的 Start 取出資料再放到 SyncQueue 裡面(可參考 controller/nginx.go#L359)   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 // New creates a new object store to be used in the ingress controller. // //nolint:gocyclo // Ignore function complexity error. func New(     namespace string,     namespaceSelector labels.Selector,     configmap, tcp, udp, defaultSSLCertificate string,     resyncPeriod time.Duration,     client clientset.Interface,     updateCh *channels.RingChannel,     disableCatchAll bool,     deepInspector bool,     icConfig *ingressclass.Configuration,     disableSyncEvents bool, ) Storer {     ingEventHandler := cache.ResourceEventHandlerFuncs{         AddFunc: func(obj interface{}) {           ing, _ := toIngress(obj)            if !watchedNamespace(ing.Namespace) {               return           }            ic, err := store.GetIngressClass(ing, icConfig)           if err != nil {               klog.InfoS(\"Ignoring ingress because of error while validating ingress class\", \"ingress\", klog.KObj(ing), \"error\", err)               return           }            klog.InfoS(\"Found valid IngressClass\", \"ingress\", klog.KObj(ing), \"ingressclass\", ic)            ...          }     } }   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 // Start starts a new NGINX master process running in the foreground. func (n *NGINXController) Start() {     for {         select {         case err := &lt;-n.ngxErrCh:             if n.isShuttingDown {                 return             }              // if the nginx master process dies, the workers continue to process requests             // until the failure of the configured livenessProbe and restart of the pod.             if process.IsRespawnIfRequired(err) {                 return             }          case event := &lt;-n.updateCh.Out():             if n.isShuttingDown {                 break             }              if evt, ok := event.(store.Event); ok {                 klog.V(3).InfoS(\"Event received\", \"type\", evt.Type, \"object\", evt.Obj)                 if evt.Type == store.ConfigurationEvent {                     // TODO: is this necessary? Consider removing this special case                     n.syncQueue.EnqueueTask(task.GetDummyObject(\"configmap-change\"))                     continue                 }                  n.syncQueue.EnqueueSkippableTask(evt.Obj)             } else {                 klog.Warningf(\"Unexpected event type received %T\", event)             }         case &lt;-n.stopCh:             return         }     } }      有關 Ring Buffer 可以參考 Goroutine 與 Channel 的共舞 | Shawn Hsu       有關 Informer 可以參考 Kubernetes 從零開始 - Informer 架構以及 Controller Pattern | Shawn Hsu    那這個 SyncQueue 在初始化的時候會給一個 callback 負責處理 queue 裡面的資料  以 Ingress Nginx Controller 來說，就是 syncIngress(controller/store.go#L908)   那設定檔最終會被交給 Nginx 內部並且 reload  Resource 的新增刪除更新某種程度上都會影響 reload 的頻率  比如說     Ingress 被新增   Ingress, Secrets, Services 被刪除   Ingress 的內部路由規則，TLS 或者 annotation 有變化   等等的都會要 reload      如果你的 Ingress 是非法的，比如說打錯字之類的  為了避免無法正常運作，Admission Webhook 會被用於驗證，通過之後才會送到 Controller 進行處理    Downtime of Ingress Nginx Controller  那這個 reload 其實會造成問題  Nginx 的 reload 是透過新的 worker process 接替(新的連線都由他處理)  然後舊的 worker process 會被 graceful shutdown(i.e. 處理完當前的連接就會停止)  看起來不會造成 downtime?        ref: 为什么 NGINX 的 reload 不是热加载？    考慮到 keepalive 的長連接設定  因為 config 更新會造成 reload，Nginx 便會主動通知 connection 關閉  然而在系統負載過高的情況下，client 可能會沒收到關閉的通知，進一步造成 downtime      可以參考 K000153144: How to extend Graceful Shutdown Time for NGINX worker Process during Configuration Reload    另一個點是，舊的 worker process 需要處理連接的時間其實是無法預測的  所以說如果你一直 trigger reload，old worker process 就會一直長出來，並且因為執行時間是無法預測的  進而導致 process 數量過多，提升系統壓力，進而導致 downtime 的出現   Traefik Ingress Controller  而 Traefik 就不會有 downtime 的問題，因為 Traefik 不需要將 configuration reload，然後再交給新的 worker process 去處理  所有的設定檔都是 in-memory 進行 hot reload 的  整體的實作也同樣是基於 Informer 的衍伸架構      有關 Informer 可以參考 Kubernetes 從零開始 - Informer 架構以及 Controller Pattern | Shawn Hsu    監聽 Ingress Resource 的變化是使用 WatchAll 這個 function 註冊 EventHandler(可以參考 ingress/client.go#L141)  並且透過 eventCh 的 Golang channel 接收 event      有關 Golang channel 可以參考 Goroutine 與 Channel 的共舞 | Shawn Hsu    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 const (     resyncPeriod   = 10 * time.Minute     defaultTimeout = 5 * time.Second )  // WatchAll starts namespace-specific controllers for all relevant kinds. func (c *clientWrapper) WatchAll(namespaces []string, stopCh &lt;-chan struct{}) (&lt;-chan interface{}, error) {     eventCh := make(chan interface{}, 1)     eventHandler := &amp;k8s.ResourceEventHandler{Ev: eventCh}      ...      factoryIngress := kinformers.NewSharedInformerFactoryWithOptions(c.clientset, resyncPeriod, kinformers.WithNamespace(ns), kinformers.WithTweakListOptions(matchesLabelSelector))      _, err := factoryIngress.Networking().V1().Ingresses().Informer().AddEventHandler(eventHandler)     if err != nil {         return nil, err     }      ... }   這個 eventCh 的資料會被 Provide 的 function 接收並處理  初步來看，會讀取 Ingress Resource 的資料並且計算 hash 值，只有在不同的 hash 值才會進行下一步  下一步會被進一步送到 configurationChan 裡面(可以參考 ingress/kubernetes.go#L136)      eventCh 是一個 Ring Buffer(code 裡面稱為 RingChannel)，可以參考 aggregator/ring_channel.go  有關 Ring Buffer 可以參考 Goroutine 與 Channel 的共舞 | Shawn Hsu       所有的 Provider 都是被 Provider Aggregator 所管理    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 // Provide allows the k8s provider to provide configurations to traefik // using the given configuration channel. func (p *Provider) Provide(configurationChan chan&lt;- dynamic.Message, pool *safe.Pool) error {     logger := log.With().Str(logs.ProviderName, \"kubernetes\").Logger()     ctxLog := logger.WithContext(context.Background())      k8sClient, err := p.newK8sClient(ctxLog)     if err != nil {         return err     }      if p.AllowExternalNameServices {         logger.Info().Msg(\"ExternalName service loading is enabled, please ensure that this is expected (see AllowExternalNameServices option)\")     }      pool.GoCtx(func(ctxPool context.Context) {         operation := func() error {             eventsChan, err := k8sClient.WatchAll(p.Namespaces, ctxPool.Done())             if err != nil {                 logger.Error().Err(err).Msg(\"Error watching kubernetes events\")                 timer := time.NewTimer(1 * time.Second)                 select {                 case &lt;-timer.C:                     return err                 case &lt;-ctxPool.Done():                     return nil                 }             }            throttleDuration := time.Duration(p.ThrottleDuration)           throttledChan := throttleEvents(ctxLog, throttleDuration, pool, eventsChan)           if throttledChan != nil {               eventsChan = throttledChan           }            for {               select {               case &lt;-ctxPool.Done():                   return nil               case event := &lt;-eventsChan:                   // Note that event is the *first* event that came in during this                   // throttling interval -- if we're hitting our throttle, we may have                   // dropped events. This is fine, because we don't treat different                   // event types differently. But if we do in the future, we'll need to                   // track more information about the dropped events.                   conf := p.loadConfigurationFromIngresses(ctxLog, k8sClient)                    confHash, err := hashstructure.Hash(conf, nil)                   switch {                   case err != nil:                       logger.Error().Msg(\"Unable to hash the configuration\")                   case p.lastConfiguration.Get() == confHash:                       logger.Debug().Msgf(\"Skipping Kubernetes event kind %T\", event)                   default:                       p.lastConfiguration.Set(confHash)                       configurationChan &lt;- dynamic.Message{                         ProviderName:  \"kubernetes\",                         Configuration: conf,                       }                   }                    // If we're throttling, we sleep here for the throttle duration to                   // enforce that we don't refresh faster than our throttle. time.Sleep                   // returns immediately if p.ThrottleDuration is 0 (no throttle).                   time.Sleep(throttleDuration)               }           }         }          notify := func(err error, time time.Duration) {             logger.Error().Err(err).Msgf(\"Provider error, retrying in %s\", time)         }          err := backoff.RetryNotify(safe.OperationWithRecover(operation), backoff.WithContext(job.NewBackOff(backoff.NewExponentialBackOff()), ctxPool), notify)         if err != nil {             logger.Error().Err(err).Msg(\"Cannot retrieve data\")         }     })      return nil }   那，是誰 consume configurationChan 呢？  主程式 traefik 會在啟動的時候初始化 ConfigurationWatcher(可參考 cmd/traefik/traefik.go#L313)  同時 configurationChan 也是在 ConfigurationWatcher 裡面被建立以及被提供給 Provider Aggregator 使用   1 2 3 4 5 6 7 8 9 10 func (c *ConfigurationWatcher) startProviderAggregator() {     log.Info().Msgf(\"Starting provider aggregator %T\", c.providerAggregator)      safe.Go(func() {         err := c.providerAggregator.Provide(c.allProvidersConfigs, c.routinesPool)         if err != nil {             log.Error().Err(err).Msgf(\"Error starting provider aggregator %T\", c.providerAggregator)         }     }) }   真正的魔法發生在 receiveConfigurations 以及 applyConfigurations 裡面  ConfigurationWatcher 裡面有兩個 channel，分別是 allProvidersConfigs 以及 newConfigs  receiveConfigurations 負責從 allProvidersConfigs 接收資料，並且進行處理  然後再將資料送到 newConfigs 裡面，被 applyConfigurations 所使用   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 // receiveConfigurations receives configuration changes from the providers. // The configuration message then gets passed along a series of check, notably // to verify that, for a given provider, the configuration that was just received // is at least different from the previously received one. // The full set of configurations is then sent to the throttling goroutine, // (throttleAndApplyConfigurations) via a RingChannel, which ensures that we can // constantly send in a non-blocking way to the throttling goroutine the last // global state we are aware of. func (c *ConfigurationWatcher) receiveConfigurations(ctx context.Context) {     newConfigurations := make(dynamic.Configurations)     var output chan dynamic.Configurations     for {         select {         case &lt;-ctx.Done():             return         // DeepCopy is necessary because newConfigurations gets modified later by the consumer of c.newConfigs         case output &lt;- newConfigurations.DeepCopy():             output = nil          default:             select {             case &lt;-ctx.Done():                 return             case configMsg, ok := &lt;-c.allProvidersConfigs:                 if !ok {                     return                 }                  logger := log.Ctx(ctx).With().Str(logs.ProviderName, configMsg.ProviderName).Logger()                  if configMsg.Configuration == nil {                     logger.Debug().Msg(\"Skipping nil configuration\")                     continue                 }                  if isEmptyConfiguration(configMsg.Configuration) {                     logger.Debug().Msg(\"Skipping empty configuration\")                     continue                 }                  logConfiguration(logger, configMsg)                  if reflect.DeepEqual(newConfigurations[configMsg.ProviderName], configMsg.Configuration) {                     // no change, do nothing                     logger.Debug().Msg(\"Skipping unchanged configuration\")                     continue                 }                  newConfigurations[configMsg.ProviderName] = configMsg.Configuration.DeepCopy()                  output = c.newConfigs              // DeepCopy is necessary because newConfigurations gets modified later by the consumer of c.newConfigs             case output &lt;- newConfigurations.DeepCopy():                 output = nil             }         }     } }   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 // applyConfigurations blocks on a RingChannel that receives the new // set of configurations that is compiled and sent by receiveConfigurations as soon // as a provider change occurs. If the new set is different from the previous set // that had been applied, the new set is applied, and we sleep for a while before // listening on the channel again. func (c *ConfigurationWatcher) applyConfigurations(ctx context.Context) {     var lastConfigurations dynamic.Configurations     for {         select {         case &lt;-ctx.Done():             return         case newConfigs, ok := &lt;-c.newConfigs:             if !ok {                 return             }              // We wait for first configuration of the required provider before applying configurations.             if _, ok := newConfigs[c.requiredProvider]; c.requiredProvider != \"\" &amp;&amp; !ok {                 continue             }              if reflect.DeepEqual(newConfigs, lastConfigurations) {                 continue             }              conf := mergeConfiguration(newConfigs.DeepCopy(), c.defaultEntryPoints)             conf = applyModel(conf)              for _, listener := range c.configurationListeners {                 listener(conf)             }              lastConfigurations = newConfigs         }     } }   Apache APISIX Ingress Controller  與 Ingress Nginx Controller 以及 Traefik Ingress Controller 不同的是  APISIX 他的設定檔是存放在 etcd 裡面，而不是直接 in-memory      Traefik Enterprise 則是由 Control Plane 負責推播所有資料(包含 events, certificates 以及 Traefik 設定檔等等)，跟 Traefik OSS 的實作是不同的  這些資料是存放在 distributed 的 key-value store 裡面    Apache APISIX 本身是為了解決 Ingress Nginx Controller 的 reload 問題而誕生的  而實作上是基於 Nginx 以及 LuaJIT(OpenResty)  透過將動態路由的設定檔置於 APISIX Core 中，所有的 request 都會先經過 Nginx 的單點入口，再經過 APISIX Core 動態的指定 upstream(i.e. 你的後端)，從而避免 Nginx reload 帶來的影響  也因為 APISIX 本人與 etcd 都支援多點部署，單點失效並不會影響整體 proxy 的運作      APISIX 內，Nginx 只會有一個 server 一個 location  所以不管 APISIX Core 怎麼變化，Nginx 都不需要 reload    Ingress Controller to Support Gateway API  Ingress 的 Resource 的設計是很粗略的  除了缺少比較進階的功能之外，他也沒辦法進行擴充，移植性也很差   如果更換 Controller，Ingress 大概率無法重複利用  比方說 Ingress Nginx Controller 會透過自定義的 annotation 來達到特殊的功能  如果你換成 Traefik Ingress Controller，那這個 annotation 就無法使用  每一家都有他自己特殊的設定檔，其實是會造成混亂  比較好的做法是例如說 L4 routing 這種比較常見的設定應該是由 Resource 來管理，而非依靠 Controller 自己的實作，所以 標準化是必要的   而且 Ingress 需要開發者知道不同底層的設定與實作(例如 Ingress Nginx Controller 需要使用這個 annotation 達到某某功能而其他的則是另一個)  身為 Application Developer 的我們其實根本不需要知道這些  於是 Kubernetes 想要利用新的 API 來達到     更細緻的控制(L4 routing, TLS … etc.)   提供標準化的接口(避免 vendor lock-in)   Policy 權責分離   稱之為 Gateway API   Gateway API  但 Gateway API 其實一個統稱，他包含多個 Resource 如 GatewayClass, Gateway, HTTPRoute 以及 GRPCRoute  分成不同的 Resource 其實是對應到不同的角色(權責分離)   Route  比如說身為應用程式開發者的我們其實比較在乎的是 HTTPRoute 以及 GRPCRoute 等等的  底層要用什麼來實現(Ingress Nginx Controller 或者 Traefik Ingress Controller)其實對我們來說沒差  反正他會動就好了嘛   我只在乎，/foo 要導向 whoami 這個 service 的 80 port  這才是重點   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 apiVersion: gateway.networking.k8s.io/v1 kind: HTTPRoute metadata:   name: http-app   namespace: default spec:   parentRefs:     - name: my-gateway   hostnames:     - whoami   rules:     - matches:         - path:             type: Exact             value: /foo       backendRefs:         - name: whoami           port: 80   GatewayClass  那誰會在乎要底層用什麼來實現呢？  肯定是管理 cluster 的人  比方說我想要使用 traefik.io/gateway-controller   1 2 3 4 5 6 apiVersion: gateway.networking.k8s.io/v1 kind: GatewayClass metadata:   name: my-gateway-class spec:   controllerName: traefik.io/gateway-controller   Gateway  你指定了 Route，選定了 Ingress Controller(GatewayClass)  其實還有一個東西要指定，是進入集群的入口   這個設定有點陌生  如果回顧 From Service to Ingress Controller 的時候，你會發現  即使在 Ingress Resource 裡面也沒有相對應的東西啊？   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 apiVersion: networking.k8s.io/v1 kind: Ingress metadata:   name: ingress-wildcard-host spec:   rules:   - host: \"foo.bar.com\"     http:       paths:       - pathType: Prefix         path: \"/bar\"         backend:           service:             name: service1             port:               number: 80   - host: \"*.foo.com\"     http:       paths:       - pathType: Prefix         path: \"/foo\"         backend:           service:             name: service2             port:               number: 80   那這個集群的入口是什麼東西呢？  你沒看到的原因在於 Ingress Controller 其實幫你做掉了  舉 Ingress Nginx Controller 為例，根據 Basic usage - host based routing      On many cloud providers ingress-nginx will also create the corresponding Load Balancer resource.  All you have to do is get the external IP and add a DNS A record inside your DNS provider that point myservicea.foo.org and myserviceb.foo.org to the nginx external IP.    從上述你可以很清楚的看到說，所有的流量會先進到 Load Balancer Resource  所以入口就是這個東西   如果以新的 API 來實作就是   1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion: gateway.networking.k8s.io/v1 kind: Gateway metadata:   name: my-gateway spec:   gatewayClassName: my-gateway-class   listeners:     - name: https   \t  protocol: HTTPS   \t  port: 443   \t  tls:         certificateRefs:           - kind: Secret         \tname: mysecret   Put it All Together       ref: Kubernetes Gateway API: What Is It and Why Do You Need It?    你可以看到說，Gateway API 實際上是將每個東西都拆得很開  根據不同的角色，各自維護他們所關心的東西      Cluster Admin 負責決定要使用哪一個 Ingress Controller 並指定於 GatewayClass   Devops Team 負責管理集群的入口並指定於 Gateway   Application Developer 負責管理路由規則並指定於 Route   以前的 Ingress 把所有東西都放在一起  他沒辦法很好的做到權責分離，Application Developer 只需要改一個 route  但卻需要取得完整的設定檔，他有可能去動到其他東西，進而造成不必要的錯誤   Adoption of Gateway API  身為 Ingress 的後繼者，Gateway API 的進度卻不盡人意  雖然新的標準立意良善並且修正了許多缺點，但一次性的 migration 是必須的  俗話說的好，會動就不要動它，很顯然的這些優點並沒有說服開發者們進行升級   並且也不是現有的 Ingress Controller 都支援 Gateway API  舉例來說，Ingress Nginx Controller 不支援 Gateway API(可參考 ⚠️ Ingress NGINX Project Status Update ⚠️)  取而代之的是一個新的專案 kubernetes-sigs/ingate   更換新的 Ingress Controller 除了要升級 Ingress 到 Gateway API 之外  Controller 的穩定性也是需要考慮的  種種原因之下導致現在 Gateway API 的採用率並不高   References     Round Robin Load Balancing Definition   DNS for Services and Pods   Service   EndpointSlices   Ingress   Ingress controller V Gateway API   I’m a newb to Kubernetes. Why do I need NGINX/Traefic/etc. ingress controllers?   How it works   How nginx reload work ? why it is zero-downtime   https://nginx.org/en/docs/control.html   为什么 NGINX 的 reload 不是热加载？   Why do you need Apache APISIX when you have NGINX and Kong?   Concepts   为什么 Apache APISIX 选择 NGINX+Lua 技术栈？   Ingress controller V Gateway API   Kubernetes Gateway API: What Is It and Why Do You Need It?   Basic usage - host based routing   Gateway API   ⚠️ Ingress NGINX Project Status Update ⚠️   負載平衡演算法類型   什麼是負載平衡？  ","categories": ["kubernetes"],
        "tags": ["ingress","egress","inbound","outbound","load balancer","nginx","reverse proxy","proxy","round robin","weighted round robin","dns round robin","least connection","sticky session","gateway api","ingress nginx controller","traefik ingress controller","ingress controller","gateway class","gateway","route","httproute","grpcroute","nginx","traefik","apache apisix","apisix","kubernetes-sigs/ingate","controller","informer","zero downtime","endpointslice","topology aware routing","golang","golang channel","goroutine","forward proxy","reverse proxy","service discovery","l4","l7"],
        "url": "/kubernetes/kubernetes-traffic/",
        "teaser": null
      },{
        "title": "Kubernetes 從零開始 - 你的 Volume 到底 Mount 到哪裡去了？",
        "excerpt":"Introduction to Kubernetes Volume  從 Docker 的年代開始，掛載 host 的檔案系統不是什麼新鮮事  而 mount 的方式也很直覺，直接指定 host 的檔案路徑，你就能夠在 container 裡面存取相關的資料  到了 Kubernetes，也是有一樣的概念，但是 mount 的方式變得五花八門，相對複雜   本文將會帶你了解那些最常用的 Volume，以及他們的特性   Quick Look to Volume  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 apiVersion: batch/v1 kind: Job metadata:   name: my-pod spec:   template:     spec:       containers:       - name: busybox         image: busybox         command: ['sh', '-c', 'ls -al /mnt/data; echo \"----\"; ls -al /mnt/cm']         volumeMounts:           - mountPath: /mnt/data             name: my-volume             subPath: dataset1           - mountPath: /mnt/cm/fn             name: my-volume-cm             subPath: firstName.txt       volumes:       - name: my-volume         emptyDir: {}       - name: my-volume-cm         configMap:           name: my-cm           items:             - key: firstName               path: firstName.txt       restartPolicy: Never   為了因應不同的情境，Kubernetes 在設計上是相對彈性的  從上述的 Job spec 你可以看到，你需要定義這個 Pod 需要哪一些 volume(i.e. .spec.volumes)  然後在裡面詳細的描述你要怎麼掛載進去要掛到哪(i.e. .spec.containers[*].volumeMounts)      Pod 本身可以同時掛載 一或多個不同的 volume    當然這種簡單的寫法是屬於 Ephemeral Volume 的範疇  如果使用 Persistent Volume 的話，你需要更複雜的定義(i.e. Persistent Volume Claim)   How to Mount?  但具體來說怎麼掛載，可以說，概念上與 Docker 是一模一樣的  你需要有兩個路徑     Container 內部掛載路徑            掛載路徑是你自己決定的，比方說 /mnt/data           Node 上的資源路徑            有一個固定的路徑，規則是 /var/lib/kubelet/pods/&lt;pod uid&gt;/volumes/&lt;volume type&gt;/&lt;volume name&gt;           1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion: v1 kind: Pod metadata:   name: my-pod spec:   containers:   - name: my-container     volumeMounts:     - mountPath: /mnt/data       name: my-volume   volumes:   - name: my-volume     emptyDir: {}   當容器啟動的時候，container runtime 會根據以上資訊逐一進行綁定  考慮以上 yaml，執行順序會是     在 Node 上建立 /var/lib/kubelet/pods/1234/volumes/kubernetes.io~empty-dir/my-volume   container runtime 將以下進行綁定            /var/lib/kubelet/pods/1234/volumes/kubernetes.io~empty-dir/my-volume :arrow_right: /mnt/data           啟動容器   Precision Control with subPath  一個常見的問題是，掛載這件事情沒弄好會造成資料遺失的問題  比方說如果你掛載到的地方本來就有資料，把它蓋過去可能東西就壞了  又或者是你只要掛一部分的資料而已，而不是整個 volume   以上的情境催生出了 subPath 這個概念   Mount Path Obscuration  但實際上也不是蓋過去啦，是被遮蔽了  linux 掛載有一個機制是這樣子的，當你掛載的路徑有資料，它會把原本的舊資料隱藏起來，直到你移除掛載  這個概念也同樣適用於 Kubernetes   如果在 linux 上你可以這樣測試  透過將 tmpfs 掛載到測試資料夾上可以很明顯的觀察到 obscuration 的效果  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 # 建立測試資料夾並填入測試資料 $ mkdir test $ echo \"test\" &gt; test/test.txt $ ls -al test total 1 drwxr-xr-x 2 root root 4096 Nov  1 10:00 . drwxr-xr-x 4 root root 4096 Nov  1 10:00 .. -rw-r--r-- 1 root root    4 Nov  1 10:00 test.txt $ mountpoint test test is a mountpoint # 掛載 tmpfs 到 test 資料夾 $ sudo mount -t tmpfs tmpfs test $ ls -al test # 檔案消失了 $ mountpoint test test is a mountpoint # 移除掛載 $ sudo umount test test is not a mountpoint $ ls -al test # 檔案又出現了 total 1 drwxr-xr-x 2 root root 4096 Nov  1 10:00 . drwxr-xr-x 4 root root 4096 Nov  1 10:00 .. -rw-r--r-- 1 root root    4 Nov  1 10:00 test.txt      你也可以建立一個 K8s Job 測試    系統也不會阻止你掛上去，所以這可能會造成一些問題，比方說你蓋掉了他的啟動 script  像是掛在 /etc 有可能造成錯誤，/usr 會是比較好的選擇  你需要足夠細心，確保原本 image 內部的資料不會被覆蓋   How to use subPath?  subPath 很好的解決了覆蓋資料以及部份掛載的問題  考慮以下範例   1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion: v1 kind: Pod metadata:   name: my-pod spec:   containers:   - name: my-container     volumeMounts:     - mountPath: /mnt/data       name: my-volume       subPath: dataset1   volumes:   - name: my-volume     emptyDir: {}   概念上還是一樣的  container volume 一樣會對回去 Node 上的資源路徑   原本是這樣嘛     /var/lib/kubelet/pods/1234/volumes/kubernetes.io~empty-dir/my-volume   /mnt/data   多了 subPath 就是新增在 Node 上的資源路徑的最後方  所以掛載路徑會變成     /var/lib/kubelet/pods/1234/volumes/kubernetes.io~empty-dir/my-volume/dataset1   /mnt/data   注意到 subPath 是 相對於 volume root 的 相對路徑  你的 volume root 是 /var/lib/kubelet/pods/1234/volumes/kubernetes.io~empty-dir/my-volume  mountPath 永遠指向你想要東西在哪裡可以存取到      subPath 是加在要使用的 volume 上面，不是 mountPath    如果你掛載在 /mnt/data/inner/myfile  上層的資料夾並不會被掛載進去，意思是當你改動 /mnt/data/inner 的資料，並不會反應回去 Node 上  因為你只掛載了單一檔案到 container 內部      mountPath 如果配合 subPath 使用，可以掛載單一檔案  沒有指定 subPath 的時候(i.e. subPath: \"\")，會掛載整個 volume root 到 mountPath    Kubernetes Volume Type  不同的 Volume Type 有不同的特性  Ephemeral Volume 的生命週期與 Pod 本身的生命週期是綁定的  缺點在於說如果 Pod 被刪除，volume 也會被刪除  針對需要資料持久化的情境，使用 Persistent Volume 會是比較好的選擇   如果你想要把所有的 volume 都掛到同一個地方，那可以參考 Projected Volume   Ephemeral Volume  如果這個 volume 是與 Pod 本身的生命週期綁定的，那我們稱這類 volume 為 Ephemeral Volume   Ephemeral Volume 很適合用於一些暫存的資料，這使得 Pod 可以很輕鬆的被建立以及刪除  而不需要擔心資料持久化的問題   也因為異揮發的特性，在 Pod spec 裡面就可以完整定義 volume 的掛載方式  而不需要仰賴其他的機制   hostPath  如果你要做到跟 Docker volume 一模一樣的功能，Kubernetes 裡面對應到的是 hostPath 這個 volume  指定 host 的路徑，他就能夠讀取到 host 的檔案  不過，這裡的 host 是指哪裡的 host 呢？  什麼意思，因為 Kubernetes 支援多個節點，所以本質上你的 app 有可能執行在不同的節點上  而 hostPath 裡面的 host 是指定到 節點上 的資源路徑   不過，hostPath 沒有到很安全，因為你可以直接存取到 host 的檔案  如果操作不當，可能會洩漏 system credential，甚至是能夠讓 container 存取 privileged 的 API  有些使用情境下是可以接受的，比如說你需要讀取系統層級的 log   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion: v1 kind: Pod metadata:   name: hostpath-example-linux spec:   containers:   - name: example-container     image: registry.k8s.io/test-webserver     volumeMounts:     - mountPath: /foo       name: example-volume       readOnly: true   volumes:   - name: example-volume     hostPath:       path: /data/foo       type: Directory   emptyDir  如果你只是單純的需要一個暫存的空間，那可以考慮使用 emptyDir  在 Pod 被建立的時候，volume 會被建立，反之被刪除的時候也會跟著一起被刪掉   所有 Pod 裡面的 container 都可以任一讀寫到這個 volume  比方說你可以掛一個 sidecar container 來做 log 的收集與處理  主要的 container 將 log 寫到相同的 volume 裡面      有關 sidecar container 可以參考 Kubernetes 從零開始 - Sidecar 與 Lifecycle Hook 組合技 | Shawn Hsu    1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion: v1 kind: Pod metadata:   name: test-pd spec:   containers:   - image: registry.k8s.io/test-webserver     name: test-container     volumeMounts:     - mountPath: /cache       name: cache-volume   volumes:   - name: cache-volume     emptyDir: {}   根據節點的儲存空間，比方說 SSD 還是 HDD，預設就是會走到該媒介上面  你可以使用 emptyDir.medium 額外指定使用記憶體(但注意到不能說我 SSD HDD 都有所以我想指定其中一個，這是不行的)  以及 emptyDir.sizeLimit 指定 volume 的大小限制      medium 類別         ”“ : 預設，使用節點上的儲存空間(SSD 或是 HDD)     Memory : 使用記憶體      1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 apiVersion: v1 kind: Pod metadata:   name: test-pd spec:   containers:   - image: registry.k8s.io/test-webserver     name: test-container     volumeMounts:     - mountPath: /cache       name: cache-volume   volumes:   - name: cache-volume     emptyDir:       sizeLimit: 500Mi       medium: Memory   configMap and secret  另一種很好用的方式是直接將外掛的參數以及資料直接掛載進 container 裡面  在未習得這個方法之前，如果有類似的需求我都是傳 environment variable 進去  透過 init container 掛載 emptyDir 並寫入檔案然後主 container 掛載相同 emptyDir 存取   但很顯然的，這種方式是有問題的  將資料，尤其是密碼之類的透過 env 傳入是很不安全的行為  所以原生支援掛載還是挺方便的      需要注意的是，無論是 configMap 還是 secret，掛載前他們都必須先被建立  並且以上兩者的掛載都是 readonly 的    具體點來說，掛載進去容器內部的方式是以檔案為主  你可能會想說，secret 也以檔案的方式掛載進去？ 不會有安全隱患嗎？  所以其實針對 secret 的部份，其實是使用 tmpfs  而其本身是將資料都放在 “易揮發” 的儲存空間中，確保不會有任何機會可以寫入永久儲存      資料放在記憶體就一定安全嗎？ 並不是  只是這樣子的手段可以一定程度上降低風險    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 apiVersion: v1 kind: Pod metadata:   name: configmap-pod spec:   containers:   - name: test     image: busybox:1.28     command: ['sh', '-c', 'echo \"The app is running!\" &amp;&amp; tail -f /dev/null']     volumeMounts:       - name: config-vol         mountPath: /etc/config   volumes:   - name: config-vol     configMap:       name: log-config       items:         - key: log_level           path: log_level.conf   以上的例子是只有將一部分的 configMap 掛載進入容器內部  config-vol 是一個 volume 的名稱，目標是 log-config 這個 configMap 的 log_level 這個 key  將這份資料掛載到 log_level.conf 這個檔案裡面   那這個檔案在哪裡呢？  volumeMounts 裡面的 mountPath 就是檔案掛載的路徑  所以檔案完整路徑會是 /etc/config/log_level.conf   你也可以使用 subPath 的方式掛載資料，需要注意的是  Pod 並不會自動拉取新的資料，等於說你需要手動重啟 Pod 才能看到新的資料   Projected Volume  Projected Volume 並不是介於 Ephemeral Volume 與 Persistent Volume 之間的東西  而是一種特殊型態的 volume，將不同類型的 volume 掛載到相同資料夾下並統一管理  目前來說僅有少數 volume 支持這種特性，其中最著名的是 configMap 以及 secret      要想掛載到同一個資料夾底下，這些 volume 必須處於同一個 namespace 下    掛載到相同資料夾底下有什麼好處？  有些 use case 是你的 deployment 需要設定檔，也同時需要 API key 這種東西  如果分開掛載，是不是就分隔兩地了？ 所以開發者們希望有一個統一管理的地方，至少他們是這樣覺得的(可參考 all-in-one volume design)   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 apiVersion: batch/v1 kind: Job metadata:   name: volume-test spec:   template:     spec:       containers:       - name: container-test         image: busybox         command: [           \"sh\",            \"-c\",            \"echo /projected-volume/my-group/my-username;            echo /projected-volume/my-group/my-config\"         ]         volumeMounts:         - name: all-in-one           mountPath: \"/projected-volume\"           readOnly: true       volumes:       - name: all-in-one         projected:           sources:           - secret:               name: mysecret               items:                 - key: username                   path: my-group/my-username           - configMap:               name: myconfigmap               items:                 - key: config                   path: my-group/my-config   如果你的 volume 都掛載到同一個檔案是會錯誤的  比如說都是 mygroup/my-data 就會出錯   Persistent Volume  如果這個 volume 是與 Pod 本身的生命週期相互獨立的，那我們稱這類 volume 為 Persistent Volume   local  local 是指定到 node 上的 掛載資源路徑(如 外接硬碟)  它最終還是依賴於節點本身，你需要依靠節點去存取這些資源      local volume 不支援動態配置    舉例來說，libfuse/sshfs 可以透過 SSH 將遠端的檔案系統掛載到本機上  一樣的概念，檔案系統不在本機上，但你可以 mount 到本機，使得操作起來跟本機的檔案系統一樣   話雖如此，如果你的節點掛了，那你就無法存取到這個 volume 了  volume 還是好的，是因為 node 掛掉  因此，local volume 會在一定程度上受到節點可用性的影響   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 apiVersion: v1 kind: PersistentVolume metadata:   name: example-pv spec:   capacity:     storage: 100Gi   volumeMode: Filesystem   accessModes:   - ReadWriteOnce   persistentVolumeReclaimPolicy: Delete   storageClassName: local-storage   local:     path: /mnt/disks/ssd1   nodeAffinity:     required:       nodeSelectorTerms:       - matchExpressions:         - key: kubernetes.io/hostname           operator: In           values:           - example-node   How should you Use Persistent Volume?  相比於 Ephemeral Volume，Persistent Volume 的使用是比較複雜的  有了 Persistent Volume 其實是不夠的，因為他的生命週期與 Pod 本身獨立  如果 PV 允許直接掛上去，那等於綁定其 lifecycle，而這不是我們所想要的   Persistent Volume Claim 就是用來解決這個問題的  這個 Resource 表示的是 a request for storage by a user  等於說是個令牌的概念，這個 Claim 裡面包含了許多資訊，比方說你要的這個空間要多大、權限是什麼等等的  因為 Persistent Volume 包含了滿多底層的資訊  講好聽點是細節豐富，但實際上就是複雜，使用者不需要知道這麼多東西，因此多個一個 Claim 的概念簡化   如果你要不到想要的資源，可能是目前可用的 PV 他們的空間不夠，或者是權限沒辦法滿足你的需求  其實這些需求可以被寫成所謂的 StorageClass，算是一個分組的概念  每個 PV 基本上可以被歸類到某個 StorageClass 底下(但有些不能)  歸類有啥用呢？ 因為某些 PV 是可以根據 StorageClass 動態建立的      需要動態建立的 StorageClass 必須要啟用 DefaultStorageClass Admission Controller    如果真的遇到只能靜態建立的 StorageClass，那就只能等了   Persistent Volume Claim  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion: v1 kind: PersistentVolumeClaim metadata:   name: myclaim spec:   accessModes:     - ReadWriteOnce   volumeMode: Filesystem   resources:     requests:       storage: 8Gi   storageClassName: slow   selector:     matchLabels:       release: \"stable\"     matchExpressions:       - {key: environment, operator: In, values: [dev]}   PVC 表示的是你希望拿到什麼樣子的 PV  包含他的 Access Modes, Storage Class, 以及大小等等的   如果 PVC 中有指定 StorageClass，則 Control Plane 除了基本大小的確認，也會保證 StorageClass 是相同的  設定成 \"\" 呢？ 你也只能挑選 StorageClass 為 \"\" 的 PV 中選擇      請注意到，storageClassName: \"\" 與沒有設定 storageClassName 是不同的    當你想要改變預設的 default StorageClass 的時候，需要特別注意的是  如果在這中間有任何 PVC 被建立，他們的 StorageClass 不會有預設值的(\"\" 並不是預設值)  所以針對那些沒有 StorageClass 的 PVC，我會自己幫你填入 Default StorageClass   什麼意思？  當新的 Default StorageClass 被指定，所有缺少 StorageClass 的 PVC 都會被自動填入 Default StorageClass  包含像是     空值   storageClassName 這個 key 不存在   的時候，會被填入當前 Default StorageClass  這個行為被稱作 Retroactive default storageClass assignment     那 PVC 要如何被使用，就跟 Ephemeral Volume 一樣，透過 volumeMounts 來掛載   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 apiVersion: v1 kind: Pod metadata:   name: mypod spec:   containers:     - name: myfrontend       image: nginx       volumeMounts:       - mountPath: \"/var/www/html\"         name: mypd   volumes:     - name: mypd       persistentVolumeClaim:         claimName: myclaim   Storage Classes  不同的儲存空間擁有不同的特性，例如說不同的 backup 策略、不同的等級或者是不同的 SLA  cluster administrator 透過定義一或多個 StorageClass 來表示說本 cluster 提供哪些類型的儲存空間   有些 StorageClass 只能透過靜態的方式建立，比如說 local   1 $ kubectl get storageclasses.storage.k8s.io local-path -o yaml    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 apiVersion: storage.k8s.io/v1 kind: StorageClass metadata:   annotations:     defaultVolumeType: local     objectset.rio.cattle.io/applied: H4sIAAAAAAAA/4     objectset.rio.cattle.io/id: \"\"     objectset.rio.cattle.io/owner-gvk: k3s.cattle.io/v1, Kind=Addon     objectset.rio.cattle.io/owner-name: local-storage     objectset.rio.cattle.io/owner-namespace: kube-system     storageclass.kubernetes.io/is-default-class: \"true\"   creationTimestamp: \"2025-10-31T15:16:17Z\"   labels:     objectset.rio.cattle.io/hash: 183f35c65ffbc3064603f43f1580d8c68a2dabd4   name: local-path   resourceVersion: \"282\"   uid: 55db47f6-e45f-4308-b085-cf68e8c9b159 provisioner: rancher.io/local-path reclaimPolicy: Delete volumeBindingMode: WaitForFirstConsumer   一般來說，StorageClass 會有這些 properties     provisioner   parameters   reclaim policy      storageclass.kubernetes.io/is-default-class 是用來標示說這個 StorageClass 是預設的       有些 Volume 是允許你擴展大小的，注意到只能增加不能減少(i.e. allowVolumeExpansion: true)    Put it All Together  在 Pod 層面，你透過 Persistent Volume Claim 來申請一個 volume  Control Plane 會根據你的需求，幫你找到適合的 Persistent Volume  你能拿到的 PV 一定是符合你的要求，某些時候甚至會是超出你的需求  注意到，PV 與 PVC 之間是 1:1 的關係，一個 PVC 只能對應到一個 PV  比方說 50GB 的 PV 是不可能被申請 100GB 的 PVC 所使用的   綁定(i.e. Binding)過後的 PV 可以使用多久呢？  你想要用多久就可以用多久的那種程度   在使用的過程中，任何的刪除都會造成嚴重的傷害  所以基本的保護需要做到(i.e. Use Protection)，比如說     PVC 只有在 “沒有任何 Pod” 使用的情況下才會真正被刪除   PV 只有在 “沒有任何 PVC” 使用的情況下才會真正被刪除   即使正在使用中，你強行刪除也會被阻擋(c.f. Finalizers)   當你使用完畢，PV 可以被其他人重複利用，不過複用之前可能要稍微處理一下  他有以下幾種選擇(i.e. Reclaim Policy)     Retain 保留，將資料保留在磁碟上，由 admin 決定如何處置   Delete 刪除，將資料刪除，磁碟空間釋出   Recycle 回收，本質上就是將資料刪除然後重新使用，這個選項已經被棄用，建議使用 Dynamic Provisioning   不過 1:1 的關係，會不會造成使用上的困擾，比方說我只是想讀取資料而已沒有要寫  有沒有一種辦法允許多個 Pod 使用同一個 PV？  可以透過 Access Modes 來實現     ReadWriteOnce: 單一節點，可讀寫   ReadOnlyMany: 多節點，可讀   ReadWriteMany: 多節點，可讀寫   ReadWriteOncePod: 單一節點內的單一 Pod，可讀寫   PV 與 PVC 是一對一  可沒說 Pod 跟 PVC 是一對一  事實上，透過以上不同的存取模式，同一個 PVC 可以被多個 Pod 使用   Difference Between hostPath and local  同樣都是存取節點上的資料，區分 hostPath 與 local 是有意義的  不單單只是因為掛載的方式差異，更多的是 scheduler 對於兩者有著不同的處理方式      我當然可以用 hostPath 指到一個掛載上去的硬碟  這不會錯，是可以正常運行的    我們知道，Pod 最終會被 scheduler 排程到某一個節點上運行  並且 Pod 會因為各種原因被重新排程，跑到不同的節點上執行  hostPath 在不同的節點上，代表著不同的實體儲存空間   比方說     Node A: /var/log/auth.log   Node B: /var/log/auth.log   即使 hostPath 的路徑相同，它也是在不同的節點上，資料當然是不一樣的  而且 scheduler 並不知曉這件事情  所以這是為什麼 hostPath 被設計成是 Ephemeral Volume   相反的，local 雖然也是依賴於節點，但是它會透過 node affinity 來標示  啥意思？ 外接硬碟理論上同一時間只能被掛載到唯一的節點上，你需要標示說這個 volume 是在哪一個節點上的  如果需要 local 的 Pod 則會被排程到相同的節點上(就是依靠 node affinity 來實現)      為什麼要同一個節點？ 設計 persistent volume 不就是為了抽象化嗎？  是這樣沒錯，但是它不像 object storage 從哪裡都拿的到對吧    References     Persistent Volumes   Projected Volumes   Ephemeral Volumes   Volumes   Using subPath   What is the difference between subPath and mountPath in Kubernetes   Fixing the Subpath Volume Vulnerability in Kubernetes   Mounting a volume over existing data   详解 Kubernetes Volume 的实现原理   Storage Classes   Can we connect multiple pods to the same PVC?  ","categories": ["kubernetes"],
        "tags": ["volume","container","mount","hostpath","local","persistent volume claim","storage class","projected volume","ephemeral volume","persistent volume","subpath","finalizers","reclaim policy","access modes","binding","use protection","dynamic provisioning","default storage class","retroactive default storage class assignment","pv","pvc","readwriteonce","readonlymany","readwritemany","readwriteoncepod"],
        "url": "/kubernetes/kubernetes-volume/",
        "teaser": null
      },{
        "title": "Kubernetes 從零開始 - Pod 高級抽象 Workload Resources",
        "excerpt":"High Level Abstraction  之前提到 Pod 是 Kubernetes 當中部署的最小單位，因為其設計關係(比如說容易被 reschedule 以及被刪除等等)，是比較不適合直接操作的  因此 Kubernetes 提供了更高層次的抽象，讓我們可以更方便的管理 Pod  本文將會走過一遍這些更高階的抽象實作，並且介紹他們的特性      可參考 Kubernetes 從零開始 - 容器基本抽象 Pod | Shawn Hsu    Workload, Workload Resources and Resources  Workload  一個執行在 Kubernetes 上的應用程式，稱之為 Workload   Workload Resources  比方說剛開始認識 Kubernetes 的時候，你會使用 Deployment 建構你的前後端服務  Deployment 就是其中的一種，也是最常見的 Workload Resources   與直接操作 Pod 不同，Workload Resources 提供了更高階的抽象，讓你可以更方便的管理 Pod  假設該節點發生故障，那上面的 Pod 都會被消失嘛，透過 Controller 它會自動幫你恢復到預期狀態  等於說讓 Kubernetes 幫我們管理全部的狀態  我們就能夠專注在應用程式的開發上      有關 Controller 可以參考 Kubernetes 從零開始 - 從自幹 Controller 到理解狀態管理 | Shawn Hsu    除了基本的 Workload Resource 之外，為了讓開發者能有更彈性的資源管理  Custom Resource Definition(CRD) 被引入了  利用 CRD 你可以定義自己的 Resource 將它利用於你的應用程式中  所以 CRD 也可以算是 Workload Resource 的一種      有關 CRD 可以參考 Kubernetes 從零開始 - client-go 實操 CRD | Shawn Hsu    Resources  注意到 Workload Resources 與 Resources 之間的差異  Resources 泛指所有你可以在 Kubernetes 中使用的物件，而 Workload Resources 是其中一個類別  像是你可能有印象的 ConfigMap, Secret, Service 等等都是所謂的 Resource  你可以用 $ kubectl api-resources 來查看所有的 Resources   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 $ kubectl api-resources NAME                              SHORTNAMES   APIVERSION                        NAMESPACED   KIND bindings                                       v1                                true         Binding configmaps                        cm           v1                                true         ConfigMap endpoints                         ep           v1                                true         Endpoints events                            ev           v1                                true         Event limitranges                       limits       v1                                true         LimitRange namespaces                        ns           v1                                false        Namespace nodes                             no           v1                                false        Node persistentvolumeclaims            pvc          v1                                true         PersistentVolumeClaim persistentvolumes                 pv           v1                                false        PersistentVolume pods                              po           v1                                true         Pod podtemplates                                   v1                                true         PodTemplate replicationcontrollers            rc           v1                                true         ReplicationController resourcequotas                    quota        v1                                true         ResourceQuota secrets                                        v1                                true         Secret serviceaccounts                   sa           v1                                true         ServiceAccount services                          svc          v1                                true         Service apiservices                                    apiregistration.k8s.io/v1         false        APIService controllerrevisions                            apps/v1                           true         ControllerRevision daemonsets                        ds           apps/v1                           true         DaemonSet deployments                       deploy       apps/v1                           true         Deployment replicasets                       rs           apps/v1                           true         ReplicaSet statefulsets                      sts          apps/v1                           true         StatefulSet selfsubjectreviews                             authentication.k8s.io/v1          false        SelfSubjectReview tokenreviews                                   authentication.k8s.io/v1          false        TokenReview      也可以用 $ kubectl api-versions 查看所有支援的版本  api-resources 只會列出最低支援的版本，如果同時有多個版本，需要 api-versions 來確認    Brief Introduction to Workload Resources  ReplicaSet  在 Deployment 有寫一句話 A Deployment provides declarative updates for Pods and ReplicaSets.  我們已知 Pod 是最小的部署單位，也可以說是整個 Kubernetes Cluster 的基礎，但現在看起來 ReplicaSet 好像也是 low level abstraction?   ReplicaSet 是用來維護一定數量的 Pod  這些 Pod 是使用相同的 template 定義的，亦即所有內部執行的東西是一樣的  這些 Pods 被稱之為 replica   Why Manage ReplicaSet through Deployment?  ReplicaSet 貴為一種 Workload Resources，他的目的是確保一定數量的 Pod 在運行  但是為什麼文件希望我們使用 Deployment 而不是 ReplicaSet 呢？  ReplicaSet 沒辦法做到 declarative update, 所以他彈性沒有到那麼高  因此官方才會推薦使用 Deployment 來管理 ReplicaSet   只有當     你需要 customize 更新策略   不需要任何更新   的時候，你才應該使用 ReplicaSet      所以 ReplicaSet 並不是 low level abstraction, 他只是一個比較基礎的 workload resource    Deployment  針對不需要管理狀態的 service, 你可以使用 deployment  狀態是啥意思？ 簡單的想法是，能夠被丟掉重開然後不會有任何影響的服務，就稱為不需要管理狀態的服務   資料庫，cache 這些基本上會認定為需要管理狀態的  因為他底層是需要儲存資料在硬碟上的，如果 scheduler 把它排到另一台機器上，那資料就會不見了  因此這種服務，就要避免使用 deployment   相反的，像是 backend server 這種，如果沒有使用 session 之類的來管理，那本質上他也是 stateless 的  因此很適合放在 deployment 裡面  scheduler 可以根據不同的需求將 backend server 安排到不同的機器上，同一時間也不會影響到服務的運作   Stateless Matters  Deployment 可以管理一或多個 Pod 的狀態，而他們通常是 stateless 的服務      你也可以跑 Redis 在 Deployment 上面這樣也不是不行    k8s Controller 會幫你管理 Deployment 的狀態，將他逐步的往你想要的狀態靠近  簡單來說，我預期要有 3 個 Pod 在運行，但是現在只有 2 個，那 Controller 會幫你自動的補上那一個 Pod  Controller 會根據你定義好的 理想的狀態，逐步的實現，並且在達到之後保持在那個狀態  當出錯的時候，Controller 也會自動的幫你修復      k8s 是屬於 declarative programming 也就是告訴你想要的狀態，具體要怎麼做我不管    StatefulSet  說 StatefulSet 就是 Deployment 加上狀態其實不準確  我原本以為單純的就是可能 volume 之類的狀態，事實上 StatefulSet 管理的狀態不只如此   考慮以下     Pod 在設計上就是可以被隨時丟棄的，換言之，如果它被丟棄了，它就不是原本的那個 Pod 對吧   如果使用 Ephemeral Volume，當 Pod 被重新排程，它就不是原本那個 Ephemeral Volume 了對吧   如果透過 Service 訪問一群 Pods，下一次能確定訪問到的是原本的那個 Pod 嗎？   如果啟動順序不同，對於這整組 Pods 來說，是不是也可以視為是不同的？   以上這些問題，是 StatefulSet 要解決的  它想要確保的是，我訪問的、存取的以及連線的，都要是原本的那個  大致上可以歸類為，”穩定的” 以及 “有序的”  符合以上就可以考慮使用 StatefulSet   針對上述提問，解決辦法如下     給予每一個 Pod 一個 unique 的名字(i.e. pod-0, pod-1, … pod-N)，我們就當作他是相同的            可是 Pod 生命是短暫的這件事情還是成立阿？ 它還是會被刪除或重新排程  相同的 identity 才是重點，因為這個 identity 有可能需要跟其他穩定資源綁定之類的，如 Persistent Volume           volumeClaimTemplates 允許 StatefulSet 獨立生成一個 PVC，每個獨立的 Pod 都有一個獨立的 PVC，進而取得獨立的 PV            其實你也可以單純的用單一 Volume，共享 PV(存取模式可能要稍微注意就是)           service 有一種東西叫做 Headless Service，你可以直接透過 domain name 的方式直接連到 Pod 本身            比如說 redis-master.default.svc.cluster.local 這種，原本是 Service name 開頭嘛，但因為你需要直接指定 Pod 本身，所以就變成 Pod name 開頭。以這個例子來說就是有一個 Pod 叫做 redis-master       即使你可以用 domain name 的方式存取，不代表有 load balancing 的功能、cluster IP 或者 kube-proxy 處理的功能。它只是單純的把 domain name 轉換成 ip address 而已           啟動、刪除以及更新順序會嚴格遵照 0 到 N - 1 的順序，啟動的時候是升冪，刪除的時候是降冪            如果前一個沒啟動成功，下一個就會卡住，如果設定不當，可能會導致整組服務無法正常運作(可參考 正式環境上踩到 StatefulSet 的雷，拿到 P1 的教訓)              有關 volume 相關可以參考 Kubernetes 從零開始 - 你的 Volume 到底 Mount 到哪裡去了？ | Shawn Hsu    DaemonSet  daemon 在計算機當中通常指的是背景程式如 sshd  Kubernetes 中，DaemonSet 就是跑在每個節點上的應用程式  注意到，是 每個節點   這些應用程式通常是 node-level 的設施，像是節點等級的 logging, monitoring 等等   那為什麼不直接在節點寫啟動 script?  擁有統一的管理機制是比較方便的，相同的設定語言，相同的習慣   至於 Pod 呢？ 由於其短暫生命週期，執行 daemon 類的服務並不是一個好的選擇  這樣說，是不是 Deployment 也能做到？ 是的，但是 Deployment 並不是最佳選擇  我們說 DaemonSet 會部署在 “每一個節點” 上，Deployment 無法輕易做到   縱使 node affinity 以及 node selector 可以做到  但是變成你要針對每一個節點寫 node selector，這樣你要維護的東西會變很多，而且幾乎長一樣  當有一個節點新增進 cluster 的時候，Deployment 需要手動更新 yaml 來部署新的節點  所以才有 DaemonSet 的出現   Jobs and CronJob  針對單次的任務，使用 Job 會是合理的選擇  為什麼不用 Pod? 答案還是一樣，因為 Pod 的生命週期是短暫的，況且如果遇到那種要重試幾次的任務也不合適   Job 本身可以設定幾個有意思的參數  比如說 .spec.completions 以及 .spec.parallelism  你需要完成這個任務幾次就是 completions，同時執行幾個就是 parallelism  另外還有 .spec.suspend 可以停止 Job 的執行   針對那種要平行處理的任務，parallelism 可以很方便的做到  不過，每個 Pod 執行一樣的任務嗎？ 其實有時候你想要的是每個 Pod 執行一小部份的內容  在 Kubernetes 1.21 中引入了 Indexed Job 的概念   其實概念滿簡單的，就是給每一個 Pod 一個 index，在設計上你就可以根據 index 來分配任務(i.e. 0 ~ N - 1)  有了它，你就不需要額外設置一個 work queue 來分配任務  說是這樣說，等於你在 code 裡面需要自己根據 index 來分配任務，有點麻煩   這部份是透过指定 .spec.completionMode 來實現的  預設情況下是 NonIndexed 但你不需要特別寫上去   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 apiVersion: batch/v1 kind: Job metadata:   name: 'sample-job' spec:   completions: 3   parallelism: 3   completionMode: Indexed   template:     spec:       restartPolicy: Never       containers:       - command:         - 'bash'         - '-c'         - 'echo \"My partition: ${JOB_COMPLETION_INDEX}\"'         image: 'docker.io/library/bash'         name: 'sample-load'     至於說重複的任務，Kubernetes 也有 CronJob  寫法與 Job 類似，最大的差別就是要設定排程(i.e. .spec.schedule)  語法與 crontab 一致   Comparison                  Workload Resource       Goal       State                       ReplicaSet       提供足夠數量的 Pod，需要客製化策略的時候適用       :x:                 Deployment       提供足夠數量的 Pod，底下是 ReplicaSet       :x:                 StatefulSet       給予穩定的 Pod、儲存空間以及網路       :heavy_check_mark:                 DaemonSet       部屬於每個節點上       :heavy_check_mark:                 Job/CronJob       一次或多次重複的任務       :heavy_check_mark:           Rolling Update for Workload Resources  對於高階的 Workload Resources，最重要的是服務不中斷  Deployment, StatefulSet 以及 DaemonSet 都支援 Rolling Update   Rolling Update 的意思是，逐步更新你的服務，至少有一個 Pod 是可以服務使用者的  所以看起來就像是沒有中斷過一樣  針對部屬策略，可以參考 Kubernetes 從零開始 - 部署策略 101 | Shawn Hsu  這邊專注於 “怎麼做”      注意到只有更新 label 或者是 Pod template 才會觸發 Rolling Update  更新 replica 數量並不會觸發 Rolling Update    Rolling Update Example  先建立一個 Deployment 來測試  1 $ kubectl create deployment dd --image nginx:1.16.1 --replicas 3   更新 Deployment 所使用的 image 為 nginx:1.14.2  可以 $ kubectl set 或是 $ kubectl edit 來更新  如果你同步觀察 Pod 的狀態，應該可以觀察到一上一下的現象   然後你可以透過 $ kubectl rollout status 來查看 Rolling Update 的狀態  以及 $ kubectl rollout history 來查看 Rolling Update 的歷史   1 2 3 4 5 $ kubectl rollout history deployment dd  deployment.apps/dd  REVISION  CHANGE-CAUSE 1         &lt;none&gt; 2         &lt;none&gt;   很顯然，這種 revision 資訊滿簡陋的  指定 revision 可以查看該 revision 的詳細資訊   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 $ kubectl rollout history deployment dd --revision 1 deployment.apps/dd with revision #1 Pod Template:   Labels:       app=dd         pod-template-hash=844587458f   Containers:    nginx:     Image:      nginx:1.16.1     Port:       &lt;none&gt;     Host Port:  &lt;none&gt;     Environment:        &lt;none&gt;     Mounts:     &lt;none&gt;   Volumes:      &lt;none&gt;   Node-Selectors:       &lt;none&gt;   Tolerations:  &lt;none&gt;   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 kubectl rollout history deployment dd --revision 2 deployment.apps/dd with revision #2 Pod Template:   Labels:       app=dd         pod-template-hash=6b88c4858d   Containers:    nginx:     Image:      nginx:1.14.2     Port:       &lt;none&gt;     Host Port:  &lt;none&gt;     Environment:        &lt;none&gt;     Mounts:     &lt;none&gt;   Volumes:      &lt;none&gt;   Node-Selectors:       &lt;none&gt;   Tolerations:  &lt;none&gt;   另外，在更新的時候其實也可以暫停的   1 2 $ kubectl rollout pause/resume deployment dd deployment.apps/dd paused/resumed   Revert the Update  你可以透過 $ kubectl rollout undo 來回退到上一個 revision  或者如果你要跳到特定的 revision，可以加上 --to-revision 參數   1 2 $ kubectl rollout undo deployment dd --to-revision 1 deployment.apps/dd rolled back   Scaling Workload Resources  Scaling 就是調整 replica 的數量  在 Kubernetes 中，Deployment, StatefulSet 以及 DaemonSet 都支援 Scaling  以 Deployment 為例，可以手動設定 yaml 中的 replicas 數量   1 2 $ kubectl scale deployment dd --replicas 5 deployment.apps/dd scaled   抑或者是使用 Autoscaler，注意到是 Horizontal Pod Autoscaler(HPA)  而不是 Vertical Pod Autoscaler(VPA)   1 2 $ kubectl autoscale deployment dd --min=1 --max=10 --cpu-percent=50 horizontalpodautoscaler.autoscaling/dd autoscaled      可以參考 Kubernetes 從零開始 - 部署策略 101 | Shawn Hsu    References     Workloads   Workload Management   Kubernetes: what’s the difference between Deployment and Replica set?   Deployment   ReplicaSet   DaemonSet   Job   CronJob  ","categories": ["kubernetes"],
        "tags": ["deployment","statefulset","daemonset","job","cronjob","replicaset","abstraction","workload","workload resources","resources","rolling update","indexed job","job completion mode","headless service"],
        "url": "/kubernetes/kubernetes-workloads/",
        "teaser": null
      },{
        "title": "多開 Goroutine 的效能瓶頸以及 Garbage Collection 對其的影響",
        "excerpt":"Preface  Goroutine 作為 Golang 的一大特色  其輕量且快速的特性，使得其的開銷相比 process 以及 thread 要來的低上許多  因此人們往往會將 Goroutine 用在非同步的操作上，比如說 I/O 或者是 CPU-bound 的操作  用以達到併發的效果      有關 Goroutine 的介紹可以參考 Goroutine 與 Golang Runtime Scheduler | Shawn Hsu    Goroutine 本身屬於 coroutine，縱使他的開銷相對較低，也還是會有一定的成本  比如說 Golang Runtime Scheduler 必須要管理這些 Goroutine 進行任務調度  頻繁的 context switch 會導致 scheduler 本身都在處理調度任務，反而實際的任務根本沒在執行   這些種種的問題都是我們在開發時需要注意的地方  而本篇文章將會詳細的探討 Goroutine 本身的開銷以及 Garbage Collection 是如何處理這些 Goroutine 的   Stack vs. Heap Allocation  有哪些東西是不會被 GC 處理的?  non-pointer 的 value 是不需要被 GC 處理的(slice 那些除外)  因為在 compile time 的時候就可以決定他的生命週期了  這種資料位於 stack 上，普遍被稱為 stack allocation   針對那些動態的記憶體存取，比方說根據使用者的輸入決定記憶體大小  或者是指標類型的，因為你不太能確定這塊記憶體會被怎麼用以及哪時候可以安全的被回收  這種動態的記憶體存取就是 GC 需要主動介入的   Introduction to Garbage Collection  以前在學 C 語言的時候，或多或少可能都聽過 malloc 完要記得 free 這句話  這是因為在 C 語言中，記憶體的管理是由開發者自己來負責的  如果你忘記 free 掉你 malloc 的記憶體，就會造成 memory leak   阿 人類就很懶惰嘛，所以就想說能不能有一個機制來幫我們管理記憶體  Garbage Collection 就是這樣一個機制，它會自動幫你管理記憶體  當你沒有要使用記憶體的時候，GC 會自動幫你回收這些記憶體(所以你可以用完就丟著，有人會幫你收拾)      local variable 不需要讓 GC 處理，因為 compile time 就可以決定他的生命週期了  需要 GC 通常是你不知道哪時候不會用到，可以被回收，比如說 slice    GC 有很多種實作，比如 Reference Counting 以及 Mark and Sweep   GC Mechanism  Reference Counting  怎麼定義這塊記憶體可以被回收？ 最簡單的想法就是看他有沒有正在被使用  假設這塊記憶體正在被 2 個人使用，那想當然不要回收它是比較正確的  這種作法被稱為 reference counting   當你把每個變數的 reference count 都記錄下來  當 reference count 為 0 的時候，就可以把這塊記憶體回收了   Mark and Sweep  Mark and Sweep 是另一種 GC 的實作方式  他是 Tracing GC 的一種，這種方式會建立一個 依賴關係圖(稱為 object graph)  這個關係圖會紀錄每一個 object 彼此使用的關係，所以你可以很輕易的觀察哪些資源正在被使用  之後就可以透過這張關係圖去區分說哪些資源是可以被回收的   你需要兩個步驟執行它     建立 object graph(i.e. scanning)   掃描 object graph 並回收不需要的記憶體(i.e. mark and sweep)   具體一點來說，GC 會走訪整個 object graph(從 root 開始)     mark            每一個走到的 object 它都會標記一下，表示這個 object 是活著的(i.e. live)           sweep            全部都標完之後，那些沒有被標記到的 object 就可以被回收了           這個就是 mark and sweep 的概念        ref: The Green Tea Garbage Collector    Cyclic Reference  你會想，建立 object graph 不就是 Reference Counting？  不一樣！ 因為 Reference Counting 沒辦法區分出 循環引用的問題  cycle import 的情況下，reference count 會永遠不會歸零，所以這塊記憶體永遠不會被回收  但是 Mark and Sweep 不會   因為 object graph 其實是有 root 節點的，這些 root 節點可以是 local/global variables  從 root 節點往下找所有 object 使用情況，如果遇到 循環引用 但是它沒辦法從 root 節點走到，那它還是會被回收  所以 Mark and Sweep 可以正確的回收這些循環引用的記憶體   Moving vs Non-moving GC  各位不曉得還記不記得所謂的 Fragmentation 問題  你動態分配的記憶體 free 掉之後，就會這邊一塊那邊一塊的  等到你真的需要一塊很大的連續記憶體的時候，你就沒辦法操作   這種情況是所謂的 External Fragmentation        ref: Compact Forwarding Information    GC 同樣也會面臨到相同的挑戰，既然記憶體是分散的  你是不是能手動把它整理在一起(就是每一塊記憶體都是緊緊貼著彼此的)，這樣碎裂化的問題就可以得到緩解？  這種方式稱為 Moving GC   具體來說怎麼做呢？  GC 會幫記憶體搬家，但是呢 所有使用到相同記憶體的地方都要更新(很合理嘛不然東西會錯掉)  為了確保你不會存取到錯誤的記憶體，在這個階段你的程式會被暫停   舊的記憶體位置放所謂的 forwarding pointer，這個 pointer 指向新的記憶體位置  GC 會走訪整個 object graph，當碰到 forwarding pointer 的時候它就會更新成新的記憶體位置  到最後所有相對應的 reference 都會被更新   1 2 object -&gt; old memory location object -&gt; forwarding pointer -&gt; new memory location   Garbage Collection Trade-offs  在上計算機組織的時候，有一個概念是 要讓常跑的東西跑得快，這樣整體效能就會有很大的提昇  對於 Garbage Collection 來說，也是一樣的道理   GC 需要不間斷的運行，才能確保最高的記憶體利用效率  但是一直執行垃圾回收，對你本身的 application 來說也是會有一定的影響的  比如說你的程式會被暫停，這樣的話你的程式就會變得很慢   垃圾回收機制本身會有哪些開銷呢？                  type       description                       CPU time       能夠影響 CPU time 的會是 object graph 的大小 因為如果圖很大，走訪的時間就會比較久                 Memory       object graph 會直接影響到記憶體的使用量 並且加上其他 metadata，這些都會佔用記憶體           真實世界中，你的記憶體不大可能是一直平穩的(i.e. steady-state)，比較多時候會是一下高一下低的狀況  比方說 web application, 可能遇到做活動流量突然爆增，這時候 GC 的開銷就會變大  而且你還要考慮這些記憶體，有可能會經過很多個 GC cycle, 也就是它執行很久(釋放的少，使用的多)   我們可以藉由調整 GC 執行間隔換取不同的效能  比如說你想要節省 CPU 你就可以把 GC 的執行間隔拉長，但是這樣的話記憶體就會被佔用很多(因為沒清理)  同樣的，如果你想要節省記憶體，你就可以把 GC 的執行間隔拉短，但是這樣的話 CPU 就會被佔用很多(因為一直跑)   舉例來說     假設一個 golang app 處於 steady state，記憶體使用量是 10MiB/s(意思就是每秒固定多 alloc 10MiB 的記憶體)   假設當前 live heap 的大小是 10MiB   GC 需要花費 1 cpu-second 掃描 100MiB 的記憶體   如果     GC 執行 cycle 是 1 cpu-second，那麼表示            在 1-cpu second 內，有 額外的 10MiB 記憶體被索取，目前的 live heap 大小是原本的 10MiB 加上 10MiB，總共 20MiB       GC 需要花費 0.1 cpu-second 來掃描 當前的 live heap(也就是 10MiB)       所以 GC 佔用 CPU 的使用率是 10%           GC 執行 cycle 是 2 cpu-second，那麼表示            在 2-cpu second 內，有 額外的 20MiB 記憶體被索取，目前的 live heap 大小是原本的 10MiB 加上 20MiB，總共 30MiB       GC 需要花費 仍然是 0.1 cpu-second 來掃描 當前的 live heap(也就是 10MiB)       所以 GC 佔用 CPU 的使用率是 5%              為什麼 GC 只掃描 live heap 而不是整個 heap  你不需要掃描完整的 heap，如果套用 Mark and Sweep  從當前 live heap 如果走不到，那就代表它可以被回收  所以實際上的開銷會是 1.) 當前 live heap 的大小以及 2.) 能夠從 live heap 被走到的少量記憶體(額外的記憶體)    當你調整 GC cycle 的速度調慢 50% 的時候，雖然 GC 的 CPU 使用率 降低 50%，但是記憶體使用量 增加 50%   GOGC  你可以透過調整 GOGC 的參數  GOGC 的數值是 CPU 與 memory 的 trade-off  當數值越大，Target Heap 的容量越大，GC 的間隔會被拉長，進而導致 CPU overhead 降低，反之亦然      設計上 Target Heap memory 會影響 GC 的間隔，它就是這樣設計的    GOMEMLIMIT  不過無限上調 GOGC 的數值是不合理的，因為記憶體數量終其有上限  所以可以設定 memory limit(i.e. GOMEMLIMIT)  這個限制主要是針對 Heap memory 的限制   不過如果你設定了 memory limit 又把 GC 關閉，就會造成 Thrashing 的狀況  你把 GC 關了意味著記憶體不會回收，同時設定記憶體上限，最終記憶體就會被用光  進而導致你的程式逐漸失去回應(i.e. stall)   為了避免這種狀況發生，Golang Runtime 其實將 memory limit 視為是 非硬性規定  因為如果你兩個都設定了，最終一定會導致 Thrashing 的狀況發生  那不如讓你 bypass 掉這個限制，讓你的程式能夠繼續跑比較重要 是吧？  也就是說 application 可以拿到 CPU time 來跑，換句話說，GC 分到的 CPU time 就會變少  等於 GC 的 cycle 會被拉長，可以理解吧      cycle 拉長，一個 cpu-second 內原本能跑 0.3 cpu-second 的 GC，現在只有拿到 0.1 cpu-second 的 GC  原本等待時間是 0.7 cpu-second 變成 0.9 cpu-second    GC 在一個 cycle 內能使用的 CPU time 通常會有一個上限，比如說最多只能佔 50% 的 CPU time  這樣的好處在於說，你的 application 最差只會慢 一倍      GC 關掉，100% CPU time 都是 application 拿，假設花 10 秒  GC 開啟，50% CPU time 是 GC 拿，50% CPU time 是 application 拿，因為每個 cycle application 執行時間少了一半，所以總共花 20 秒  因此是 10秒 變 20秒，所以是一倍    Evolution of Golang Garbage Collector  Stop-the-World Mark and Sweep  Golang 早期的實作是 Mark and Sweep 並且屬於 Non-moving GC  根據不同的 GC 實作，有的會需要 Stop-the-World(i.e. STW) 的機制，也就是說你的程式會被暫停，直到 GC 完成  這種作法無疑是犧牲了應用程式的效能，Golang 在這方面，選擇 GC 與 application 同步執行(i.e. not fully Stop-the-World GC)   即使是 Stop-the-World 的作法，Mark and Sweep 還是要走過完整的 Live Heap  也就是說 STW 的情況下，GC 暫停的時間與 Live Heap 的大小成正比  又因為 STW 會導致應用程式暫停，所以 latency 會被拉高   latency 維持比較低通常是比較好的狀況，代表使用者可以有比較好的體驗(按下按鈕到看到結果的時間是相對短暫的)  既然 STW 的作法會導致較高的延遲，自然而然就變成 not fully Stop-the-World GC 的實作      low latency != low throughput    Tricolor Mark and Sweep  相比於 Mark and Sweep，Tricolor Mark and Sweep 在實作上引入了額外的一種狀態(處理中 的狀態)  使 GC 可以針對不同狀態的 object 進行不同的處理  這些狀態我們會使用三種顏色表示                  color       description                       white       沒辦法被拜訪到的 object，之後可以被回收                 gray       可以拜訪到的 object，但是需要進一步掃描                 black       確定正在使用中的 object           當 GC 開始的時候，所有的 object 都會被標記為 white  從 GC root 開始先把走得到的標記成 gray  再把 gray 標記為 black 直到所有 處理中(gray) 的 object 都消失  這時候只剩下 white 跟 black 的 object   因為我們確定說 black 的 object 是正在使用中的 object，所以就繼續放著  至於 white 的 object 就是可以被回收的 object  至此，GC 就完成了        ref: Tracing garbage collection      但是光有 Tricolor 的機制其實還不足  既然 Golang 選擇 not fully Stop-the-World GC，其中一個挑戰就是如何確保資料的正確性   不同於 Mark and Sweep 這種 STW GC  同時執行 GC 與 application，因為 app 會去動記憶體嘛，那是不是有可能之前標記過得資料被移動，導致 GC 標記錯誤  然後執行就會有問題   Write Barrier  即使是 Tricolor Mark and Sweep，GC 本身也需要額外透過 Write Barrier 的幫忙來解決記憶體移動，然後標記的問題  注意到 Race 本身還是存在的，只是說 Write Barrier 可以確保這個 Race 的結果會被 GC 所得知，進而避免錯誤的處理   舉例來說，假設 A 已經被標記為 黑色 (Black)（代表 GC 已掃描完 A），而 B 是 灰色 (Gray)  原本的連結是 B -&gt; C  但是呢，app 突然將連結改成 A -&gt; C 並切斷了 B -&gt; C  在沒有 Write Barrier 的情況下：     下一個掃描步驟會把 B 標記為黑色，但因為 B -&gt; C 已斷開，GC 沒機會把 C 染灰   而 A 已經是黑色，GC 不會回頭再掃描 A，所以也不會發現 A -&gt; C   最後，C 仍維持 白色 (White)，導致這個還在被使用的物件被當成垃圾回收掉   所以 Write Barrier 就是用來解決此類問題的   Dijkstra barrier  至於說他是怎麼做的呢？   首先，Tricolor 的 GC 需要嚴格遵守以下的 invariant      No black object may contain a pointer to a white object.    啥意思？ 因為 white object 會被 GC 掉，但如果被 reference 到，它應該要是 gray 的   再來的問題是，我要檢查誰？ 只有指標是需要的，一般變數因為生命週期是可以預期的，所以 GC 可以不處理  當任何 pointer update(i.e. read、write 或是 read and write) 發生的時候都需要做再一次的檢查  為什麼是當 pointer update? 因為 GC 是透過 pointer 來拜訪 object 的  它指到誰 指到哪 很重要   怎麼做就有趣了  Golang 原本只有單純的 Dijkstra barrier  實作上來說也挺簡單的   1 2 3 writePointer(slot, ptr):     shade(ptr)     *slot = ptr   shade() 必須先將 ptr 標記為 gray  因為如果他是白色的，就會違反 invariant  把它變成 gray 的另一個目的是讓 GC 不要把 ptr 當成是 white object 回收掉        ref: Understanding Garbage Collection in Go    不過他也有缺點  Write Barrier 中提到的問題，Dijkstra barrier 本身有兩種方式可以解決     你需要一個 Stack Memory Barrier   你必須確保 stack 上的資料是 permagray(永久灰)   Golang 選擇後者(因為 Stack Memory Barrier 的成本太高)，但是要怎麼保證 stack 上的資料是 permagray 呢？  你需要一直去檢查 goroutine 的 stack 內的東西是不是合法的(i.e. 符合 invariant)   舉例來說，Heap 內的指標可能會指向 Stack 裡的 White Object  當碰到這種狀況，你必須要確保 Stack 內的 White Object 不會被回收掉  所以重新掃描是必要的(i.e. Stack Rescan)   但這個掃描是很頻繁且耗時的  GC cycle 開始之前要掃一次，結束之前也要再掃一次  每一次的掃描其實都需要 Stop-the-World，不然 stack 內的資料有可能會被改動      Re-scanning the stacks can take 10’s to 100’s of milliseconds in an application with a large number of active goroutines.    Yuasa-style Deletion Write Barrier  Yuasa-style deletion write barrier 與 Dijkstra barrier 的差別在於，Yuasa-style 是標記 被刪除的 object 為 gray  而 Dijkstra barrier 是標記 被更新的 pointer 為 gray  所以 Yuasa-style 才稱為 Deletion Write Barrier      ptr 不需要 shade 嗎？ 因為它其實是被 “chain” 起來的，所以 GC 會找到它    實作上就會是  1 2 3 writePointer(slot, ptr):     shade(*slot)     *slot = ptr   範例圖中要記得，shade() 的對象是 B，因為他才是被拔掉的 object        ref: Understanding Garbage Collection in Go    不過它有比 Dijkstra barrier 還要好嗎？  Yuasa 在 pointer update 之前就會先 shade 舊值，因此不論它被移動到哪裡去，GC 都會找到它  也因為這個特性，Yuasa 的作法不需要 Stack Rescan   Hybrid write barrier  而為了要解決 Dijkstra barrier Stack Rescan 的缺點  新版 Golang GC 的架構採用 hybrid write barrier 來解決這個問題(結合了 Dijkstra barrier 與 Yuasa-style deletion write barrier)   最終 write barrier 的實作會長這樣   1 2 3 4 5 writePointer(slot, ptr):     shade(*slot)     if current stack is grey:         shade(ptr)     *slot = ptr   結合了兩種作法的優點     Dijkstra barrier: 允許 concurrent scan   Yuasa-style deletion write barrier: 不需要 Stack Rescan      雖然 Dijkstra 與 Yuasa 都允許 concurrent scan，但是 Dijkstra 的 concurrent 程度更高    但是新的 hybrid write barrier 實際上無法滿足 Tricolor 的 invariant  相反的是，提供了一個 稍微弱的 invariant      No black object may contain a pointer to a white object. :x:  Any white object pointed to by a black object is reachable from a grey object via a chain of white pointers (it is grey-protected). :heavy_check_mark:    自始至終 GC 的挑戰都是要能夠避免 “mutator” 隱藏 object 的問題  這個稍微弱的 invariant 允許隱藏得很深的 white object 能夠被 GC 所感知，那基本上要能夠走訪就代表它應該是在同一個 “chain” 上的      shade(*slot)            借鑒 Yuasa-style deletion write barrier 的作法，使其不需要進行 Stack Rescan。這個操作可以在 “從 Heap 搬遷到內部 Stack 的過程中” 防止 mutator 試圖隱藏 object           shade(ptr)            借鑒 Dijkstra barrier 的作法。這個操作可以在 “從內部 Stack 搬遷到 Heap 的過程中” 防止 mutator 試圖隱藏 object，不過如果 stack 已經掃完然後變 black，那就不需要了(原本要把 ptr 標記為 gray 做後續處理，但因為 ptr 本身在的 stack 已經掃完了，所以不需要)           Green Tea Garbage Collector  針對 Mark and Sweep 的算法來說，幾乎所有的成本都是在 mark 的過程，sweep 一直以來都沒什麼大問題  具體的差別大概是 90% mark 以及 10% sweep  而在 mark 階段中，大約會花費 35% 的時間是在等待 heap memory 的存取，這顯然是一個可以優化的空間   而造成這個問題的原因是，多數資料是分散在記憶體中的  GC 在執行的時候如果東西不在旁邊(i.e. cache)，就需要跑到 main memory 去拿  而這會浪費很多時間   與其這樣做，不如盡量讓所有資料存取可以在附近就拿到  這樣我們可以縮短 memory 的存取時間，進而提昇 GC 的效率     新的 GC 算法旨在解決上述效能問題  不過它本質上也還是 Mark and Sweep，其核心的概念是      Work with pages, not objects    雖然說是處理 page, 但實際上你還是要處理 object，在某種程度上  所以新的架構會需要 metadata 來做這件事，這個 metadata 有兩個部分     seen: 代表這個 object 被看過   scanned: 代表這個 object 被掃描過   奇怪了？ 為什麼要區分 seen 跟 scanned 呢？  你看過不就等於掃描過了嗎  其實這正是 Green Tea 的精髓所在   早期，每一個 object 只會被加入 work list 一次，但是 page 的方法下，每一個 page 都會被加入 “很多次”  而且不同的地方在於，如果他有 reference 到其他 object，他只會跑過去 標記 seen 而已，他並不會直接跑去掃描他  等到我掃描完一個 work list 之後，我再從 metadata 中找出那些被 seen 過的 object，並且把他們加入到 work list 中  這也是 Green Tea 效率高的關鍵         ref: The Green Tea Garbage Collector    改成掃描 page 的方式會減少 scan 的次數，不過每一次 scan 的時間會變長  但這也是沒關係的，因為每一個 page 裡面的 object 都是緊密相鄰的，也就是說 cache hit 的機率更高對吧  又因為 scan 的次數減少了，也代表 CPU 的壓力會降低                  Flood       Green Tea                                               ref: The Green Tea Garbage Collector       ref: The Green Tea Garbage Collector           並且 Green Tea 可以受益於 CPU 架構的改進，例如 AVX-512 指令集擁有 512 位元的暫存器的大小允許 Green Tea 可以將 metadata 全部塞進去，而且在 vector register 上也有 bitwise operation 的支援，使得 Scanning 的過程僅需少量 CPU cycle 即可完成   Performance Evaluation  根據 Google 官方的測試，Green Tea 普遍情況下可以有 10% ~ 40% 左右的性能提升  不過在某些特定的情況下不是   Green Tea 的做法本質上是將瑣碎的 scan 的次數減少，合併成一個大型的 page scan 來達到加速的效果  問題是，並非每一次都可以這樣 scan  如果遇到 single object per page 的情況，那麼那些優勢就會蕩然無存   注意到，極端情況下，performance 甚至可能會比 Flood 還要差  雖然說實作上有針對 single object per page 的情況做優化，但並沒辦法完全消除  不過，你可能很難遇到這種情況，事實上，只要你能夠掃描 2% 的 page 資料，其效能就能夠超越 Flood 算法   Goroutine Count to Affect Garbage Collection  本次實驗，使用環境如下  1 2 3 4 5 $ uname -a Linux station 6.8.0-90-generic #91~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Nov 20 15:20:45 UTC 2 x86_64 x86_64 x86_64 GNU/Linux  $ go version go version go1.25.5 linux/amd64   實驗內容會開到 100000 個 Goroutine 一起測試  其內容就是單純的 time.Sleep 這樣      先查看，不同數量的 Goroutine 開起來的時候，會需要多少的 Heap 以及 Stack 的空間  可以看到，10w 個 Goroutine 會使用到約 200MB 的 Stack 記憶體  可以得出一個 Goroutine 會先分配 2KB 的空間，跟官方宣稱的一樣   Heap 記憶體的部份，10w 個 Goroutine 則是大概 56MB 左右         而實際測試出來的 GC 時間，也可以看到，基本上 Green Tea 的 GC 時間是比 Flood 要來的短  只不過正如前面提到的，性能提昇的幅度並不是很大  實際跑測試大概落在 14% 左右   關於 benchmark 的實作可以參考 ambersun1234/blog-labs/golang-gc   References     Tracing garbage collection   A Guide to the Go Garbage Collector   Compact Forwarding Information   The Green Tea Garbage Collector   图解Golang的GC算法-三色标记法   design/17503-eliminate-rescan.md   2020 年系統軟體系列課程討論區   Understanding Garbage Collection in Go  ","categories": ["random"],
        "tags": ["golang","goroutine","garbage collection","gc","reference counting","mark and sweep","tricolor mark and sweep","moving gc","non-moving gc","fragmentation","external fragmentation","internal fragmentation","steady state","thrashing","gogc","gomemlimit","green tea gc","flood gc","golang gc","dijkstra barrier","yuasa style deletion write barrier","object graph","live heap","cycle reference","stop the world","write barrier","steady state"],
        "url": "/random/golang-gc/",
        "teaser": null
      },{
        "title": "資料庫 - 機率型資料結構 Bloom Filter 在 Cache 中的應用",
        "excerpt":"Cache Issues  就像我們在 資料庫 - Cache Strategies 與常見的 Solutions | Shawn Hsu 當中提到的  如果碰到惡意攻擊，查詢不在 cache 也不在 database 裡面的資料，那麼所有的請求都會直接到 database，然後又會直接被打爆  所以其實在這種狀況底下，直接讓請求到 database 是不好的選項   當然除了你對它設計 rate limit，阻擋某個 IP 過來的請求  可是這樣也不是一個太好的選項，雖然會動   是不是用一個可以快速查詢的方式，就可以將所有不在 cache 也不在 database 裡面的請求，直接快速返回呢？  顯而易見的，使用 Hash Function 來實作是一個很好的選項   Probabilistic Data Structure  這種要求就會需要用到機率型資料結構  這些資料結構能夠提供 “概略的” 統計資料，比方說     這個資料存不存在   某某資料的數量是多少   等等的   相比於傳統的資料結構，雖然能夠統計出最完整的資料，不過效率上會慢非常多  這也是為什麼機率型資料結構受到歡迎的原因，犧牲一點的準確性，換取極高的效率   Hash Function  要怎麼快速的，比如說查詢資料存不存在  最直覺的作法肯定是將資料本身做個 hash  查詢的時候，就做一樣過 hash function 得出結果，然後看看有沒有這個資料就可以了   所以其實機率型資料結構的實作，就是依靠 hash function 來達成的   Why not Hash Table only?  碰撞問題在算 hash 的時候是很常會遇到的問題  以下的實作方法雖然本質上都是使用 Hash Function 來實作  但是為什麼不單純只算一次，而是比如說      Bloom Filter: 採用 多次 hash 的作法   Cuckoo Filter: 採用 雙 hash 的作法   等多次的算法，目的為何？  其實說白了就是要降低碰撞的概率，降低誤判的機率  機率型結構同時滿足，”空間換時間” 以及 “準確率換時間” 的特性   所以單純的一次 hash 實務上是不太會使用的   Different Types of Probabilistic Data Structure                  Probabilistic Data Structure       Insertion       Query       Deletion       Modification       Space Utilization                       Bloom Filter       $O(K)$       $O(K)$       :x:       :heavy_check_mark:       Very Low                 Scalable Bloom Filter       $O(K)$       $O(K * L)$       :x:       :heavy_check_mark:       Low                 Cuckoo Filter       $O(1)$       $O(1)$       :heavy_check_mark:       :heavy_check_mark:       Medium                 XOR Filter       $O(1)$       $O(1)$       :x:       :x:       High                 Binary Fuse Filter       $O(1)$       $O(1)$       :x:       :x:       Very High              每筆資料所需空間: Bloom Filter &gt; Scalable Bloom Filter &gt; Cuckoo Filter &gt; XOR Filter &gt; Binary Fuse Filter    Bloom Filter  Bloom Filter 是一種機率型資料結構，可以快速判斷資料是否存在  當資料不存在時，可以快速返回結果，避免無謂的查詢   基本的概念是，透過一個巨大的 bit array，將同一筆資料做 多次 hash，然後將結果存到 array 裡面  以下圖來說就是三次 hash        ref: Bloom Filter    假設你做了三次 hash，那麼 hash 出來的結果會被寫入三個不同的位置，不是 0 就是 1  那他要怎麼做查詢呢？  將輸入也做同樣的三次 hash，如果相對位置上的資料都是 1，那這筆資料 “應該” 是存在的   為什麼是應該呢？  因為，hash function 會有碰撞的問題嘛  有可能不同的輸入，hash 出來的結果是相同的  而採用 多次 hash 的作法，可以降低碰撞的機率  但並不是完全可以避免   只有算出來的東西 全部都是 1，那這筆資料可能是存在的(i.e. false positive，因為同一個位置有可能有其他資料 map 到這裡)  那如果有一個位置是 0，那這筆資料就絕對不存在(比如說上圖的 w)     那會不會有一種狀況是，Bloom Filter 的資料滿了呢？  會的吧？ 那到時候是不是整條 bit array 的內容都幾乎是 1 了呢？   在這樣的情況下，Bloom Filter 的準確性就會下降，就是你丟啥進去它都是 true 了  那能不能考慮將資料刪除呢？ 不行，因為有可能同一個位置有多筆資料 map 到同一個位置  強制把它清 0 會讓其他原本準確的資料也一起被清掉  所以 Bloom Filter 不支援刪除   Better Bloom Filter  不過，多次 hash 實務上也是有缺點的，就是慢  所以 Building a Better Bloom Filter 這篇論文提出了一些改進的方式  簡單講就是透過計算兩次的 hash 結果之後，利用以上結果把剩下的全部推論出來   1 2 3 4 5 bit_vector[hash1] = 1 bit_vector[hash2] = 1  for i in range(0, k-1):     bit_vector[hash1 + i * hash2] = 1   而這牽扯到一個數學問題，如果你的 bit_vector 的長度與 hash2 的結果有公因數  那你可能會很快的將 Bloom Filter 全部塞滿   就是說，假設陣列大小是 10  hash2 的結果是 2  那你之後步進的結果就是 2, 4, 6, 8, 0, 2, 4, 6, 8, ...  等於說你有很大的機率會重複，這樣誤判率上升，對吧   與其去限制 hash2 不如控制陣列的大小  既然重點是 互質，將陣列大小設定為質數不就好了？  這也是為什麼工程上通常會設定 100003 而不是 100000   Scalable(Stackable) Bloom Filter  Bloom Filter 是沒辦法做刪除的，而且也沒有辦法擴展  也就是說，在一開始的時候 bit array 的大小就要事先決定好  但是這樣就不好用了阿？ 所以一個變種 Scalable Bloom Filter 就出現了   既然一個 Filter 不夠，那就再加一個  而新增的這一層通常會比原本的大一倍，為了不夠然後又繼續加層數   這樣下去，查詢的時間除了原本的 hash 次數，還要再加上層數(L)  所以是 $O(K \\times L)$  寫入的部份因為只需要寫入最新的那一層(因為舊的那層滿了嘛)，所以是 $O(K)$      如果我重複寫入相同的資料呢？  會需要先檢查，確定不存在再寫嗎？  實務上通常不這樣做，因為新的層數會是先前的一倍大，多幾個 bit 的資料影響不大  而且先查再寫會大幅度的增加 overhead(因為查詢是 $O(K \\times L)$)    Cuckoo Filter  Cuckoo Filter 不同於 Bloom Filter，它採用 雙 hash 的作法，計算出資料的 fingerprint  然後在相對應的位置上標記  而這種作法，也會出現 false positive 的問題(i.e. 相同 fingerprint)   計算 fingerprint 的好處在於，能夠做到 刪除的功能  因為我儲存的是資料的 fingerprint，而不是資料模糊之後的結果(Bloom Filter 儲存的是去特徵化後的結果映射)  我能夠定位到唯一的資料，然後刪除它  不過，hash 的原罪就是會碰撞，有沒有可能 fingerprint 也相同呢？  所以對於刪除 你只能刪除某個，你確定有加入過的資料   那我有多個加入過的資料，然後 fingerprint 也相同呢？  這就要講回到 雙 hash 的作法了(i.e. cuckoo hashing)  兩種 hash function 分別長這樣      $h_1(x) = \\text{hash}(x)$   $h_2(x) = h_1(x) \\oplus \\text{hash}(\\text{fingerprint}(x))$      fingerprint 也可以讓整個 filter 的 size 變得更緊湊  原因在於判斷存在與否僅須看兩個位置，就算旁邊資料是滿的，也不影響判斷  而 Bloom Filter 資料如果塞太滿，會導致誤判的機率上升(就像課本上畫滿重點，整本都是重點，整本也都不是重點)    由於 $\\oplus$(xor) 的特性，你只要知道其中一個 hash 值，你就能夠推導出另一個  那為什麼要算兩個 hash，原因也是要處理碰撞的問題  如果發現 $h_1$ 已經被佔領了，它就會嘗試將資料放到 $h_2$ 的位置上  那如果兩個位置都滿了呢？  cuckoo hashing 的作法會是將舊的資料踢掉，因為你可以算另一個 hash 嘛  所以就一直找下去，如果一直都是滿的就一直踢，直到有空位  當然不太會一直無限找下去啦，所以通常會設定個 threshold，超過就放棄  也就是說 insert 是有可能會失敗的        ref: Cuckoo Hashing    XOR Filter  XOR Filter 的作法則是將儲存的資料變成是 “片段的 fingerprint” 資料  並且只有固定三個片段，然後將這三個片段進行 XOR 運算  得出來的結果，再與 fingerprint 進行比對  如果兩個長的一樣，那這筆資料應該存在   \\[h_1(p_1) \\oplus h_2(p_2) \\oplus h_3(p_3) = \\text{fingerprint(input)}\\]  本質也還是算 hash，所以片段的 fingerprint 也會有碰撞的問題  所以 XOR Filter 也會有 false positive 的問題      都是算 fingerprint，why not Cuckoo Filter?  就還是回到 Why not Hash Table only? 的問題  還是空間最大化利用以及準確率換時間的 trade offs    他的出現旨在取代 Bloom Filter，因為以下各種原因     XOR Filter 比 Bloom Filter 更快            3 + 1 次 hash 比 n 次 hash 快           XOR Filter 所需空間比 Bloom Filter 更小            Bloom Filter 塞太滿，誤判機率會上升           不過他有一個大缺點  要先知道所有儲存資料，你才能開始構件 XOR Filter  因為它本質上是在解方程式，上述的數學式你也看到了  它需要找到 3 個不同的 hash function，使得所有數值填入之後計算出來的等式是成立的  換句話說，XOR Filter 沒辦法 動態新增資料   本質上就是先找到 Degree 1 slot 然後一層一層解析  一個 slot 如果有兩個資料映射到同一個位置，那就不能拆  就是要找到所謂的突破口   很抽象？   \\[x + y = 10 \\\\ y + z = 15 \\\\ x + z = 11\\]  單純看 $x + y = 10$ 你可以很簡單的說出答案，可是這個答案他是與其他等式有相依性的  部份解不一定等於全局解  況且它本質上還是 hash function 你更難猜      上述作法稱為 peeling    所以它就沒用了嗎？  其實有些系統是 read heavy 的，將 XOR Filter 應用在此種狀況可以獲得很好的效果   Binary Fuse Filter  你會發現 XOR Filter 的 peeling 過程其實挺容易失敗的  找到共同解沒有這麼簡單，尤其資料量大的時候，萬一失敗它就要重新設定 hash   那既然問題是資料量大的時候，容易失敗  那麼把資料分組不就解決了，所以現在是 分組 也 分片段     分組: 將整個 array 切成多個大小相同的小 array 且互不重疊，稱為 segment   分片段: 將輸入資料切成 3 個片段(3 次 hash)   而目的是      分組: 為了解決 peeling 失敗   分片段: 犧牲部份準確率換取時間      如果你不分組，那基本等於 XOR Filter    為了要讓 Binary Fuse Filter 跑得又快又好  其中一個特殊要求是，3 組 hash function 的結果，必須座落於相鄰的 segment 中  也就是說      當你算出第一個 hash 結果，找到它應該放在哪個 segment 之後，假設位置 i   第二個 hash 結果必須放在 i + 1 的 segment 中   第三個 hash 結果必須放在 i + 2 的 segment 中      如果都放在同一個 segment，那就是小號的 XOR Filter    這樣的好處是     解決了資料量大可能會出現的 peeling 失敗問題   查詢時間更快速，因為 3 次 hash 結果在記憶體中是相鄰的，可以提高 cache hit rate   peeling 建構過程更順利，只要找到 degree 1 slot 就能夠大幅度提昇找到全局解的機率(因為相鄰，所以找到一個，就可以往後繼續找(燒)，所以才叫 fuse)      有關 cache 可以參考 資料庫 - Cache Strategies 與常見的 Solutions | Shawn Hsu    Redis Example  那就來試一下   1 2 $ docker run -itd -p 6379:6379 -p 8001:8001 --name redis redis/redis-stack $ docker exec -it redis sh      你可以造訪 localhost:8001，這個是 Redis Insight 的 GUI    Bloom Filter  那其實我想試試它撞到會發生什麼事情  所以 Bloom Filter 的錯誤率我會設定成 0.5  然後大小開 10 筆，這樣就比較容易撞到      可看到說，我沒有新增 dog 這筆資料，但它卻說存在  這就是 false positive 的問題(當然也是因為我錯誤率設定很高，所以很容易復現)      有關 Bloom Filter 的指令可以參考 BF    Cuckoo Filter  而 Cuckoo Filter 的操作也類似      注意到，拿這個範例去跟 Bloom Filter 做比較是沒有意義的  因為他們的錯誤率設定不同，大小也不同，所以不能這樣看      有關 Cuckoo Filter 的指令可以參考 CF    References     Probabilistic   Bloom filter   Cuckoo filter   Bloom filter   Cuckoo filter   Bloom Filter Datatype for Redis   資料結構大便當：Bloom Filter   Binary Fuse Filters: Fast and Smaller Than Xor Filters   What is a binary fuse filter?   What is an XOR filter?   Bloom Filters with the Kirsch Mitzenmacher optimization  ","categories": ["database"],
        "tags": ["redis","redis-stack","bloom filter","cuckoo filter","hash function","false positive","scalable bloom filter","cuckoo hashing","probabilistic data structure","data structure","cache","cache penetration","cache avalanche","cache hotspot invalid","xor filter","binary fuse filter","peeling"],
        "url": "/database/database-filter/",
        "teaser": null
      },{
    "title": null,
    "excerpt":"404   Page not found! :(  ","url": "https://ambersuncreates.com/404.html"
  },{
    "title": null,
    "excerpt":"About  Welcome to my blog   I’m a software engineer, mainly specializing in back-end development  In this blog you can find a variety of articles, but they are mainly related to backend, cloud native and blockchain  All of the content is based on my own experiences and insights that I have learned along the way.   Hope you’ll find something interesting here :)   Resume   ","url": "https://ambersuncreates.com/about/"
  },{
    "title": "Posts by Category",
    "excerpt":" ","url": "https://ambersuncreates.com/categories/"
  },{
    "title": null,
    "excerpt":"","url": "https://ambersuncreates.com/"
  },{
    "title": null,
    "excerpt":"var idx = lunr(function () {   this.field('title')   this.field('excerpt')   this.field('categories')   this.field('tags')   this.ref('id')    this.pipeline.remove(lunr.trimmer)    for (var item in store) {     this.add({       title: store[item].title,       excerpt: store[item].excerpt,       categories: store[item].categories,       tags: store[item].tags,       id: item     })   } });  $(document).ready(function() {   $('input#search').on('keyup', function () {     var resultdiv = $('#results');     var query = $(this).val().toLowerCase();     var result =       idx.query(function (q) {         query.split(lunr.tokenizer.separator).forEach(function (term) {           q.term(term, { boost: 100 })           if(query.lastIndexOf(\" \") != query.length-1){             q.term(term, {  usePipeline: false, wildcard: lunr.Query.wildcard.TRAILING, boost: 10 })           }           if (term != \"\"){             q.term(term, {  usePipeline: false, editDistance: 1, boost: 1 })           }         })       });     resultdiv.empty();     resultdiv.prepend(''+result.length+' Result(s) found ');     for (var item in result) {       var ref = result[item].ref;       if(store[ref].teaser){         var searchitem =           ''+             ''+               ''+                 ''+store[ref].title+''+               ' '+               ''+                 ''+               ''+               ''+store[ref].excerpt.split(\" \").splice(0,20).join(\" \")+'... '+             ''+           '';       }       else{     \t  var searchitem =           ''+             ''+               ''+                 ''+store[ref].title+''+               ' '+               ''+store[ref].excerpt.split(\" \").splice(0,20).join(\" \")+'... '+             ''+           '';       }       resultdiv.append(searchitem);     }   }); }); ","url": "https://ambersuncreates.com/assets/js/lunr/lunr-en.js"
  },{
    "title": null,
    "excerpt":"step1list = new Array(); step1list[\"ΦΑΓΙΑ\"] = \"ΦΑ\"; step1list[\"ΦΑΓΙΟΥ\"] = \"ΦΑ\"; step1list[\"ΦΑΓΙΩΝ\"] = \"ΦΑ\"; step1list[\"ΣΚΑΓΙΑ\"] = \"ΣΚΑ\"; step1list[\"ΣΚΑΓΙΟΥ\"] = \"ΣΚΑ\"; step1list[\"ΣΚΑΓΙΩΝ\"] = \"ΣΚΑ\"; step1list[\"ΟΛΟΓΙΟΥ\"] = \"ΟΛΟ\"; step1list[\"ΟΛΟΓΙΑ\"] = \"ΟΛΟ\"; step1list[\"ΟΛΟΓΙΩΝ\"] = \"ΟΛΟ\"; step1list[\"ΣΟΓΙΟΥ\"] = \"ΣΟ\"; step1list[\"ΣΟΓΙΑ\"] = \"ΣΟ\"; step1list[\"ΣΟΓΙΩΝ\"] = \"ΣΟ\"; step1list[\"ΤΑΤΟΓΙΑ\"] = \"ΤΑΤΟ\"; step1list[\"ΤΑΤΟΓΙΟΥ\"] = \"ΤΑΤΟ\"; step1list[\"ΤΑΤΟΓΙΩΝ\"] = \"ΤΑΤΟ\"; step1list[\"ΚΡΕΑΣ\"] = \"ΚΡΕ\"; step1list[\"ΚΡΕΑΤΟΣ\"] = \"ΚΡΕ\"; step1list[\"ΚΡΕΑΤΑ\"] = \"ΚΡΕ\"; step1list[\"ΚΡΕΑΤΩΝ\"] = \"ΚΡΕ\"; step1list[\"ΠΕΡΑΣ\"] = \"ΠΕΡ\"; step1list[\"ΠΕΡΑΤΟΣ\"] = \"ΠΕΡ\"; step1list[\"ΠΕΡΑΤΑ\"] = \"ΠΕΡ\"; step1list[\"ΠΕΡΑΤΩΝ\"] = \"ΠΕΡ\"; step1list[\"ΤΕΡΑΣ\"] = \"ΤΕΡ\"; step1list[\"ΤΕΡΑΤΟΣ\"] = \"ΤΕΡ\"; step1list[\"ΤΕΡΑΤΑ\"] = \"ΤΕΡ\"; step1list[\"ΤΕΡΑΤΩΝ\"] = \"ΤΕΡ\"; step1list[\"ΦΩΣ\"] = \"ΦΩ\"; step1list[\"ΦΩΤΟΣ\"] = \"ΦΩ\"; step1list[\"ΦΩΤΑ\"] = \"ΦΩ\"; step1list[\"ΦΩΤΩΝ\"] = \"ΦΩ\"; step1list[\"ΚΑΘΕΣΤΩΣ\"] = \"ΚΑΘΕΣΤ\"; step1list[\"ΚΑΘΕΣΤΩΤΟΣ\"] = \"ΚΑΘΕΣΤ\"; step1list[\"ΚΑΘΕΣΤΩΤΑ\"] = \"ΚΑΘΕΣΤ\"; step1list[\"ΚΑΘΕΣΤΩΤΩΝ\"] = \"ΚΑΘΕΣΤ\"; step1list[\"ΓΕΓΟΝΟΣ\"] = \"ΓΕΓΟΝ\"; step1list[\"ΓΕΓΟΝΟΤΟΣ\"] = \"ΓΕΓΟΝ\"; step1list[\"ΓΕΓΟΝΟΤΑ\"] = \"ΓΕΓΟΝ\"; step1list[\"ΓΕΓΟΝΟΤΩΝ\"] = \"ΓΕΓΟΝ\";  v = \"[ΑΕΗΙΟΥΩ]\"; v2 = \"[ΑΕΗΙΟΩ]\"  function stemWord(w) {   var stem;   var suffix;   var firstch;   var origword = w;   test1 = new Boolean(true);    if(w.length '+result.length+' Result(s) found ');     for (var item in result) {       var ref = result[item].ref;       if(store[ref].teaser){         var searchitem =           ''+             ''+               ''+                 ''+store[ref].title+''+               ' '+               ''+                 ''+               ''+               ''+store[ref].excerpt.split(\" \").splice(0,20).join(\" \")+'... '+             ''+           '';       }       else{     \t  var searchitem =           ''+             ''+               ''+                 ''+store[ref].title+''+               ' '+               ''+store[ref].excerpt.split(\" \").splice(0,20).join(\" \")+'... '+             ''+           '';       }       resultdiv.append(searchitem);     }   }); }); ","url": "https://ambersuncreates.com/assets/js/lunr/lunr-gr.js"
  },{
    "title": null,
    "excerpt":"var store = [   {%- for c in site.collections -%}     {%- if forloop.last -%}       {%- assign l = true -%}     {%- endif -%}     {%- assign docs = c.docs | where_exp:'doc','doc.search != false' -%}     {%- for doc in docs -%}       {%- if doc.header.teaser -%}         {%- capture teaser -%}{{ doc.header.teaser }}{%- endcapture -%}       {%- else -%}         {%- assign teaser = site.teaser -%}       {%- endif -%}       {         \"title\": {{ doc.title | jsonify }},         \"excerpt\":           {%- if site.search_full_content == true -%}             {{ doc.content | newline_to_br |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \"|             strip_html | strip_newlines | jsonify }},           {%- else -%}             {{ doc.content | newline_to_br |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \"|             strip_html | strip_newlines | truncatewords: 50 | jsonify }},           {%- endif -%}         \"categories\": {{ doc.categories | jsonify }},         \"tags\": {{ doc.tags | jsonify }},         \"url\": {{ doc.url | relative_url | jsonify }},         \"teaser\": {{ teaser | relative_url | jsonify }}       }{%- unless forloop.last and l -%},{%- endunless -%}     {%- endfor -%}   {%- endfor -%}{%- if site.lunr.search_within_pages -%},   {%- assign pages = site.pages | where_exp:'doc','doc.search != false' -%}   {%- for doc in pages -%}     {%- if forloop.last -%}       {%- assign l = true -%}     {%- endif -%}   {     \"title\": {{ doc.title | jsonify }},     \"excerpt\":         {%- if site.search_full_content == true -%}           {{ doc.content | newline_to_br |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \"|           strip_html | strip_newlines | jsonify }},         {%- else -%}           {{ doc.content | newline_to_br |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \"|           strip_html | strip_newlines | truncatewords: 50 | jsonify }},         {%- endif -%}       \"url\": {{ doc.url | absolute_url | jsonify }}   }{%- unless forloop.last and l -%},{%- endunless -%}   {%- endfor -%} {%- endif -%}] ","url": "https://ambersuncreates.com/assets/js/lunr/lunr-store.js"
  },{
    "title": "Posts by Tag",
    "excerpt":"","url": "https://ambersuncreates.com/tags/"
  },{
    "title": "operating system",
    "excerpt":"","url": "https://ambersuncreates.com/tags/operating-system/"
  },{
    "title": "c",
    "excerpt":"","url": "https://ambersuncreates.com/tags/c/"
  },{
    "title": "linux",
    "excerpt":"","url": "https://ambersuncreates.com/tags/linux/"
  },{
    "title": "version control",
    "excerpt":"","url": "https://ambersuncreates.com/tags/version-control/"
  },{
    "title": "github",
    "excerpt":"","url": "https://ambersuncreates.com/tags/github/"
  },{
    "title": "blog",
    "excerpt":"","url": "https://ambersuncreates.com/tags/blog/"
  },{
    "title": "docker",
    "excerpt":"","url": "https://ambersuncreates.com/tags/docker/"
  },{
    "title": "kubernetes",
    "excerpt":"","url": "https://ambersuncreates.com/tags/kubernetes/"
  },{
    "title": "container",
    "excerpt":"","url": "https://ambersuncreates.com/tags/container/"
  },{
    "title": "distroless image",
    "excerpt":"","url": "https://ambersuncreates.com/tags/distroless-image/"
  },{
    "title": "network",
    "excerpt":"","url": "https://ambersuncreates.com/tags/network/"
  },{
    "title": "clock",
    "excerpt":"","url": "https://ambersuncreates.com/tags/clock/"
  },{
    "title": "namespaces",
    "excerpt":"","url": "https://ambersuncreates.com/tags/namespaces/"
  },{
    "title": "python",
    "excerpt":"","url": "https://ambersuncreates.com/tags/python/"
  },{
    "title": "gil",
    "excerpt":"","url": "https://ambersuncreates.com/tags/gil/"
  },{
    "title": "atomic operation",
    "excerpt":"","url": "https://ambersuncreates.com/tags/atomic-operation/"
  },{
    "title": "parallelism",
    "excerpt":"","url": "https://ambersuncreates.com/tags/parallelism/"
  },{
    "title": "concurrency",
    "excerpt":"","url": "https://ambersuncreates.com/tags/concurrency/"
  },{
    "title": "race condition",
    "excerpt":"","url": "https://ambersuncreates.com/tags/race-condition/"
  },{
    "title": "cpython",
    "excerpt":"","url": "https://ambersuncreates.com/tags/cpython/"
  },{
    "title": "ironpython",
    "excerpt":"","url": "https://ambersuncreates.com/tags/ironpython/"
  },{
    "title": "jython",
    "excerpt":"","url": "https://ambersuncreates.com/tags/jython/"
  },{
    "title": "mutex",
    "excerpt":"","url": "https://ambersuncreates.com/tags/mutex/"
  },{
    "title": "threading",
    "excerpt":"","url": "https://ambersuncreates.com/tags/threading/"
  },{
    "title": "multiprocessing",
    "excerpt":"","url": "https://ambersuncreates.com/tags/multiprocessing/"
  },{
    "title": "subprocess",
    "excerpt":"","url": "https://ambersuncreates.com/tags/subprocess/"
  },{
    "title": "fork",
    "excerpt":"","url": "https://ambersuncreates.com/tags/fork/"
  },{
    "title": "yield",
    "excerpt":"","url": "https://ambersuncreates.com/tags/yield/"
  },{
    "title": "timeout",
    "excerpt":"","url": "https://ambersuncreates.com/tags/timeout/"
  },{
    "title": "cooperative scheduling",
    "excerpt":"","url": "https://ambersuncreates.com/tags/cooperative-scheduling/"
  },{
    "title": "specialized",
    "excerpt":"","url": "https://ambersuncreates.com/tags/specialized/"
  },{
    "title": "pep-703",
    "excerpt":"","url": "https://ambersuncreates.com/tags/pep-703/"
  },{
    "title": "thread-safe",
    "excerpt":"","url": "https://ambersuncreates.com/tags/thread-safe/"
  },{
    "title": "lock",
    "excerpt":"","url": "https://ambersuncreates.com/tags/lock/"
  },{
    "title": "shared memory",
    "excerpt":"","url": "https://ambersuncreates.com/tags/shared-memory/"
  },{
    "title": "message passing",
    "excerpt":"","url": "https://ambersuncreates.com/tags/message-passing/"
  },{
    "title": "python virtual machine",
    "excerpt":"","url": "https://ambersuncreates.com/tags/python-virtual-machine/"
  },{
    "title": "pvm",
    "excerpt":"","url": "https://ambersuncreates.com/tags/pvm/"
  },{
    "title": "cprofile",
    "excerpt":"","url": "https://ambersuncreates.com/tags/cprofile/"
  },{
    "title": "pep-659",
    "excerpt":"","url": "https://ambersuncreates.com/tags/pep-659/"
  },{
    "title": "starving",
    "excerpt":"","url": "https://ambersuncreates.com/tags/starving/"
  },{
    "title": "disassemble",
    "excerpt":"","url": "https://ambersuncreates.com/tags/disassemble/"
  },{
    "title": "api",
    "excerpt":"","url": "https://ambersuncreates.com/tags/api/"
  },{
    "title": "restful",
    "excerpt":"","url": "https://ambersuncreates.com/tags/restful/"
  },{
    "title": "stateless",
    "excerpt":"","url": "https://ambersuncreates.com/tags/stateless/"
  },{
    "title": "url length limit",
    "excerpt":"","url": "https://ambersuncreates.com/tags/url-length-limit/"
  },{
    "title": "backward compatibility",
    "excerpt":"","url": "https://ambersuncreates.com/tags/backward-compatibility/"
  },{
    "title": "batch endpoint",
    "excerpt":"","url": "https://ambersuncreates.com/tags/batch-endpoint/"
  },{
    "title": "bulk operation",
    "excerpt":"","url": "https://ambersuncreates.com/tags/bulk-operation/"
  },{
    "title": "batch operation",
    "excerpt":"","url": "https://ambersuncreates.com/tags/batch-operation/"
  },{
    "title": "http 414",
    "excerpt":"","url": "https://ambersuncreates.com/tags/http-414/"
  },{
    "title": "http 301",
    "excerpt":"","url": "https://ambersuncreates.com/tags/http-301/"
  },{
    "title": "redirect",
    "excerpt":"","url": "https://ambersuncreates.com/tags/redirect/"
  },{
    "title": "session",
    "excerpt":"","url": "https://ambersuncreates.com/tags/session/"
  },{
    "title": "jwt",
    "excerpt":"","url": "https://ambersuncreates.com/tags/jwt/"
  },{
    "title": "cookie",
    "excerpt":"","url": "https://ambersuncreates.com/tags/cookie/"
  },{
    "title": "http",
    "excerpt":"","url": "https://ambersuncreates.com/tags/http/"
  },{
    "title": "grpc",
    "excerpt":"","url": "https://ambersuncreates.com/tags/grpc/"
  },{
    "title": "rpc",
    "excerpt":"","url": "https://ambersuncreates.com/tags/rpc/"
  },{
    "title": "json-rpc",
    "excerpt":"","url": "https://ambersuncreates.com/tags/json-rpc/"
  },{
    "title": "design pattern",
    "excerpt":"","url": "https://ambersuncreates.com/tags/design-pattern/"
  },{
    "title": "protobuf",
    "excerpt":"","url": "https://ambersuncreates.com/tags/protobuf/"
  },{
    "title": "grpc-gateway",
    "excerpt":"","url": "https://ambersuncreates.com/tags/grpc-gateway/"
  },{
    "title": "stub",
    "excerpt":"","url": "https://ambersuncreates.com/tags/stub/"
  },{
    "title": "github action",
    "excerpt":"","url": "https://ambersuncreates.com/tags/github-action/"
  },{
    "title": "ci",
    "excerpt":"","url": "https://ambersuncreates.com/tags/ci/"
  },{
    "title": "cd",
    "excerpt":"","url": "https://ambersuncreates.com/tags/cd/"
  },{
    "title": "gpg",
    "excerpt":"","url": "https://ambersuncreates.com/tags/gpg/"
  },{
    "title": "ssh",
    "excerpt":"","url": "https://ambersuncreates.com/tags/ssh/"
  },{
    "title": "yubikey",
    "excerpt":"","url": "https://ambersuncreates.com/tags/yubikey/"
  },{
    "title": "mfa",
    "excerpt":"","url": "https://ambersuncreates.com/tags/mfa/"
  },{
    "title": "extension-cable",
    "excerpt":"","url": "https://ambersuncreates.com/tags/extension-cable/"
  },{
    "title": "subkey",
    "excerpt":"","url": "https://ambersuncreates.com/tags/subkey/"
  },{
    "title": "keyserver",
    "excerpt":"","url": "https://ambersuncreates.com/tags/keyserver/"
  },{
    "title": "gpg agent",
    "excerpt":"","url": "https://ambersuncreates.com/tags/gpg-agent/"
  },{
    "title": "osi",
    "excerpt":"","url": "https://ambersuncreates.com/tags/osi/"
  },{
    "title": "rfc",
    "excerpt":"","url": "https://ambersuncreates.com/tags/rfc/"
  },{
    "title": "endian",
    "excerpt":"","url": "https://ambersuncreates.com/tags/endian/"
  },{
    "title": "http0.9",
    "excerpt":"","url": "https://ambersuncreates.com/tags/http0-9/"
  },{
    "title": "http1.0",
    "excerpt":"","url": "https://ambersuncreates.com/tags/http1-0/"
  },{
    "title": "http1.1",
    "excerpt":"","url": "https://ambersuncreates.com/tags/http1-1/"
  },{
    "title": "cache",
    "excerpt":"","url": "https://ambersuncreates.com/tags/cache/"
  },{
    "title": "etag",
    "excerpt":"","url": "https://ambersuncreates.com/tags/etag/"
  },{
    "title": "nginx",
    "excerpt":"","url": "https://ambersuncreates.com/tags/nginx/"
  },{
    "title": "unit test",
    "excerpt":"","url": "https://ambersuncreates.com/tags/unit-test/"
  },{
    "title": "TDD",
    "excerpt":"","url": "https://ambersuncreates.com/tags/tdd/"
  },{
    "title": "dependency injection",
    "excerpt":"","url": "https://ambersuncreates.com/tags/dependency-injection/"
  },{
    "title": "dual boot",
    "excerpt":"","url": "https://ambersuncreates.com/tags/dual-boot/"
  },{
    "title": "redis",
    "excerpt":"","url": "https://ambersuncreates.com/tags/redis/"
  },{
    "title": "transaction",
    "excerpt":"","url": "https://ambersuncreates.com/tags/transaction/"
  },{
    "title": "rdp",
    "excerpt":"","url": "https://ambersuncreates.com/tags/rdp/"
  },{
    "title": "aof",
    "excerpt":"","url": "https://ambersuncreates.com/tags/aof/"
  },{
    "title": "memory hierarchy",
    "excerpt":"","url": "https://ambersuncreates.com/tags/memory-hierarchy/"
  },{
    "title": "cache warming",
    "excerpt":"","url": "https://ambersuncreates.com/tags/cache-warming/"
  },{
    "title": "cache aside",
    "excerpt":"","url": "https://ambersuncreates.com/tags/cache-aside/"
  },{
    "title": "read through",
    "excerpt":"","url": "https://ambersuncreates.com/tags/read-through/"
  },{
    "title": "write through",
    "excerpt":"","url": "https://ambersuncreates.com/tags/write-through/"
  },{
    "title": "write back",
    "excerpt":"","url": "https://ambersuncreates.com/tags/write-back/"
  },{
    "title": "write around",
    "excerpt":"","url": "https://ambersuncreates.com/tags/write-around/"
  },{
    "title": "redis cluster",
    "excerpt":"","url": "https://ambersuncreates.com/tags/redis-cluster/"
  },{
    "title": "memcached",
    "excerpt":"","url": "https://ambersuncreates.com/tags/memcached/"
  },{
    "title": "distributed lock",
    "excerpt":"","url": "https://ambersuncreates.com/tags/distributed-lock/"
  },{
    "title": "bloom filter",
    "excerpt":"","url": "https://ambersuncreates.com/tags/bloom-filter/"
  },{
    "title": "cache avalanche",
    "excerpt":"","url": "https://ambersuncreates.com/tags/cache-avalanche/"
  },{
    "title": "cache hotspot invalid",
    "excerpt":"","url": "https://ambersuncreates.com/tags/cache-hotspot-invalid/"
  },{
    "title": "cache penetration",
    "excerpt":"","url": "https://ambersuncreates.com/tags/cache-penetration/"
  },{
    "title": "database",
    "excerpt":"","url": "https://ambersuncreates.com/tags/database/"
  },{
    "title": "isolation level",
    "excerpt":"","url": "https://ambersuncreates.com/tags/isolation-level/"
  },{
    "title": "mvcc",
    "excerpt":"","url": "https://ambersuncreates.com/tags/mvcc/"
  },{
    "title": "read uncommitted",
    "excerpt":"","url": "https://ambersuncreates.com/tags/read-uncommitted/"
  },{
    "title": "read committed",
    "excerpt":"","url": "https://ambersuncreates.com/tags/read-committed/"
  },{
    "title": "repeatable reads",
    "excerpt":"","url": "https://ambersuncreates.com/tags/repeatable-reads/"
  },{
    "title": "serializable",
    "excerpt":"","url": "https://ambersuncreates.com/tags/serializable/"
  },{
    "title": "snapshot",
    "excerpt":"","url": "https://ambersuncreates.com/tags/snapshot/"
  },{
    "title": "two-phase locking",
    "excerpt":"","url": "https://ambersuncreates.com/tags/two-phase-locking/"
  },{
    "title": "optimistic locking",
    "excerpt":"","url": "https://ambersuncreates.com/tags/optimistic-locking/"
  },{
    "title": "pessimistic locking",
    "excerpt":"","url": "https://ambersuncreates.com/tags/pessimistic-locking/"
  },{
    "title": "write skew",
    "excerpt":"","url": "https://ambersuncreates.com/tags/write-skew/"
  },{
    "title": "phantom read",
    "excerpt":"","url": "https://ambersuncreates.com/tags/phantom-read/"
  },{
    "title": "lost update",
    "excerpt":"","url": "https://ambersuncreates.com/tags/lost-update/"
  },{
    "title": "read-modify-write",
    "excerpt":"","url": "https://ambersuncreates.com/tags/read-modify-write/"
  },{
    "title": "lock promotion",
    "excerpt":"","url": "https://ambersuncreates.com/tags/lock-promotion/"
  },{
    "title": "predicate lock",
    "excerpt":"","url": "https://ambersuncreates.com/tags/predicate-lock/"
  },{
    "title": "index range lock",
    "excerpt":"","url": "https://ambersuncreates.com/tags/index-range-lock/"
  },{
    "title": "serializable snapshot isolation",
    "excerpt":"","url": "https://ambersuncreates.com/tags/serializable-snapshot-isolation/"
  },{
    "title": "ssi",
    "excerpt":"","url": "https://ambersuncreates.com/tags/ssi/"
  },{
    "title": "dirty read",
    "excerpt":"","url": "https://ambersuncreates.com/tags/dirty-read/"
  },{
    "title": "non-repeatable read",
    "excerpt":"","url": "https://ambersuncreates.com/tags/non-repeatable-read/"
  },{
    "title": "unit of work",
    "excerpt":"","url": "https://ambersuncreates.com/tags/unit-of-work/"
  },{
    "title": "atomicity",
    "excerpt":"","url": "https://ambersuncreates.com/tags/atomicity/"
  },{
    "title": "consistency",
    "excerpt":"","url": "https://ambersuncreates.com/tags/consistency/"
  },{
    "title": "isolation",
    "excerpt":"","url": "https://ambersuncreates.com/tags/isolation/"
  },{
    "title": "durability",
    "excerpt":"","url": "https://ambersuncreates.com/tags/durability/"
  },{
    "title": "base",
    "excerpt":"","url": "https://ambersuncreates.com/tags/base/"
  },{
    "title": "basically available",
    "excerpt":"","url": "https://ambersuncreates.com/tags/basically-available/"
  },{
    "title": "soft state",
    "excerpt":"","url": "https://ambersuncreates.com/tags/soft-state/"
  },{
    "title": "eventually consistent",
    "excerpt":"","url": "https://ambersuncreates.com/tags/eventually-consistent/"
  },{
    "title": "orm",
    "excerpt":"","url": "https://ambersuncreates.com/tags/orm/"
  },{
    "title": "sql",
    "excerpt":"","url": "https://ambersuncreates.com/tags/sql/"
  },{
    "title": "n+1",
    "excerpt":"","url": "https://ambersuncreates.com/tags/n-1/"
  },{
    "title": "temp table",
    "excerpt":"","url": "https://ambersuncreates.com/tags/temp-table/"
  },{
    "title": "temporary table",
    "excerpt":"","url": "https://ambersuncreates.com/tags/temporary-table/"
  },{
    "title": "index",
    "excerpt":"","url": "https://ambersuncreates.com/tags/index/"
  },{
    "title": "histogram",
    "excerpt":"","url": "https://ambersuncreates.com/tags/histogram/"
  },{
    "title": "partial index",
    "excerpt":"","url": "https://ambersuncreates.com/tags/partial-index/"
  },{
    "title": "secondary index",
    "excerpt":"","url": "https://ambersuncreates.com/tags/secondary-index/"
  },{
    "title": "clustered index",
    "excerpt":"","url": "https://ambersuncreates.com/tags/clustered-index/"
  },{
    "title": "non-clustered index",
    "excerpt":"","url": "https://ambersuncreates.com/tags/non-clustered-index/"
  },{
    "title": "hash index",
    "excerpt":"","url": "https://ambersuncreates.com/tags/hash-index/"
  },{
    "title": "b-tree index",
    "excerpt":"","url": "https://ambersuncreates.com/tags/b-tree-index/"
  },{
    "title": "b+ tree index",
    "excerpt":"","url": "https://ambersuncreates.com/tags/b-tree-index/"
  },{
    "title": "index seek",
    "excerpt":"","url": "https://ambersuncreates.com/tags/index-seek/"
  },{
    "title": "index scan",
    "excerpt":"","url": "https://ambersuncreates.com/tags/index-scan/"
  },{
    "title": "table scan",
    "excerpt":"","url": "https://ambersuncreates.com/tags/table-scan/"
  },{
    "title": "full table scan",
    "excerpt":"","url": "https://ambersuncreates.com/tags/full-table-scan/"
  },{
    "title": "low cardinality",
    "excerpt":"","url": "https://ambersuncreates.com/tags/low-cardinality/"
  },{
    "title": "high cardinality",
    "excerpt":"","url": "https://ambersuncreates.com/tags/high-cardinality/"
  },{
    "title": "cardinality",
    "excerpt":"","url": "https://ambersuncreates.com/tags/cardinality/"
  },{
    "title": "composite index",
    "excerpt":"","url": "https://ambersuncreates.com/tags/composite-index/"
  },{
    "title": "bitmap index",
    "excerpt":"","url": "https://ambersuncreates.com/tags/bitmap-index/"
  },{
    "title": "dense index",
    "excerpt":"","url": "https://ambersuncreates.com/tags/dense-index/"
  },{
    "title": "sparse index",
    "excerpt":"","url": "https://ambersuncreates.com/tags/sparse-index/"
  },{
    "title": "reverse index",
    "excerpt":"","url": "https://ambersuncreates.com/tags/reverse-index/"
  },{
    "title": "inverted index",
    "excerpt":"","url": "https://ambersuncreates.com/tags/inverted-index/"
  },{
    "title": "sstable",
    "excerpt":"","url": "https://ambersuncreates.com/tags/sstable/"
  },{
    "title": "lsm tree",
    "excerpt":"","url": "https://ambersuncreates.com/tags/lsm-tree/"
  },{
    "title": "avl tree",
    "excerpt":"","url": "https://ambersuncreates.com/tags/avl-tree/"
  },{
    "title": "red-black tree",
    "excerpt":"","url": "https://ambersuncreates.com/tags/red-black-tree/"
  },{
    "title": "linear probing",
    "excerpt":"","url": "https://ambersuncreates.com/tags/linear-probing/"
  },{
    "title": "double hashing",
    "excerpt":"","url": "https://ambersuncreates.com/tags/double-hashing/"
  },{
    "title": "fragmentation",
    "excerpt":"","url": "https://ambersuncreates.com/tags/fragmentation/"
  },{
    "title": "external fragmentation",
    "excerpt":"","url": "https://ambersuncreates.com/tags/external-fragmentation/"
  },{
    "title": "internal fragmentation",
    "excerpt":"","url": "https://ambersuncreates.com/tags/internal-fragmentation/"
  },{
    "title": "locality",
    "excerpt":"","url": "https://ambersuncreates.com/tags/locality/"
  },{
    "title": "predicates",
    "excerpt":"","url": "https://ambersuncreates.com/tags/predicates/"
  },{
    "title": "access predicates",
    "excerpt":"","url": "https://ambersuncreates.com/tags/access-predicates/"
  },{
    "title": "filter predicates",
    "excerpt":"","url": "https://ambersuncreates.com/tags/filter-predicates/"
  },{
    "title": "index only scan",
    "excerpt":"","url": "https://ambersuncreates.com/tags/index-only-scan/"
  },{
    "title": "explain",
    "excerpt":"","url": "https://ambersuncreates.com/tags/explain/"
  },{
    "title": "execution plan",
    "excerpt":"","url": "https://ambersuncreates.com/tags/execution-plan/"
  },{
    "title": "golang",
    "excerpt":"","url": "https://ambersuncreates.com/tags/golang/"
  },{
    "title": "coroutine",
    "excerpt":"","url": "https://ambersuncreates.com/tags/coroutine/"
  },{
    "title": "thread",
    "excerpt":"","url": "https://ambersuncreates.com/tags/thread/"
  },{
    "title": "scheduler",
    "excerpt":"","url": "https://ambersuncreates.com/tags/scheduler/"
  },{
    "title": "gm",
    "excerpt":"","url": "https://ambersuncreates.com/tags/gm/"
  },{
    "title": "gmp",
    "excerpt":"","url": "https://ambersuncreates.com/tags/gmp/"
  },{
    "title": "work steal",
    "excerpt":"","url": "https://ambersuncreates.com/tags/work-steal/"
  },{
    "title": "kadane's algorithm",
    "excerpt":"","url": "https://ambersuncreates.com/tags/kadane-s-algorithm/"
  },{
    "title": "subarray",
    "excerpt":"","url": "https://ambersuncreates.com/tags/subarray/"
  },{
    "title": "subarray sum",
    "excerpt":"","url": "https://ambersuncreates.com/tags/subarray-sum/"
  },{
    "title": "cumulative sum",
    "excerpt":"","url": "https://ambersuncreates.com/tags/cumulative-sum/"
  },{
    "title": "dynamic programming",
    "excerpt":"","url": "https://ambersuncreates.com/tags/dynamic-programming/"
  },{
    "title": "prefix sum",
    "excerpt":"","url": "https://ambersuncreates.com/tags/prefix-sum/"
  },{
    "title": "jws",
    "excerpt":"","url": "https://ambersuncreates.com/tags/jws/"
  },{
    "title": "jwe",
    "excerpt":"","url": "https://ambersuncreates.com/tags/jwe/"
  },{
    "title": "jwk",
    "excerpt":"","url": "https://ambersuncreates.com/tags/jwk/"
  },{
    "title": "oauth",
    "excerpt":"","url": "https://ambersuncreates.com/tags/oauth/"
  },{
    "title": "realm",
    "excerpt":"","url": "https://ambersuncreates.com/tags/realm/"
  },{
    "title": "httponly cookie",
    "excerpt":"","url": "https://ambersuncreates.com/tags/httponly-cookie/"
  },{
    "title": "authorization",
    "excerpt":"","url": "https://ambersuncreates.com/tags/authorization/"
  },{
    "title": "cors",
    "excerpt":"","url": "https://ambersuncreates.com/tags/cors/"
  },{
    "title": "website",
    "excerpt":"","url": "https://ambersuncreates.com/tags/website/"
  },{
    "title": "preflight request",
    "excerpt":"","url": "https://ambersuncreates.com/tags/preflight-request/"
  },{
    "title": "same origin",
    "excerpt":"","url": "https://ambersuncreates.com/tags/same-origin/"
  },{
    "title": "chrome",
    "excerpt":"","url": "https://ambersuncreates.com/tags/chrome/"
  },{
    "title": "gin",
    "excerpt":"","url": "https://ambersuncreates.com/tags/gin/"
  },{
    "title": "goroutine",
    "excerpt":"","url": "https://ambersuncreates.com/tags/goroutine/"
  },{
    "title": "channel",
    "excerpt":"","url": "https://ambersuncreates.com/tags/channel/"
  },{
    "title": "ring buffer",
    "excerpt":"","url": "https://ambersuncreates.com/tags/ring-buffer/"
  },{
    "title": "process",
    "excerpt":"","url": "https://ambersuncreates.com/tags/process/"
  },{
    "title": "inter-process communication",
    "excerpt":"","url": "https://ambersuncreates.com/tags/inter-process-communication/"
  },{
    "title": "makechan",
    "excerpt":"","url": "https://ambersuncreates.com/tags/makechan/"
  },{
    "title": "send",
    "excerpt":"","url": "https://ambersuncreates.com/tags/send/"
  },{
    "title": "receive",
    "excerpt":"","url": "https://ambersuncreates.com/tags/receive/"
  },{
    "title": "sudog",
    "excerpt":"","url": "https://ambersuncreates.com/tags/sudog/"
  },{
    "title": "select",
    "excerpt":"","url": "https://ambersuncreates.com/tags/select/"
  },{
    "title": "producer consumer",
    "excerpt":"","url": "https://ambersuncreates.com/tags/producer-consumer/"
  },{
    "title": "blocking send",
    "excerpt":"","url": "https://ambersuncreates.com/tags/blocking-send/"
  },{
    "title": "blocking receive",
    "excerpt":"","url": "https://ambersuncreates.com/tags/blocking-receive/"
  },{
    "title": "non-blocking send",
    "excerpt":"","url": "https://ambersuncreates.com/tags/non-blocking-send/"
  },{
    "title": "non-blocking receive",
    "excerpt":"","url": "https://ambersuncreates.com/tags/non-blocking-receive/"
  },{
    "title": "counter approach",
    "excerpt":"","url": "https://ambersuncreates.com/tags/counter-approach/"
  },{
    "title": "last operation approach",
    "excerpt":"","url": "https://ambersuncreates.com/tags/last-operation-approach/"
  },{
    "title": "store only size - 1 element",
    "excerpt":"","url": "https://ambersuncreates.com/tags/store-only-size-1-element/"
  },{
    "title": "mirror approach",
    "excerpt":"","url": "https://ambersuncreates.com/tags/mirror-approach/"
  },{
    "title": "typescript",
    "excerpt":"","url": "https://ambersuncreates.com/tags/typescript/"
  },{
    "title": "hooks",
    "excerpt":"","url": "https://ambersuncreates.com/tags/hooks/"
  },{
    "title": "obfuscation",
    "excerpt":"","url": "https://ambersuncreates.com/tags/obfuscation/"
  },{
    "title": "webpack",
    "excerpt":"","url": "https://ambersuncreates.com/tags/webpack/"
  },{
    "title": "blockchain",
    "excerpt":"","url": "https://ambersuncreates.com/tags/blockchain/"
  },{
    "title": "ethereum",
    "excerpt":"","url": "https://ambersuncreates.com/tags/ethereum/"
  },{
    "title": "hardhat",
    "excerpt":"","url": "https://ambersuncreates.com/tags/hardhat/"
  },{
    "title": "block",
    "excerpt":"","url": "https://ambersuncreates.com/tags/block/"
  },{
    "title": "observer",
    "excerpt":"","url": "https://ambersuncreates.com/tags/observer/"
  },{
    "title": "observable",
    "excerpt":"","url": "https://ambersuncreates.com/tags/observable/"
  },{
    "title": "subject",
    "excerpt":"","url": "https://ambersuncreates.com/tags/subject/"
  },{
    "title": "publisher",
    "excerpt":"","url": "https://ambersuncreates.com/tags/publisher/"
  },{
    "title": "subscriber",
    "excerpt":"","url": "https://ambersuncreates.com/tags/subscriber/"
  },{
    "title": "topic",
    "excerpt":"","url": "https://ambersuncreates.com/tags/topic/"
  },{
    "title": "vscode",
    "excerpt":"","url": "https://ambersuncreates.com/tags/vscode/"
  },{
    "title": "decorator",
    "excerpt":"","url": "https://ambersuncreates.com/tags/decorator/"
  },{
    "title": "prisma",
    "excerpt":"","url": "https://ambersuncreates.com/tags/prisma/"
  },{
    "title": "query",
    "excerpt":"","url": "https://ambersuncreates.com/tags/query/"
  },{
    "title": "pagination",
    "excerpt":"","url": "https://ambersuncreates.com/tags/pagination/"
  },{
    "title": "cursor",
    "excerpt":"","url": "https://ambersuncreates.com/tags/cursor/"
  },{
    "title": "cursor based pagination",
    "excerpt":"","url": "https://ambersuncreates.com/tags/cursor-based-pagination/"
  },{
    "title": "previous page",
    "excerpt":"","url": "https://ambersuncreates.com/tags/previous-page/"
  },{
    "title": "next page",
    "excerpt":"","url": "https://ambersuncreates.com/tags/next-page/"
  },{
    "title": "uuid",
    "excerpt":"","url": "https://ambersuncreates.com/tags/uuid/"
  },{
    "title": "ulid",
    "excerpt":"","url": "https://ambersuncreates.com/tags/ulid/"
  },{
    "title": "page limit",
    "excerpt":"","url": "https://ambersuncreates.com/tags/page-limit/"
  },{
    "title": "page offset",
    "excerpt":"","url": "https://ambersuncreates.com/tags/page-offset/"
  },{
    "title": "benchmark",
    "excerpt":"","url": "https://ambersuncreates.com/tags/benchmark/"
  },{
    "title": "binary indexed tree",
    "excerpt":"","url": "https://ambersuncreates.com/tags/binary-indexed-tree/"
  },{
    "title": "binary tree",
    "excerpt":"","url": "https://ambersuncreates.com/tags/binary-tree/"
  },{
    "title": "fenwick tree",
    "excerpt":"","url": "https://ambersuncreates.com/tags/fenwick-tree/"
  },{
    "title": "cumulative",
    "excerpt":"","url": "https://ambersuncreates.com/tags/cumulative/"
  },{
    "title": "distributed",
    "excerpt":"","url": "https://ambersuncreates.com/tags/distributed/"
  },{
    "title": "cluster",
    "excerpt":"","url": "https://ambersuncreates.com/tags/cluster/"
  },{
    "title": "CAP",
    "excerpt":"","url": "https://ambersuncreates.com/tags/cap/"
  },{
    "title": "single leader",
    "excerpt":"","url": "https://ambersuncreates.com/tags/single-leader/"
  },{
    "title": "multi leader",
    "excerpt":"","url": "https://ambersuncreates.com/tags/multi-leader/"
  },{
    "title": "replication",
    "excerpt":"","url": "https://ambersuncreates.com/tags/replication/"
  },{
    "title": "scale up",
    "excerpt":"","url": "https://ambersuncreates.com/tags/scale-up/"
  },{
    "title": "scale out",
    "excerpt":"","url": "https://ambersuncreates.com/tags/scale-out/"
  },{
    "title": "leaderless",
    "excerpt":"","url": "https://ambersuncreates.com/tags/leaderless/"
  },{
    "title": "sequential io",
    "excerpt":"","url": "https://ambersuncreates.com/tags/sequential-io/"
  },{
    "title": "random io",
    "excerpt":"","url": "https://ambersuncreates.com/tags/random-io/"
  },{
    "title": "quorum consensus",
    "excerpt":"","url": "https://ambersuncreates.com/tags/quorum-consensus/"
  },{
    "title": "raft consensus",
    "excerpt":"","url": "https://ambersuncreates.com/tags/raft-consensus/"
  },{
    "title": "write ahead log",
    "excerpt":"","url": "https://ambersuncreates.com/tags/write-ahead-log/"
  },{
    "title": "logical log",
    "excerpt":"","url": "https://ambersuncreates.com/tags/logical-log/"
  },{
    "title": "store procedure",
    "excerpt":"","url": "https://ambersuncreates.com/tags/store-procedure/"
  },{
    "title": "split brain",
    "excerpt":"","url": "https://ambersuncreates.com/tags/split-brain/"
  },{
    "title": "quorum",
    "excerpt":"","url": "https://ambersuncreates.com/tags/quorum/"
  },{
    "title": "raft",
    "excerpt":"","url": "https://ambersuncreates.com/tags/raft/"
  },{
    "title": "swagger",
    "excerpt":"","url": "https://ambersuncreates.com/tags/swagger/"
  },{
    "title": "openapi",
    "excerpt":"","url": "https://ambersuncreates.com/tags/openapi/"
  },{
    "title": "nodejs",
    "excerpt":"","url": "https://ambersuncreates.com/tags/nodejs/"
  },{
    "title": "swagger-ui",
    "excerpt":"","url": "https://ambersuncreates.com/tags/swagger-ui/"
  },{
    "title": "anchor",
    "excerpt":"","url": "https://ambersuncreates.com/tags/anchor/"
  },{
    "title": "mock server",
    "excerpt":"","url": "https://ambersuncreates.com/tags/mock-server/"
  },{
    "title": "json pointer",
    "excerpt":"","url": "https://ambersuncreates.com/tags/json-pointer/"
  },{
    "title": "postgresql",
    "excerpt":"","url": "https://ambersuncreates.com/tags/postgresql/"
  },{
    "title": "mysql",
    "excerpt":"","url": "https://ambersuncreates.com/tags/mysql/"
  },{
    "title": "sql standard",
    "excerpt":"","url": "https://ambersuncreates.com/tags/sql-standard/"
  },{
    "title": "array",
    "excerpt":"","url": "https://ambersuncreates.com/tags/array/"
  },{
    "title": "stack",
    "excerpt":"","url": "https://ambersuncreates.com/tags/stack/"
  },{
    "title": "monotonic",
    "excerpt":"","url": "https://ambersuncreates.com/tags/monotonic/"
  },{
    "title": "next greater element",
    "excerpt":"","url": "https://ambersuncreates.com/tags/next-greater-element/"
  },{
    "title": "leetcode",
    "excerpt":"","url": "https://ambersuncreates.com/tags/leetcode/"
  },{
    "title": "leetcode-496",
    "excerpt":"","url": "https://ambersuncreates.com/tags/leetcode-496/"
  },{
    "title": "leetcode-503",
    "excerpt":"","url": "https://ambersuncreates.com/tags/leetcode-503/"
  },{
    "title": "leetcode-1475",
    "excerpt":"","url": "https://ambersuncreates.com/tags/leetcode-1475/"
  },{
    "title": "leetcode-2434",
    "excerpt":"","url": "https://ambersuncreates.com/tags/leetcode-2434/"
  },{
    "title": "gist",
    "excerpt":"","url": "https://ambersuncreates.com/tags/gist/"
  },{
    "title": "pg_trgm",
    "excerpt":"","url": "https://ambersuncreates.com/tags/pg-trgm/"
  },{
    "title": "fuzzy search",
    "excerpt":"","url": "https://ambersuncreates.com/tags/fuzzy-search/"
  },{
    "title": "integration test",
    "excerpt":"","url": "https://ambersuncreates.com/tags/integration-test/"
  },{
    "title": "mock",
    "excerpt":"","url": "https://ambersuncreates.com/tags/mock/"
  },{
    "title": "e2e test",
    "excerpt":"","url": "https://ambersuncreates.com/tags/e2e-test/"
  },{
    "title": "backtracking",
    "excerpt":"","url": "https://ambersuncreates.com/tags/backtracking/"
  },{
    "title": "divide-and-conquer",
    "excerpt":"","url": "https://ambersuncreates.com/tags/divide-and-conquer/"
  },{
    "title": "recursion",
    "excerpt":"","url": "https://ambersuncreates.com/tags/recursion/"
  },{
    "title": "fixed point",
    "excerpt":"","url": "https://ambersuncreates.com/tags/fixed-point/"
  },{
    "title": "ieee754",
    "excerpt":"","url": "https://ambersuncreates.com/tags/ieee754/"
  },{
    "title": "floating point",
    "excerpt":"","url": "https://ambersuncreates.com/tags/floating-point/"
  },{
    "title": "binary",
    "excerpt":"","url": "https://ambersuncreates.com/tags/binary/"
  },{
    "title": "overflow",
    "excerpt":"","url": "https://ambersuncreates.com/tags/overflow/"
  },{
    "title": "underflow",
    "excerpt":"","url": "https://ambersuncreates.com/tags/underflow/"
  },{
    "title": "message queue",
    "excerpt":"","url": "https://ambersuncreates.com/tags/message-queue/"
  },{
    "title": "consumer",
    "excerpt":"","url": "https://ambersuncreates.com/tags/consumer/"
  },{
    "title": "producer",
    "excerpt":"","url": "https://ambersuncreates.com/tags/producer/"
  },{
    "title": "event",
    "excerpt":"","url": "https://ambersuncreates.com/tags/event/"
  },{
    "title": "kafka",
    "excerpt":"","url": "https://ambersuncreates.com/tags/kafka/"
  },{
    "title": "amqp",
    "excerpt":"","url": "https://ambersuncreates.com/tags/amqp/"
  },{
    "title": "mqtt",
    "excerpt":"","url": "https://ambersuncreates.com/tags/mqtt/"
  },{
    "title": "jms",
    "excerpt":"","url": "https://ambersuncreates.com/tags/jms/"
  },{
    "title": "rabbitmq",
    "excerpt":"","url": "https://ambersuncreates.com/tags/rabbitmq/"
  },{
    "title": "dlq",
    "excerpt":"","url": "https://ambersuncreates.com/tags/dlq/"
  },{
    "title": "pubsub",
    "excerpt":"","url": "https://ambersuncreates.com/tags/pubsub/"
  },{
    "title": "p2p",
    "excerpt":"","url": "https://ambersuncreates.com/tags/p2p/"
  },{
    "title": "pull protocol",
    "excerpt":"","url": "https://ambersuncreates.com/tags/pull-protocol/"
  },{
    "title": "push protocol",
    "excerpt":"","url": "https://ambersuncreates.com/tags/push-protocol/"
  },{
    "title": "polling",
    "excerpt":"","url": "https://ambersuncreates.com/tags/polling/"
  },{
    "title": "long polling",
    "excerpt":"","url": "https://ambersuncreates.com/tags/long-polling/"
  },{
    "title": "webhook",
    "excerpt":"","url": "https://ambersuncreates.com/tags/webhook/"
  },{
    "title": "webrtc",
    "excerpt":"","url": "https://ambersuncreates.com/tags/webrtc/"
  },{
    "title": "websocket",
    "excerpt":"","url": "https://ambersuncreates.com/tags/websocket/"
  },{
    "title": "tcp",
    "excerpt":"","url": "https://ambersuncreates.com/tags/tcp/"
  },{
    "title": "file descriptor",
    "excerpt":"","url": "https://ambersuncreates.com/tags/file-descriptor/"
  },{
    "title": "socket",
    "excerpt":"","url": "https://ambersuncreates.com/tags/socket/"
  },{
    "title": "data migration",
    "excerpt":"","url": "https://ambersuncreates.com/tags/data-migration/"
  },{
    "title": "idempotent",
    "excerpt":"","url": "https://ambersuncreates.com/tags/idempotent/"
  },{
    "title": "upsert",
    "excerpt":"","url": "https://ambersuncreates.com/tags/upsert/"
  },{
    "title": "on-premise",
    "excerpt":"","url": "https://ambersuncreates.com/tags/on-premise/"
  },{
    "title": "saas",
    "excerpt":"","url": "https://ambersuncreates.com/tags/saas/"
  },{
    "title": "alembic",
    "excerpt":"","url": "https://ambersuncreates.com/tags/alembic/"
  },{
    "title": "reversibility",
    "excerpt":"","url": "https://ambersuncreates.com/tags/reversibility/"
  },{
    "title": "atlas",
    "excerpt":"","url": "https://ambersuncreates.com/tags/atlas/"
  },{
    "title": "checkpoint",
    "excerpt":"","url": "https://ambersuncreates.com/tags/checkpoint/"
  },{
    "title": "kafka connect",
    "excerpt":"","url": "https://ambersuncreates.com/tags/kafka-connect/"
  },{
    "title": "cdc",
    "excerpt":"","url": "https://ambersuncreates.com/tags/cdc/"
  },{
    "title": "change data capture",
    "excerpt":"","url": "https://ambersuncreates.com/tags/change-data-capture/"
  },{
    "title": "statement replication",
    "excerpt":"","url": "https://ambersuncreates.com/tags/statement-replication/"
  },{
    "title": "kubectl wait",
    "excerpt":"","url": "https://ambersuncreates.com/tags/kubectl-wait/"
  },{
    "title": "argocd",
    "excerpt":"","url": "https://ambersuncreates.com/tags/argocd/"
  },{
    "title": "logical replication slot",
    "excerpt":"","url": "https://ambersuncreates.com/tags/logical-replication-slot/"
  },{
    "title": "datamesh",
    "excerpt":"","url": "https://ambersuncreates.com/tags/datamesh/"
  },{
    "title": "netflix",
    "excerpt":"","url": "https://ambersuncreates.com/tags/netflix/"
  },{
    "title": "greedy",
    "excerpt":"","url": "https://ambersuncreates.com/tags/greedy/"
  },{
    "title": "greedy algorithm",
    "excerpt":"","url": "https://ambersuncreates.com/tags/greedy-algorithm/"
  },{
    "title": "optimal",
    "excerpt":"","url": "https://ambersuncreates.com/tags/optimal/"
  },{
    "title": "leetcode-322",
    "excerpt":"","url": "https://ambersuncreates.com/tags/leetcode-322/"
  },{
    "title": "leetcode-55",
    "excerpt":"","url": "https://ambersuncreates.com/tags/leetcode-55/"
  },{
    "title": "leetcode-222",
    "excerpt":"","url": "https://ambersuncreates.com/tags/leetcode-222/"
  },{
    "title": "infrastructure as code",
    "excerpt":"","url": "https://ambersuncreates.com/tags/infrastructure-as-code/"
  },{
    "title": "configuration as code",
    "excerpt":"","url": "https://ambersuncreates.com/tags/configuration-as-code/"
  },{
    "title": "iac",
    "excerpt":"","url": "https://ambersuncreates.com/tags/iac/"
  },{
    "title": "cac",
    "excerpt":"","url": "https://ambersuncreates.com/tags/cac/"
  },{
    "title": "pm2",
    "excerpt":"","url": "https://ambersuncreates.com/tags/pm2/"
  },{
    "title": "logging",
    "excerpt":"","url": "https://ambersuncreates.com/tags/logging/"
  },{
    "title": "contextual logging",
    "excerpt":"","url": "https://ambersuncreates.com/tags/contextual-logging/"
  },{
    "title": "slack",
    "excerpt":"","url": "https://ambersuncreates.com/tags/slack/"
  },{
    "title": "sentry",
    "excerpt":"","url": "https://ambersuncreates.com/tags/sentry/"
  },{
    "title": "integration",
    "excerpt":"","url": "https://ambersuncreates.com/tags/integration/"
  },{
    "title": "archive",
    "excerpt":"","url": "https://ambersuncreates.com/tags/archive/"
  },{
    "title": "log rotation",
    "excerpt":"","url": "https://ambersuncreates.com/tags/log-rotation/"
  },{
    "title": "logrotate",
    "excerpt":"","url": "https://ambersuncreates.com/tags/logrotate/"
  },{
    "title": "log4j",
    "excerpt":"","url": "https://ambersuncreates.com/tags/log4j/"
  },{
    "title": "wallet",
    "excerpt":"","url": "https://ambersuncreates.com/tags/wallet/"
  },{
    "title": "seed",
    "excerpt":"","url": "https://ambersuncreates.com/tags/seed/"
  },{
    "title": "hash",
    "excerpt":"","url": "https://ambersuncreates.com/tags/hash/"
  },{
    "title": "mnemonic",
    "excerpt":"","url": "https://ambersuncreates.com/tags/mnemonic/"
  },{
    "title": "wordlist",
    "excerpt":"","url": "https://ambersuncreates.com/tags/wordlist/"
  },{
    "title": "bitcoin",
    "excerpt":"","url": "https://ambersuncreates.com/tags/bitcoin/"
  },{
    "title": "master key",
    "excerpt":"","url": "https://ambersuncreates.com/tags/master-key/"
  },{
    "title": "hierarchical deterministic wallet",
    "excerpt":"","url": "https://ambersuncreates.com/tags/hierarchical-deterministic-wallet/"
  },{
    "title": "deterministic wallet",
    "excerpt":"","url": "https://ambersuncreates.com/tags/deterministic-wallet/"
  },{
    "title": "udp",
    "excerpt":"","url": "https://ambersuncreates.com/tags/udp/"
  },{
    "title": "ssl",
    "excerpt":"","url": "https://ambersuncreates.com/tags/ssl/"
  },{
    "title": "tls",
    "excerpt":"","url": "https://ambersuncreates.com/tags/tls/"
  },{
    "title": "dns",
    "excerpt":"","url": "https://ambersuncreates.com/tags/dns/"
  },{
    "title": "nc",
    "excerpt":"","url": "https://ambersuncreates.com/tags/nc/"
  },{
    "title": "tcpdump",
    "excerpt":"","url": "https://ambersuncreates.com/tags/tcpdump/"
  },{
    "title": "wireshark",
    "excerpt":"","url": "https://ambersuncreates.com/tags/wireshark/"
  },{
    "title": "broadcast",
    "excerpt":"","url": "https://ambersuncreates.com/tags/broadcast/"
  },{
    "title": "load balance",
    "excerpt":"","url": "https://ambersuncreates.com/tags/load-balance/"
  },{
    "title": "ip",
    "excerpt":"","url": "https://ambersuncreates.com/tags/ip/"
  },{
    "title": "telnet",
    "excerpt":"","url": "https://ambersuncreates.com/tags/telnet/"
  },{
    "title": "multicast",
    "excerpt":"","url": "https://ambersuncreates.com/tags/multicast/"
  },{
    "title": "unicast",
    "excerpt":"","url": "https://ambersuncreates.com/tags/unicast/"
  },{
    "title": "cname",
    "excerpt":"","url": "https://ambersuncreates.com/tags/cname/"
  },{
    "title": "dp",
    "excerpt":"","url": "https://ambersuncreates.com/tags/dp/"
  },{
    "title": "fibonacci",
    "excerpt":"","url": "https://ambersuncreates.com/tags/fibonacci/"
  },{
    "title": "coin change",
    "excerpt":"","url": "https://ambersuncreates.com/tags/coin-change/"
  },{
    "title": "leetcode-120",
    "excerpt":"","url": "https://ambersuncreates.com/tags/leetcode-120/"
  },{
    "title": "leetcode-97",
    "excerpt":"","url": "https://ambersuncreates.com/tags/leetcode-97/"
  },{
    "title": "javascript",
    "excerpt":"","url": "https://ambersuncreates.com/tags/javascript/"
  },{
    "title": "queue",
    "excerpt":"","url": "https://ambersuncreates.com/tags/queue/"
  },{
    "title": "event loop",
    "excerpt":"","url": "https://ambersuncreates.com/tags/event-loop/"
  },{
    "title": "first class function",
    "excerpt":"","url": "https://ambersuncreates.com/tags/first-class-function/"
  },{
    "title": "closure",
    "excerpt":"","url": "https://ambersuncreates.com/tags/closure/"
  },{
    "title": "callback",
    "excerpt":"","url": "https://ambersuncreates.com/tags/callback/"
  },{
    "title": "microtask queue",
    "excerpt":"","url": "https://ambersuncreates.com/tags/microtask-queue/"
  },{
    "title": "macrotask queue",
    "excerpt":"","url": "https://ambersuncreates.com/tags/macrotask-queue/"
  },{
    "title": "libuv",
    "excerpt":"","url": "https://ambersuncreates.com/tags/libuv/"
  },{
    "title": "libeio",
    "excerpt":"","url": "https://ambersuncreates.com/tags/libeio/"
  },{
    "title": "lexical scoping",
    "excerpt":"","url": "https://ambersuncreates.com/tags/lexical-scoping/"
  },{
    "title": "solid",
    "excerpt":"","url": "https://ambersuncreates.com/tags/solid/"
  },{
    "title": "architecture",
    "excerpt":"","url": "https://ambersuncreates.com/tags/architecture/"
  },{
    "title": "couple",
    "excerpt":"","url": "https://ambersuncreates.com/tags/couple/"
  },{
    "title": "cohesion",
    "excerpt":"","url": "https://ambersuncreates.com/tags/cohesion/"
  },{
    "title": "scalable",
    "excerpt":"","url": "https://ambersuncreates.com/tags/scalable/"
  },{
    "title": "ai",
    "excerpt":"","url": "https://ambersuncreates.com/tags/ai/"
  },{
    "title": "byzantine fault",
    "excerpt":"","url": "https://ambersuncreates.com/tags/byzantine-fault/"
  },{
    "title": "monotonic read",
    "excerpt":"","url": "https://ambersuncreates.com/tags/monotonic-read/"
  },{
    "title": "vector clock",
    "excerpt":"","url": "https://ambersuncreates.com/tags/vector-clock/"
  },{
    "title": "version vector",
    "excerpt":"","url": "https://ambersuncreates.com/tags/version-vector/"
  },{
    "title": "last write wins",
    "excerpt":"","url": "https://ambersuncreates.com/tags/last-write-wins/"
  },{
    "title": "lamport timestamp",
    "excerpt":"","url": "https://ambersuncreates.com/tags/lamport-timestamp/"
  },{
    "title": "2PC",
    "excerpt":"","url": "https://ambersuncreates.com/tags/2pc/"
  },{
    "title": "3PC",
    "excerpt":"","url": "https://ambersuncreates.com/tags/3pc/"
  },{
    "title": "tudum",
    "excerpt":"","url": "https://ambersuncreates.com/tags/tudum/"
  },{
    "title": "raw hollow",
    "excerpt":"","url": "https://ambersuncreates.com/tags/raw-hollow/"
  },{
    "title": "hollow",
    "excerpt":"","url": "https://ambersuncreates.com/tags/hollow/"
  },{
    "title": "read after write",
    "excerpt":"","url": "https://ambersuncreates.com/tags/read-after-write/"
  },{
    "title": "linearizability",
    "excerpt":"","url": "https://ambersuncreates.com/tags/linearizability/"
  },{
    "title": "zookeeper",
    "excerpt":"","url": "https://ambersuncreates.com/tags/zookeeper/"
  },{
    "title": "atomic broadcast",
    "excerpt":"","url": "https://ambersuncreates.com/tags/atomic-broadcast/"
  },{
    "title": "sequential consistency",
    "excerpt":"","url": "https://ambersuncreates.com/tags/sequential-consistency/"
  },{
    "title": "2pc",
    "excerpt":"","url": "https://ambersuncreates.com/tags/2pc/"
  },{
    "title": "3pc",
    "excerpt":"","url": "https://ambersuncreates.com/tags/3pc/"
  },{
    "title": "same prefix read",
    "excerpt":"","url": "https://ambersuncreates.com/tags/same-prefix-read/"
  },{
    "title": "sharding",
    "excerpt":"","url": "https://ambersuncreates.com/tags/sharding/"
  },{
    "title": "table partitioning",
    "excerpt":"","url": "https://ambersuncreates.com/tags/table-partitioning/"
  },{
    "title": "store program",
    "excerpt":"","url": "https://ambersuncreates.com/tags/store-program/"
  },{
    "title": "rebalance",
    "excerpt":"","url": "https://ambersuncreates.com/tags/rebalance/"
  },{
    "title": "k3d",
    "excerpt":"","url": "https://ambersuncreates.com/tags/k3d/"
  },{
    "title": "service",
    "excerpt":"","url": "https://ambersuncreates.com/tags/service/"
  },{
    "title": "deployment",
    "excerpt":"","url": "https://ambersuncreates.com/tags/deployment/"
  },{
    "title": "configmap",
    "excerpt":"","url": "https://ambersuncreates.com/tags/configmap/"
  },{
    "title": "secret",
    "excerpt":"","url": "https://ambersuncreates.com/tags/secret/"
  },{
    "title": "dig",
    "excerpt":"","url": "https://ambersuncreates.com/tags/dig/"
  },{
    "title": "aws s3",
    "excerpt":"","url": "https://ambersuncreates.com/tags/aws-s3/"
  },{
    "title": "minio",
    "excerpt":"","url": "https://ambersuncreates.com/tags/minio/"
  },{
    "title": "storage",
    "excerpt":"","url": "https://ambersuncreates.com/tags/storage/"
  },{
    "title": "erasure set",
    "excerpt":"","url": "https://ambersuncreates.com/tags/erasure-set/"
  },{
    "title": "bit rot healing",
    "excerpt":"","url": "https://ambersuncreates.com/tags/bit-rot-healing/"
  },{
    "title": "erasure coding",
    "excerpt":"","url": "https://ambersuncreates.com/tags/erasure-coding/"
  },{
    "title": "lifecycle management",
    "excerpt":"","url": "https://ambersuncreates.com/tags/lifecycle-management/"
  },{
    "title": "object expiration",
    "excerpt":"","url": "https://ambersuncreates.com/tags/object-expiration/"
  },{
    "title": "object tiering",
    "excerpt":"","url": "https://ambersuncreates.com/tags/object-tiering/"
  },{
    "title": "object transition",
    "excerpt":"","url": "https://ambersuncreates.com/tags/object-transition/"
  },{
    "title": "object versioning",
    "excerpt":"","url": "https://ambersuncreates.com/tags/object-versioning/"
  },{
    "title": "object lifecycle management",
    "excerpt":"","url": "https://ambersuncreates.com/tags/object-lifecycle-management/"
  },{
    "title": "mc ping",
    "excerpt":"","url": "https://ambersuncreates.com/tags/mc-ping/"
  },{
    "title": "mc admin",
    "excerpt":"","url": "https://ambersuncreates.com/tags/mc-admin/"
  },{
    "title": "mc event",
    "excerpt":"","url": "https://ambersuncreates.com/tags/mc-event/"
  },{
    "title": "mc alias",
    "excerpt":"","url": "https://ambersuncreates.com/tags/mc-alias/"
  },{
    "title": "mc stat",
    "excerpt":"","url": "https://ambersuncreates.com/tags/mc-stat/"
  },{
    "title": "mc admin config",
    "excerpt":"","url": "https://ambersuncreates.com/tags/mc-admin-config/"
  },{
    "title": "mc admin info",
    "excerpt":"","url": "https://ambersuncreates.com/tags/mc-admin-info/"
  },{
    "title": "provisioning",
    "excerpt":"","url": "https://ambersuncreates.com/tags/provisioning/"
  },{
    "title": "multipart",
    "excerpt":"","url": "https://ambersuncreates.com/tags/multipart/"
  },{
    "title": "permission",
    "excerpt":"","url": "https://ambersuncreates.com/tags/permission/"
  },{
    "title": "acl",
    "excerpt":"","url": "https://ambersuncreates.com/tags/acl/"
  },{
    "title": "rbac",
    "excerpt":"","url": "https://ambersuncreates.com/tags/rbac/"
  },{
    "title": "abac",
    "excerpt":"","url": "https://ambersuncreates.com/tags/abac/"
  },{
    "title": "pbac",
    "excerpt":"","url": "https://ambersuncreates.com/tags/pbac/"
  },{
    "title": "casbin",
    "excerpt":"","url": "https://ambersuncreates.com/tags/casbin/"
  },{
    "title": "permission granularity",
    "excerpt":"","url": "https://ambersuncreates.com/tags/permission-granularity/"
  },{
    "title": "role inheritance",
    "excerpt":"","url": "https://ambersuncreates.com/tags/role-inheritance/"
  },{
    "title": "role",
    "excerpt":"","url": "https://ambersuncreates.com/tags/role/"
  },{
    "title": "policy",
    "excerpt":"","url": "https://ambersuncreates.com/tags/policy/"
  },{
    "title": "request",
    "excerpt":"","url": "https://ambersuncreates.com/tags/request/"
  },{
    "title": "matcher",
    "excerpt":"","url": "https://ambersuncreates.com/tags/matcher/"
  },{
    "title": "effect",
    "excerpt":"","url": "https://ambersuncreates.com/tags/effect/"
  },{
    "title": "aslr",
    "excerpt":"","url": "https://ambersuncreates.com/tags/aslr/"
  },{
    "title": "prng",
    "excerpt":"","url": "https://ambersuncreates.com/tags/prng/"
  },{
    "title": "pie",
    "excerpt":"","url": "https://ambersuncreates.com/tags/pie/"
  },{
    "title": "local development",
    "excerpt":"","url": "https://ambersuncreates.com/tags/local-development/"
  },{
    "title": "yaml",
    "excerpt":"","url": "https://ambersuncreates.com/tags/yaml/"
  },{
    "title": "skaffold",
    "excerpt":"","url": "https://ambersuncreates.com/tags/skaffold/"
  },{
    "title": "registry",
    "excerpt":"","url": "https://ambersuncreates.com/tags/registry/"
  },{
    "title": "local cluster",
    "excerpt":"","url": "https://ambersuncreates.com/tags/local-cluster/"
  },{
    "title": "pod",
    "excerpt":"","url": "https://ambersuncreates.com/tags/pod/"
  },{
    "title": "lifecycle",
    "excerpt":"","url": "https://ambersuncreates.com/tags/lifecycle/"
  },{
    "title": "node",
    "excerpt":"","url": "https://ambersuncreates.com/tags/node/"
  },{
    "title": "binding",
    "excerpt":"","url": "https://ambersuncreates.com/tags/binding/"
  },{
    "title": "ephemeral",
    "excerpt":"","url": "https://ambersuncreates.com/tags/ephemeral/"
  },{
    "title": "scale",
    "excerpt":"","url": "https://ambersuncreates.com/tags/scale/"
  },{
    "title": "scheduling",
    "excerpt":"","url": "https://ambersuncreates.com/tags/scheduling/"
  },{
    "title": "workloads",
    "excerpt":"","url": "https://ambersuncreates.com/tags/workloads/"
  },{
    "title": "redis sentinel",
    "excerpt":"","url": "https://ambersuncreates.com/tags/redis-sentinel/"
  },{
    "title": "master-slave",
    "excerpt":"","url": "https://ambersuncreates.com/tags/master-slave/"
  },{
    "title": "distributed system",
    "excerpt":"","url": "https://ambersuncreates.com/tags/distributed-system/"
  },{
    "title": "replication id",
    "excerpt":"","url": "https://ambersuncreates.com/tags/replication-id/"
  },{
    "title": "strong consistency",
    "excerpt":"","url": "https://ambersuncreates.com/tags/strong-consistency/"
  },{
    "title": "failover",
    "excerpt":"","url": "https://ambersuncreates.com/tags/failover/"
  },{
    "title": "sentinel",
    "excerpt":"","url": "https://ambersuncreates.com/tags/sentinel/"
  },{
    "title": "hash slot",
    "excerpt":"","url": "https://ambersuncreates.com/tags/hash-slot/"
  },{
    "title": "partition",
    "excerpt":"","url": "https://ambersuncreates.com/tags/partition/"
  },{
    "title": "cluster bus",
    "excerpt":"","url": "https://ambersuncreates.com/tags/cluster-bus/"
  },{
    "title": "no downtime",
    "excerpt":"","url": "https://ambersuncreates.com/tags/no-downtime/"
  },{
    "title": "high availability",
    "excerpt":"","url": "https://ambersuncreates.com/tags/high-availability/"
  },{
    "title": "local runner",
    "excerpt":"","url": "https://ambersuncreates.com/tags/local-runner/"
  },{
    "title": "helm chart",
    "excerpt":"","url": "https://ambersuncreates.com/tags/helm-chart/"
  },{
    "title": "rancher",
    "excerpt":"","url": "https://ambersuncreates.com/tags/rancher/"
  },{
    "title": "customize runner image",
    "excerpt":"","url": "https://ambersuncreates.com/tags/customize-runner-image/"
  },{
    "title": "helm controller",
    "excerpt":"","url": "https://ambersuncreates.com/tags/helm-controller/"
  },{
    "title": "crd",
    "excerpt":"","url": "https://ambersuncreates.com/tags/crd/"
  },{
    "title": "terratest",
    "excerpt":"","url": "https://ambersuncreates.com/tags/terratest/"
  },{
    "title": "json patch patch",
    "excerpt":"","url": "https://ambersuncreates.com/tags/json-patch-patch/"
  },{
    "title": "strategic merge patch",
    "excerpt":"","url": "https://ambersuncreates.com/tags/strategic-merge-patch/"
  },{
    "title": "json patch",
    "excerpt":"","url": "https://ambersuncreates.com/tags/json-patch/"
  },{
    "title": "kubectl patch",
    "excerpt":"","url": "https://ambersuncreates.com/tags/kubectl-patch/"
  },{
    "title": "helm upgrade",
    "excerpt":"","url": "https://ambersuncreates.com/tags/helm-upgrade/"
  },{
    "title": "kubernetes controller",
    "excerpt":"","url": "https://ambersuncreates.com/tags/kubernetes-controller/"
  },{
    "title": "state",
    "excerpt":"","url": "https://ambersuncreates.com/tags/state/"
  },{
    "title": "wrangler",
    "excerpt":"","url": "https://ambersuncreates.com/tags/wrangler/"
  },{
    "title": "kubernetes operator",
    "excerpt":"","url": "https://ambersuncreates.com/tags/kubernetes-operator/"
  },{
    "title": "reconcile",
    "excerpt":"","url": "https://ambersuncreates.com/tags/reconcile/"
  },{
    "title": "control loop",
    "excerpt":"","url": "https://ambersuncreates.com/tags/control-loop/"
  },{
    "title": "controller pattern",
    "excerpt":"","url": "https://ambersuncreates.com/tags/controller-pattern/"
  },{
    "title": "operator pattern",
    "excerpt":"","url": "https://ambersuncreates.com/tags/operator-pattern/"
  },{
    "title": "self healing",
    "excerpt":"","url": "https://ambersuncreates.com/tags/self-healing/"
  },{
    "title": "operator sdk",
    "excerpt":"","url": "https://ambersuncreates.com/tags/operator-sdk/"
  },{
    "title": "fsm",
    "excerpt":"","url": "https://ambersuncreates.com/tags/fsm/"
  },{
    "title": "finalizer",
    "excerpt":"","url": "https://ambersuncreates.com/tags/finalizer/"
  },{
    "title": "namespaced operator",
    "excerpt":"","url": "https://ambersuncreates.com/tags/namespaced-operator/"
  },{
    "title": "livenessprobe",
    "excerpt":"","url": "https://ambersuncreates.com/tags/livenessprobe/"
  },{
    "title": "readinessprobe",
    "excerpt":"","url": "https://ambersuncreates.com/tags/readinessprobe/"
  },{
    "title": "health check",
    "excerpt":"","url": "https://ambersuncreates.com/tags/health-check/"
  },{
    "title": "leader election",
    "excerpt":"","url": "https://ambersuncreates.com/tags/leader-election/"
  },{
    "title": "leader with lease",
    "excerpt":"","url": "https://ambersuncreates.com/tags/leader-with-lease/"
  },{
    "title": "leader for life",
    "excerpt":"","url": "https://ambersuncreates.com/tags/leader-for-life/"
  },{
    "title": "event filter",
    "excerpt":"","url": "https://ambersuncreates.com/tags/event-filter/"
  },{
    "title": "predicate",
    "excerpt":"","url": "https://ambersuncreates.com/tags/predicate/"
  },{
    "title": "conversion webhook",
    "excerpt":"","url": "https://ambersuncreates.com/tags/conversion-webhook/"
  },{
    "title": "crd migration",
    "excerpt":"","url": "https://ambersuncreates.com/tags/crd-migration/"
  },{
    "title": "saga",
    "excerpt":"","url": "https://ambersuncreates.com/tags/saga/"
  },{
    "title": "microservices",
    "excerpt":"","url": "https://ambersuncreates.com/tags/microservices/"
  },{
    "title": "compensating transaction",
    "excerpt":"","url": "https://ambersuncreates.com/tags/compensating-transaction/"
  },{
    "title": "job",
    "excerpt":"","url": "https://ambersuncreates.com/tags/job/"
  },{
    "title": "kueue",
    "excerpt":"","url": "https://ambersuncreates.com/tags/kueue/"
  },{
    "title": "resource",
    "excerpt":"","url": "https://ambersuncreates.com/tags/resource/"
  },{
    "title": "taints",
    "excerpt":"","url": "https://ambersuncreates.com/tags/taints/"
  },{
    "title": "toleration",
    "excerpt":"","url": "https://ambersuncreates.com/tags/toleration/"
  },{
    "title": "affinity",
    "excerpt":"","url": "https://ambersuncreates.com/tags/affinity/"
  },{
    "title": "taskset",
    "excerpt":"","url": "https://ambersuncreates.com/tags/taskset/"
  },{
    "title": "cluster queue",
    "excerpt":"","url": "https://ambersuncreates.com/tags/cluster-queue/"
  },{
    "title": "local queue",
    "excerpt":"","url": "https://ambersuncreates.com/tags/local-queue/"
  },{
    "title": "resource flavor",
    "excerpt":"","url": "https://ambersuncreates.com/tags/resource-flavor/"
  },{
    "title": "cohort",
    "excerpt":"","url": "https://ambersuncreates.com/tags/cohort/"
  },{
    "title": "dynamic client",
    "excerpt":"","url": "https://ambersuncreates.com/tags/dynamic-client/"
  },{
    "title": "client-go",
    "excerpt":"","url": "https://ambersuncreates.com/tags/client-go/"
  },{
    "title": "clientset",
    "excerpt":"","url": "https://ambersuncreates.com/tags/clientset/"
  },{
    "title": "unsturctured object",
    "excerpt":"","url": "https://ambersuncreates.com/tags/unsturctured-object/"
  },{
    "title": "fake client",
    "excerpt":"","url": "https://ambersuncreates.com/tags/fake-client/"
  },{
    "title": "GVR",
    "excerpt":"","url": "https://ambersuncreates.com/tags/gvr/"
  },{
    "title": "cr",
    "excerpt":"","url": "https://ambersuncreates.com/tags/cr/"
  },{
    "title": "api aggregation",
    "excerpt":"","url": "https://ambersuncreates.com/tags/api-aggregation/"
  },{
    "title": "custom resource definition",
    "excerpt":"","url": "https://ambersuncreates.com/tags/custom-resource-definition/"
  },{
    "title": "custom resource",
    "excerpt":"","url": "https://ambersuncreates.com/tags/custom-resource/"
  },{
    "title": "crd versioning",
    "excerpt":"","url": "https://ambersuncreates.com/tags/crd-versioning/"
  },{
    "title": "clusterrole",
    "excerpt":"","url": "https://ambersuncreates.com/tags/clusterrole/"
  },{
    "title": "conversion",
    "excerpt":"","url": "https://ambersuncreates.com/tags/conversion/"
  },{
    "title": "migration",
    "excerpt":"","url": "https://ambersuncreates.com/tags/migration/"
  },{
    "title": "argo-workflows",
    "excerpt":"","url": "https://ambersuncreates.com/tags/argo-workflows/"
  },{
    "title": "dag",
    "excerpt":"","url": "https://ambersuncreates.com/tags/dag/"
  },{
    "title": "service account",
    "excerpt":"","url": "https://ambersuncreates.com/tags/service-account/"
  },{
    "title": "conditional execution",
    "excerpt":"","url": "https://ambersuncreates.com/tags/conditional-execution/"
  },{
    "title": "telemetry",
    "excerpt":"","url": "https://ambersuncreates.com/tags/telemetry/"
  },{
    "title": "opentelemetry",
    "excerpt":"","url": "https://ambersuncreates.com/tags/opentelemetry/"
  },{
    "title": "trace",
    "excerpt":"","url": "https://ambersuncreates.com/tags/trace/"
  },{
    "title": "log",
    "excerpt":"","url": "https://ambersuncreates.com/tags/log/"
  },{
    "title": "metric",
    "excerpt":"","url": "https://ambersuncreates.com/tags/metric/"
  },{
    "title": "uptrace",
    "excerpt":"","url": "https://ambersuncreates.com/tags/uptrace/"
  },{
    "title": "slog",
    "excerpt":"","url": "https://ambersuncreates.com/tags/slog/"
  },{
    "title": "slog-multi",
    "excerpt":"","url": "https://ambersuncreates.com/tags/slog-multi/"
  },{
    "title": "otelgin",
    "excerpt":"","url": "https://ambersuncreates.com/tags/otelgin/"
  },{
    "title": "distributed trace",
    "excerpt":"","url": "https://ambersuncreates.com/tags/distributed-trace/"
  },{
    "title": "context propagation",
    "excerpt":"","url": "https://ambersuncreates.com/tags/context-propagation/"
  },{
    "title": "baggage",
    "excerpt":"","url": "https://ambersuncreates.com/tags/baggage/"
  },{
    "title": "span",
    "excerpt":"","url": "https://ambersuncreates.com/tags/span/"
  },{
    "title": "span attribute",
    "excerpt":"","url": "https://ambersuncreates.com/tags/span-attribute/"
  },{
    "title": "span link",
    "excerpt":"","url": "https://ambersuncreates.com/tags/span-link/"
  },{
    "title": "trace id",
    "excerpt":"","url": "https://ambersuncreates.com/tags/trace-id/"
  },{
    "title": "trace parent",
    "excerpt":"","url": "https://ambersuncreates.com/tags/trace-parent/"
  },{
    "title": "sampling",
    "excerpt":"","url": "https://ambersuncreates.com/tags/sampling/"
  },{
    "title": "textmappropagator",
    "excerpt":"","url": "https://ambersuncreates.com/tags/textmappropagator/"
  },{
    "title": "uniswap",
    "excerpt":"","url": "https://ambersuncreates.com/tags/uniswap/"
  },{
    "title": "the-graph",
    "excerpt":"","url": "https://ambersuncreates.com/tags/the-graph/"
  },{
    "title": "web3",
    "excerpt":"","url": "https://ambersuncreates.com/tags/web3/"
  },{
    "title": "alchemy",
    "excerpt":"","url": "https://ambersuncreates.com/tags/alchemy/"
  },{
    "title": "infura",
    "excerpt":"","url": "https://ambersuncreates.com/tags/infura/"
  },{
    "title": "indexer",
    "excerpt":"","url": "https://ambersuncreates.com/tags/indexer/"
  },{
    "title": "curator",
    "excerpt":"","url": "https://ambersuncreates.com/tags/curator/"
  },{
    "title": "delegator",
    "excerpt":"","url": "https://ambersuncreates.com/tags/delegator/"
  },{
    "title": "subgraph",
    "excerpt":"","url": "https://ambersuncreates.com/tags/subgraph/"
  },{
    "title": "assemblyscript",
    "excerpt":"","url": "https://ambersuncreates.com/tags/assemblyscript/"
  },{
    "title": "graphql",
    "excerpt":"","url": "https://ambersuncreates.com/tags/graphql/"
  },{
    "title": "aggregate",
    "excerpt":"","url": "https://ambersuncreates.com/tags/aggregate/"
  },{
    "title": "probe",
    "excerpt":"","url": "https://ambersuncreates.com/tags/probe/"
  },{
    "title": "liveness probe",
    "excerpt":"","url": "https://ambersuncreates.com/tags/liveness-probe/"
  },{
    "title": "readiness probe",
    "excerpt":"","url": "https://ambersuncreates.com/tags/readiness-probe/"
  },{
    "title": "startup probe",
    "excerpt":"","url": "https://ambersuncreates.com/tags/startup-probe/"
  },{
    "title": "gRPC",
    "excerpt":"","url": "https://ambersuncreates.com/tags/grpc/"
  },{
    "title": "TCP",
    "excerpt":"","url": "https://ambersuncreates.com/tags/tcp/"
  },{
    "title": "EXEC",
    "excerpt":"","url": "https://ambersuncreates.com/tags/exec/"
  },{
    "title": "healthz",
    "excerpt":"","url": "https://ambersuncreates.com/tags/healthz/"
  },{
    "title": "controller",
    "excerpt":"","url": "https://ambersuncreates.com/tags/controller/"
  },{
    "title": "operator",
    "excerpt":"","url": "https://ambersuncreates.com/tags/operator/"
  },{
    "title": "self-healing",
    "excerpt":"","url": "https://ambersuncreates.com/tags/self-healing/"
  },{
    "title": "kubelet",
    "excerpt":"","url": "https://ambersuncreates.com/tags/kubelet/"
  },{
    "title": "k8s requests",
    "excerpt":"","url": "https://ambersuncreates.com/tags/k8s-requests/"
  },{
    "title": "k8s limits",
    "excerpt":"","url": "https://ambersuncreates.com/tags/k8s-limits/"
  },{
    "title": "test double",
    "excerpt":"","url": "https://ambersuncreates.com/tags/test-double/"
  },{
    "title": "dummy",
    "excerpt":"","url": "https://ambersuncreates.com/tags/dummy/"
  },{
    "title": "fake object",
    "excerpt":"","url": "https://ambersuncreates.com/tags/fake-object/"
  },{
    "title": "spy",
    "excerpt":"","url": "https://ambersuncreates.com/tags/spy/"
  },{
    "title": "state verification",
    "excerpt":"","url": "https://ambersuncreates.com/tags/state-verification/"
  },{
    "title": "behaviour verification",
    "excerpt":"","url": "https://ambersuncreates.com/tags/behaviour-verification/"
  },{
    "title": "mockery",
    "excerpt":"","url": "https://ambersuncreates.com/tags/mockery/"
  },{
    "title": "httptest",
    "excerpt":"","url": "https://ambersuncreates.com/tags/httptest/"
  },{
    "title": "http request",
    "excerpt":"","url": "https://ambersuncreates.com/tags/http-request/"
  },{
    "title": "mux",
    "excerpt":"","url": "https://ambersuncreates.com/tags/mux/"
  },{
    "title": "kubernetes resource",
    "excerpt":"","url": "https://ambersuncreates.com/tags/kubernetes-resource/"
  },{
    "title": "kubernetes object",
    "excerpt":"","url": "https://ambersuncreates.com/tags/kubernetes-object/"
  },{
    "title": "informer",
    "excerpt":"","url": "https://ambersuncreates.com/tags/informer/"
  },{
    "title": "reflector",
    "excerpt":"","url": "https://ambersuncreates.com/tags/reflector/"
  },{
    "title": "workqueue",
    "excerpt":"","url": "https://ambersuncreates.com/tags/workqueue/"
  },{
    "title": "lister",
    "excerpt":"","url": "https://ambersuncreates.com/tags/lister/"
  },{
    "title": "etcd",
    "excerpt":"","url": "https://ambersuncreates.com/tags/etcd/"
  },{
    "title": "kubernetes api server",
    "excerpt":"","url": "https://ambersuncreates.com/tags/kubernetes-api-server/"
  },{
    "title": "resync",
    "excerpt":"","url": "https://ambersuncreates.com/tags/resync/"
  },{
    "title": "bookmark event",
    "excerpt":"","url": "https://ambersuncreates.com/tags/bookmark-event/"
  },{
    "title": "priority queue",
    "excerpt":"","url": "https://ambersuncreates.com/tags/priority-queue/"
  },{
    "title": "heap",
    "excerpt":"","url": "https://ambersuncreates.com/tags/heap/"
  },{
    "title": "min heap",
    "excerpt":"","url": "https://ambersuncreates.com/tags/min-heap/"
  },{
    "title": "max heap",
    "excerpt":"","url": "https://ambersuncreates.com/tags/max-heap/"
  },{
    "title": "linked list",
    "excerpt":"","url": "https://ambersuncreates.com/tags/linked-list/"
  },{
    "title": "realloc",
    "excerpt":"","url": "https://ambersuncreates.com/tags/realloc/"
  },{
    "title": "kustomize",
    "excerpt":"","url": "https://ambersuncreates.com/tags/kustomize/"
  },{
    "title": "package manager",
    "excerpt":"","url": "https://ambersuncreates.com/tags/package-manager/"
  },{
    "title": "template",
    "excerpt":"","url": "https://ambersuncreates.com/tags/template/"
  },{
    "title": "manifests",
    "excerpt":"","url": "https://ambersuncreates.com/tags/manifests/"
  },{
    "title": "crd upgrade",
    "excerpt":"","url": "https://ambersuncreates.com/tags/crd-upgrade/"
  },{
    "title": "subchart",
    "excerpt":"","url": "https://ambersuncreates.com/tags/subchart/"
  },{
    "title": "artifact hub",
    "excerpt":"","url": "https://ambersuncreates.com/tags/artifact-hub/"
  },{
    "title": "chart hook",
    "excerpt":"","url": "https://ambersuncreates.com/tags/chart-hook/"
  },{
    "title": "pre-install hook",
    "excerpt":"","url": "https://ambersuncreates.com/tags/pre-install-hook/"
  },{
    "title": "post-install hook",
    "excerpt":"","url": "https://ambersuncreates.com/tags/post-install-hook/"
  },{
    "title": "pre-upgrade hook",
    "excerpt":"","url": "https://ambersuncreates.com/tags/pre-upgrade-hook/"
  },{
    "title": "post-upgrade hook",
    "excerpt":"","url": "https://ambersuncreates.com/tags/post-upgrade-hook/"
  },{
    "title": "pre-rollback hook",
    "excerpt":"","url": "https://ambersuncreates.com/tags/pre-rollback-hook/"
  },{
    "title": "post-rollback hook",
    "excerpt":"","url": "https://ambersuncreates.com/tags/post-rollback-hook/"
  },{
    "title": "pre-uninstall hook",
    "excerpt":"","url": "https://ambersuncreates.com/tags/pre-uninstall-hook/"
  },{
    "title": "post-uninstall hook",
    "excerpt":"","url": "https://ambersuncreates.com/tags/post-uninstall-hook/"
  },{
    "title": "test hook",
    "excerpt":"","url": "https://ambersuncreates.com/tags/test-hook/"
  },{
    "title": "hook lifecycle",
    "excerpt":"","url": "https://ambersuncreates.com/tags/hook-lifecycle/"
  },{
    "title": "hook weight",
    "excerpt":"","url": "https://ambersuncreates.com/tags/hook-weight/"
  },{
    "title": "hook delete policy",
    "excerpt":"","url": "https://ambersuncreates.com/tags/hook-delete-policy/"
  },{
    "title": "publish chart",
    "excerpt":"","url": "https://ambersuncreates.com/tags/publish-chart/"
  },{
    "title": "helm repository",
    "excerpt":"","url": "https://ambersuncreates.com/tags/helm-repository/"
  },{
    "title": "oci",
    "excerpt":"","url": "https://ambersuncreates.com/tags/oci/"
  },{
    "title": "helm 3",
    "excerpt":"","url": "https://ambersuncreates.com/tags/helm-3/"
  },{
    "title": "helm release",
    "excerpt":"","url": "https://ambersuncreates.com/tags/helm-release/"
  },{
    "title": "helm install",
    "excerpt":"","url": "https://ambersuncreates.com/tags/helm-install/"
  },{
    "title": "helm list",
    "excerpt":"","url": "https://ambersuncreates.com/tags/helm-list/"
  },{
    "title": "helm rollback",
    "excerpt":"","url": "https://ambersuncreates.com/tags/helm-rollback/"
  },{
    "title": "helm uninstall",
    "excerpt":"","url": "https://ambersuncreates.com/tags/helm-uninstall/"
  },{
    "title": "helm test",
    "excerpt":"","url": "https://ambersuncreates.com/tags/helm-test/"
  },{
    "title": "helm dependency",
    "excerpt":"","url": "https://ambersuncreates.com/tags/helm-dependency/"
  },{
    "title": "helm dependency update",
    "excerpt":"","url": "https://ambersuncreates.com/tags/helm-dependency-update/"
  },{
    "title": "helm dependency build",
    "excerpt":"","url": "https://ambersuncreates.com/tags/helm-dependency-build/"
  },{
    "title": "oci-based registries",
    "excerpt":"","url": "https://ambersuncreates.com/tags/oci-based-registries/"
  },{
    "title": "binary search",
    "excerpt":"","url": "https://ambersuncreates.com/tags/binary-search/"
  },{
    "title": "lower bound",
    "excerpt":"","url": "https://ambersuncreates.com/tags/lower-bound/"
  },{
    "title": "upper bound",
    "excerpt":"","url": "https://ambersuncreates.com/tags/upper-bound/"
  },{
    "title": "sorted array",
    "excerpt":"","url": "https://ambersuncreates.com/tags/sorted-array/"
  },{
    "title": "leetcode-704",
    "excerpt":"","url": "https://ambersuncreates.com/tags/leetcode-704/"
  },{
    "title": "leetcode-2560",
    "excerpt":"","url": "https://ambersuncreates.com/tags/leetcode-2560/"
  },{
    "title": "leetcode-2529",
    "excerpt":"","url": "https://ambersuncreates.com/tags/leetcode-2529/"
  },{
    "title": "dynamic import",
    "excerpt":"","url": "https://ambersuncreates.com/tags/dynamic-import/"
  },{
    "title": "static import",
    "excerpt":"","url": "https://ambersuncreates.com/tags/static-import/"
  },{
    "title": "cjs",
    "excerpt":"","url": "https://ambersuncreates.com/tags/cjs/"
  },{
    "title": "service worker",
    "excerpt":"","url": "https://ambersuncreates.com/tags/service-worker/"
  },{
    "title": "webpack chunk",
    "excerpt":"","url": "https://ambersuncreates.com/tags/webpack-chunk/"
  },{
    "title": "magic comment",
    "excerpt":"","url": "https://ambersuncreates.com/tags/magic-comment/"
  },{
    "title": "webpack magic comment",
    "excerpt":"","url": "https://ambersuncreates.com/tags/webpack-magic-comment/"
  },{
    "title": "webpackignore",
    "excerpt":"","url": "https://ambersuncreates.com/tags/webpackignore/"
  },{
    "title": "sidecar",
    "excerpt":"","url": "https://ambersuncreates.com/tags/sidecar/"
  },{
    "title": "container pattern",
    "excerpt":"","url": "https://ambersuncreates.com/tags/container-pattern/"
  },{
    "title": "ambassador",
    "excerpt":"","url": "https://ambersuncreates.com/tags/ambassador/"
  },{
    "title": "adapter",
    "excerpt":"","url": "https://ambersuncreates.com/tags/adapter/"
  },{
    "title": "monitoring",
    "excerpt":"","url": "https://ambersuncreates.com/tags/monitoring/"
  },{
    "title": "init container",
    "excerpt":"","url": "https://ambersuncreates.com/tags/init-container/"
  },{
    "title": "liveness",
    "excerpt":"","url": "https://ambersuncreates.com/tags/liveness/"
  },{
    "title": "readiness",
    "excerpt":"","url": "https://ambersuncreates.com/tags/readiness/"
  },{
    "title": "lifecycle hook",
    "excerpt":"","url": "https://ambersuncreates.com/tags/lifecycle-hook/"
  },{
    "title": "post start",
    "excerpt":"","url": "https://ambersuncreates.com/tags/post-start/"
  },{
    "title": "pre stop",
    "excerpt":"","url": "https://ambersuncreates.com/tags/pre-stop/"
  },{
    "title": "sigterm",
    "excerpt":"","url": "https://ambersuncreates.com/tags/sigterm/"
  },{
    "title": "sigkill",
    "excerpt":"","url": "https://ambersuncreates.com/tags/sigkill/"
  },{
    "title": "exit code",
    "excerpt":"","url": "https://ambersuncreates.com/tags/exit-code/"
  },{
    "title": "signal",
    "excerpt":"","url": "https://ambersuncreates.com/tags/signal/"
  },{
    "title": "trap",
    "excerpt":"","url": "https://ambersuncreates.com/tags/trap/"
  },{
    "title": "wait",
    "excerpt":"","url": "https://ambersuncreates.com/tags/wait/"
  },{
    "title": "sleep",
    "excerpt":"","url": "https://ambersuncreates.com/tags/sleep/"
  },{
    "title": "background process",
    "excerpt":"","url": "https://ambersuncreates.com/tags/background-process/"
  },{
    "title": "foreground process",
    "excerpt":"","url": "https://ambersuncreates.com/tags/foreground-process/"
  },{
    "title": "netshoot",
    "excerpt":"","url": "https://ambersuncreates.com/tags/netshoot/"
  },{
    "title": "killing event",
    "excerpt":"","url": "https://ambersuncreates.com/tags/killing-event/"
  },{
    "title": "terminationGracePeriodSeconds",
    "excerpt":"","url": "https://ambersuncreates.com/tags/terminationgraceperiodseconds/"
  },{
    "title": "feature gate",
    "excerpt":"","url": "https://ambersuncreates.com/tags/feature-gate/"
  },{
    "title": "image pull policy",
    "excerpt":"","url": "https://ambersuncreates.com/tags/image-pull-policy/"
  },{
    "title": "union find",
    "excerpt":"","url": "https://ambersuncreates.com/tags/union-find/"
  },{
    "title": "disjoint set",
    "excerpt":"","url": "https://ambersuncreates.com/tags/disjoint-set/"
  },{
    "title": "tree compression",
    "excerpt":"","url": "https://ambersuncreates.com/tags/tree-compression/"
  },{
    "title": "leetcode-1061",
    "excerpt":"","url": "https://ambersuncreates.com/tags/leetcode-1061/"
  },{
    "title": "leetcode-1584",
    "excerpt":"","url": "https://ambersuncreates.com/tags/leetcode-1584/"
  },{
    "title": "syslog",
    "excerpt":"","url": "https://ambersuncreates.com/tags/syslog/"
  },{
    "title": "syslog-ng",
    "excerpt":"","url": "https://ambersuncreates.com/tags/syslog-ng/"
  },{
    "title": "rfc3164",
    "excerpt":"","url": "https://ambersuncreates.com/tags/rfc3164/"
  },{
    "title": "rfc5424",
    "excerpt":"","url": "https://ambersuncreates.com/tags/rfc5424/"
  },{
    "title": "non-transparent framing",
    "excerpt":"","url": "https://ambersuncreates.com/tags/non-transparent-framing/"
  },{
    "title": "octet counting",
    "excerpt":"","url": "https://ambersuncreates.com/tags/octet-counting/"
  },{
    "title": "priority",
    "excerpt":"","url": "https://ambersuncreates.com/tags/priority/"
  },{
    "title": "facility",
    "excerpt":"","url": "https://ambersuncreates.com/tags/facility/"
  },{
    "title": "severity",
    "excerpt":"","url": "https://ambersuncreates.com/tags/severity/"
  },{
    "title": "timestamp",
    "excerpt":"","url": "https://ambersuncreates.com/tags/timestamp/"
  },{
    "title": "hostname",
    "excerpt":"","url": "https://ambersuncreates.com/tags/hostname/"
  },{
    "title": "app-name",
    "excerpt":"","url": "https://ambersuncreates.com/tags/app-name/"
  },{
    "title": "procid",
    "excerpt":"","url": "https://ambersuncreates.com/tags/procid/"
  },{
    "title": "msgid",
    "excerpt":"","url": "https://ambersuncreates.com/tags/msgid/"
  },{
    "title": "sd-id",
    "excerpt":"","url": "https://ambersuncreates.com/tags/sd-id/"
  },{
    "title": "sd-param-name",
    "excerpt":"","url": "https://ambersuncreates.com/tags/sd-param-name/"
  },{
    "title": "sd-param-value",
    "excerpt":"","url": "https://ambersuncreates.com/tags/sd-param-value/"
  },{
    "title": "msg",
    "excerpt":"","url": "https://ambersuncreates.com/tags/msg/"
  },{
    "title": "rfc-3339",
    "excerpt":"","url": "https://ambersuncreates.com/tags/rfc-3339/"
  },{
    "title": "rfc-1034",
    "excerpt":"","url": "https://ambersuncreates.com/tags/rfc-1034/"
  },{
    "title": "rfc-5426",
    "excerpt":"","url": "https://ambersuncreates.com/tags/rfc-5426/"
  },{
    "title": "rfc-6587",
    "excerpt":"","url": "https://ambersuncreates.com/tags/rfc-6587/"
  },{
    "title": "client-server",
    "excerpt":"","url": "https://ambersuncreates.com/tags/client-server/"
  },{
    "title": "wc",
    "excerpt":"","url": "https://ambersuncreates.com/tags/wc/"
  },{
    "title": "delayed queue",
    "excerpt":"","url": "https://ambersuncreates.com/tags/delayed-queue/"
  },{
    "title": "linux at",
    "excerpt":"","url": "https://ambersuncreates.com/tags/linux-at/"
  },{
    "title": "cronjob",
    "excerpt":"","url": "https://ambersuncreates.com/tags/cronjob/"
  },{
    "title": "linux atd",
    "excerpt":"","url": "https://ambersuncreates.com/tags/linux-atd/"
  },{
    "title": "dynomite",
    "excerpt":"","url": "https://ambersuncreates.com/tags/dynomite/"
  },{
    "title": "cassandra",
    "excerpt":"","url": "https://ambersuncreates.com/tags/cassandra/"
  },{
    "title": "mnesia",
    "excerpt":"","url": "https://ambersuncreates.com/tags/mnesia/"
  },{
    "title": "erlang",
    "excerpt":"","url": "https://ambersuncreates.com/tags/erlang/"
  },{
    "title": "ttl",
    "excerpt":"","url": "https://ambersuncreates.com/tags/ttl/"
  },{
    "title": "dlx",
    "excerpt":"","url": "https://ambersuncreates.com/tags/dlx/"
  },{
    "title": "quorum queue",
    "excerpt":"","url": "https://ambersuncreates.com/tags/quorum-queue/"
  },{
    "title": "classic queue",
    "excerpt":"","url": "https://ambersuncreates.com/tags/classic-queue/"
  },{
    "title": "cluster replication",
    "excerpt":"","url": "https://ambersuncreates.com/tags/cluster-replication/"
  },{
    "title": "ack",
    "excerpt":"","url": "https://ambersuncreates.com/tags/ack/"
  },{
    "title": "nack",
    "excerpt":"","url": "https://ambersuncreates.com/tags/nack/"
  },{
    "title": "dead letter",
    "excerpt":"","url": "https://ambersuncreates.com/tags/dead-letter/"
  },{
    "title": "dyno queue",
    "excerpt":"","url": "https://ambersuncreates.com/tags/dyno-queue/"
  },{
    "title": "sorted set",
    "excerpt":"","url": "https://ambersuncreates.com/tags/sorted-set/"
  },{
    "title": "fifo",
    "excerpt":"","url": "https://ambersuncreates.com/tags/fifo/"
  },{
    "title": "activemq",
    "excerpt":"","url": "https://ambersuncreates.com/tags/activemq/"
  },{
    "title": "activemq classic",
    "excerpt":"","url": "https://ambersuncreates.com/tags/activemq-classic/"
  },{
    "title": "activemq artemis",
    "excerpt":"","url": "https://ambersuncreates.com/tags/activemq-artemis/"
  },{
    "title": "java",
    "excerpt":"","url": "https://ambersuncreates.com/tags/java/"
  },{
    "title": "scheduledthreadpoolexecutor",
    "excerpt":"","url": "https://ambersuncreates.com/tags/scheduledthreadpoolexecutor/"
  },{
    "title": "mainloop",
    "excerpt":"","url": "https://ambersuncreates.com/tags/mainloop/"
  },{
    "title": "scaling",
    "excerpt":"","url": "https://ambersuncreates.com/tags/scaling/"
  },{
    "title": "hpa",
    "excerpt":"","url": "https://ambersuncreates.com/tags/hpa/"
  },{
    "title": "vpa",
    "excerpt":"","url": "https://ambersuncreates.com/tags/vpa/"
  },{
    "title": "rolling update",
    "excerpt":"","url": "https://ambersuncreates.com/tags/rolling-update/"
  },{
    "title": "recreate",
    "excerpt":"","url": "https://ambersuncreates.com/tags/recreate/"
  },{
    "title": "blue green",
    "excerpt":"","url": "https://ambersuncreates.com/tags/blue-green/"
  },{
    "title": "canary",
    "excerpt":"","url": "https://ambersuncreates.com/tags/canary/"
  },{
    "title": "shadow",
    "excerpt":"","url": "https://ambersuncreates.com/tags/shadow/"
  },{
    "title": "rollout",
    "excerpt":"","url": "https://ambersuncreates.com/tags/rollout/"
  },{
    "title": "argo rollouts",
    "excerpt":"","url": "https://ambersuncreates.com/tags/argo-rollouts/"
  },{
    "title": "keda",
    "excerpt":"","url": "https://ambersuncreates.com/tags/keda/"
  },{
    "title": "keda concepts",
    "excerpt":"","url": "https://ambersuncreates.com/tags/keda-concepts/"
  },{
    "title": "horizontal pod autoscaler",
    "excerpt":"","url": "https://ambersuncreates.com/tags/horizontal-pod-autoscaler/"
  },{
    "title": "vertical pod autoscaler",
    "excerpt":"","url": "https://ambersuncreates.com/tags/vertical-pod-autoscaler/"
  },{
    "title": "hpa and vpa",
    "excerpt":"","url": "https://ambersuncreates.com/tags/hpa-and-vpa/"
  },{
    "title": "rollout crd",
    "excerpt":"","url": "https://ambersuncreates.com/tags/rollout-crd/"
  },{
    "title": "manual scaling",
    "excerpt":"","url": "https://ambersuncreates.com/tags/manual-scaling/"
  },{
    "title": "auto scaling",
    "excerpt":"","url": "https://ambersuncreates.com/tags/auto-scaling/"
  },{
    "title": "stabilization window",
    "excerpt":"","url": "https://ambersuncreates.com/tags/stabilization-window/"
  },{
    "title": "tolerance",
    "excerpt":"","url": "https://ambersuncreates.com/tags/tolerance/"
  },{
    "title": "scaling policy",
    "excerpt":"","url": "https://ambersuncreates.com/tags/scaling-policy/"
  },{
    "title": "metric data",
    "excerpt":"","url": "https://ambersuncreates.com/tags/metric-data/"
  },{
    "title": "resize container resources",
    "excerpt":"","url": "https://ambersuncreates.com/tags/resize-container-resources/"
  },{
    "title": "observed generation",
    "excerpt":"","url": "https://ambersuncreates.com/tags/observed-generation/"
  },{
    "title": "ab testing",
    "excerpt":"","url": "https://ambersuncreates.com/tags/ab-testing/"
  },{
    "title": "shadow deployment",
    "excerpt":"","url": "https://ambersuncreates.com/tags/shadow-deployment/"
  },{
    "title": "ingress",
    "excerpt":"","url": "https://ambersuncreates.com/tags/ingress/"
  },{
    "title": "egress",
    "excerpt":"","url": "https://ambersuncreates.com/tags/egress/"
  },{
    "title": "inbound",
    "excerpt":"","url": "https://ambersuncreates.com/tags/inbound/"
  },{
    "title": "outbound",
    "excerpt":"","url": "https://ambersuncreates.com/tags/outbound/"
  },{
    "title": "load balancer",
    "excerpt":"","url": "https://ambersuncreates.com/tags/load-balancer/"
  },{
    "title": "reverse proxy",
    "excerpt":"","url": "https://ambersuncreates.com/tags/reverse-proxy/"
  },{
    "title": "proxy",
    "excerpt":"","url": "https://ambersuncreates.com/tags/proxy/"
  },{
    "title": "round robin",
    "excerpt":"","url": "https://ambersuncreates.com/tags/round-robin/"
  },{
    "title": "weighted round robin",
    "excerpt":"","url": "https://ambersuncreates.com/tags/weighted-round-robin/"
  },{
    "title": "dns round robin",
    "excerpt":"","url": "https://ambersuncreates.com/tags/dns-round-robin/"
  },{
    "title": "least connection",
    "excerpt":"","url": "https://ambersuncreates.com/tags/least-connection/"
  },{
    "title": "sticky session",
    "excerpt":"","url": "https://ambersuncreates.com/tags/sticky-session/"
  },{
    "title": "gateway api",
    "excerpt":"","url": "https://ambersuncreates.com/tags/gateway-api/"
  },{
    "title": "ingress nginx controller",
    "excerpt":"","url": "https://ambersuncreates.com/tags/ingress-nginx-controller/"
  },{
    "title": "traefik ingress controller",
    "excerpt":"","url": "https://ambersuncreates.com/tags/traefik-ingress-controller/"
  },{
    "title": "ingress controller",
    "excerpt":"","url": "https://ambersuncreates.com/tags/ingress-controller/"
  },{
    "title": "gateway class",
    "excerpt":"","url": "https://ambersuncreates.com/tags/gateway-class/"
  },{
    "title": "gateway",
    "excerpt":"","url": "https://ambersuncreates.com/tags/gateway/"
  },{
    "title": "route",
    "excerpt":"","url": "https://ambersuncreates.com/tags/route/"
  },{
    "title": "httproute",
    "excerpt":"","url": "https://ambersuncreates.com/tags/httproute/"
  },{
    "title": "grpcroute",
    "excerpt":"","url": "https://ambersuncreates.com/tags/grpcroute/"
  },{
    "title": "traefik",
    "excerpt":"","url": "https://ambersuncreates.com/tags/traefik/"
  },{
    "title": "apache apisix",
    "excerpt":"","url": "https://ambersuncreates.com/tags/apache-apisix/"
  },{
    "title": "apisix",
    "excerpt":"","url": "https://ambersuncreates.com/tags/apisix/"
  },{
    "title": "kubernetes-sigs/ingate",
    "excerpt":"","url": "https://ambersuncreates.com/tags/kubernetes-sigs-ingate/"
  },{
    "title": "zero downtime",
    "excerpt":"","url": "https://ambersuncreates.com/tags/zero-downtime/"
  },{
    "title": "endpointslice",
    "excerpt":"","url": "https://ambersuncreates.com/tags/endpointslice/"
  },{
    "title": "topology aware routing",
    "excerpt":"","url": "https://ambersuncreates.com/tags/topology-aware-routing/"
  },{
    "title": "golang channel",
    "excerpt":"","url": "https://ambersuncreates.com/tags/golang-channel/"
  },{
    "title": "forward proxy",
    "excerpt":"","url": "https://ambersuncreates.com/tags/forward-proxy/"
  },{
    "title": "service discovery",
    "excerpt":"","url": "https://ambersuncreates.com/tags/service-discovery/"
  },{
    "title": "l4",
    "excerpt":"","url": "https://ambersuncreates.com/tags/l4/"
  },{
    "title": "l7",
    "excerpt":"","url": "https://ambersuncreates.com/tags/l7/"
  },{
    "title": "volume",
    "excerpt":"","url": "https://ambersuncreates.com/tags/volume/"
  },{
    "title": "mount",
    "excerpt":"","url": "https://ambersuncreates.com/tags/mount/"
  },{
    "title": "hostpath",
    "excerpt":"","url": "https://ambersuncreates.com/tags/hostpath/"
  },{
    "title": "local",
    "excerpt":"","url": "https://ambersuncreates.com/tags/local/"
  },{
    "title": "persistent volume claim",
    "excerpt":"","url": "https://ambersuncreates.com/tags/persistent-volume-claim/"
  },{
    "title": "storage class",
    "excerpt":"","url": "https://ambersuncreates.com/tags/storage-class/"
  },{
    "title": "projected volume",
    "excerpt":"","url": "https://ambersuncreates.com/tags/projected-volume/"
  },{
    "title": "ephemeral volume",
    "excerpt":"","url": "https://ambersuncreates.com/tags/ephemeral-volume/"
  },{
    "title": "persistent volume",
    "excerpt":"","url": "https://ambersuncreates.com/tags/persistent-volume/"
  },{
    "title": "subpath",
    "excerpt":"","url": "https://ambersuncreates.com/tags/subpath/"
  },{
    "title": "finalizers",
    "excerpt":"","url": "https://ambersuncreates.com/tags/finalizers/"
  },{
    "title": "reclaim policy",
    "excerpt":"","url": "https://ambersuncreates.com/tags/reclaim-policy/"
  },{
    "title": "access modes",
    "excerpt":"","url": "https://ambersuncreates.com/tags/access-modes/"
  },{
    "title": "use protection",
    "excerpt":"","url": "https://ambersuncreates.com/tags/use-protection/"
  },{
    "title": "dynamic provisioning",
    "excerpt":"","url": "https://ambersuncreates.com/tags/dynamic-provisioning/"
  },{
    "title": "default storage class",
    "excerpt":"","url": "https://ambersuncreates.com/tags/default-storage-class/"
  },{
    "title": "retroactive default storage class assignment",
    "excerpt":"","url": "https://ambersuncreates.com/tags/retroactive-default-storage-class-assignment/"
  },{
    "title": "pv",
    "excerpt":"","url": "https://ambersuncreates.com/tags/pv/"
  },{
    "title": "pvc",
    "excerpt":"","url": "https://ambersuncreates.com/tags/pvc/"
  },{
    "title": "readwriteonce",
    "excerpt":"","url": "https://ambersuncreates.com/tags/readwriteonce/"
  },{
    "title": "readonlymany",
    "excerpt":"","url": "https://ambersuncreates.com/tags/readonlymany/"
  },{
    "title": "readwritemany",
    "excerpt":"","url": "https://ambersuncreates.com/tags/readwritemany/"
  },{
    "title": "readwriteoncepod",
    "excerpt":"","url": "https://ambersuncreates.com/tags/readwriteoncepod/"
  },{
    "title": "statefulset",
    "excerpt":"","url": "https://ambersuncreates.com/tags/statefulset/"
  },{
    "title": "daemonset",
    "excerpt":"","url": "https://ambersuncreates.com/tags/daemonset/"
  },{
    "title": "replicaset",
    "excerpt":"","url": "https://ambersuncreates.com/tags/replicaset/"
  },{
    "title": "abstraction",
    "excerpt":"","url": "https://ambersuncreates.com/tags/abstraction/"
  },{
    "title": "workload",
    "excerpt":"","url": "https://ambersuncreates.com/tags/workload/"
  },{
    "title": "workload resources",
    "excerpt":"","url": "https://ambersuncreates.com/tags/workload-resources/"
  },{
    "title": "resources",
    "excerpt":"","url": "https://ambersuncreates.com/tags/resources/"
  },{
    "title": "indexed job",
    "excerpt":"","url": "https://ambersuncreates.com/tags/indexed-job/"
  },{
    "title": "job completion mode",
    "excerpt":"","url": "https://ambersuncreates.com/tags/job-completion-mode/"
  },{
    "title": "headless service",
    "excerpt":"","url": "https://ambersuncreates.com/tags/headless-service/"
  },{
    "title": "garbage collection",
    "excerpt":"","url": "https://ambersuncreates.com/tags/garbage-collection/"
  },{
    "title": "gc",
    "excerpt":"","url": "https://ambersuncreates.com/tags/gc/"
  },{
    "title": "reference counting",
    "excerpt":"","url": "https://ambersuncreates.com/tags/reference-counting/"
  },{
    "title": "mark and sweep",
    "excerpt":"","url": "https://ambersuncreates.com/tags/mark-and-sweep/"
  },{
    "title": "tricolor mark and sweep",
    "excerpt":"","url": "https://ambersuncreates.com/tags/tricolor-mark-and-sweep/"
  },{
    "title": "moving gc",
    "excerpt":"","url": "https://ambersuncreates.com/tags/moving-gc/"
  },{
    "title": "non-moving gc",
    "excerpt":"","url": "https://ambersuncreates.com/tags/non-moving-gc/"
  },{
    "title": "steady state",
    "excerpt":"","url": "https://ambersuncreates.com/tags/steady-state/"
  },{
    "title": "thrashing",
    "excerpt":"","url": "https://ambersuncreates.com/tags/thrashing/"
  },{
    "title": "gogc",
    "excerpt":"","url": "https://ambersuncreates.com/tags/gogc/"
  },{
    "title": "gomemlimit",
    "excerpt":"","url": "https://ambersuncreates.com/tags/gomemlimit/"
  },{
    "title": "green tea gc",
    "excerpt":"","url": "https://ambersuncreates.com/tags/green-tea-gc/"
  },{
    "title": "flood gc",
    "excerpt":"","url": "https://ambersuncreates.com/tags/flood-gc/"
  },{
    "title": "golang gc",
    "excerpt":"","url": "https://ambersuncreates.com/tags/golang-gc/"
  },{
    "title": "dijkstra barrier",
    "excerpt":"","url": "https://ambersuncreates.com/tags/dijkstra-barrier/"
  },{
    "title": "yuasa style deletion write barrier",
    "excerpt":"","url": "https://ambersuncreates.com/tags/yuasa-style-deletion-write-barrier/"
  },{
    "title": "object graph",
    "excerpt":"","url": "https://ambersuncreates.com/tags/object-graph/"
  },{
    "title": "live heap",
    "excerpt":"","url": "https://ambersuncreates.com/tags/live-heap/"
  },{
    "title": "cycle reference",
    "excerpt":"","url": "https://ambersuncreates.com/tags/cycle-reference/"
  },{
    "title": "stop the world",
    "excerpt":"","url": "https://ambersuncreates.com/tags/stop-the-world/"
  },{
    "title": "write barrier",
    "excerpt":"","url": "https://ambersuncreates.com/tags/write-barrier/"
  },{
    "title": "redis-stack",
    "excerpt":"","url": "https://ambersuncreates.com/tags/redis-stack/"
  },{
    "title": "cuckoo filter",
    "excerpt":"","url": "https://ambersuncreates.com/tags/cuckoo-filter/"
  },{
    "title": "hash function",
    "excerpt":"","url": "https://ambersuncreates.com/tags/hash-function/"
  },{
    "title": "false positive",
    "excerpt":"","url": "https://ambersuncreates.com/tags/false-positive/"
  },{
    "title": "scalable bloom filter",
    "excerpt":"","url": "https://ambersuncreates.com/tags/scalable-bloom-filter/"
  },{
    "title": "cuckoo hashing",
    "excerpt":"","url": "https://ambersuncreates.com/tags/cuckoo-hashing/"
  },{
    "title": "probabilistic data structure",
    "excerpt":"","url": "https://ambersuncreates.com/tags/probabilistic-data-structure/"
  },{
    "title": "data structure",
    "excerpt":"","url": "https://ambersuncreates.com/tags/data-structure/"
  },{
    "title": "xor filter",
    "excerpt":"","url": "https://ambersuncreates.com/tags/xor-filter/"
  },{
    "title": "binary fuse filter",
    "excerpt":"","url": "https://ambersuncreates.com/tags/binary-fuse-filter/"
  },{
    "title": "peeling",
    "excerpt":"","url": "https://ambersuncreates.com/tags/peeling/"
  },{
    "title": "csapp",
    "excerpt":"","url": "https://ambersuncreates.com/categories/csapp/"
  },{
    "title": "git",
    "excerpt":"","url": "https://ambersuncreates.com/categories/git/"
  },{
    "title": "random",
    "excerpt":"","url": "https://ambersuncreates.com/categories/random/"
  },{
    "title": "container",
    "excerpt":"","url": "https://ambersuncreates.com/categories/container/"
  },{
    "title": "linux-kernel",
    "excerpt":"","url": "https://ambersuncreates.com/categories/linux-kernel/"
  },{
    "title": "website",
    "excerpt":"","url": "https://ambersuncreates.com/categories/website/"
  },{
    "title": "devops",
    "excerpt":"","url": "https://ambersuncreates.com/categories/devops/"
  },{
    "title": "network",
    "excerpt":"","url": "https://ambersuncreates.com/categories/network/"
  },{
    "title": "database",
    "excerpt":"","url": "https://ambersuncreates.com/categories/database/"
  },{
    "title": "algorithm",
    "excerpt":"","url": "https://ambersuncreates.com/categories/algorithm/"
  },{
    "title": "angular",
    "excerpt":"","url": "https://ambersuncreates.com/categories/angular/"
  },{
    "title": "blockchain",
    "excerpt":"","url": "https://ambersuncreates.com/categories/blockchain/"
  },{
    "title": "design pattern",
    "excerpt":"","url": "https://ambersuncreates.com/categories/design-pattern/"
  },{
    "title": "kubernetes",
    "excerpt":"","url": "https://ambersuncreates.com/categories/kubernetes/"
  },{
    "title": null,
    "excerpt":"","url": "https://ambersuncreates.com/posts/git-hook/"
  },{
    "title": null,
    "excerpt":"","url": "https://ambersuncreates.com/posts/git-rebase/"
  },{
    "title": null,
    "excerpt":"","url": "https://ambersuncreates.com/website/website-grpc/"
  },{
    "title": null,
    "excerpt":"","url": "https://ambersuncreates.com/http/networking-osi/"
  },{
    "title": null,
    "excerpt":"","url": "https://ambersuncreates.com/network/networking-osi/"
  },{
    "title": null,
    "excerpt":"","url": "https://ambersuncreates.com/http/networking-http1/"
  },{
    "title": null,
    "excerpt":"","url": "https://ambersuncreates.com/network/networking-http1/"
  },{
    "title": null,
    "excerpt":"","url": "https://ambersuncreates.com/posts/golang-channel/"
  },{
    "title": null,
    "excerpt":"","url": "https://ambersuncreates.com/database/database-optimization-hardware/"
  },{
    "title": null,
    "excerpt":"","url": "https://ambersuncreates.com/network/networking-basics/"
  },{
    "title": null,
    "excerpt":"","url": "https://ambersuncreates.com/database/database-optimization-software/"
  },{
    "title": null,
    "excerpt":"","url": "https://ambersuncreates.com/database/database-cache-bloom-filter/"
  },{
    "title": null,
    "excerpt":"{\"/posts/git-hook/\":\"https://ambersuncreates.com/git/git-hook/\",\"/posts/git-rebase/\":\"https://ambersuncreates.com/git/git-rebase/\",\"/website/website-grpc/\":\"https://ambersuncreates.com/website/website-rpc/\",\"/http/networking-osi/\":\"https://ambersuncreates.com/network/network-osi/\",\"/network/networking-osi/\":\"https://ambersuncreates.com/network/network-osi/\",\"/http/networking-http1/\":\"https://ambersuncreates.com/network/network-http1/\",\"/network/networking-http1/\":\"https://ambersuncreates.com/network/network-http1/\",\"/posts/golang-channel/\":\"https://ambersuncreates.com/random/golang-channel/\",\"/database/database-optimization-hardware/\":\"https://ambersuncreates.com/database/database-distributed-database/\",\"/network/networking-basics/\":\"https://ambersuncreates.com/network/network-basics/\",\"/database/database-optimization-software/\":\"https://ambersuncreates.com/database/database-optimization/\",\"/database/database-cache-bloom-filter/\":\"https://ambersuncreates.com/database/database-filter/\"}","url": "https://ambersuncreates.com/redirects.json"
  },{
    "title": null,
    "excerpt":"","url": "https://ambersuncreates.com/page2/"
  },{
    "title": null,
    "excerpt":"","url": "https://ambersuncreates.com/page3/"
  },{
    "title": null,
    "excerpt":"","url": "https://ambersuncreates.com/page4/"
  },{
    "title": null,
    "excerpt":"","url": "https://ambersuncreates.com/page5/"
  },{
    "title": null,
    "excerpt":"","url": "https://ambersuncreates.com/page6/"
  },{
    "title": null,
    "excerpt":"","url": "https://ambersuncreates.com/page7/"
  },{
    "title": null,
    "excerpt":"","url": "https://ambersuncreates.com/page8/"
  },{
    "title": null,
    "excerpt":"","url": "https://ambersuncreates.com/page9/"
  },{
    "title": null,
    "excerpt":"","url": "https://ambersuncreates.com/page10/"
  },{
    "title": null,
    "excerpt":" {% if page.xsl %} {% endif %} {% assign collections = site.collections | where_exp:'collection','collection.output != false' %}{% for collection in collections %}{% assign docs = collection.docs | where_exp:'doc','doc.sitemap != false' %}{% for doc in docs %} {{ doc.url | replace:'/index.html','/' | absolute_url | xml_escape }} {% if doc.last_modified_at or doc.date %}{{ doc.last_modified_at | default: doc.date | date_to_xmlschema }} {% endif %} {% endfor %}{% endfor %}{% assign pages = site.html_pages | where_exp:'doc','doc.sitemap != false' | where_exp:'doc','doc.url != \"/404.html\"' %}{% for page in pages %} {{ page.url | replace:'/index.html','/' | absolute_url | xml_escape }} {% if page.last_modified_at %}{{ page.last_modified_at | date_to_xmlschema }} {% endif %} {% endfor %}{% assign static_files = page.static_files | where_exp:'page','page.sitemap != false' | where_exp:'page','page.name != \"404.html\"' %}{% for file in static_files %} {{ file.path | replace:'/index.html','/' | absolute_url | xml_escape }} {{ file.modified_time | date_to_xmlschema }}  {% endfor %} ","url": "https://ambersuncreates.com/sitemap.xml"
  },{
    "title": null,
    "excerpt":"Sitemap: {{ \"sitemap.xml\" | absolute_url }} ","url": "https://ambersuncreates.com/robots.txt"
  },{
    "title": null,
    "excerpt":"{% if page.xsl %}{% endif %}Jekyll{{ site.time | date_to_xmlschema }}{{ page.url | absolute_url | xml_escape }}{% assign title = site.title | default: site.name %}{% if page.collection != \"posts\" %}{% assign collection = page.collection | capitalize %}{% assign title = title | append: \" | \" | append: collection %}{% endif %}{% if page.category %}{% assign category = page.category | capitalize %}{% assign title = title | append: \" | \" | append: category %}{% endif %}{% if title %}{{ title | smartify | xml_escape }}{% endif %}{% if site.description %}{{ site.description | xml_escape }}{% endif %}{% if site.author %}{{ site.author.name | default: site.author | xml_escape }}{% if site.author.email %}{{ site.author.email | xml_escape }}{% endif %}{% if site.author.uri %}{{ site.author.uri | xml_escape }}{% endif %}{% endif %}{% if page.tags %}{% assign posts = site.tags[page.tags] %}{% else %}{% assign posts = site[page.collection] %}{% endif %}{% if page.category %}{% assign posts = posts | where: \"categories\", page.category %}{% endif %}{% unless site.show_drafts %}{% assign posts = posts | where_exp: \"post\", \"post.draft != true\" %}{% endunless %}{% assign posts = posts | sort: \"date\" | reverse %}{% assign posts_limit = site.feed.posts_limit | default: 10 %}{% for post in posts limit: posts_limit %}{% assign post_title = post.title | smartify | strip_html | normalize_whitespace | xml_escape %}{{ post_title }}{{ post.date | date_to_xmlschema }}{{ post.last_modified_at | default: post.date | date_to_xmlschema }}{{ post.id | absolute_url | xml_escape }}{% assign excerpt_only = post.feed.excerpt_only | default: site.feed.excerpt_only %}{% unless excerpt_only %}{% endunless %}{% assign post_author = post.author | default: post.authors[0] | default: site.author %}{% assign post_author = site.data.authors[post_author] | default: post_author %}{% assign post_author_email = post_author.email | default: nil %}{% assign post_author_uri = post_author.uri | default: nil %}{% assign post_author_name = post_author.name | default: post_author %}{{ post_author_name | default: \"\" | xml_escape }}{% if post_author_email %}{{ post_author_email | xml_escape }}{% endif %}{% if post_author_uri %}{{ post_author_uri | xml_escape }}{% endif %}{% if post.category %}{% elsif post.categories %}{% for category in post.categories %}{% endfor %}{% endif %}{% for tag in post.tags %}{% endfor %}{% assign post_summary = post.description | default: post.excerpt %}{% if post_summary and post_summary != empty %}{% endif %}{% assign post_image = post.image.path | default: post.image %}{% if post_image %}{% unless post_image contains \"://\" %}{% assign post_image = post_image | absolute_url %}{% endunless %}{% endif %}{% endfor %}","url": "https://ambersuncreates.com/feed.xml"
  }]
